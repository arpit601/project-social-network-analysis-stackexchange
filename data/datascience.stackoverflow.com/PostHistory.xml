<?xml version="1.0" encoding="utf-8"?>
<posthistory>
  <row Id="7" PostHistoryTypeId="2" PostId="5" RevisionGUID="009bca93-fce2-44ed-a277-a8452650a627" CreationDate="2014-05-13T23:58:30.457" UserId="5" Text="I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &quot;Hello World&quot; example - how can I avoid hard-coding behavior?&#xD;&#xA;&#xD;&#xA;For example, if I wanted to &quot;teach&quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&#xD;&#xA;&#xD;&#xA;Obviously, randomly generating code would be impractical, so how could I do this?" />
  <row Id="8" PostHistoryTypeId="1" PostId="5" RevisionGUID="009bca93-fce2-44ed-a277-a8452650a627" CreationDate="2014-05-13T23:58:30.457" UserId="5" Text="How can I do simple machine learning without hard-coding behavior?" />
  <row Id="9" PostHistoryTypeId="3" PostId="5" RevisionGUID="009bca93-fce2-44ed-a277-a8452650a627" CreationDate="2014-05-13T23:58:30.457" UserId="5" Text="&lt;machine-learning&gt;" />
  <row Id="12" PostHistoryTypeId="2" PostId="7" RevisionGUID="ea5a5642-ed30-43ea-9be5-8e8de0e1c660" CreationDate="2014-05-14T00:11:06.457" UserId="36" Text="As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers." />
  <row Id="13" PostHistoryTypeId="1" PostId="7" RevisionGUID="ea5a5642-ed30-43ea-9be5-8e8de0e1c660" CreationDate="2014-05-14T00:11:06.457" UserId="36" Text="What open-source books (or other materials) provide a relatively thorough overview of data science?" />
  <row Id="14" PostHistoryTypeId="3" PostId="7" RevisionGUID="ea5a5642-ed30-43ea-9be5-8e8de0e1c660" CreationDate="2014-05-14T00:11:06.457" UserId="36" Text="&lt;machine-learning&gt;" />
  <row Id="18" PostHistoryTypeId="2" PostId="9" RevisionGUID="7482c6e8-a216-4648-b725-dd7463e87b7c" CreationDate="2014-05-14T00:36:31.077" UserId="51" Text="Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.&#xD;&#xA;&#xD;&#xA;With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are &quot;fixed&quot; and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.&#xD;&#xA;&#xD;&#xA;You rarely hard-code behavior in AI/ML solutions. It's all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.&#xD;&#xA;&#xD;&#xA;In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving." />
  <row Id="19" PostHistoryTypeId="2" PostId="10" RevisionGUID="f9b92c74-c38f-419e-967a-006663d28a75" CreationDate="2014-05-14T00:53:43.273" UserId="22" Text="One book that's freely available is &quot;The Elements of Statistical Learning&quot; by Hastie, Tibshirani, and Friedman (published by Springer): [see Tibshirani's website][1].&#xD;&#xA;&#xD;&#xA;Another fantastic source, although it isn't a book, is Andrew Ng's Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.&#xD;&#xA;&#xD;&#xA;  [1]: http://statweb.stanford.edu/~tibs/ElemStatLearn/" />
  <row Id="29" PostHistoryTypeId="2" PostId="14" RevisionGUID="bd8a5c03-7143-4cc0-9d50-beafcdffd19a" CreationDate="2014-05-14T01:25:59.677" UserId="66" Text="I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.&#xD;&#xA;&#xD;&#xA;My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?" />
  <row Id="30" PostHistoryTypeId="1" PostId="14" RevisionGUID="bd8a5c03-7143-4cc0-9d50-beafcdffd19a" CreationDate="2014-05-14T01:25:59.677" UserId="66" Text="Is Data Science the Same as Data Mining?" />
  <row Id="31" PostHistoryTypeId="3" PostId="14" RevisionGUID="bd8a5c03-7143-4cc0-9d50-beafcdffd19a" CreationDate="2014-05-14T01:25:59.677" UserId="66" Text="&lt;data-mining&gt;" />
  <row Id="32" PostHistoryTypeId="2" PostId="15" RevisionGUID="42817f34-3736-4798-bbed-b07364f263a5" CreationDate="2014-05-14T01:41:23.110" UserId="64" Text="In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?" />
  <row Id="33" PostHistoryTypeId="1" PostId="15" RevisionGUID="42817f34-3736-4798-bbed-b07364f263a5" CreationDate="2014-05-14T01:41:23.110" UserId="64" Text="What are the advantages and disadvantages of SQL versus NoSQL in data science?" />
  <row Id="34" PostHistoryTypeId="3" PostId="15" RevisionGUID="42817f34-3736-4798-bbed-b07364f263a5" CreationDate="2014-05-14T01:41:23.110" UserId="64" Text="&lt;databases&gt;" />
  <row Id="36" PostHistoryTypeId="2" PostId="16" RevisionGUID="3b75652d-4d40-448c-9242-9579560f4634" CreationDate="2014-05-14T01:57:56.880" UserId="63" Text="I use [Libsvm][1] to training data and predict classification on **semantic analysis**.&#xD;&#xA;&#xD;&#xA;But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** issue.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release and it can solve performance issue.&#xD;&#xA;But it cost too much **memory**. &#xD;&#xA;&#xD;&#xA;Is **MapReduce** the only way to solve semantic analysis on big data?&#xD;&#xA;&#xD;&#xA;Or have any other methods can improve memory issue on **Liblinear**.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/" />
  <row Id="37" PostHistoryTypeId="1" PostId="16" RevisionGUID="3b75652d-4d40-448c-9242-9579560f4634" CreationDate="2014-05-14T01:57:56.880" UserId="63" Text="Use liblinear on big data" />
  <row Id="38" PostHistoryTypeId="3" PostId="16" RevisionGUID="3b75652d-4d40-448c-9242-9579560f4634" CreationDate="2014-05-14T01:57:56.880" UserId="63" Text="&lt;machine-learning&gt;&lt;bigdata&gt;&lt;liblinear&gt;&lt;libsvm&gt;&lt;semantic-analysis&gt;" />
  <row Id="40" PostHistoryTypeId="5" PostId="16" RevisionGUID="f49ef7e0-7a84-4e16-a28c-520e7948d0f4" CreationDate="2014-05-14T02:04:42.460" UserId="63" Comment="added 12 characters in body" Text="I use [Libsvm][1] to training data and predict classification on **semantic analysis** issue.&#xD;&#xA;&#xD;&#xA;But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** issue.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release and it can solve performance issue.&#xD;&#xA;But it cost too much **memory**. &#xD;&#xA;&#xD;&#xA;Is **MapReduce** the only way to solve semantic analysis issue on big data?&#xD;&#xA;&#xD;&#xA;Or have any other methods can improve memory issue on **Liblinear**.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/" />
  <row Id="41" PostHistoryTypeId="2" PostId="17" RevisionGUID="52e22583-f290-4b1d-a859-bdecef1cc8e6" CreationDate="2014-05-14T02:49:14.580" UserId="-1" Text="" />
  <row Id="42" PostHistoryTypeId="2" PostId="18" RevisionGUID="27193173-dce1-4133-9237-25d44e84dc62" CreationDate="2014-05-14T02:49:14.580" UserId="-1" Text="" />
  <row Id="43" PostHistoryTypeId="5" PostId="16" RevisionGUID="dfabe8d8-bc68-4f23-8978-86f1d0f44aba" CreationDate="2014-05-14T03:40:25.897" UserId="63" Comment="added 16 characters in body" Text="I use [Libsvm][1] to training data and predict classification on **semantic analysis** problem.&#xD;&#xA;&#xD;&#xA;But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** problem.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release and it can solve performance bottleneck.&#xD;&#xA;But it cost too much **memory**. &#xD;&#xA;&#xD;&#xA;Is **MapReduce** the only way to solve semantic analysis problem on big data?&#xD;&#xA;&#xD;&#xA;Or have any other methods can improve memory bottleneck on **Liblinear**.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/" />
  <row Id="44" PostHistoryTypeId="2" PostId="19" RevisionGUID="ec94df15-c4ec-44db-aae7-d75b1ad487ef" CreationDate="2014-05-14T03:56:20.963" UserId="84" Text="Lots of people use the term *big data* in a rather *comercial* way, as a means of pointing that large datasets are involved in the computation, and therefore upcoming solutions must have nice performance. Of course, *big data* always carry associated terms, like scalability and efficiency, but what exactly defines a problem as a *big data* problem?&#xD;&#xA;&#xD;&#xA;Does the computation have to be related to some set of specific purposes, like data mining/information retrieval, or could an algorithm for general graph problems be labeled *big data* if data dataset was *big enough*? Also, how *big* is *big enough*? (In case this is possible to define)." />
  <row Id="45" PostHistoryTypeId="1" PostId="19" RevisionGUID="ec94df15-c4ec-44db-aae7-d75b1ad487ef" CreationDate="2014-05-14T03:56:20.963" UserId="84" Text="How big is big data?" />
  <row Id="46" PostHistoryTypeId="3" PostId="19" RevisionGUID="ec94df15-c4ec-44db-aae7-d75b1ad487ef" CreationDate="2014-05-14T03:56:20.963" UserId="84" Text="&lt;bigdata&gt;&lt;scalability&gt;&lt;efficiency&gt;&lt;performance&gt;" />
  <row Id="48" PostHistoryTypeId="5" PostId="16" RevisionGUID="e9edc983-c391-4af9-b89d-bb37e571010d" CreationDate="2014-05-14T05:15:27.797" UserId="63" Comment="edited body" Text="I use [Libsvm][1] to training data and predict classification on **semantic analysis** problem.&#xD;&#xA;&#xD;&#xA;But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** problem.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release and it can solve performance bottleneck.&#xD;&#xA;But it cost too much **memory**. &#xD;&#xA;&#xD;&#xA;Is **MapReduce** the only way to solve semantic analysis problem on big data?&#xD;&#xA;&#xD;&#xA;Or have any other methods can improve memory bottleneck on **Liblinear**?&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/" />
  <row Id="49" PostHistoryTypeId="2" PostId="20" RevisionGUID="7b2b9b15-c532-4414-bd4e-3f9d367b2cf3" CreationDate="2014-05-14T05:37:46.780" UserId="96" Text="we created this social network application for eLearning purposes, it's an experimental thing we are researching on in our lab. it has been used by some case studies for a while and the data on our relational DBMS (SQL Server 2008) is getting big, it's a few gigabytes now and the tables are highly connected to each other. the performance is still fine, but when should we consider other options? is it the matter of performance?  " />
  <row Id="50" PostHistoryTypeId="1" PostId="20" RevisionGUID="7b2b9b15-c532-4414-bd4e-3f9d367b2cf3" CreationDate="2014-05-14T05:37:46.780" UserId="96" Text="the data on our relational DBMS is getting big, is it the time to move to NoSQL?" />
  <row Id="51" PostHistoryTypeId="3" PostId="20" RevisionGUID="7b2b9b15-c532-4414-bd4e-3f9d367b2cf3" CreationDate="2014-05-14T05:37:46.780" UserId="96" Text="&lt;nosql&gt;&lt;relational-dbms&gt;" />
  <row Id="52" PostHistoryTypeId="2" PostId="21" RevisionGUID="0ca00e0c-4f01-4ed4-8e63-08a9cbeb6ffe" CreationDate="2014-05-14T05:44:29.340" UserId="14" Text="As you rightly note, these days &quot;big data&quot; is something everyone wants to say they've got, which entails a certain looseness in how people define the term.  Generally, though, I'd say you're certainly dealing with big data if the scale is such that it's no longer feasible to manage with more traditional technologies such as RDBMS, at least without complementing them with big data technologies such as Hadoop.&#xD;&#xA;&#xD;&#xA;How big your data has to actually be for that to be the case is debatable.  Here's a (somewhat provocative) [blog post][1] that claims that it's not really the case for less than 5 TB of data.  (To be clear, it doesn't claim &quot;Less than 5 TB isn't big data&quot;, but just &quot;Less than 5 TB isn't big enough that you need Hadoop&quot;.)&#xD;&#xA;&#xD;&#xA;But even on smaller datasets, big data technologies like Hadoop can have other advantages, including being well suited to batch operations, playing well with unstructured data (as well as data whose structure isn't known in advance or could change), horizontal scalability (scaling by adding more nodes instead of beefing up your existing servers), and (as one of the commenters on the above-linked post notes) the ability to integrate your data processing with external data sets (think of a map-reduce where the mapper makes a call to another server).  Other technologies associated with big data, like NoSql databases, emphasize fast performance and consistent availability while dealing with large sets of data, as well also being able to handle semi-unstructured data and to scale horizontally.&#xD;&#xA;&#xD;&#xA;Of course, traditional RDBMS have their own advantages including ACID guarantees (Atomicity, Consistency, Isolation, Durability) and better performance for certain operations, as well as being more standardized, more mature, and (for many users) more familiar.  So even for indisputably &quot;big&quot; data, it may make sense to load at least a portion of your data into a traditional SQL database and use that in conjunction with big data technologies.&#xD;&#xA;&#xD;&#xA;So, a more generous definition would be that you have big data so long as it's big enough that big data technologies provide some added value for you.  But as you can see, that can depend not just on the size of your data but on how you want to work with it and what sort of requirements you have in terms of flexibility, consistency, and performance.  *How* you're using your data is more relevant to the question than what you're using it *for* (e.g. data mining).  That said, uses like data mining and machine learning are more likely to yield useful results if you have a big enough data set to work with.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html" />
  <row Id="53" PostHistoryTypeId="2" PostId="22" RevisionGUID="deac1274-d6c8-4534-82f2-ce6d40be137b" CreationDate="2014-05-14T05:58:21.927" UserId="97" Text="My set of data contains a set of numeric attributes and one categorical.&#xD;&#xA;&#xD;&#xA;Say, `NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr`, &#xD;&#xA;&#xD;&#xA;where `CategoricalAttr` takes one of three possible values: `CategoricalAttrValue1`, `CategoricalAttrValue2` or `CategoricalAttrValue3`.&#xD;&#xA;&#xD;&#xA;I'm using default k-means clustering algorithm implementation for Octave https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/.&#xD;&#xA;It works with numeric data only.&#xD;&#xA;&#xD;&#xA;So my question: is it correct to split the categorical attribute `CategoricalAttr` into three numeric (binary) variables, like `IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3` ?" />
  <row Id="54" PostHistoryTypeId="1" PostId="22" RevisionGUID="deac1274-d6c8-4534-82f2-ce6d40be137b" CreationDate="2014-05-14T05:58:21.927" UserId="97" Text="K-Mean clustering for mixed numeric and categorical data" />
  <row Id="55" PostHistoryTypeId="3" PostId="22" RevisionGUID="deac1274-d6c8-4534-82f2-ce6d40be137b" CreationDate="2014-05-14T05:58:21.927" UserId="97" Text="&lt;data-mining&gt;&lt;clustering&gt;&lt;octave&gt;" />
  <row Id="56" PostHistoryTypeId="2" PostId="23" RevisionGUID="50b00a66-20b3-4b36-af40-c372709c61c6" CreationDate="2014-05-14T06:06:13.603" UserId="97" Text="Data Science specialization from Johns Hopkins University at Coursera would be a great start.&#xD;&#xA;https://www.coursera.org/specialization/jhudatascience/1" />
  <row Id="57" PostHistoryTypeId="4" PostId="22" RevisionGUID="a6461050-177b-4a17-b842-aaa274e039b1" CreationDate="2014-05-14T06:11:22.330" UserId="97" Comment="Renamed 'K-Mean' to 'K-Means' and added tag" Text="K-Means clustering for mixed numeric and categorical data" />
  <row Id="58" PostHistoryTypeId="6" PostId="22" RevisionGUID="a6461050-177b-4a17-b842-aaa274e039b1" CreationDate="2014-05-14T06:11:22.330" UserId="97" Comment="Renamed 'K-Mean' to 'K-Means' and added tag" Text="&lt;data-mining&gt;&lt;clustering&gt;&lt;octave&gt;&lt;k-means&gt;" />
  <row Id="59" PostHistoryTypeId="2" PostId="24" RevisionGUID="73829641-ff19-4c62-a227-95d453a2f312" CreationDate="2014-05-14T06:26:27.163" UserId="14" Text="The standard k-means algorithm isn't directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn't have a natural origin.  A Euclidean distance function on such a space isn't really meaningful.  As someone put it, &quot;The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs.&quot; (from [here][1])&#xD;&#xA;&#xD;&#xA;There's a variation of k-means known as k-modes which is suitable for categorical data, although it has the flaw that the solutions you get are sensitive to initial conditions.  See [here][2] (PDF), for instance.&#xD;&#xA;&#xD;&#xA;A Google search for &quot;k-means mix of categorical data&quot; turns up quite a few papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven't yet read them, so I can't comment on their merits.)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html&#xD;&#xA;  [2]: http://arxiv.org/ftp/cs/papers/0603/0603120.pdf" />
  <row Id="60" PostHistoryTypeId="2" PostId="25" RevisionGUID="a1e2232d-7803-40a4-87e1-d439cbcfed5d" CreationDate="2014-05-14T07:26:04.390" UserId="104" Text="Big Data is defined by the volume of data, that's right, but not only. The particularity of big data is that you need to store a **lots** of **various** and sometimes **unstructured** stuffs **all the times** and from a **tons of sensors**, usually **for years or decade**.&#xD;&#xA;&#xD;&#xA;Furthermore you need something scalable, so that it doesn't take you half a year to find a data back.&#xD;&#xA;&#xD;&#xA;So here's come Big Data, where traditional method won't work anymore. SQL is not scalable. And SQL works with very structured and linked data (with all those Primary and foreign key mess, innerjoin, imbricated request...). &#xD;&#xA;&#xD;&#xA;Basically, because storage becomes cheaper and cheaper and data becomes more and more valuable, big manager ask engineer to records everything. Add to this tons of new sensors with all those mobile, social network, embeded stuff ...etc. So as classic methods won't work, they have to find new technologies (storing everything in files, in json format, with big index, what we call noSQL). &#xD;&#xA;&#xD;&#xA;So Big Data may be very big but can be not so big but complexe unstructured or various data which has to be store quickly and on-the-run in a raw format. We focus and storing at first, and then we look at how to link everything together." />
  <row Id="61" PostHistoryTypeId="2" PostId="26" RevisionGUID="0a0d04e3-da6d-4e63-9ee8-99daec0b75ad" CreationDate="2014-05-14T07:38:31.103" UserId="115" Text="A few gigabytes is not very &quot;**big**&quot;. It's more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don't get TB's of data a day).&#xD;&#xA;&#xD;&#xA;Most professionals working in a big data environment consider **&gt; ~5TB** as the *beginning* of the term big data. But even then it's not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.&#xD;&#xA;&#xD;&#xA;i.e. if you do alot of searches in you database it would probably be better to run a solr cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it in solr instead of just moving the data from sql to nosql in term of persistence and performance." />
  <row Id="62" PostHistoryTypeId="5" PostId="24" RevisionGUID="9e8b4e67-db0d-48c2-a91a-e9f73417dfa3" CreationDate="2014-05-14T07:40:44.160" UserId="14" Comment="added 641 characters in body" Text="The standard k-means algorithm isn't directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn't have a natural origin.  A Euclidean distance function on such a space isn't really meaningful.  As someone put it, &quot;The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs.&quot; (from [here][1])&#xD;&#xA;&#xD;&#xA;There's a variation of k-means known as k-modes, introduced in [this paper][2] by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed [here][3] (PDF), for instance.&#xD;&#xA;&#xD;&#xA;A Google search for &quot;k-means mix of categorical data&quot; turns up quite a few papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven't yet read them, so I can't comment on their merits.)  Huang's paper (linked above) also has a section on a &quot;k-protoypes&quot; which I believe serves this purpose.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:1147–1157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html&#xD;&#xA;  [2]: http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf&#xD;&#xA;  [3]: http://arxiv.org/ftp/cs/papers/0603/0603120.pdf" />
  <row Id="63" PostHistoryTypeId="10" PostId="15" RevisionGUID="85163b77-9e22-44b0-b1a5-1ee8422ff63f" CreationDate="2014-05-14T07:41:49.437" UserId="-1" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:5,&quot;DisplayName&quot;:&quot;Doorknob&quot;},{&quot;Id&quot;:51,&quot;DisplayName&quot;:&quot;Ansari&quot;},{&quot;Id&quot;:63,&quot;DisplayName&quot;:&quot;Puffin GDI&quot;},{&quot;Id&quot;:101,&quot;DisplayName&quot;:&quot;Didzis Elferts&quot;},{&quot;Id&quot;:115,&quot;DisplayName&quot;:&quot;Johnny000&quot;}]}" />
  <row Id="64" PostHistoryTypeId="2" PostId="27" RevisionGUID="39dce2b3-e25f-456a-896a-188d64522640" CreationDate="2014-05-14T07:53:02.560" UserId="108" Text="To answer this question you have o answer which kind of compromise you can afford. RDBMs implements [ACID][1]. This is expensive in terms of resources. There are no NoSQL solution which is ACID. See [CAP theorem][2] to dive deep into these ideas. &#xD;&#xA;&#xD;&#xA;So you have to understand each compromise given by each solution and choose the one which is the most appropriate for your problem.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/ACID&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/CAP_theorem" />
  <row Id="66" PostHistoryTypeId="5" PostId="26" RevisionGUID="d5747a83-5139-486b-9be3-9f5468dcc61d" CreationDate="2014-05-14T07:53:53.657" UserId="115" Comment="added 2 characters in body" Text="A few gigabytes is not very &quot;**big**&quot;. It's more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don't get TB's of data a day).&#xD;&#xA;&#xD;&#xA;Most professionals working in a big data environment consider **&gt; ~5TB** as the *beginning* of the term big data. But even then it's not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.&#xD;&#xA;&#xD;&#xA;i.e. if you do alot of searches in you database it would probably be better to run a solr cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance." />
  <row Id="67" PostHistoryTypeId="5" PostId="24" RevisionGUID="a6d83474-d91a-4153-9b57-f9dac0d907c2" CreationDate="2014-05-14T07:55:04.747" UserId="14" Comment="added 182 characters in body" Text="The standard k-means algorithm isn't directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn't have a natural origin.  A Euclidean distance function on such a space isn't really meaningful.  As someone put it, &quot;The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs.&quot; (from [here][1])&#xD;&#xA;&#xD;&#xA;There's a variation of k-means known as k-modes, introduced in [this paper][2] by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed [here][3] (PDF), for instance.&#xD;&#xA;&#xD;&#xA;Huang's paper (linked above) also has a section on &quot;k-protoypes&quot; which applies to data with a mix of categorical and numeric features.  It uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features.&#xD;&#xA;&#xD;&#xA;A Google search for &quot;k-means mix of categorical data&quot; turns up quite a few more recent papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven't yet read them, so I can't comment on their merits.)  &#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:1147–1157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html&#xD;&#xA;  [2]: http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf&#xD;&#xA;  [3]: http://arxiv.org/ftp/cs/papers/0603/0603120.pdf" />
  <row Id="68" PostHistoryTypeId="2" PostId="28" RevisionGUID="d398a744-6316-41a9-b3c6-1a6e509528da" CreationDate="2014-05-14T07:55:40.133" UserId="118" Text="There is free ebook &quot;[Introduction to Data Science][1]&quot; based on [tag:R] language&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://jsresearch.net/" />
  <row Id="69" PostHistoryTypeId="2" PostId="29" RevisionGUID="b66aeb2c-28b4-450f-8520-91b8f676999b" CreationDate="2014-05-14T07:56:34.437" UserId="53" Text="[@statsRus][statsRus] starts to lay the groundwork for your answer in another question http://datascience.stackexchange.com/questions/1/what-characterises-the-difference-between-data-science-and-statistics:&#xD;&#xA;&#xD;&#xA;&gt;  - **Data collection**: web scraping and online surveys&#xD;&#xA;&gt;  - **Data manipulation**: recoding messy data and extracting meaning from linguistic and social network data&#xD;&#xA;&gt;  - **Data scale**: working with extremely large data sets&#xD;&#xA;&gt;  - **Data mining**: finding patterns in large, complex data sets, with an emphasis on algorithmic techniques&#xD;&#xA;&gt;  - **Data communication**: helping turn &quot;machine-readable&quot; data into &quot;human-readable&quot; information via visualization&#xD;&#xA;&#xD;&#xA;Definition&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;[tag:data-mining] can be seen as one item (or set of skills and applications) in the toolkit of the data scientist.  I like how he separates the definition of mining from collection in a sort of trade-specific jargon.&#xD;&#xA;&#xD;&#xA;However, I think that *data-mining* would be synonymous with *data-collection* in a US-English colloquial definition.&#xD;&#xA;&#xD;&#xA;*As to where to go to become proficient?*  I think that question is too broad as it is currently stated and would receive answers that are primarily opinion based.  Perhaps if you could refine your question, it might be easier to see what you are asking.&#xD;&#xA;&#xD;&#xA;  [statsRus]: http://datascience.stackexchange.com/users/36/statsrus" />
  <row Id="70" PostHistoryTypeId="2" PostId="30" RevisionGUID="7f1db063-e957-4964-85d3-e3de3d198e86" CreationDate="2014-05-14T08:03:28.117" UserId="26" Text="The top-down answer:&#xD;&#xA;&#xD;&#xA;Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 ([source][1]) and with a doubling time of 40 months. Can't get bigger than that :)&#xD;&#xA;&#xD;&#xA;As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 ([source][2])  Their current warehouse is &gt;300 petabytes.&#xD;&#xA;&#xD;&#xA;Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it's a spectrum not a single number).&#xD;&#xA;&#xD;&#xA;Some of the features that make it &quot;big&quot; are:&#xD;&#xA;&#xD;&#xA;- it is actively analyzed, not just stored  (quote &quot;If you aren’t taking advantage of big data, then you don’t have big data, you have just a pile of data&quot; Jay Parikh @ Facebook)&#xD;&#xA;&#xD;&#xA;- building and running a data warehouse is a major infrastructure project&#xD;&#xA;&#xD;&#xA;- it is growing at a significant rate&#xD;&#xA;&#xD;&#xA;- it is unstructured or has irregular structure&#xD;&#xA;&#xD;&#xA;Gartner definition: &quot;Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing&quot; (The 3Vs)  So they also think &quot;bigness&quot; isn't entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://siliconangle.com/blog/2012/05/21/when-will-the-world-reach-8-zetabytes-of-stored-data-infographic/&#xD;&#xA;  [2]: http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/" />
  <row Id="71" PostHistoryTypeId="5" PostId="27" RevisionGUID="10c74c4b-2467-4513-89c5-6ae7a6752977" CreationDate="2014-05-14T08:03:37.890" UserId="14" Comment="minor typos/grammar" Text="To answer this question you have to answer which kind of compromise you can afford. RDBMs implements [ACID][1]. This is expensive in terms of resources. There are no NoSQL solutions which are ACID. See [CAP theorem][2] to dive deep into these ideas. &#xD;&#xA;&#xD;&#xA;So you have to understand each compromise given by each solution and choose the one which is the most appropriate for your problem.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/ACID&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/CAP_theorem" />
  <row Id="72" PostHistoryTypeId="24" PostId="27" RevisionGUID="10c74c4b-2467-4513-89c5-6ae7a6752977" CreationDate="2014-05-14T08:03:37.890" Comment="Proposed by 14 approved by 108 edit id of 5" />
  <row Id="73" PostHistoryTypeId="5" PostId="30" RevisionGUID="e952e344-5431-43bf-aa0e-4f105660d6c6" CreationDate="2014-05-14T08:09:20.747" UserId="26" Comment="added 15 characters in body" Text="The top-down answer:&#xD;&#xA;&#xD;&#xA;Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 ([source][1]) and with a doubling time of 40 months. Can't get bigger than that :)&#xD;&#xA;&#xD;&#xA;As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 ([source][2])  Their current warehouse is &gt;300 petabytes.&#xD;&#xA;&#xD;&#xA;Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it's a spectrum not a single number).&#xD;&#xA;&#xD;&#xA;In addition to size, some of the features that make it &quot;big&quot; are:&#xD;&#xA;&#xD;&#xA;- it is actively analyzed, not just stored  (quote &quot;If you aren’t taking advantage of big data, then you don’t have big data, you have just a pile of data&quot; Jay Parikh @ Facebook)&#xD;&#xA;&#xD;&#xA;- building and running a data warehouse is a major infrastructure project&#xD;&#xA;&#xD;&#xA;- it is growing at a significant rate&#xD;&#xA;&#xD;&#xA;- it is unstructured or has irregular structure&#xD;&#xA;&#xD;&#xA;Gartner definition: &quot;Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing&quot; (The 3Vs)  So they also think &quot;bigness&quot; isn't entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://siliconangle.com/blog/2012/05/21/when-will-the-world-reach-8-zetabytes-of-stored-data-infographic/&#xD;&#xA;  [2]: http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/" />
  <row Id="74" PostHistoryTypeId="2" PostId="31" RevisionGUID="26fa9a8e-80d6-4965-9725-009a067b91fc" CreationDate="2014-05-14T08:38:07.007" UserId="118" Text="I have a bunch of customer profiles stroed in [tag:elasticsearch] cluster. These profiles are now used for creatinon of target groups for our email subscriptions. &#xD;&#xA;&#xD;&#xA;Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).&#xD;&#xA;&#xD;&#xA;I wonder how can I search for interesting groups automatically - using datascience, machine learning, clustering or somewhat else.&#xD;&#xA;&#xD;&#xA;[tag:R] programming langugae seems to be a good tool for this task, but I can't form a methodology of such group search. One solution is to find somehow largest clusters of customers and use them as target groups, so the question is:&#xD;&#xA;&#xD;&#xA;**How can I automatically choose largest clusters of similar customers (similar by parameters that I don't know at this moment)?**&#xD;&#xA;&#xD;&#xA;For example: my program will connect elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="75" PostHistoryTypeId="1" PostId="31" RevisionGUID="26fa9a8e-80d6-4965-9725-009a067b91fc" CreationDate="2014-05-14T08:38:07.007" UserId="118" Text="Clustering customer data stored in ElasticSearch" />
  <row Id="76" PostHistoryTypeId="3" PostId="31" RevisionGUID="26fa9a8e-80d6-4965-9725-009a067b91fc" CreationDate="2014-05-14T08:38:07.007" UserId="118" Text="&lt;data-mining&gt;&lt;clustering&gt;" />
  <row Id="77" PostHistoryTypeId="10" PostId="7" RevisionGUID="87bf261d-1e23-4df7-a2d3-fded7c04c34d" CreationDate="2014-05-14T08:40:54.950" UserId="-1" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:66,&quot;DisplayName&quot;:&quot;demongolem&quot;},{&quot;Id&quot;:48,&quot;DisplayName&quot;:&quot;senshin&quot;},{&quot;Id&quot;:78,&quot;DisplayName&quot;:&quot;Bill the Lizard&quot;},{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;Konstantin V. Salikhov&quot;}]}" />
  <row Id="81" PostHistoryTypeId="5" PostId="31" RevisionGUID="d92b7e62-ad68-41e8-bf53-df02eaf2e2c7" CreationDate="2014-05-14T09:07:44.440" UserId="118" Comment="fix typo" Text="I have a bunch of customer profiles stroed in [tag:elasticsearch] cluster. These profiles are now used for creatinon of target groups for our email subscriptions. &#xD;&#xA;&#xD;&#xA;Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).&#xD;&#xA;&#xD;&#xA;I wonder how can I search for interesting groups automatically - using datascience, machine learning, clustering or somewhat else.&#xD;&#xA;&#xD;&#xA;[tag:R] programming language seems to be a good tool for this task, but I can't form a methodology of such group search. One solution is to find somehow largest clusters of customers and use them as target groups, so the question is:&#xD;&#xA;&#xD;&#xA;**How can I automatically choose largest clusters of similar customers (similar by parameters that I don't know at this moment)?**&#xD;&#xA;&#xD;&#xA;For example: my program will connect elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="82" PostHistoryTypeId="2" PostId="33" RevisionGUID="76691869-e20b-4928-b0b5-408547d8d201" CreationDate="2014-05-14T09:34:15.477" UserId="132" Text="Depends on 2 things: the nature/structure of your data; and your performance.&#xD;&#xA;&#xD;&#xA;SQL databases excel when your data is well structured (e.g. when it can be modeled as a table or Excel spreadsheet). A set of rows with a fixed # of columns. Also, when you need to do a lot of table joins (which it sounds like you do).&#xD;&#xA;&#xD;&#xA;NoSQL databases excel when the data is UN-structured beyond key-value pairs.&#xD;&#xA;&#xD;&#xA;Performance wise, you gotta ask yourself one question: is your current SQL solution slow? &#xD;&#xA;If it isn't, go with &quot;IIABDFI&quot; principle. " />
  <row Id="86" PostHistoryTypeId="2" PostId="35" RevisionGUID="de7f9431-9c8c-4f4a-b9e0-546d761b7f67" CreationDate="2014-05-14T09:51:54.753" UserId="26" Text="In working on exploratory data analysis, and developing algorithms, I find that most of my time is spent in a cycle of visualize, write some code, run on small dataset, repeat.   The data I have tends to be computer vision/sensor fusion type stuff, and algorithms are vision-heavy (for example object detection and tracking, etc), and the off the shelf algorithms don't work in this context.  I find that this takes a lot of iterations (for example, to dial in the type of algorithm or tune the parameters in the algorithm, or to get a visualization right) and also the run times even on a small dataset are quite long, so all together it takes a while. &#xD;&#xA;&#xD;&#xA;How can the algorithm development itself be sped up and made more scalable?&#xD;&#xA;&#xD;&#xA;Some specific challenges: &#xD;&#xA;&#xD;&#xA;How can the number of iterations be reduced?  (Esp. when what kind of algorithm, let alone the specifics of it, does not seem to be easily foreseeable without trying different versions and examining their behavior)&#xD;&#xA;&#xD;&#xA;How to run on bigger datasets during development?  (Often going from small to large dataset is when a bunch of new behavior and new issues is seen)&#xD;&#xA;&#xD;&#xA;How can algorithm parameters be tuned faster?&#xD;&#xA;&#xD;&#xA;How to apply machine learning type tools to algorithm development itself?  (For example, instead of writing the algorithm by hand, write some simple building blocks and combine them in a way learned from the problem, etc)&#xD;&#xA;" />
  <row Id="87" PostHistoryTypeId="1" PostId="35" RevisionGUID="de7f9431-9c8c-4f4a-b9e0-546d761b7f67" CreationDate="2014-05-14T09:51:54.753" UserId="26" Text="How to scale up algorithm development?" />
  <row Id="88" PostHistoryTypeId="3" PostId="35" RevisionGUID="de7f9431-9c8c-4f4a-b9e0-546d761b7f67" CreationDate="2014-05-14T09:51:54.753" UserId="26" Text="&lt;algorithms&gt;&lt;rapid-development&gt;" />
  <row Id="91" PostHistoryTypeId="2" PostId="37" RevisionGUID="d85541b1-1350-411d-b28c-a7915a0d4979" CreationDate="2014-05-14T10:41:23.823" UserId="9" Text="To me (coming from a relational database background), &quot;Big Data&quot; is not primarily about the data size (which is the bulk of what the other answers are so far).&#xD;&#xA;&#xD;&#xA;&quot;Big Data&quot; and &quot;Bad Data&quot; are closely related. Relational Databases require 'pristine data'. If the data is in the database, it is accurate, clean, and 100% reliable. Relational Databases require &quot;Great Data&quot;  and a huge amount of time, money, and accountability is put on to making sure the data is well prepared before loading it in to the database. If the data is in the database, it is 'gospel', and it defines the system understanding of reality.&#xD;&#xA;&#xD;&#xA;&quot;Big Data&quot; tackles this problem from the other direction. The data is poorly defined, much of it may be inaccurate, and much of it may in fact be missing. The structure and layout of the data is linear as opposed to relational.&#xD;&#xA;&#xD;&#xA;Big Data has to have enough volume so that the amount of bad data, or missing data becomes statistically insignificant. When the errors in your data are common enough to cancel each other out, and when the missing data is proportionally small enough to be negligible. When your data access requirements and algorithms are functional even with incomplete and inaccurate data, then you have &quot;Big Data&quot;.&#xD;&#xA;&#xD;&#xA;&quot;Big Data&quot; is not really about the volume, it is about the characteristics of the data." />
  <row Id="92" PostHistoryTypeId="2" PostId="38" RevisionGUID="8c1b70f8-6111-4299-9d8e-fe376bf25e3f" CreationDate="2014-05-14T10:44:58.933" UserId="134" Text="I heard about many tools / frameworks for helping people to process their data (big data environment). &#xD;&#xA;&#xD;&#xA;One is called Hadoop and the other is the noSQL concept. What is the difference in point of processing? &#xD;&#xA;&#xD;&#xA;Are they complementary?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="93" PostHistoryTypeId="1" PostId="38" RevisionGUID="8c1b70f8-6111-4299-9d8e-fe376bf25e3f" CreationDate="2014-05-14T10:44:58.933" UserId="134" Text="What is the difference between hadoop and noSQL" />
  <row Id="94" PostHistoryTypeId="3" PostId="38" RevisionGUID="8c1b70f8-6111-4299-9d8e-fe376bf25e3f" CreationDate="2014-05-14T10:44:58.933" UserId="134" Text="&lt;nosql&gt;&lt;tools&gt;&lt;processing&gt;&lt;hadoop&gt;" />
  <row Id="96" PostHistoryTypeId="5" PostId="26" RevisionGUID="1ebdfbcc-8a38-4a85-bb37-bf6885081bf6" CreationDate="2014-05-14T11:03:51.577" UserId="115" Comment="added 2 characters in body" Text="A few gigabytes is not very &quot;**big**&quot;. It's more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don't get TB's of data a day).&#xD;&#xA;&#xD;&#xA;Most professionals working in a big data environment consider **&gt; ~5TB** as the *beginning* of the term big data. But even then it's not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.&#xD;&#xA;&#xD;&#xA;i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance." />
  <row Id="99" PostHistoryTypeId="2" PostId="40" RevisionGUID="5f347c00-3376-4c79-a3ff-e3dd9c64bf45" CreationDate="2014-05-14T11:12:03.880" UserId="104" Text="Big Data is actually not so about the &quot;how big it is&quot;. &#xD;&#xA;&#xD;&#xA;First, few gigabytes is not big at all, it's almost nothing. So don't bother yourself, your system will continu to work efficiently for some time I think.&#xD;&#xA;&#xD;&#xA;Then you have to think of how do you use your data. &#xD;&#xA;&#xD;&#xA; - SQL approach: Every data is precious, well collected and selected, and the focus is put on storing high valuable and well structured data. This can be costly, everything is interlink, and it's good for well stuctured system and functionnal data.&#xD;&#xA; - Big Data approach: In big data you basically store almost everything, regardless of the value it has, and then do a active analytics process. Things are not linked, they are copied. For example let's say I have a blog entry. In Big Data there will not be a link to its author, but the author will be embedded inside the blog entry. Way more scalable, but require a different and more complex approach.&#xD;&#xA;&#xD;&#xA;If your storing &quot;functionnal&quot; data use by your application, I will suggest you to remain on SQL. If your storing data in order to search on them later or to do reporting, and if this amount of data may increase quickly, I will suggest big data.&#xD;&#xA;In my opinion, big data is useful when you are dealing with real data that have to be collect and analyzed continuously." />
  <row Id="100" PostHistoryTypeId="5" PostId="22" RevisionGUID="2322a126-81bc-49e2-ab1e-c55af2f9dfa8" CreationDate="2014-05-14T11:13:12.423" UserId="97" Comment="Cosmetic change in text." Text="My data set contains a number of numeric attributes and one categorical.&#xD;&#xA;&#xD;&#xA;Say, `NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr`, &#xD;&#xA;&#xD;&#xA;where `CategoricalAttr` takes one of three possible values: `CategoricalAttrValue1`, `CategoricalAttrValue2` or `CategoricalAttrValue3`.&#xD;&#xA;&#xD;&#xA;I'm using default k-means clustering algorithm implementation for Octave https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/.&#xD;&#xA;It works with numeric data only.&#xD;&#xA;&#xD;&#xA;So my question: is it correct to split the categorical attribute `CategoricalAttr` into three numeric (binary) variables, like `IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3` ?" />
  <row Id="101" PostHistoryTypeId="2" PostId="41" RevisionGUID="09a9e93c-d137-4792-b869-17e427812538" CreationDate="2014-05-14T11:15:40.907" UserId="136" Text="R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, &quot;Machine Learning with R&quot;.&#xD;&#xA;&#xD;&#xA;I've seen a guideline of 5TB for a dataset to be considered as Big Data.&#xD;&#xA;&#xD;&#xA;My question is: Is R suitable for the amount of Data typically seen in Big Data problems? &#xD;&#xA;Are there strategies to be employed when using R with this size of dataset?" />
  <row Id="102" PostHistoryTypeId="1" PostId="41" RevisionGUID="09a9e93c-d137-4792-b869-17e427812538" CreationDate="2014-05-14T11:15:40.907" UserId="136" Text="Is the R language suitable for Big Data" />
  <row Id="103" PostHistoryTypeId="3" PostId="41" RevisionGUID="09a9e93c-d137-4792-b869-17e427812538" CreationDate="2014-05-14T11:15:40.907" UserId="136" Text="&lt;bigdata&gt;" />
  <row Id="104" PostHistoryTypeId="2" PostId="42" RevisionGUID="78a1f18a-a62d-4c86-8364-409caed1b125" CreationDate="2014-05-14T11:21:31.500" UserId="59" Text="NoSQL is a way to store data that does not require there to be some sort of relation. The simplicity of its design and horizontal scale-ability, one way they store data is the `key : value` pair design. This lends itself to processing that is similar to Hadoop. The use of a NoSQL db really depends on the type of problem that one is after.&#xD;&#xA;&#xD;&#xA;Here is a good wikipedia link [NoSQL][1]&#xD;&#xA;&#xD;&#xA;Hadoop is a system that is meant to store and process huge chunks of data. It is a distributed file system dfs. The reason it does this is that central to its design it makes the assumption that hardware failures are common, thus making multiple copies of the same piece of information and spreading it across multiple machines and racks, so if one goes down, no problem, we have two more copies. Here is a great link for Hadoop from wikipedia as well, you will see that it is, in my opinion more than just storage, but also processing:&#xD;&#xA; [Hadoop][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/NoSQL&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Apache_Hadoop" />
  <row Id="105" PostHistoryTypeId="2" PostId="43" RevisionGUID="7245175c-1974-400a-9a1d-06e390f01e83" CreationDate="2014-05-14T11:23:25.913" UserId="115" Text="**Hadoop is not a database**, hadoop runs mapreduce jobs while splitting big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.&#xD;&#xA;&#xD;&#xA;i.e. you load into hadoop a set of `&lt;String, Integer&gt;` with the population of some cities and you want to get the average population over the whole dataset.&#xD;&#xA;&#xD;&#xA;    [new york, 40394]&#xD;&#xA;    [new york, 134]&#xD;&#xA;    [la, 44]&#xD;&#xA;    [la, 647]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Now hadoop will first map and reduce the keys, in our example the cities to a new set of integers:&#xD;&#xA;&#xD;&#xA;    [new york, [40394,134]]&#xD;&#xA;    [la, [44,647]]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;after this task is completed it will just get like in SQL the average (sum/size) of the interger set for each key:&#xD;&#xA;&#xD;&#xA;    [new york, [20264]]&#xD;&#xA;    [la, [346]]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;now hadoop would be done with everything. You can now load the result into the HDFS (hadoop file system) or into any DBMS or file.&#xD;&#xA;&#xD;&#xA;Thats just one **very basic** and **simple** example of what hadoop can do. You can run much more complicated tasks in hadoop." />
  <row Id="106" PostHistoryTypeId="2" PostId="44" RevisionGUID="1a2acc74-0bab-4f8e-9f30-49abfac53511" CreationDate="2014-05-14T11:24:39.530" UserId="59" Text="Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it's work in memory, so you are basically limited to the amount of RAM you have available to you.&#xD;&#xA;&#xD;&#xA;A mature project for R and Hadoop is RHadoop" />
  <row Id="107" PostHistoryTypeId="2" PostId="45" RevisionGUID="89a145b1-ee32-4acd-85a2-409b3e7166ee" CreationDate="2014-05-14T11:26:40.580" UserId="84" Text="First off, if your data has as many variations (in function of time, context, and others) as to make it hard to apply a single strategy to cope with it, you may be interested in doing a prior temporal/contextual/... characterization of the dataset. Characterizing data, i.e., extracting information about how the volume or specifics of the content varies according to some criteria, usually provides with a better understanding (more consise and precise) than simply inferring algorithms on a brute-force fashion.&#xD;&#xA;&#xD;&#xA;So, answering each question:&#xD;&#xA;&#xD;&#xA;1. characterization is definitely a means of reducing the number of *iterations* while trying to select proper algorithms for specific data;&#xD;&#xA;2. if you have a discrete set of criterias on which your data varies, it becomes much easier to *scale up* solutions, as will know what information you'd gain/lose if simpler/specific solutions were applied;&#xD;&#xA;3. after a characterization, you should be also easier to select parameters, since you'd know what kind of *specific data* you'd be dealing with;&#xD;&#xA;4. finally, you may use data mining/machine learning algorithms to support this characterization. This includes using:&#xD;&#xA;- clustering algorithms, to reduce the dimensionality of data;&#xD;&#xA;- classification algorithms, to help deciding on specific properties the data in function of time/context/... may present;&#xD;&#xA;- association rules, to predict particular knowledge from the dataset, while also improving/fine-graining the data used for later analysis;&#xD;&#xA;- and other possible strategies and analyses." />
  <row Id="108" PostHistoryTypeId="5" PostId="43" RevisionGUID="5bc7bf11-9b69-4072-83ce-dad8b699fd3b" CreationDate="2014-05-14T11:28:54.380" UserId="115" Comment="added 2 characters in body" Text="**Hadoop is not a database**, hadoop runs mapreduce jobs while splitting big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.&#xD;&#xA;&#xD;&#xA;i.e. you load into hadoop a set of `&lt;String, Integer&gt;` with the population of some cities and you want to get the average population over the whole dataset.&#xD;&#xA;&#xD;&#xA;    [new york, 40394]&#xD;&#xA;    [new york, 134]&#xD;&#xA;    [la, 44]&#xD;&#xA;    [la, 647]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Now hadoop will first map and reduce the keys, in our example the cities to a new set of integers:&#xD;&#xA;&#xD;&#xA;    [new york, [40394,134]]&#xD;&#xA;    [la, [44,647]]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;after this task is completed it will just get like in SQL the average (sum/length) of the interger set for each key:&#xD;&#xA;&#xD;&#xA;    [new york, [20264]]&#xD;&#xA;    [la, [346]]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;now hadoop would be done with everything. You can now load the result into the HDFS (hadoop file system) or into any DBMS or file.&#xD;&#xA;&#xD;&#xA;Thats just one **very basic** and **simple** example of what hadoop can do. You can run much more complicated tasks in hadoop.&#xD;&#xA;&#xD;&#xA;As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS." />
  <row Id="109" PostHistoryTypeId="5" PostId="45" RevisionGUID="f471a3b8-42db-4a6d-986a-7ac31108c948" CreationDate="2014-05-14T11:33:27.770" UserId="84" Comment="added 143 characters in body" Text="First off, if your data has as many variations (in function of time, context, and others) as to make it hard to apply a single strategy to cope with it, you may be interested in doing a prior temporal/contextual/... characterization of the dataset. Characterizing data, i.e., extracting information about how the volume or specifics of the content varies according to some criteria, usually provides with a better understanding (more consise and precise) than simply inferring algorithms on a brute-force fashion.&#xD;&#xA;&#xD;&#xA;So, answering each question:&#xD;&#xA;&#xD;&#xA;1. characterization is definitely a means of reducing the number of *iterations* while trying to select proper algorithms for specific data;&#xD;&#xA;2. if you have a discrete set of criterias on which your data varies, it becomes much easier to *scale up* solutions, as will know what information you'd gain/lose if simpler/specific solutions were applied;&#xD;&#xA;3. after a characterization, you should be also easier to select parameters, since you'd know what kind of *specific data* you'd be dealing with;&#xD;&#xA;4. finally, you may use data mining/machine learning algorithms to support this characterization. This includes using:&#xD;&#xA;- clustering algorithms, to reduce the dimensionality of data;&#xD;&#xA;- classification algorithms, to help deciding on specific properties the data in function of time/context/... may present;&#xD;&#xA;- association rules, to predict particular knowledge from the dataset, while also improving/fine-graining the data used for later analysis;&#xD;&#xA;- and other possible strategies and analyses.&#xD;&#xA;&#xD;&#xA;And [here](http://www.cs.cmu.edu/Groups/sage/sagedc.html) is a list of some criterias on which to analyse data, which you may find helpful." />
  <row Id="110" PostHistoryTypeId="5" PostId="43" RevisionGUID="7935a41a-181b-4245-9395-32170abf962c" CreationDate="2014-05-14T12:08:46.913" UserId="115" Comment="added 9 characters in body" Text="**Hadoop is not a database**, hadoop runs mapreduce jobs while splitting big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.&#xD;&#xA;&#xD;&#xA;i.e. you load into hadoop a set of `&lt;String, Integer&gt;` with the population of some cities and you want to get the average population over the whole dataset.&#xD;&#xA;&#xD;&#xA;    [new york, 40394]&#xD;&#xA;    [new york, 134]&#xD;&#xA;    [la, 44]&#xD;&#xA;    [la, 647]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Now hadoop will first map and reduce by using the keys, in our example the cities to a new set of integers:&#xD;&#xA;&#xD;&#xA;    [new york, [40394,134]]&#xD;&#xA;    [la, [44,647]]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;after this task is completed it will just get like in SQL the average (sum/length) of the interger set for each key:&#xD;&#xA;&#xD;&#xA;    [new york, [20264]]&#xD;&#xA;    [la, [346]]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;now hadoop would be done with everything. You can now load the result into the HDFS (hadoop file system) or into any DBMS or file.&#xD;&#xA;&#xD;&#xA;Thats just one **very basic** and **simple** example of what hadoop can do. You can run much more complicated tasks in hadoop.&#xD;&#xA;&#xD;&#xA;As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS." />
  <row Id="111" PostHistoryTypeId="2" PostId="46" RevisionGUID="38cec2c1-c571-466a-a640-358a3c1348bf" CreationDate="2014-05-14T12:32:29.503" UserId="21" Text="Note that there is an early version of LIBLINEAR ported to [Apache Spark][1]. See: http://apache-spark-user-list.1001560.n3.nabble.com/Spark-LIBLINEAR-td5546.html&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://spark.apache.org" />
  <row Id="112" PostHistoryTypeId="2" PostId="47" RevisionGUID="7992303c-d1a3-40ac-8c8a-2fafacabfdb8" CreationDate="2014-05-14T12:39:41.197" UserId="62" Text="The main problem with using R for large data sets is the RAM constraint. The reason behind keeping all the data in RAM is that it provides much faster access and data manipulations than would storing on HDDs. If you are willing to take a hit on performance, then yes, it is quite practical to work with [large datasets in R][1].&#xD;&#xA;&#xD;&#xA;- RODBC Package: Allows connecting to external DB from R to retrieve and handle data. Hence, the data being *manipulated* is restricted to your RAM. The overall data set can go much larger.&#xD;&#xA;- The ff package allows using larger than RAM data sets by utilising memory-mapped pages.&#xD;&#xA;- BigLM: It builds generalized linear models on big data. It loads data into memory in chunks.&#xD;&#xA;- bigmemory : An R package which allows powerful and memory-efficient parallel &#xD;&#xA;analyses and data mining of massive data sets. It permits storing large objects (matrices etc.) in memory (on the RAM) using external pointer objects to refer to them. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://statistics.org.il/wp-content/uploads/2010/04/Big_Memory%20V0.pdf" />
  <row Id="113" PostHistoryTypeId="6" PostId="41" RevisionGUID="2a9da2b2-a929-4f6d-baf4-1f346da68021" CreationDate="2014-05-14T13:06:28.407" UserId="118" Comment="added tag r" Text="&lt;bigdata&gt;&lt;r&gt;" />
  <row Id="114" PostHistoryTypeId="24" PostId="41" RevisionGUID="2a9da2b2-a929-4f6d-baf4-1f346da68021" CreationDate="2014-05-14T13:06:28.407" Comment="Proposed by 118 approved by 136 edit id of 6" />
  <row Id="115" PostHistoryTypeId="2" PostId="48" RevisionGUID="c69229e9-4232-4319-b59c-fa6ddb624b69" CreationDate="2014-05-14T13:08:26.647" UserId="-1" Text="" />
  <row Id="116" PostHistoryTypeId="2" PostId="49" RevisionGUID="5b223f01-fc39-41c2-933c-1d047f02f76b" CreationDate="2014-05-14T13:08:26.647" UserId="-1" Text="" />
  <row Id="120" PostHistoryTypeId="2" PostId="50" RevisionGUID="1d4c4cc3-161f-42a3-9c9c-5a344ce7660e" CreationDate="2014-05-14T14:26:54.313" UserId="151" Text="I have an R script that generates a report based on the current contents of a database. This database is constantly in flux with records being added/deleted many times each day. How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning? Or perhaps I want it to re-run once a certain number of new records have been added to the database. How might I go about automating this? I should mention I'm on Windows, but I could easily put this script on my Linux machine if that would simplify the process. " />
  <row Id="121" PostHistoryTypeId="1" PostId="50" RevisionGUID="1d4c4cc3-161f-42a3-9c9c-5a344ce7660e" CreationDate="2014-05-14T14:26:54.313" UserId="151" Text="Running an R script programmatically" />
  <row Id="122" PostHistoryTypeId="3" PostId="50" RevisionGUID="1d4c4cc3-161f-42a3-9c9c-5a344ce7660e" CreationDate="2014-05-14T14:26:54.313" UserId="151" Text="&lt;r&gt;&lt;databases&gt;&lt;efficiency&gt;&lt;tools&gt;" />
  <row Id="123" PostHistoryTypeId="10" PostId="5" RevisionGUID="3251bee5-5c96-43ef-bae7-9dd14bd950b1" CreationDate="2014-05-14T14:40:25.950" UserId="-1" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;},{&quot;Id&quot;:115,&quot;DisplayName&quot;:&quot;Johnny000&quot;},{&quot;Id&quot;:118,&quot;DisplayName&quot;:&quot;Konstantin V. Salikhov&quot;},{&quot;Id&quot;:148,&quot;DisplayName&quot;:&quot;ProgramFOX&quot;},{&quot;Id&quot;:108,&quot;DisplayName&quot;:&quot;rapaio&quot;}]}" />
  <row Id="124" PostHistoryTypeId="2" PostId="51" RevisionGUID="8d8451a2-7e08-488e-98ea-20a3d783ce49" CreationDate="2014-05-14T14:48:32.180" UserId="62" Text="&gt; How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning?&#xD;&#xA;&#xD;&#xA;You can set up a cronjob on a Linux system. These are run at the set time, if the computer is on. To do so, open a terminal and type:&#xD;&#xA;&#xD;&#xA;    crontab -e&#xD;&#xA;&#xD;&#xA;And add:&#xD;&#xA;&#xD;&#xA;    00 4 * * * /your/command&#xD;&#xA;&#xD;&#xA;Source: [Stack Overflow][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/a/14710307/1488917" />
  <row Id="125" PostHistoryTypeId="5" PostId="51" RevisionGUID="f12fc059-e868-446f-a45f-dd6eb3bc05e6" CreationDate="2014-05-14T14:57:32.243" UserId="151" Comment="more explicit code for running an R script rather than just any command" Text="&gt; How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning?&#xD;&#xA;&#xD;&#xA;You can set up a cronjob on a Linux system. These are run at the set time, if the computer is on. To do so, open a terminal and type:&#xD;&#xA;&#xD;&#xA;    crontab -e&#xD;&#xA;&#xD;&#xA;And add:&#xD;&#xA;&#xD;&#xA;    00 4 * * * r source(/home/FilePath/.../myRscript.R)&#xD;&#xA;&#xD;&#xA;Source: [Stack Overflow][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/a/14710307/1488917" />
  <row Id="126" PostHistoryTypeId="24" PostId="51" RevisionGUID="f12fc059-e868-446f-a45f-dd6eb3bc05e6" CreationDate="2014-05-14T14:57:32.243" Comment="Proposed by 151 approved by 62 edit id of 8" />
  <row Id="127" PostHistoryTypeId="2" PostId="52" RevisionGUID="c45e9098-e4e9-42bc-a3a7-96197b0e0e4a" CreationDate="2014-05-14T15:25:21.700" UserId="157" Text="From my limited dabbling with data science using R, I realized that cleaning bad data is a very important part of preparing data for analysis. &#xD;&#xA;&#xD;&#xA;Are there any best practices or processes for cleaning data before processing it? If so, are there any automated or semi-automated tools which implement some of these best practices?" />
  <row Id="128" PostHistoryTypeId="1" PostId="52" RevisionGUID="c45e9098-e4e9-42bc-a3a7-96197b0e0e4a" CreationDate="2014-05-14T15:25:21.700" UserId="157" Text="Organized processes to clean data" />
  <row Id="129" PostHistoryTypeId="3" PostId="52" RevisionGUID="c45e9098-e4e9-42bc-a3a7-96197b0e0e4a" CreationDate="2014-05-14T15:25:21.700" UserId="157" Text="&lt;bad-data&gt;&lt;data-cleaning&gt;" />
  <row Id="130" PostHistoryTypeId="2" PostId="53" RevisionGUID="221a9386-1dd1-4d0a-9f44-cd5318094c6c" CreationDate="2014-05-14T15:42:02.393" UserId="116" Text="For windows, use the task scheduler to set the task to run for example daily at 4:00 AM&#xD;&#xA;&#xD;&#xA;It gives you many other options regarding frequency etc.&#xD;&#xA;http://en.wikipedia.org/wiki/Windows_Task_Scheduler" />
  <row Id="140" PostHistoryTypeId="2" PostId="57" RevisionGUID="957647d9-0ef5-40f4-aaca-1e4ff9a2f9bb" CreationDate="2014-05-14T16:29:39.927" UserId="84" Text="From my point of view, this question is suitable for a two-step answer. The first part, let us call it *soft preprocessing*, could be taken as the usage of different data mining algorithms to preprocess data in such a way that makes it suitable for further analyses. Notice that this could be the analysis itself, in case the goal is simple enough to be tackled in a single shot.&#xD;&#xA;&#xD;&#xA;The second part, the *hard preprocessing*, actually comes prior to any other process, and is may be taken as the usage of simple tools or scripts to clean up data, selecting specific contents to be processed. To this problem, POSIX provides us with a wonderous set of magic tools, which can be used to compose concise -- and very powerful -- preprocessing scripts.&#xD;&#xA;&#xD;&#xA;For example, for people who deal with data coming from social websites (twitter, facebook, ...), the *data retrieval* usually yields files with very specific format -- although not always nicely structure, as they may contain missing fields, and so. For these cases, a simple `awk` script could clean up the data, producing a *valid* input file for later processing. From the magic set, one may also point out `grep`, `sed`, `cut`, `join`, `paste`, `sort`, and a whole multitude of other tools.&#xD;&#xA;&#xD;&#xA;In case simple the source file has too many nitty-gritties, it may also be necessary to produce a bundle of methods to clean up data. In such cases, it is usually better to use scripting languages (other than shell ones), such as Python, Ruby, and Perl. This allows for building up *API*'s to select specific data in a very straightforward and reusable way. Such *API*'s are sometimes made public by their writers, such as [IMDbPY](http://imdbpy.sourceforge.net/), [Stack Exchange API](https://api.stackexchange.com/docs), and many others.&#xD;&#xA;&#xD;&#xA;So, answering the question: are there any best practices? It usually depends on your task. If you will always deal with the same data format, it's commonly best to write an *organized* script to preprocess it; whereas, if you just need a simple and fast clean up on some dataset, count on POSIX tools for concise shell scripts that will do the whole job **much** faster than a Python script, or so. Since the *clean up* depends both on the dataset and on your purposes, it's hard to have everything already done. Yet, there are lots of API's that puts you halfway through with the problem." />
  <row Id="145" PostHistoryTypeId="2" PostId="58" RevisionGUID="2c437422-f2f7-4e70-9bdb-22bd10e05265" CreationDate="2014-05-14T17:06:33.337" UserId="119" Text="You can check out [vowpal wabbit][1]. It is quite popular for large-scale learning and includes parallel provisions.&#xD;&#xA;&#xD;&#xA;From their website:&#xD;&#xA;&#xD;&#xA;&gt; VW is the essence of speed in machine learning, able to learn from terafeature datasets with ease. Via parallel learning, it can exceed the throughput of any single machine network interface when doing linear learning, a first amongst learning algorithms.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://hunch.net/~vw/" />
  <row Id="147" PostHistoryTypeId="2" PostId="59" RevisionGUID="c8666588-5a3b-4d49-ba50-3258dc13338e" CreationDate="2014-05-14T17:48:21.240" UserId="158" Text="In reviewing “[Applied Predictive Modeling][1]&quot; a [reviewer states][2]:&#xD;&#xA;&#xD;&#xA;&gt; One critique I have of statistical learning (SL) pedagogy is the&#xD;&#xA;&gt; absence of computation performance considerations in the evaluation of&#xD;&#xA;&gt; different modeling techniques. With its emphases on bootstrapping and&#xD;&#xA;&gt; cross-validation to tune/test models, SL is quite compute-intensive.&#xD;&#xA;&gt; Add to that the re-sampling that's embedded in techniques like bagging&#xD;&#xA;&gt; and boosting, and you have the specter of computation hell for&#xD;&#xA;&gt; supervised learning of large data sets. **In fact, R's memory&#xD;&#xA;&gt; constraints impose pretty severe limits on the size of models that can&#xD;&#xA;&gt; be fit by top-performing methods like random forests.** Though SL does a&#xD;&#xA;&gt; good job calibrating model performance against small data sets, it'd&#xD;&#xA;&gt; sure be nice to understand performance versus computational cost for&#xD;&#xA;&gt; larger data.&#xD;&#xA;&#xD;&#xA;What are R's memory constraints, and do they impose severe limits on the size of models that can be fit by top-performing methods like [random forests][3]?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485&#xD;&#xA;  [2]: http://www.information-management.com/blogs/applied-predictive-modeling-10024771-1.html&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Random_forest" />
  <row Id="148" PostHistoryTypeId="1" PostId="59" RevisionGUID="c8666588-5a3b-4d49-ba50-3258dc13338e" CreationDate="2014-05-14T17:48:21.240" UserId="158" Text="What are R's memory constraints?" />
  <row Id="149" PostHistoryTypeId="3" PostId="59" RevisionGUID="c8666588-5a3b-4d49-ba50-3258dc13338e" CreationDate="2014-05-14T17:48:21.240" UserId="158" Text="&lt;r&gt;" />
  <row Id="150" PostHistoryTypeId="5" PostId="43" RevisionGUID="09e1df24-dfc9-47ee-9ea8-b3ea35f5ab6b" CreationDate="2014-05-14T17:51:58.163" UserId="115" Comment="added 1 character in body" Text="**Hadoop is not a database**, hadoop runs mapreduce jobs while splitting big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.&#xD;&#xA;&#xD;&#xA;i.e. you load into hadoop a set of `&lt;String, Integer&gt;` with the population of some cities and you want to get the average population over the whole dataset.&#xD;&#xA;&#xD;&#xA;    [new york, 40394]&#xD;&#xA;    [new york, 134]&#xD;&#xA;    [la, 44]&#xD;&#xA;    [la, 647]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;Now hadoop will first map and reduce by using the keys (in our example the cities) to a new set of integers:&#xD;&#xA;&#xD;&#xA;    [new york, [40394,134]]&#xD;&#xA;    [la, [44,647]]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;after this task is completed it will just get like in SQL the average (sum/length) of the interger set for each key:&#xD;&#xA;&#xD;&#xA;    [new york, [20264]]&#xD;&#xA;    [la, [346]]&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;now hadoop would be done with everything. You can now load the result into the HDFS (hadoop file system) or into any DBMS or file.&#xD;&#xA;&#xD;&#xA;Thats just one **very basic** and **simple** example of what hadoop can do. You can run much more complicated tasks in hadoop.&#xD;&#xA;&#xD;&#xA;As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS." />
  <row Id="151" PostHistoryTypeId="2" PostId="60" RevisionGUID="9d921d53-259d-4dcc-aebe-f8dacb8a8f29" CreationDate="2014-05-14T17:58:48.297" UserId="118" Text="R performs all computation in-memory so you can't perform operation on a dataset that is larger than available RAM amount. However there are some libraries that allow bigdata processing using R and one of popular libraries for bigdata processing like Hadoop." />
  <row Id="153" PostHistoryTypeId="2" PostId="61" RevisionGUID="ab1149f0-cc8b-41da-921e-d6f9ea709f6f" CreationDate="2014-05-14T18:09:01.940" UserId="158" Text="Logic often states that by overfitting a model, it's capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why?" />
  <row Id="154" PostHistoryTypeId="1" PostId="61" RevisionGUID="ab1149f0-cc8b-41da-921e-d6f9ea709f6f" CreationDate="2014-05-14T18:09:01.940" UserId="158" Text="Why Is Overfitting Bad?" />
  <row Id="155" PostHistoryTypeId="3" PostId="61" RevisionGUID="ab1149f0-cc8b-41da-921e-d6f9ea709f6f" CreationDate="2014-05-14T18:09:01.940" UserId="158" Text="&lt;machine-learning&gt;" />
  <row Id="156" PostHistoryTypeId="2" PostId="62" RevisionGUID="5ddffccf-c5fd-48df-870b-172aba4263bf" CreationDate="2014-05-14T18:27:56.043" UserId="26" Text="Overfitting is *empirically* bad.  Suppose you have a data set which you split in two (test and training).  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on a test dataset than models which are not overfitted.  &#xD;&#xA;&#xD;&#xA;One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).&#xD;&#xA;&#xD;&#xA;Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal." />
  <row Id="157" PostHistoryTypeId="5" PostId="30" RevisionGUID="8bb31230-592a-4a83-991a-caf55c0e6fc6" CreationDate="2014-05-14T18:30:59.180" UserId="26" Comment="deleted 24 characters in body" Text="Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 ([source][1]) and with a doubling time of 40 months. Can't get bigger than that :)&#xD;&#xA;&#xD;&#xA;As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 ([source][2])  Their current warehouse is &gt;300 petabytes.&#xD;&#xA;&#xD;&#xA;Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it's a spectrum not a single number).&#xD;&#xA;&#xD;&#xA;In addition to size, some of the features that make it &quot;big&quot; are:&#xD;&#xA;&#xD;&#xA;- it is actively analyzed, not just stored  (quote &quot;If you aren’t taking advantage of big data, then you don’t have big data, you have just a pile of data&quot; Jay Parikh @ Facebook)&#xD;&#xA;&#xD;&#xA;- building and running a data warehouse is a major infrastructure project&#xD;&#xA;&#xD;&#xA;- it is growing at a significant rate&#xD;&#xA;&#xD;&#xA;- it is unstructured or has irregular structure&#xD;&#xA;&#xD;&#xA;Gartner definition: &quot;Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing&quot; (The 3Vs)  So they also think &quot;bigness&quot; isn't entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://siliconangle.com/blog/2012/05/21/when-will-the-world-reach-8-zetabytes-of-stored-data-infographic/&#xD;&#xA;  [2]: http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/" />
  <row Id="161" PostHistoryTypeId="2" PostId="64" RevisionGUID="5a086527-e70c-4061-8f73-d2705614e26f" CreationDate="2014-05-14T18:37:52.333" UserId="84" Text="Overfitting, in a nutshell, means take into account **too much** information from your data and/or prior knowledge, and use it in a model. To make it more straightforward, consider the following example: you're hired by some scientists to provide them with a model to predict the growth of some kind of plants. The scientists have given you information collected from their work &#xD;&#xA;with such plants throughout a whole year, and they shall continuously give you information on the future development of their plantation.&#xD;&#xA;&#xD;&#xA;So, you run through the data received, and build up a model out of it. Now suppose that, in your model, you considered just as many characteristics as possible to always find out the exact behavior of the plants you saw in the initial dataset. Now, as the production continues, you'll always take into account those characteristics, and will produce very *fine-grained* results. However, if the plantation eventually suffer from some seasonal change, the results you will receive may fit your model in such a way that your predictions will begin to fail (either saying that the growth will slow down, while it shall actually speed up, or the opposite).&#xD;&#xA;&#xD;&#xA;Apart from being unable to detect such small variations, and to usually classify your entries incorrectly, the *fine-grain* on the model, i.e., the great amount of variables, may cause the processing to be too costly. Now, imagine that your data is already complex. Overfitting your model to the data not only will make the classification/evaluation very complex, but will most probably make you error the prediction over the slightest variation you may have on the input.&#xD;&#xA;" />
  <row Id="162" PostHistoryTypeId="5" PostId="62" RevisionGUID="6e07951a-1131-47e6-b5a7-1220f6f4bdd1" CreationDate="2014-05-14T18:39:59.027" UserId="26" Comment="added 305 characters in body" Text="Overfitting is *empirically* bad.  Suppose you have a data set which you split in two (test and training).  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on a test dataset than models which are not overfitted.  &#xD;&#xA;&#xD;&#xA;One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data.&#xD;&#xA;&#xD;&#xA;Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.&#xD;&#xA;&#xD;&#xA;P.S. On the &quot;ability to generalize&quot; part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail." />
  <row Id="163" PostHistoryTypeId="2" PostId="65" RevisionGUID="e762ca7b-c543-42a2-a6aa-20e2d60079ed" CreationDate="2014-05-14T18:45:23.917" UserId="-1" Text="" />
  <row Id="164" PostHistoryTypeId="2" PostId="66" RevisionGUID="8de39075-4f35-4ca3-ae76-448c5284d426" CreationDate="2014-05-14T18:45:23.917" UserId="-1" Text="" />
  <row Id="165" PostHistoryTypeId="2" PostId="67" RevisionGUID="34fedb95-2b78-4ed8-9e01-093ec98e1e53" CreationDate="2014-05-14T18:48:42.263" UserId="-1" Text="" />
  <row Id="166" PostHistoryTypeId="2" PostId="68" RevisionGUID="3696c1bf-49a3-4f38-90a2-2268b13ccf89" CreationDate="2014-05-14T18:48:42.263" UserId="-1" Text="" />
  <row Id="170" PostHistoryTypeId="5" PostId="62" RevisionGUID="15678292-7405-4bee-b0f0-9162a7ab5982" CreationDate="2014-05-14T19:46:32.213" UserId="26" Comment="added 24 characters in body" Text="Overfitting is *empirically* bad.  Suppose you have a data set which you split in two (test and training).  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on a test dataset than models which are not overfitted.  &#xD;&#xA;&#xD;&#xA;One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data (including future data).&#xD;&#xA;&#xD;&#xA;Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.&#xD;&#xA;&#xD;&#xA;P.S. On the &quot;ability to generalize&quot; part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail." />
  <row Id="171" PostHistoryTypeId="2" PostId="69" RevisionGUID="b26e1d4c-681c-4c18-8216-4e7ac96d298d" CreationDate="2014-05-14T20:03:15.233" UserId="158" Text="Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it's planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility without it just being a replication of what has already been done. How do you define the difference between replication and reproducibility, and how to you execute a data science workflow that allows for reproducibility without simply being a replication of what has already been done?" />
  <row Id="172" PostHistoryTypeId="1" PostId="69" RevisionGUID="b26e1d4c-681c-4c18-8216-4e7ac96d298d" CreationDate="2014-05-14T20:03:15.233" UserId="158" Text="What is a reproducibile data science workflow?" />
  <row Id="173" PostHistoryTypeId="3" PostId="69" RevisionGUID="b26e1d4c-681c-4c18-8216-4e7ac96d298d" CreationDate="2014-05-14T20:03:15.233" UserId="158" Text="&lt;processing&gt;" />
  <row Id="174" PostHistoryTypeId="5" PostId="46" RevisionGUID="ffcfb3f1-b78e-425d-bff6-ed9f0147dcb0" CreationDate="2014-05-14T21:03:05.313" UserId="21" Comment="added 162 characters in body" Text="Note that there is an early version of LIBLINEAR ported to [Apache Spark][1]. See [mailing list comments][2] for some early details, and the [project site][3].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://spark.apache.org&#xD;&#xA;  [2]: http://apache-spark-user-list.1001560.n3.nabble.com/Spark-LIBLINEAR-td5546.html&#xD;&#xA;  [3]: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/" />
  <row Id="175" PostHistoryTypeId="5" PostId="69" RevisionGUID="e1e5b55f-8480-4e5e-b391-eaf60d40577b" CreationDate="2014-05-14T21:34:19.977" UserId="158" Comment="added 394 characters in body" Text="Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it's planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility without it just being a replication of what has already been done. How do you define the difference between replication and reproducibility, and how to you execute a data science workflow that allows for reproducibility without simply being a replication of what has already been done?&#xD;&#xA;&#xD;&#xA;**UPDATE:** Possible my use of the terms is my own, though I view replication as an exact copy that's a result of having access to all inputs &amp; processes required to reach the same outcome. Replication to me means that the concepts related to how the input was collected &amp; processed is known, but independently achieves an outcome that expresses an identental outcome semantically speaking." />
  <row Id="176" PostHistoryTypeId="2" PostId="70" RevisionGUID="9940efa6-419d-480c-97e5-83b114d7fa59" CreationDate="2014-05-14T22:03:50.597" UserId="178" Text="To be reproducible without being just a replication, you would need to redo the experiment with new data, following the same technique as before.  The work flow is not as important as the techniques used.  Sample data in the same way, use the same type of models.  It doesn't matter if you switch from one language to another, so long as the models and the data manipulations are the same.&#xD;&#xA;&#xD;&#xA;This type of replication will show that the results you got in the first experiment are less likely to be a fluke than they were earlier." />
  <row Id="177" PostHistoryTypeId="2" PostId="71" RevisionGUID="e02a35c8-d701-4aab-ac60-943550b81965" CreationDate="2014-05-14T22:12:37.203" UserId="179" Text="What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?" />
  <row Id="178" PostHistoryTypeId="1" PostId="71" RevisionGUID="e02a35c8-d701-4aab-ac60-943550b81965" CreationDate="2014-05-14T22:12:37.203" UserId="179" Text="When are p-values deceptive?" />
  <row Id="179" PostHistoryTypeId="3" PostId="71" RevisionGUID="e02a35c8-d701-4aab-ac60-943550b81965" CreationDate="2014-05-14T22:12:37.203" UserId="179" Text="&lt;bigdata&gt;&lt;statistics&gt;" />
  <row Id="180" PostHistoryTypeId="4" PostId="69" RevisionGUID="a3106baa-327f-4430-b6c0-280ebdeeb0b1" CreationDate="2014-05-14T22:19:52.970" UserId="158" Comment="edited title" Text="Is it possible to automate generating reproducibility documentation?" />
  <row Id="181" PostHistoryTypeId="4" PostId="38" RevisionGUID="b461a733-4231-456a-aa4f-bfd23207b2f7" CreationDate="2014-05-14T22:26:59.453" UserId="134" Comment="edited title" Text="What is the difference between Hadoop and noSQL" />
  <row Id="182" PostHistoryTypeId="7" PostId="69" RevisionGUID="b9210a50-45d3-4029-b2c2-5cd4677be88b" CreationDate="2014-05-14T22:28:50.800" UserId="158" Comment="Rollback to [e1e5b55f-8480-4e5e-b391-eaf60d40577b]" Text="What is a reproducibile data science workflow?" />
  <row Id="183" PostHistoryTypeId="2" PostId="72" RevisionGUID="dfebb0af-4088-4bec-8dcb-52266bfe44b2" CreationDate="2014-05-14T22:40:40.363" UserId="22" Text="One algorithm that can be used for this is the [k-means clustering algorithm](http://en.wikipedia.org/wiki/K-means_clustering).&#xD;&#xA;&#xD;&#xA;Basically:&#xD;&#xA;&#xD;&#xA;1. Randomly choose k datapoints from your set, m_1, ..., m_k.&#xD;&#xA;2. &quot;Until convergence&quot;:&#xD;&#xA;&#xD;&#xA;    1. Assign your data points to k clusters, where cluster i is the set of points for which m_i is the closest of your current means&#xD;&#xA;    2. Replace each m_i by the mean of all points assigned to cluster i.&#xD;&#xA;&#xD;&#xA;It is good practice to repeat this algorithm several times, then choose the outcome that minimizes distances between the points of each cluster i and the center m_i.&#xD;&#xA;&#xD;&#xA;Of course, you have to know k to start here; you can use cross-validation to choose this parameter, though." />
  <row Id="184" PostHistoryTypeId="2" PostId="73" RevisionGUID="c999b644-b087-4c34-8536-c9584f41951f" CreationDate="2014-05-14T22:43:23.587" UserId="14" Text="You shouldn't consider the p-value out of context.&#xD;&#xA;&#xD;&#xA;One rather basic point (as illustrated by [xkcd][1]) is that you need to consider how many tests you're actually doing.  Obviously, you shouldn't be shocked to see p &lt; 0.05 for one out of 20 tests, even if the null hypothesis is true every time.  &#xD;&#xA;&#xD;&#xA;A more subtle example of this occurs in high-energy physics, and is known as the [look-elsewhere effect][2].  The larger the parameter space you search for a signal that might represent a new particle, the more likely you are to see an apparent signal that's really just due to random fluctuations. &#xD;&#xA;&#xD;&#xA;  [1]: http://xkcd.com/882/&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Look-elsewhere_effect" />
  <row Id="185" PostHistoryTypeId="2" PostId="74" RevisionGUID="dfd37513-608e-4c4d-99f8-929c1c3c80f6" CreationDate="2014-05-14T22:58:11.583" UserId="64" Text="One thing you should be aware of is the sample size you are using. Very large samples, such as economists using census data, will lead to deflated p-values. This paper [&quot;Too Big to Fail: Large Samples and the p-Value Problem&quot;][1] covers some of the issues. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://galitshmueli.com/system/files/Print%20Version.pdf &quot;Too Big to Fail: Large Samples and the p-Value Problem&quot;" />
  <row Id="187" PostHistoryTypeId="2" PostId="75" RevisionGUID="c39875a3-fbd2-4329-b31c-f9eeab2b8195" CreationDate="2014-05-15T00:26:11.387" UserId="158" Text="If small p-values are plentiful in big data, what is a comparable replacement for p-values in data with million of samples?" />
  <row Id="188" PostHistoryTypeId="1" PostId="75" RevisionGUID="c39875a3-fbd2-4329-b31c-f9eeab2b8195" CreationDate="2014-05-15T00:26:11.387" UserId="158" Text="Is there a replacement for small p-values in big data?" />
  <row Id="189" PostHistoryTypeId="3" PostId="75" RevisionGUID="c39875a3-fbd2-4329-b31c-f9eeab2b8195" CreationDate="2014-05-15T00:26:11.387" UserId="158" Text="&lt;statistics&gt;" />
  <row Id="190" PostHistoryTypeId="2" PostId="76" RevisionGUID="7d923163-3138-4eba-9910-fd9681c60547" CreationDate="2014-05-15T00:39:33.433" UserId="158" Text="*(Note: Pulled this question from the [list of questions in Area51](http://area51.stackexchange.com/proposals/55053/data-science/57398#57398), but believe the question is self explanatory. That said, believe I get the general intent of the question, and as a result likely able to field any questions on the question that might pop-up.)* &#xD;&#xA;&#xD;&#xA;**Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?**" />
  <row Id="191" PostHistoryTypeId="1" PostId="76" RevisionGUID="7d923163-3138-4eba-9910-fd9681c60547" CreationDate="2014-05-15T00:39:33.433" UserId="158" Text="Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?" />
  <row Id="192" PostHistoryTypeId="3" PostId="76" RevisionGUID="7d923163-3138-4eba-9910-fd9681c60547" CreationDate="2014-05-15T00:39:33.433" UserId="158" Text="&lt;bigdata&gt;" />
  <row Id="193" PostHistoryTypeId="2" PostId="77" RevisionGUID="2b33043e-0e40-47ad-b826-df1b08fd70d8" CreationDate="2014-05-15T01:22:35.167" UserId="158" Text="**Background:** Following is from the book [Graph Databases][1], which covers a performance test mentioned in the book [Neo4j in Action][2]:&#xD;&#xA;&#xD;&#xA;&gt; Relationships in a graph naturally form paths. Querying, or&#xD;&#xA;&gt; traversing, the graph involves following paths. Because of the&#xD;&#xA;&gt; fundamentally path-oriented nature of the datamodel, the majority of&#xD;&#xA;&gt; path-based graph database operations are highly aligned with the way&#xD;&#xA;&gt; in which the data is laid out, making them extremely efficient. In&#xD;&#xA;&gt; their book Neo4j in Action, Partner and Vukotic perform an experiment&#xD;&#xA;&gt; using a relational store and Neo4j.&#xD;&#xA;&gt; &#xD;&#xA;&gt; The comparison shows that the graph database is substantially quicker&#xD;&#xA;&gt; for connected data than a relational store.Partner and Vukotic’s&#xD;&#xA;&gt; experiment seeks to find friends-of-friends in a social network, to a&#xD;&#xA;&gt; maximum depth of five. Given any two persons chosen at random, is&#xD;&#xA;&gt; there a path that connects them which is at most five relationships&#xD;&#xA;&gt; long? For a social network containing 1,000,000 people, each with&#xD;&#xA;&gt; approximately 50 friends, the results strongly suggest that graph&#xD;&#xA;&gt; databases are the best choice for connected data, as we see in Table&#xD;&#xA;&gt; 2-1.&#xD;&#xA;&gt; &#xD;&#xA;Table 2-1. Finding extended friends in a relational database versus efficient finding in Neo4j&#xD;&#xA;&#xD;&#xA;&gt;     Depth   RDBMS Execution time (s)    Neo4j Execution time (s)     Records returned&#xD;&#xA;&gt;     2	      0.016                       0.01	                       ~2500	&#xD;&#xA;&gt;     3	      30.267                      0.168	                       ~110,000	&#xD;&#xA;&gt;     4	      1543.505	                  1.359	                       ~600,000	&#xD;&#xA;&gt;     5	      Unfinished	              2.132	                       ~800,000&#xD;&#xA;At depth two (friends-of-friends) both the relational database and the graph database perform well enough for us to consider using them in an online system. While the Neo4j query runs in two-thirds the time of the relational one, an end-user would barely notice the the difference in milliseconds between the two. By the time we reach depth three (friend-of-friend-of-friend), however, it’s clear that the relational database can no longer deal with the query in a reasonable timeframe: the thirty seconds it takes to complete would be completely unacceptable for an online system. In contrast, Neo4j’s response time remains relatively flat: just a fraction of a second to perform the query—definitely quick enough for an online system.&#xD;&#xA;&gt; &#xD;&#xA;&gt; At depth four the relational database exhibits crippling latency,&#xD;&#xA;&gt; making it practically useless for an online system. Neo4j’s timings&#xD;&#xA;&gt; have deteriorated a little too, but the latency here is at the&#xD;&#xA;&gt; periphery of being acceptable for a responsive online system. Finally,&#xD;&#xA;&gt; at depth five, the relational database simply takes too long to&#xD;&#xA;&gt; complete the query. Neo4j, in contrast, returns a result in around two&#xD;&#xA;&gt; seconds. At depth five, it transpires almost the entire network is our&#xD;&#xA;&gt; friend: for many real-world use cases, we’d likely trim the results,&#xD;&#xA;&gt; and the timings.&#xD;&#xA;&#xD;&#xA;**Questions are:** &#xD;&#xA;&#xD;&#xA; - Is this a reasonable test to emulate what one might except to find in a social network? *(Meaning do real social networks normally have nodes with approximately 50 friends for example; seems like the &quot;[rich get richer][3]&quot; model would be more natural for social networks, though might be wrong.)*&#xD;&#xA; - Regardless of the naturalness of the emulation, is there any reason to believe the results are off, or unreproducible? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.amazon.com/Graph-Databases-Ian-Robinson/dp/1449356265&#xD;&#xA;  [2]: http://www.amazon.com/Neo4j-Action-Jonas-Partner/dp/1617290769/&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/The_rich_get_richer_%28statistics%29" />
  <row Id="194" PostHistoryTypeId="1" PostId="77" RevisionGUID="2b33043e-0e40-47ad-b826-df1b08fd70d8" CreationDate="2014-05-15T01:22:35.167" UserId="158" Text="Is this Neo4j comparison to RDBMS execution time correct?" />
  <row Id="195" PostHistoryTypeId="3" PostId="77" RevisionGUID="2b33043e-0e40-47ad-b826-df1b08fd70d8" CreationDate="2014-05-15T01:22:35.167" UserId="158" Text="&lt;databases&gt;&lt;nosql&gt;" />
  <row Id="196" PostHistoryTypeId="7" PostId="69" RevisionGUID="037b9d5c-041b-4b57-8190-ddad79760f46" CreationDate="2014-05-15T01:43:13.557" UserId="158" Comment="Rollback to [a3106baa-327f-4430-b6c0-280ebdeeb0b1]" Text="Is it possible to automate generating reproducibility documentation?" />
  <row Id="197" PostHistoryTypeId="2" PostId="78" RevisionGUID="ab847ef5-c38c-48de-94f6-1bb15f515356" CreationDate="2014-05-15T01:46:28.467" UserId="178" Text="There is no replacement in the strict sense of the word.  Instead you should look at other measures.&#xD;&#xA;&#xD;&#xA;The other measures you look at depend on what you type of problem you are solving.  In general, if you have a small p-value, also consider the magnitude of the effect size.  It may be highly statistically significant but in practice meaningless.  It is also helpful to report the confidence interval of the effect size.&#xD;&#xA;&#xD;&#xA;I would consider [this paper](http://galitshmueli.com/system/files/Print%20Version.pdf) as mentoned in DanC's answer to [this question](http://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive)." />
  <row Id="198" PostHistoryTypeId="5" PostId="69" RevisionGUID="63d6e665-76ae-4475-b731-6c89c2c9889b" CreationDate="2014-05-15T02:02:08.010" UserId="158" Comment="added 900 characters in body" Text="First, think it's worth me stating what I mean by replication &amp; reproducibility:&#xD;&#xA;&#xD;&#xA;- Replication of analysis A results in an exact copy of all inputs and processes that are supply and result in incidental outputs in analysis B.&#xD;&#xA;- Reproducibility of analysis A results in inputs, processes, and outputs that are semantically incidental to analysis A, without access to the exact inputs and processes.&#xD;&#xA;&#xD;&#xA;Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it's planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility.&#xD;&#xA;&#xD;&#xA;The closet comparison I'm able to think of is [documentation generators][1] that generates software documentation intended for programmers - though the main difference I see is that in theory, if two sets of analysis ran the &quot;reproducibility documentation generators&quot; the documentation should match.&#xD;&#xA;&#xD;&#xA;Another issue, is that while I get the concept of reproducibility documentation, I am having a hard time imagining what it would look like in usable form without just being a guide to replicating the analysis.&#xD;&#xA;&#xD;&#xA;Lastly, whole intent of this is to understand if it's possible to &quot;bake-in&quot; reproducibility documentation as you build out a stack, not after the stack is built.&#xD;&#xA;&#xD;&#xA;So, Is it possible to automate generating reproducibility documentation, and if so how, and what would it look like?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;***UPDATE:** Please note that this is the second draft of this question and that [Christopher Louden][2] was kind enough to let me edit the question after I realized it was likely the first draft was unclear. Thanks!*&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Documentation_generator&#xD;&#xA;  [2]: http://datascience.stackexchange.com/users/178/christopher-louden" />
  <row Id="200" PostHistoryTypeId="4" PostId="16" RevisionGUID="c09c6eba-b8ad-452a-bce4-0efaf3f1c8d0" CreationDate="2014-05-15T02:41:27.063" UserId="63" Comment="edited title" Text="Use liblinear on big data for semantic analysis" />
  <row Id="201" PostHistoryTypeId="2" PostId="79" RevisionGUID="5dfc7d33-d8a0-4c02-afd0-33a6254c96d5" CreationDate="2014-05-15T03:19:40.360" UserId="-1" Text="" />
  <row Id="202" PostHistoryTypeId="2" PostId="80" RevisionGUID="2c2e2962-b018-4361-a69d-c4453d4c18a6" CreationDate="2014-05-15T03:19:40.360" UserId="-1" Text="" />
  <row Id="203" PostHistoryTypeId="2" PostId="81" RevisionGUID="b78b5118-181c-44a3-8643-ef88723e8712" CreationDate="2014-05-15T04:59:54.317" UserId="84" Text="What is(are) the difference(s) between parallel and distributed computing? When it comes to scalability and efficiency, it is very common to see solutions dealing with computations in clusters of machines, and sometimes it is referred to as a parallel processing, or as distributed processing.&#xD;&#xA;&#xD;&#xA;In a certain way, the computation seems to be always parallel, since there are things running concurrently. But is the distributed computation simply related to the use of more than one machine, or are there any further specificities that distinguishes these two kinds of processing? Wouldn't it be redundant to say, for example, that a computation is *parallel AND distributed*?" />
  <row Id="204" PostHistoryTypeId="1" PostId="81" RevisionGUID="b78b5118-181c-44a3-8643-ef88723e8712" CreationDate="2014-05-15T04:59:54.317" UserId="84" Text="Parallel and distributed computing" />
  <row Id="205" PostHistoryTypeId="3" PostId="81" RevisionGUID="b78b5118-181c-44a3-8643-ef88723e8712" CreationDate="2014-05-15T04:59:54.317" UserId="84" Text="&lt;bigdata&gt;&lt;scalability&gt;&lt;efficiency&gt;&lt;parallelism&gt;&lt;distributed&gt;" />
  <row Id="206" PostHistoryTypeId="2" PostId="82" RevisionGUID="c91ddd3c-baef-4666-aefc-50a3b3905fa2" CreationDate="2014-05-15T05:19:34.757" UserId="172" Text="Simply set, 'parallel' means running concurrently on distinct resources (CPUs), while 'distributed' means running accross distinct computers, involving issues related to networks.&#xD;&#xA;&#xD;&#xA;Parallel computing using for instance [OpenMP][1] is not distributed, while parallel computing with [Message Passing][2] is often distributed.&#xD;&#xA;&#xD;&#xA;Being in a 'distributed but not parallel' setting would mean under-using resources so it is seldom encoutered.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/OpenMP&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Message_Passing_Interface" />
  <row Id="207" PostHistoryTypeId="5" PostId="82" RevisionGUID="bd188207-62ec-4231-8217-c7457e73ad4d" CreationDate="2014-05-15T05:25:39.970" UserId="172" Comment="added 32 characters in body" Text="Simply set, 'parallel' means running concurrently on distinct resources (CPUs), while 'distributed' means running across distinct computers, involving issues related to networks.&#xD;&#xA;&#xD;&#xA;Parallel computing using for instance [OpenMP][1] is not distributed, while parallel computing with [Message Passing][2] is often distributed.&#xD;&#xA;&#xD;&#xA;Being in a 'distributed but not parallel' setting would mean under-using resources so it is seldom encountered but it is conceptually possible.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/OpenMP&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Message_Passing_Interface" />
  <row Id="209" PostHistoryTypeId="5" PostId="31" RevisionGUID="2f7af5bc-6e56-47a6-acc3-3563efb7c185" CreationDate="2014-05-15T05:49:39.140" UserId="24" Comment="Just tightening things up a bit to improve readability" Text="I have a bunch of customer profiles stored in a [tag:elasticsearch] cluster. These profiles are now used for creation of target groups for our email subscriptions. &#xD;&#xA;&#xD;&#xA;Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).&#xD;&#xA;&#xD;&#xA;How could I search for interesting groups **automatically** - using data science, machine learning, clustering or something else?&#xD;&#xA;&#xD;&#xA;[tag:R] programming language seems to be a good tool for this task, but I can't form a methodology of such group search. One solution is to somehow find the largest clusters of customers and use them as target groups, so the question is:&#xD;&#xA;&#xD;&#xA;**How can I automatically choose largest clusters of similar customers (similar by parameters that I don't know at this moment)?**&#xD;&#xA;&#xD;&#xA;For example: my program will connect to elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.&#xD;&#xA;" />
  <row Id="210" PostHistoryTypeId="24" PostId="31" RevisionGUID="2f7af5bc-6e56-47a6-acc3-3563efb7c185" CreationDate="2014-05-15T05:49:39.140" Comment="Proposed by 24 approved by 118 edit id of 12" />
  <row Id="211" PostHistoryTypeId="2" PostId="83" RevisionGUID="9b451f8f-1c75-4ceb-beba-199b5ecb55c3" CreationDate="2014-05-15T07:47:44.710" UserId="26" Text="I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767&#xD;&#xA;&#xD;&#xA;Summary:&#xD;&#xA;&#xD;&#xA;- for small stuff, go with whatever tools you are familiar with&#xD;&#xA;&#xD;&#xA;- a few gigabytes is definitely small stuff: it doesn't get big until it is too big to fit in a single [MySQL Cluster][1] with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second.&#xD;&#xA;&#xD;&#xA;- if you're stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware.&#xD;&#xA;&#xD;&#xA;- once you have data larger than a few TB, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL.&#xD;&#xA;&#xD;&#xA;- Cassandra :)&#xD;&#xA;&#xD;&#xA;  [1]: http://www.mysql.com/products/cluster/scalability.html" />
  <row Id="212" PostHistoryTypeId="5" PostId="83" RevisionGUID="a7bc5315-02ff-4cab-b382-bd2921d1053f" CreationDate="2014-05-15T07:53:40.170" UserId="26" Comment="added 147 characters in body" Text="I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database, here:  &#xD;&#xA;&#xD;&#xA;**[Motivations for using relational database / ORM or document database / ODM][1]**&#xD;&#xA;&#xD;&#xA;Summary:&#xD;&#xA;&#xD;&#xA;- for small stuff, go with whatever tools you are familiar with&#xD;&#xA;&#xD;&#xA;- a few gigabytes is definitely small stuff: it doesn't get big until it is too big to fit in a single [MySQL Cluster][2] with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second (or a more conventional hard-drive-based database with up to 100's of TB data and a few thousand transactions per second).&#xD;&#xA;&#xD;&#xA;- if you're stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware.&#xD;&#xA;&#xD;&#xA;- once you have data larger than a few TB and faster than thousands of transactions per second, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL.&#xD;&#xA;&#xD;&#xA;- [Cassandra][3] :)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767&#xD;&#xA;  [2]: http://www.mysql.com/products/cluster/scalability.html&#xD;&#xA;  [3]: http://cassandra.apache.org/" />
  <row Id="213" PostHistoryTypeId="5" PostId="83" RevisionGUID="d7deb5de-5a20-4dd9-a23b-245049feb89a" CreationDate="2014-05-15T07:59:05.497" UserId="26" Comment="added 2 characters in body" Text="I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database, here:  &#xD;&#xA;&#xD;&#xA;**[Motivations for using relational database / ORM or document database / ODM][1]**&#xD;&#xA;&#xD;&#xA;Summary:&#xD;&#xA;&#xD;&#xA;- for small stuff, go with whatever tools you are familiar with&#xD;&#xA;&#xD;&#xA;- a few gigabytes is definitely small stuff: it doesn't get big until it is too big to fit in a single [MySQL Cluster][2] with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second (or a more conventional hard-drive-based database with up to 100's of TB data and a few thousand transactions per second).&#xD;&#xA;&#xD;&#xA;- if you're stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware.&#xD;&#xA;&#xD;&#xA;- once you have data larger than a few TB *and* faster than thousands of transactions per second, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL.&#xD;&#xA;&#xD;&#xA;- [Cassandra][3] :)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767&#xD;&#xA;  [2]: http://www.mysql.com/products/cluster/scalability.html&#xD;&#xA;  [3]: http://cassandra.apache.org/" />
  <row Id="214" PostHistoryTypeId="2" PostId="84" RevisionGUID="05ca6d96-2075-4781-a0de-6a2a2fd6d085" CreationDate="2014-05-15T08:19:40.577" UserId="26" Text="You are asking about [Data Dredging][1], which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggested by the same data.  &#xD;&#xA;&#xD;&#xA;In particular, check out [Multiple hypothesis hazard][2], and [Testing hypotheses suggested by the data][3].&#xD;&#xA;&#xD;&#xA;The solution is to use some kind of correction for [False discovery rate][4] or [Familywise error rate][5], such as [Scheffé's method][6] or the (very old-school) [Bonferroni correction][7].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Data_dredging&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Multiple_comparisons&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/False_discovery_rate&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/Familywise_error_rate&#xD;&#xA;  [6]: http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method&#xD;&#xA;  [7]: http://en.wikipedia.org/wiki/Bonferroni_correction" />
  <row Id="215" PostHistoryTypeId="5" PostId="84" RevisionGUID="2279680a-d93e-489a-8ceb-3b7f9a5a8e18" CreationDate="2014-05-15T08:25:47.933" UserId="26" Comment="added 444 characters in body" Text="You are asking about [Data Dredging][1], which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggested by the same data.  &#xD;&#xA;&#xD;&#xA;In particular, check out [Multiple hypothesis hazard][2], and [Testing hypotheses suggested by the data][3].&#xD;&#xA;&#xD;&#xA;The solution is to use some kind of correction for [False discovery rate][4] or [Familywise error rate][5], such as [Scheffé's method][6] or the (very old-school) [Bonferroni correction][7].&#xD;&#xA;&#xD;&#xA;In a somewhat less rigorous way, it may help to filter your discoveries by the confidence interval for the odds ratio (OR) for each statistical result.  If the 99% confidence interval for the odds ratio is 10-12, then the OR is &lt;= 1 with some *extremely* small probability, especially if the sample size is also large.  If you find something like this, it is probably a strong effect even if it came out of a test of millions of hypotheses.  &#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Data_dredging&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Multiple_comparisons&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/False_discovery_rate&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/Familywise_error_rate&#xD;&#xA;  [6]: http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method&#xD;&#xA;  [7]: http://en.wikipedia.org/wiki/Bonferroni_correction" />
  <row Id="216" PostHistoryTypeId="2" PostId="85" RevisionGUID="2bae8f49-b645-4604-a778-3b22a96bf126" CreationDate="2014-05-15T08:44:47.327" UserId="26" Text="See http://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive/84#84&#xD;&#xA;&#xD;&#xA;The replacement is to use any of the corrections for [False discovery rate][4] (to limit probability that any given discovery is false) or [Familywise error rate][5] (to limit probability of one or more false discoveries).  For example, you might use the Holm–Bonferroni method.&#xD;&#xA;&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/False_discovery_rate&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/Familywise_error_rate" />
  <row Id="219" PostHistoryTypeId="2" PostId="86" RevisionGUID="c95e75b7-866f-493a-99ed-183c6aa16afb" CreationDate="2014-05-15T09:04:09.710" UserId="116" Text="Given website access data in the form `session_id, ip, user_agent`, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?&#xD;&#xA;&#xD;&#xA;`session_id`: is an id given to every new visitor. It does not expire, however if the user doesn't accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymore&#xD;&#xA;&#xD;&#xA;`IP` can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.&#xD;&#xA;&#xD;&#xA;`User_agent` is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.&#xD;&#xA;&#xD;&#xA;Data might look as the fiddle here:&#xD;&#xA;http://sqlfiddle.com/#!2/c4de40/1&#xD;&#xA;&#xD;&#xA;Of course, we are talking about assumptions, but it's about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it's the same user, with some edge case exceptions.&#xD;&#xA;&#xD;&#xA;Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:&#xD;&#xA;&#xD;&#xA;    select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_id&#xD;&#xA;    from &#xD;&#xA;    	(select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr&#xD;&#xA;    	from&#xD;&#xA;    		(select 1 as nr union all select 2 union all select 3	union all select 4 union all select 5&#xD;&#xA;    		union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a&#xD;&#xA;    	join&#xD;&#xA;    		(select 1 as nr union all select 2 union all select 3	union all select 4 union all select 5&#xD;&#xA;    		union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b&#xD;&#xA;    		order by 1&#xD;&#xA;    	)d&#xD;&#xA;    inner join&#xD;&#xA;    	(select 1 as nr union all select 2 union all select 3	union all select 4 union all select 5&#xD;&#xA;    	union all select 6 union all select 7 union all select 8 union all select 9 )e&#xD;&#xA;    	on d.nr&gt;=e.nr" />
  <row Id="220" PostHistoryTypeId="1" PostId="86" RevisionGUID="c95e75b7-866f-493a-99ed-183c6aa16afb" CreationDate="2014-05-15T09:04:09.710" UserId="116" Text="Clustering unique visitors by useragent, ip, session_id" />
  <row Id="221" PostHistoryTypeId="3" PostId="86" RevisionGUID="c95e75b7-866f-493a-99ed-183c6aa16afb" CreationDate="2014-05-15T09:04:09.710" UserId="116" Text="&lt;clustering&gt;&lt;log&gt;" />
  <row Id="222" PostHistoryTypeId="2" PostId="87" RevisionGUID="6b491179-5055-4612-a407-0d2bc4c24d1f" CreationDate="2014-05-15T09:30:36.460" UserId="108" Text="Looking at this document called [Anatomy of Facebook][1] I note that the median is 100. Looking at the cumulative function plot I can bet that the average is higher, near 200. So 50 seems to not be the best number here. However I think that this is not the main issue here. &#xD;&#xA;&#xD;&#xA;The main issue is the lack of information on how the database was used.&#xD;&#xA;&#xD;&#xA;It seems reasonable that a data storage designed specially for graph structures to be more efficient than traditional RDBMs. However, even if the RDBMs are not in the latest trends as a data storage of choice, these systems evolved continuously in a race with the data set dimensions. There are various types of possible designs, various ways of indexing data, improvements related with concurrency and so on. &#xD;&#xA;&#xD;&#xA;To conclude I think that regarding reproducibility, the study lack a proper description of how the database schema was designed. I do not expect that a database to dominate on such king of interrogations, however I would expect that with a well-tuned design the differences to not be such massive.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.facebook.com/notes/facebook-data-team/anatomy-of-facebook/10150388519243859" />
  <row Id="223" PostHistoryTypeId="6" PostId="81" RevisionGUID="33469ce8-85a3-473b-8e05-b629e511ba29" CreationDate="2014-05-15T09:31:51.370" UserId="118" Comment="retagged question" Text="&lt;definitions&gt;&lt;parallelism&gt;&lt;distributed&gt;" />
  <row Id="224" PostHistoryTypeId="24" PostId="81" RevisionGUID="33469ce8-85a3-473b-8e05-b629e511ba29" CreationDate="2014-05-15T09:31:51.370" Comment="Proposed by 118 approved by 84 edit id of 15" />
  <row Id="226" PostHistoryTypeId="5" PostId="86" RevisionGUID="88a7b1c1-97be-4f80-adb3-ab624bf989de" CreationDate="2014-05-15T10:06:06.393" UserId="116" Comment="added 132 characters in body" Text="Given website access data in the form `session_id, ip, user_agent`, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?&#xD;&#xA;&#xD;&#xA;`session_id`: is an id given to every new visitor. It does not expire, however if the user doesn't accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymore&#xD;&#xA;&#xD;&#xA;`IP` can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.&#xD;&#xA;&#xD;&#xA;`User_agent` is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.&#xD;&#xA;&#xD;&#xA;Data might look as the fiddle here:&#xD;&#xA;http://sqlfiddle.com/#!2/c4de40/1&#xD;&#xA;&#xD;&#xA;Of course, we are talking about assumptions, but it's about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it's the same user, with some edge case exceptions.&#xD;&#xA;&#xD;&#xA;Edit: Language in which the problem is solved is irellevant, it's mostly about logic and not implementation. Pseudocode is fine.&#xD;&#xA;&#xD;&#xA;Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:&#xD;&#xA;&#xD;&#xA;    select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_id&#xD;&#xA;    from &#xD;&#xA;    	(select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr&#xD;&#xA;    	from&#xD;&#xA;    		(select 1 as nr union all select 2 union all select 3	union all select 4 union all select 5&#xD;&#xA;    		union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a&#xD;&#xA;    	join&#xD;&#xA;    		(select 1 as nr union all select 2 union all select 3	union all select 4 union all select 5&#xD;&#xA;    		union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b&#xD;&#xA;    		order by 1&#xD;&#xA;    	)d&#xD;&#xA;    inner join&#xD;&#xA;    	(select 1 as nr union all select 2 union all select 3	union all select 4 union all select 5&#xD;&#xA;    	union all select 6 union all select 7 union all select 8 union all select 9 )e&#xD;&#xA;    	on d.nr&gt;=e.nr" />
  <row Id="228" PostHistoryTypeId="2" PostId="89" RevisionGUID="8173f9be-7747-4ace-bed6-500f3203cdd1" CreationDate="2014-05-15T11:22:27.293" UserId="189" Text="For example, when searching something in Google, results return nigh-instantly.&#xD;&#xA;&#xD;&#xA;I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?&#xD;&#xA;&#xD;&#xA;Moreover, wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.&#xD;&#xA;&#xD;&#xA;Does MapReduce help solve this problem?" />
  <row Id="229" PostHistoryTypeId="1" PostId="89" RevisionGUID="8173f9be-7747-4ace-bed6-500f3203cdd1" CreationDate="2014-05-15T11:22:27.293" UserId="189" Text="How do querys into huge database return with negligible latency?" />
  <row Id="230" PostHistoryTypeId="3" PostId="89" RevisionGUID="8173f9be-7747-4ace-bed6-500f3203cdd1" CreationDate="2014-05-15T11:22:27.293" UserId="189" Text="&lt;bigdata&gt;" />
  <row Id="231" PostHistoryTypeId="5" PostId="89" RevisionGUID="f4a83e57-6dc1-46d3-96f0-a609feb4877d" CreationDate="2014-05-15T11:28:39.880" UserId="189" Comment="added 10 characters in body" Text="For example, when searching something in Google, results return nigh-instantly.&#xD;&#xA;&#xD;&#xA;I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?&#xD;&#xA;&#xD;&#xA;Moreover, wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.&#xD;&#xA;&#xD;&#xA;Does MapReduce help solve this problem?" />
  <row Id="232" PostHistoryTypeId="2" PostId="90" RevisionGUID="f8c762bb-e80e-4585-b914-fa553c08b02d" CreationDate="2014-05-15T11:46:38.170" UserId="156" Text="The terms &quot;parallel computing&quot; and &quot;distributed computing&quot; certainly have a large overlap, but can be differentiated further. Actually, you already did this in your question, by later asking about &quot;parallel processing&quot; and &quot;distributed processing&quot;. &#xD;&#xA;&#xD;&#xA;One could consider &quot;distributed computing&quot; as the more general term that involves &quot;distributed processing&quot; as well as, for example, &quot;distributed storage&quot;. The common term, &quot;distributed&quot;, usually refers to some sort of [Message Passing][1] over a network, between machines that are physically separated.&#xD;&#xA;&#xD;&#xA;The term &quot;parallel computing&quot; is also in the process of being further defined, e.g. by explicitly differentiating between the terms &quot;parallel&quot; and &quot;concurrent&quot;, where - roughly - the first one refers [data parallelism][2] and the latter to [task parallelism][3], although there are hardly really strict and binding defintions. &#xD;&#xA;&#xD;&#xA;So one could say that &#xD;&#xA;&#xD;&#xA; - &quot;distributed processing&quot; usually (although not necessarily) means that it also is &quot;parallel processing&quot;&#xD;&#xA; - &quot;distributed computing&quot; is more general, and also covers aspects that are not related to parallelism&#xD;&#xA; - and obviously, &quot;parallel computing&quot;/&quot;parallel processing&quot; does not imply that it is &quot;distributed&quot;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Message_Passing_Interface&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Data_parallelism&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Task_parallelism" />
  <row Id="233" PostHistoryTypeId="2" PostId="91" RevisionGUID="2eaa478e-591b-4921-bb61-ea93d6da9415" CreationDate="2014-05-15T11:56:43.607" UserId="84" Text="Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:&#xD;&#xA;&#xD;&#xA;1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);&#xD;&#xA;2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;&#xD;&#xA;3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.&#xD;&#xA;&#xD;&#xA;Considering that, lets try to address your questions:&#xD;&#xA;&#xD;&#xA;&gt; but I imagine it infeasible for the results of every single possible query to be indexed&#xD;&#xA;&#xD;&#xA;Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number ofqueries from these `n -&gt; inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant for the user queries are the ones cached.&#xD;&#xA;&#xD;&#xA;&gt; wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs&#xD;&#xA;&#xD;&#xA;Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?&#xD;&#xA;&#xD;&#xA;The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.&#xD;&#xA;&#xD;&#xA;Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it's not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times." />
  <row Id="234" PostHistoryTypeId="5" PostId="91" RevisionGUID="01a99e57-3c1d-43cb-9500-08927a563520" CreationDate="2014-05-15T12:02:22.820" UserId="84" Comment="added 1 character in body" Text="Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:&#xD;&#xA;&#xD;&#xA;1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);&#xD;&#xA;2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;&#xD;&#xA;3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.&#xD;&#xA;&#xD;&#xA;Considering that, lets try to address your questions:&#xD;&#xA;&#xD;&#xA;&gt; but I imagine it infeasible for the results of every single possible query to be indexed&#xD;&#xA;&#xD;&#xA;Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these `n -&gt; inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.&#xD;&#xA;&#xD;&#xA;&gt; wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs&#xD;&#xA;&#xD;&#xA;Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?&#xD;&#xA;&#xD;&#xA;The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](http://i.stack.imgur.com/Uf6al.gif). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.&#xD;&#xA;&#xD;&#xA;Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it's not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/Uf6al.gif" />
  <row Id="235" PostHistoryTypeId="5" PostId="91" RevisionGUID="3198603d-6c02-41a0-95e1-085468778bf5" CreationDate="2014-05-15T12:25:07.730" UserId="84" Comment="added 526 characters in body" Text="Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:&#xD;&#xA;&#xD;&#xA;1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);&#xD;&#xA;2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;&#xD;&#xA;3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.&#xD;&#xA;&#xD;&#xA;Considering that, lets try to address your questions:&#xD;&#xA;&#xD;&#xA;&gt; but I imagine it infeasible for the results of every single possible query to be indexed&#xD;&#xA;&#xD;&#xA;Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these `n -&gt; inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.&#xD;&#xA;&#xD;&#xA;&gt; wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs&#xD;&#xA;&#xD;&#xA;Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?&#xD;&#xA;&#xD;&#xA;The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](http://i.stack.imgur.com/Uf6al.gif). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.&#xD;&#xA;&#xD;&#xA;Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it's not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.&#xD;&#xA;&#xD;&#xA;&gt; Does MapReduce help solve this problem?&#xD;&#xA;&#xD;&#xA;Although I don't think that using or not MapReduce is restricted information inside Google, I'm not conversant about this point. However, Google's implementation of MapReduce (which is surely *not* Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/Uf6al.gif" />
  <row Id="236" PostHistoryTypeId="6" PostId="77" RevisionGUID="b12d3a4f-33fc-4888-9bee-c94c051c1c46" CreationDate="2014-05-15T13:15:02.727" UserId="118" Comment="added neo4j tag" Text="&lt;databases&gt;&lt;nosql&gt;&lt;neo4j&gt;" />
  <row Id="237" PostHistoryTypeId="24" PostId="77" RevisionGUID="b12d3a4f-33fc-4888-9bee-c94c051c1c46" CreationDate="2014-05-15T13:15:02.727" Comment="Proposed by 118 approved by 158 edit id of 16" />
  <row Id="238" PostHistoryTypeId="2" PostId="92" RevisionGUID="c3ca33ad-faca-4c65-b977-056dd912bbc7" CreationDate="2014-05-15T13:18:38.693" UserId="21" Text="MapReduce has nothing to do with real-time anything. It is a batch-oriented processing framework suitable for some offline tasks, like ETL and index building. Google has moved off of MapReduce for most jobs now, and even the Hadoop ecosystem is doing the same.&#xD;&#xA;&#xD;&#xA;The answer to low latency is generally to keep precomputed indices in memory. Anything that touches disk is hard to make fast and scale. This is how newer-generation Hadoop-based SQL engines like [Impala][1] get so much speed compared to MapReduce-based infrastructure like [Hive][2], for example.&#xD;&#xA;&#xD;&#xA;Search infrastructure can't cache the results of every single query. But it sure can cache intermediate results, or, more complete results for top queries. With a little caching you can serve results for a significant minority of all queries.&#xD;&#xA;&#xD;&#xA;Search is also split across servers. So one machine can delegate to 100 to each get a part of the result and then combine them.&#xD;&#xA;&#xD;&#xA;You can also get away with some degree of approximation. Google does not literally form a thousand pages of search results; it just has to get the first page about right.&#xD;&#xA;&#xD;&#xA;Keep in mind that Google has _millions_ of computers around the globe. Your queries are going to a data center geographically near to you and that is only serving your geography. This cuts out most of the latency, which is network and not processing time in the data center.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html&#xD;&#xA;  [2]: http://hive.apache.org/" />
  <row Id="239" PostHistoryTypeId="2" PostId="93" RevisionGUID="da57273c-3a38-4957-bfb8-4cbe642d1228" CreationDate="2014-05-15T13:30:04.270" UserId="21" Text="There's not much you can do with just this data, but what little you can do does not rely on machine learning. &#xD;&#xA;&#xD;&#xA;Yes, sessions from the same IP but different User-Agents are almost certainly distinct users. Sessions with the same IP and User-Agent are usually the same user, except in the case of proxies / wi-fi access points. Those you might identify by looking at the distribution of session count per IP to identify likely 'aggregate' IPs. Sessions from the same IP / User-Agent that overlap in time are almost surely distinct.&#xD;&#xA;&#xD;&#xA;To further distinguish users you would need more info. For example, the sites or IP addresses that the user is connecting to would be a very strong basis for differentiating sessions. Then you could get into more sophisticated learning to figure out when sessions are the same or different users." />
  <row Id="240" PostHistoryTypeId="2" PostId="94" RevisionGUID="d9e7cca8-3ee4-4216-a962-52e7c0f51f09" CreationDate="2014-05-15T14:41:24.020" UserId="84" Text="While building a rank, say for a search engine, or a recommendation system, is it valid to rely on click frequency to determine the relevance of an entry?" />
  <row Id="241" PostHistoryTypeId="1" PostId="94" RevisionGUID="d9e7cca8-3ee4-4216-a962-52e7c0f51f09" CreationDate="2014-05-15T14:41:24.020" UserId="84" Text="Does click frequency account for relevance?" />
  <row Id="242" PostHistoryTypeId="3" PostId="94" RevisionGUID="d9e7cca8-3ee4-4216-a962-52e7c0f51f09" CreationDate="2014-05-15T14:41:24.020" UserId="84" Text="&lt;recommendation&gt;&lt;feedback&gt;&lt;relevance&gt;&lt;information-retrieval&gt;" />
  <row Id="243" PostHistoryTypeId="2" PostId="95" RevisionGUID="140403b0-4119-40f9-b5c7-576c4cc0e61a" CreationDate="2014-05-15T15:06:24.600" UserId="9" Text="Is it valid to *use* click frequency, then **yes**. Is it valid to use **only** the click frequency, then probably **no**.&#xD;&#xA;&#xD;&#xA;Search relevance is much more complicated than just one metric. [There are entire books on the subject][1]. Extending this answer beyond a simple yes/no would likely make the answer far too broad (and opinionated)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.amazon.ca/s/ref=nb_sb_noss?url=search-alias=aps&amp;field-keywords=search%20ranking" />
  <row Id="244" PostHistoryTypeId="2" PostId="96" RevisionGUID="fce161d6-601a-47d8-bfd4-52b1332add4c" CreationDate="2014-05-15T15:10:30.243" UserId="115" Text="For my part I can say that I use click frequency on i.e. eCommerce products. When you combine it with the days of the year it can even bring you great suggestions.&#xD;&#xA;&#xD;&#xA;i.e.: We have historical data from 1 year over 2 products (Snowboots[], Sandalettes[])&#xD;&#xA;&#xD;&#xA;    [Snowboots[1024,1253,652,123,50,12,8,4,50,148,345,896]]&#xD;&#xA;    [Sandalettes[23,50,73,100,534,701,1053,1503,1125,453,213,25]]&#xD;&#xA;    &#xD;&#xA;where [0] = January&#xD;&#xA;&#xD;&#xA;As you can see, snowboots are much more searched in January as sandalettes, so you should suggest snowboots to someone searching shoes on your site or /we on january.&#xD;&#xA;&#xD;&#xA;You can also see if something is &quot;fresh&quot; at this time, like when people often click a unknown product it could be an insight for a new comming trend or something.&#xD;&#xA;&#xD;&#xA;That are just some samples where you could use click frequency as a insight. I think there are **no rules** for what you can use or not in recommendations, **as long as it make sense**.&#xD;&#xA;" />
  <row Id="245" PostHistoryTypeId="2" PostId="97" RevisionGUID="f0467fe8-7595-4552-b0da-a1425c38161c" CreationDate="2014-05-15T17:14:36.817" UserId="158" Text="Users normally only view the first set of links, which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using click is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. &#xD;&#xA;&#xD;&#xA;That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see &quot;[YouTube's head of engineering speaking about clicks vs engagement][3]&quot; for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.&#xD;&#xA;&#xD;&#xA;Might be worth noting that historically Google was known for [PageRank algorithm][1] though it's possible your intent is only to review click-streams, so I won't delve [Google ranking factors][2]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/PageRank&#xD;&#xA;  [2]: https://www.google.com/search?q=google%20ranking%20factors&#xD;&#xA;  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8" />
  <row Id="246" PostHistoryTypeId="5" PostId="97" RevisionGUID="8a4582e5-f2ea-434d-807f-fbd741890d10" CreationDate="2014-05-15T17:29:37.650" UserId="158" Comment="added 213 characters in body" Text="[Depends on the users intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters. &#xD;&#xA;&#xD;&#xA;[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here's a [click and attention distribution heat-map][2] for  Google search results:&#xD;&#xA;&#xD;&#xA;![Google SEPR Click and Attention distribution ‘heat-map’][1]&#xD;&#xA;&#xD;&#xA;CAPTION: .&#xD;&#xA;&#xD;&#xA;Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using click is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. &#xD;&#xA;&#xD;&#xA;That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see &quot;[YouTube's head of engineering speaking about clicks vs engagement][3]&quot; for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.&#xD;&#xA;&#xD;&#xA;Might be worth noting that historically Google was known for [PageRank algorithm][4] though it's possible your intent is only to review click-streams, so I won't delve [Google ranking factors][5]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/8kO5S.jpg&#xD;&#xA;  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm&#xD;&#xA;  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/PageRank&#xD;&#xA;  [5]: https://www.google.com/search?q=google%20ranking%20factors" />
  <row Id="247" PostHistoryTypeId="5" PostId="97" RevisionGUID="a29e97eb-5c5f-4a7d-b50d-5accb8dad367" CreationDate="2014-05-15T17:36:45.363" UserId="158" Comment="deleted 14 characters in body" Text="[Depends on the users intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters. &#xD;&#xA;&#xD;&#xA;[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here's a [click and attention distribution heat-map][2] for  Google search results:&#xD;&#xA;&#xD;&#xA;![Google SEPR Click and Attention distribution ‘heat-map’][1]&#xD;&#xA;&#xD;&#xA;Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using click is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. &#xD;&#xA;&#xD;&#xA;That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see &quot;[YouTube's head of engineering speaking about clicks vs engagement][3]&quot; for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.&#xD;&#xA;&#xD;&#xA;Might be worth noting that historically Google was known for [PageRank algorithm][4] though it's possible your intent is only to review click-streams, so I won't delve [Google ranking factors][5]. Also, you might find a review of [Google's Search Quality Rating Guidelines](http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf) of use.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/8kO5S.jpg&#xD;&#xA;  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm&#xD;&#xA;  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/PageRank&#xD;&#xA;  [5]: https://www.google.com/search?q=google%20ranking%20factors" />
  <row Id="248" PostHistoryTypeId="5" PostId="97" RevisionGUID="1357d1be-9193-45dd-8746-aa6f24990710" CreationDate="2014-05-15T17:51:20.640" UserId="158" Comment="added 1 character in body" Text="[Depends on the users intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters. &#xD;&#xA;&#xD;&#xA;[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here's a [click and attention distribution heat-map][2] for  Google search results:&#xD;&#xA;&#xD;&#xA;![Google SEPR Click and Attention distribution ‘heat-map’][1]&#xD;&#xA;&#xD;&#xA;Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. &#xD;&#xA;&#xD;&#xA;That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see &quot;[YouTube's head of engineering speaking about clicks vs engagement][3]&quot; for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.&#xD;&#xA;&#xD;&#xA;Might be worth noting that historically Google was known for [PageRank algorithm][4] though it's possible your intent is only to review click-streams, so I won't delve [Google ranking factors][5]; if you are interested in the Google's approach, you might find a review of [Google's Search Quality Rating Guidelines](http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf) of use.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/8kO5S.jpg&#xD;&#xA;  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm&#xD;&#xA;  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/PageRank&#xD;&#xA;  [5]: https://www.google.com/search?q=google%20ranking%20factors" />
  <row Id="249" PostHistoryTypeId="5" PostId="97" RevisionGUID="7c00682e-e5a6-4ec6-804c-709f6ecebb00" CreationDate="2014-05-15T18:06:21.103" UserId="158" Comment="added 1 character in body" Text="[Depends on the user's intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters. &#xD;&#xA;&#xD;&#xA;[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here's a [click and attention distribution heat-map][2] for  Google search results:&#xD;&#xA;&#xD;&#xA;![Google SEPR Click and Attention distribution ‘heat-map’][1]&#xD;&#xA;&#xD;&#xA;Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. &#xD;&#xA;&#xD;&#xA;That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see &quot;[YouTube's head of engineering speaking about clicks vs engagement][3]&quot; for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.&#xD;&#xA;&#xD;&#xA;Might be worth noting that historically Google was known for [PageRank algorithm][4] though it's possible your intent is only to review click-streams, so I won't delve [Google ranking factors][5]; if you are interested in the Google's approach, you might find a review of [Google's Search Quality Rating Guidelines](http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf) of use.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/8kO5S.jpg&#xD;&#xA;  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm&#xD;&#xA;  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/PageRank&#xD;&#xA;  [5]: https://www.google.com/search?q=google%20ranking%20factors" />
  <row Id="259" PostHistoryTypeId="5" PostId="85" RevisionGUID="ea971f59-cae5-4d11-b72a-609bfa4e2b0e" CreationDate="2014-05-15T20:32:26.923" UserId="26" Comment="added 1302 characters in body" Text="See also http://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive/84#84&#xD;&#xA;&#xD;&#xA;When there are a lot of variables that can be tested for pair-wise correlation (for example), the replacement is to use any of the corrections for [False discovery rate][4] (to limit probability that any given discovery is false) or [Familywise error rate][5] (to limit probability of one or more false discoveries).  For example, you might use the Holm–Bonferroni method.&#xD;&#xA;&#xD;&#xA;In the case of a large sample rather than a lot of variables, something else is needed.  As Christopher said, magnitude of effect a way to treat this.  Combining these two ideas, you might use a confidence interval around your magnitude of effect, and apply a false discovery rate correction to the p-value of the confidence interval.  The effects for which even the lowest bound of the corrected confidence interval is high are likely to be strong effects, regardless of huge data set size.  I am not aware of any published paper that combines confidence intervals with false discovery rate correction in this way, but it seems like a straightforward and intuitively understandable approach.&#xD;&#xA;&#xD;&#xA;To make this even better, use a non-parametric way to estimate confidence intervals.  Assuming a distribution is likely to give very optimistic estimates here, and even fitting a distribution to the data is likely to be inaccurate.  Since the information about the shape of the distribution past the edges of the confidence interval comes from a relatively small subsample of the data, this is where it really pays to be careful.  You can use bootstrapping to get a non-parametric confidence interval.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/False_discovery_rate&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/Familywise_error_rate" />
  <row Id="261" PostHistoryTypeId="2" PostId="101" RevisionGUID="5e1fb228-9e1b-44df-8bb8-71727ee6efae" CreationDate="2014-05-15T21:41:22.703" UserId="92" Text="One possibility here (and this is really an extension of what Sean Owen posted) is to define a &quot;stable user.&quot;&#xD;&#xA;&#xD;&#xA;For the given info you have you can imagine making a user_id that is a hash of ip and some user agent info (pseudo code):&#xD;&#xA;    &#xD;&#xA;    uid = MD5Hash(ip + UA.device + UA.model)&#xD;&#xA;&#xD;&#xA;Then you flag these ids with &quot;stable&quot; or &quot;unstable&quot; based on usage heuristics you observe for your users.  This can be a threshold of # of visits in a given time window, length of time their cookies persist, some end action on your site (I realize this wasn't stated in your original log), etc...&#xD;&#xA;&#xD;&#xA;The idea here is to separate the users that don't drop cookies from those that do.&#xD;&#xA;&#xD;&#xA;From here you can attribute session_ids to stable uids from your logs.  You will then have &quot;left over&quot; session_ids for unstable users that you are relatively unsure about.  You may be over or under counting sessions, attributing behavior to multiple people when there is only one, etc...  But this is at least limited to the users you are now &quot;less certain&quot; about.&#xD;&#xA;&#xD;&#xA;You then perform analytics on your stable group and project that to the unstable group.  Take a user count for example, you know the total # of sessions, but you are unsure of how many users generated those sessions.  You can find the # sessions / unique stable user and use this to project the &quot;estimated&quot; number of unique users in the unstable group since you know the number of sessions attributed to that group.&#xD;&#xA;&#xD;&#xA;    projected_num_unstable_users = num_sess_unstable / num_sess_per_stable_uid&#xD;&#xA;&#xD;&#xA;This doesn't help with per user level investigation on unstable users but you can at least get some mileage out of a cohort of stable users that persist for some time.  You can, by various methods, project behavior and counts into the unstable group.  The above is a simple example of something you might want to know.  The general idea is again to define a set of users you are confident persist, measure what you want to measure, and use certain ground truths (num searches, visits, clicks, etc...) to project into the unknown user space and estimate counts for them.&#xD;&#xA;&#xD;&#xA;This is a longstanding problem in unique user counting, logging, etc... for services that don't require log in." />
  <row Id="262" PostHistoryTypeId="5" PostId="97" RevisionGUID="9a735a97-1157-4fb3-9621-9c5c6fd7dcc4" CreationDate="2014-05-15T23:08:04.300" UserId="158" Comment="deleted 7 characters in body" Text="[Depends on the user's intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters. &#xD;&#xA;&#xD;&#xA;[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it's not getting clicks; meaning you'd have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here's a [click and attention distribution heat-map][2] for  Google search results:&#xD;&#xA;&#xD;&#xA;![Google SEPR Click and Attention distribution ‘heat-map’][1]&#xD;&#xA;&#xD;&#xA;Further, using click frequency to account for relevance is not a direct measure of the resource's relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter. &#xD;&#xA;&#xD;&#xA;That said, if you're interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see &quot;[YouTube's head of engineering speaking about clicks vs engagement][3]&quot; for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.&#xD;&#xA;&#xD;&#xA;Might be worth noting that historically Google was known for [PageRank algorithm][4] though it's possible your intent is only to review click-streams, so I won't delve [Google ranking factors][5]; if you are interested in the Google's approach, you might find a review of [Google's Search Quality Rating Guidelines](http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/8kO5S.jpg&#xD;&#xA;  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm&#xD;&#xA;  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/PageRank&#xD;&#xA;  [5]: https://www.google.com/search?q=google%20ranking%20factors" />
  <row Id="263" PostHistoryTypeId="5" PostId="64" RevisionGUID="1eb111d4-5549-4b15-b1b7-932641e8054d" CreationDate="2014-05-15T23:22:39.427" UserId="84" Comment="added 149 characters in body" Text="Overfitting, in a nutshell, means take into account **too much** information from your data and/or prior knowledge, and use it in a model. To make it more straightforward, consider the following example: you're hired by some scientists to provide them with a model to predict the growth of some kind of plants. The scientists have given you information collected from their work &#xD;&#xA;with such plants throughout a whole year, and they shall continuously give you information on the future development of their plantation.&#xD;&#xA;&#xD;&#xA;So, you run through the data received, and build up a model out of it. Now suppose that, in your model, you considered just as many characteristics as possible to always find out the exact behavior of the plants you saw in the initial dataset. Now, as the production continues, you'll always take into account those characteristics, and will produce very *fine-grained* results. However, if the plantation eventually suffer from some seasonal change, the results you will receive may fit your model in such a way that your predictions will begin to fail (either saying that the growth will slow down, while it shall actually speed up, or the opposite).&#xD;&#xA;&#xD;&#xA;Apart from being unable to detect such small variations, and to usually classify your entries incorrectly, the *fine-grain* on the model, i.e., the great amount of variables, may cause the processing to be too costly. Now, imagine that your data is already complex. Overfitting your model to the data not only will make the classification/evaluation very complex, but will most probably make you error the prediction over the slightest variation you may have on the input.&#xD;&#xA;&#xD;&#xA;**Edit**: [This](https://www.youtube.com/watch?v=DQWI1kvmwRg) might as well be of some use, perhaps adding dynamicity to the above explanation :D" />
  <row Id="264" PostHistoryTypeId="4" PostId="89" RevisionGUID="1742e878-7034-4015-b8bc-4f4b23905f32" CreationDate="2014-05-16T02:46:17.830" UserId="21" Comment="Fix typo; add tags" Text="How does a query into a huge database return with negligible latency?" />
  <row Id="265" PostHistoryTypeId="6" PostId="89" RevisionGUID="1742e878-7034-4015-b8bc-4f4b23905f32" CreationDate="2014-05-16T02:46:17.830" UserId="21" Comment="Fix typo; add tags" Text="&lt;bigdata&gt;&lt;google&gt;&lt;search&gt;&lt;latency&gt;" />
  <row Id="266" PostHistoryTypeId="24" PostId="89" RevisionGUID="1742e878-7034-4015-b8bc-4f4b23905f32" CreationDate="2014-05-16T02:46:17.830" Comment="Proposed by 21 approved by 189 edit id of 17" />
  <row Id="267" PostHistoryTypeId="5" PostId="89" RevisionGUID="58c6279d-a832-41cf-be47-7568908df195" CreationDate="2014-05-16T02:46:56.510" UserId="189" Comment="added 263 characters in body" Text="For example, when searching something in Google, results return nigh-instantly.&#xD;&#xA;&#xD;&#xA;I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?&#xD;&#xA;&#xD;&#xA;Moreover, wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.&#xD;&#xA;&#xD;&#xA;Does MapReduce help solve this problem?&#xD;&#xA;&#xD;&#xA;EDIT: Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches? Even for the most obscure search I have conducted, I don't think the search has ever been reported to be larger than 5 seconds. How is this possible?" />
  <row Id="268" PostHistoryTypeId="5" PostId="91" RevisionGUID="c7904778-d48f-4ea2-bab6-8bdfabe3d1d3" CreationDate="2014-05-16T04:28:40.033" UserId="84" Comment="added 1487 characters in body" Text="Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:&#xD;&#xA;&#xD;&#xA;1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);&#xD;&#xA;2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;&#xD;&#xA;3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.&#xD;&#xA;&#xD;&#xA;Considering that, lets try to address your questions:&#xD;&#xA;&#xD;&#xA;&gt; but I imagine it infeasible for the results of every single possible query to be indexed&#xD;&#xA;&#xD;&#xA;Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these `n -&gt; inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.&#xD;&#xA;&#xD;&#xA;&gt; wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs&#xD;&#xA;&#xD;&#xA;Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?&#xD;&#xA;&#xD;&#xA;The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](http://i.stack.imgur.com/Uf6al.gif). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.&#xD;&#xA;&#xD;&#xA;Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it's not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.&#xD;&#xA;&#xD;&#xA;&gt; Does MapReduce help solve this problem?&#xD;&#xA;&#xD;&#xA;Although I don't think that using or not MapReduce is restricted information inside Google, I'm not conversant about this point. However, Google's implementation of MapReduce (which is surely *not* Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time.&#xD;&#xA;&#xD;&#xA;&gt; Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches?&#xD;&#xA;&#xD;&#xA;The graph below presents a curve of how the *kinds* of queries occur. You can see that there are three main kinds of searches, each of them holding approximately 1/3 of the volume of queries (area below curve). The plot shows power law, and reinforces the fact that smaller queries are the most popular. The second third of queries are still feasible to process, since they hold few words. But the set of so-called *obscure queries*, which usually consist of non-experienced users' queries, are not a negligible part of the queries.&#xD;&#xA;&#xD;&#xA;![Heavy-tailed distribution][1]&#xD;&#xA;&#xD;&#xA;And there lies space for novel solutions. Since it's not just one or two queries (but one third of them), they must have *relevant* results. If you type in something *much too obscure* in a Google search, it shan't take longer to return a list of results, but will most probably show you something it *inferred* you'd like to say. Or it may simply state that there was no document with such terms -- or even cut down your search to 32 words (which just happened to me in a random test here).&#xD;&#xA;&#xD;&#xA;There are dozens of appliable heuristics, which may be either to ignore some words, or try to break the query into smaller ones, and gather the most *popular* results. All these combinations, since they are simply heuristics, can be tailored and tweaked to respect *feasible waiting times* of, say, less then a second? :D&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/CpcNf.jpg" />
  <row Id="269" PostHistoryTypeId="5" PostId="91" RevisionGUID="9058136d-57c7-435f-99c8-7922e7c902fb" CreationDate="2014-05-16T04:33:52.310" UserId="84" Comment="added 1487 characters in body" Text="Well, I'm not sure if it is MapReduce that solves the problem, but it surely wouldn't be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:&#xD;&#xA;&#xD;&#xA;1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);&#xD;&#xA;2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;&#xD;&#xA;3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.&#xD;&#xA;&#xD;&#xA;Considering that, lets try to address your questions:&#xD;&#xA;&#xD;&#xA;&gt; but I imagine it infeasible for the results of every single possible query to be indexed&#xD;&#xA;&#xD;&#xA;Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these `n -&gt; inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.&#xD;&#xA;&#xD;&#xA;&gt; wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs&#xD;&#xA;&#xD;&#xA;Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?&#xD;&#xA;&#xD;&#xA;The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](http://i.stack.imgur.com/Uf6al.gif). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.&#xD;&#xA;&#xD;&#xA;Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it's not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.&#xD;&#xA;&#xD;&#xA;&gt; Does MapReduce help solve this problem?&#xD;&#xA;&#xD;&#xA;Although I don't think that using or not MapReduce is restricted information inside Google, I'm not conversant about this point. However, Google's implementation of MapReduce (which is surely *not* Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time.&#xD;&#xA;&#xD;&#xA;&gt; Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches?&#xD;&#xA;&#xD;&#xA;The graph below presents a curve of how the *kinds* of queries occur. You can see that there are three main kinds of searches, each of them holding approximately 1/3 of the volume of queries (area below curve). The plot shows power law, and reinforces the fact that smaller queries are the most popular. The second third of queries are still feasible to process, since they hold few words. But the set of so-called *obscure queries*, which usually consist of non-experienced users' queries, are not a negligible part of the queries.&#xD;&#xA;&#xD;&#xA;![Heavy-tailed distribution][1]&#xD;&#xA;&#xD;&#xA;And there lies space for novel solutions. Since it's not just one or two queries (but one third of them), they must have *relevant* results. If you type in something *much too obscure* in a Google search, it shan't take longer to return a list of results, but will most probably show you something it *inferred* you'd like to say. Or it may simply state that there was no document with such terms -- or even cut down your search to 32 words (which just happened to me in a random test here).&#xD;&#xA;&#xD;&#xA;There are dozens of appliable heuristics, which may be either to ignore some words, or to try to break the query into smaller ones, and gather the most *popular* results. And all these solutions can be tailored and tweaked to respect *feasible waiting times* of, say, less then a second? :D&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/CpcNf.jpg" />
  <row Id="271" PostHistoryTypeId="2" PostId="102" RevisionGUID="e4db9174-41f4-492b-98eb-bcf39f022eda" CreationDate="2014-05-16T05:09:33.557" UserId="199" Text="What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users history (like in app purchase) and analytics of use behavior." />
  <row Id="272" PostHistoryTypeId="1" PostId="102" RevisionGUID="e4db9174-41f4-492b-98eb-bcf39f022eda" CreationDate="2014-05-16T05:09:33.557" UserId="199" Text="Best noSQL backend for mobile game" />
  <row Id="273" PostHistoryTypeId="3" PostId="102" RevisionGUID="e4db9174-41f4-492b-98eb-bcf39f022eda" CreationDate="2014-05-16T05:09:33.557" UserId="199" Text="&lt;nosql&gt;" />
  <row Id="274" PostHistoryTypeId="5" PostId="17" RevisionGUID="20a8bb9a-c7d1-4f93-bd5f-d735df0df7c4" CreationDate="2014-05-16T13:44:53.470" UserId="63" Comment="added 196 characters in body" Text="[LIBSVM][1] is a library for support vector classification (SVM) and regression.&#xD;&#xA;It was created by Chih-Chung Chang and Chih-Jen Lin in 2001.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/" />
  <row Id="275" PostHistoryTypeId="24" PostId="17" RevisionGUID="20a8bb9a-c7d1-4f93-bd5f-d735df0df7c4" CreationDate="2014-05-16T13:44:53.470" Comment="Proposed by 63 approved by 50 edit id of 1" />
  <row Id="276" PostHistoryTypeId="6" PostId="7" RevisionGUID="44b379df-3358-477b-99e6-8a9a34da74ce" CreationDate="2014-05-16T13:45:00.237" UserId="97" Comment="Added relevant tags" Text="&lt;education&gt;&lt;open-source&gt;" />
  <row Id="277" PostHistoryTypeId="24" PostId="7" RevisionGUID="44b379df-3358-477b-99e6-8a9a34da74ce" CreationDate="2014-05-16T13:45:00.237" Comment="Proposed by 97 approved by 50 edit id of 3" />
  <row Id="278" PostHistoryTypeId="6" PostId="52" RevisionGUID="b37ebd26-025c-4574-99a4-fdcedb82e571" CreationDate="2014-05-16T13:45:07.447" UserId="136" Comment="added R tag" Text="&lt;r&gt;&lt;bad-data&gt;&lt;data-cleaning&gt;" />
  <row Id="279" PostHistoryTypeId="24" PostId="52" RevisionGUID="b37ebd26-025c-4574-99a4-fdcedb82e571" CreationDate="2014-05-16T13:45:07.447" Comment="Proposed by 136 approved by 50 edit id of 9" />
  <row Id="280" PostHistoryTypeId="5" PostId="79" RevisionGUID="e7ca3ea4-8e48-486b-93b6-b0286e44eaa3" CreationDate="2014-05-16T13:45:25.047" UserId="53" Comment="Cited source for Wiki tag content." Text="Conceptually speaking, *data-mining* can be thought of as one item (or set of skills and applications) in the toolkit of the data scientist.&#xD;&#xA;&#xD;&#xA;More specifically, data-mining is an activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal.&#xD;&#xA;&#xD;&#xA;In US-English colloquial speech, data-mining and data-collection are often used interchangeably.&#xD;&#xA;&#xD;&#xA;However, a main difference between these two related activities is *intentionality*. &#xD;&#xA;&#xD;&#xA;*Definition inspired mostly by the contributions of @statsRus to Data Science.SE*" />
  <row Id="281" PostHistoryTypeId="24" PostId="79" RevisionGUID="e7ca3ea4-8e48-486b-93b6-b0286e44eaa3" CreationDate="2014-05-16T13:45:25.047" Comment="Proposed by 53 approved by 50 edit id of 13" />
  <row Id="282" PostHistoryTypeId="6" PostId="14" RevisionGUID="b3c7d2c0-69dd-4dfb-b3f1-02172e2a0a70" CreationDate="2014-05-16T13:45:34.440" UserId="53" Comment="Propose that definitions tag be added for this question" Text="&lt;data-science&gt;&lt;data-mining&gt;&lt;definitions&gt;" />
  <row Id="283" PostHistoryTypeId="24" PostId="14" RevisionGUID="b3c7d2c0-69dd-4dfb-b3f1-02172e2a0a70" CreationDate="2014-05-16T13:45:34.440" Comment="Proposed by 53 approved by 50 edit id of 4" />
  <row Id="284" PostHistoryTypeId="5" PostId="66" RevisionGUID="3f29bc47-4f5c-4b70-bc50-6817e9323191" CreationDate="2014-05-16T13:45:57.450" UserId="118" Comment="some info about tag" Text="Big data is the term for a collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools or traditional data processing applications. The challenges include capture, curation, storage, search, sharing, transfer, analysis and visualization." />
  <row Id="285" PostHistoryTypeId="24" PostId="66" RevisionGUID="3f29bc47-4f5c-4b70-bc50-6817e9323191" CreationDate="2014-05-16T13:45:57.450" Comment="Proposed by 118 approved by 50 edit id of 10" />
  <row Id="286" PostHistoryTypeId="5" PostId="80" RevisionGUID="2f2b77af-fd7c-465e-97eb-a4edf055e04e" CreationDate="2014-05-16T13:46:05.850" UserId="53" Comment="Cited source for Wiki tag content." Text="An activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal." />
  <row Id="287" PostHistoryTypeId="24" PostId="80" RevisionGUID="2f2b77af-fd7c-465e-97eb-a4edf055e04e" CreationDate="2014-05-16T13:46:05.850" Comment="Proposed by 53 approved by 50 edit id of 14" />
  <row Id="288" PostHistoryTypeId="2" PostId="103" RevisionGUID="49925465-cc9e-411c-aa0c-5094a6d20f75" CreationDate="2014-05-16T14:26:12.270" UserId="113" Text="Assume that we have a set of elements *E* and a similarity (**not distance**) function *sim(ei, ej)* between two elements *ei,ej ∈ E*. &#xD;&#xA;&#xD;&#xA;How could we (efficiently) cluster the elements of *E*, using *sim*?&#xD;&#xA;&#xD;&#xA;*k*-means, for example, requires a given *k*, Canopy Clustering requires two threshold values. What if we don't want such predefined parameters?&#xD;&#xA;&#xD;&#xA;Note, that *sim* is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn't matter if the clusters are disjoint (partitions of *E*).&#xD;&#xA;" />
  <row Id="289" PostHistoryTypeId="1" PostId="103" RevisionGUID="49925465-cc9e-411c-aa0c-5094a6d20f75" CreationDate="2014-05-16T14:26:12.270" UserId="113" Text="Clustering based on similarity scores" />
  <row Id="290" PostHistoryTypeId="3" PostId="103" RevisionGUID="49925465-cc9e-411c-aa0c-5094a6d20f75" CreationDate="2014-05-16T14:26:12.270" UserId="113" Text="&lt;clustering&gt;&lt;algorithms&gt;&lt;similarity&gt;" />
  <row Id="291" PostHistoryTypeId="2" PostId="104" RevisionGUID="308637bf-d9b0-47a2-a2f2-a953301309d5" CreationDate="2014-05-16T15:35:51.420" UserId="-1" Text="" />
  <row Id="292" PostHistoryTypeId="2" PostId="105" RevisionGUID="2fe928f3-e272-46b5-824d-2bb061997bd1" CreationDate="2014-05-16T15:35:51.420" UserId="-1" Text="" />
  <row Id="293" PostHistoryTypeId="2" PostId="106" RevisionGUID="94618ffd-1651-40f1-a2e1-ebb656080903" CreationDate="2014-05-16T16:25:58.250" UserId="34" Text="There are many overlaps between data mining and datascience. I would say that people with the role of datamining are concerned with data collection and the extraction of features from unfiltered, unorganised and mostly raw/wild datasets. Some very important data may be difficult to extract, not do to the implementation issues but because it may have foreign artifacts. &#xD;&#xA;&#xD;&#xA;Eg. if I needed someone to look at financial data from written tax returns in the 70s which were scanned and machine read to find out if people saved more on car insurance; a dataminer would be the person to get. &#xD;&#xA;&#xD;&#xA;If I needed someone to examine the influence Nike's Twitter profile in the tweets of Brazil and identify key positive features from the profile, I would look for a datascientist." />
  <row Id="294" PostHistoryTypeId="2" PostId="107" RevisionGUID="05264096-c5d9-42aa-835c-c90ca0c8408f" CreationDate="2014-05-16T20:07:50.983" UserId="200" Text="Consider a stream containing tuples `(user, new_score)` representing users score in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players. &#xD;&#xA;&#xD;&#xA;I would like to have some standing queries like: &#xD;&#xA;&#xD;&#xA;1. Which players posted more than x scores in a sliding window of one hour&#xD;&#xA;2. Which players gained x% score in a sliding window of one hour&#xD;&#xA;&#xD;&#xA;My question is which open source tools can I employ to jumpstart this project? I am considering [Esper][1] at the moment. &#xD;&#xA;&#xD;&#xA;Note: I have just completed reading &quot;Mining Data Streams&quot; (chapter 4 of [Mining of Massive Datasets][2]) and I am quiet new to mining data streams.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://esper.codehaus.org/&#xD;&#xA;  [2]: http://infolab.stanford.edu/~ullman/mmds.html" />
  <row Id="295" PostHistoryTypeId="1" PostId="107" RevisionGUID="05264096-c5d9-42aa-835c-c90ca0c8408f" CreationDate="2014-05-16T20:07:50.983" UserId="200" Text="Opensource tools for help in mining stream of leader board scores" />
  <row Id="296" PostHistoryTypeId="3" PostId="107" RevisionGUID="05264096-c5d9-42aa-835c-c90ca0c8408f" CreationDate="2014-05-16T20:07:50.983" UserId="200" Text="&lt;data-stream-mining&gt;" />
  <row Id="297" PostHistoryTypeId="2" PostId="108" RevisionGUID="010b05ec-6994-4015-8405-273e63c584d0" CreationDate="2014-05-16T20:24:38.980" UserId="-1" Text="" />
  <row Id="298" PostHistoryTypeId="2" PostId="109" RevisionGUID="8989bab4-3ca4-45b9-ac29-0b3a65e25da4" CreationDate="2014-05-16T20:24:38.980" UserId="-1" Text="" />
  <row Id="304" PostHistoryTypeId="2" PostId="111" RevisionGUID="4b50495d-1743-48be-bfe3-665f77a64b92" CreationDate="2014-05-17T03:07:59.707" UserId="26" Text="Some factors you might consider:&#xD;&#xA;&#xD;&#xA;Developer familiarity: go with whatever you or your developers are familiar with.  Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfamiliar solution that might be slughtly better go for familiar and save a bunch of development time.&#xD;&#xA;&#xD;&#xA;Ease of cloud deployment:  for example, if you are using Amazon AWS, then DynamoDB is likely an excellent choice.  Sure, you could use Mongo on AWS, but why bother?  Other cloud providers have their own preferred db, for example if you are uusing Google AppEngine, it makes sense to use BigTable or Cloud Storage. &#xD;&#xA;&#xD;&#xA;Your use case seems both well suited to NoSQL and not very challenging since your data has a natural partition by user.  I think you'd be technically ok with anything, which is why I'm mainly covering other factors." />
  <row Id="305" PostHistoryTypeId="2" PostId="112" RevisionGUID="823a3c28-c770-4211-83f9-5bc114541157" CreationDate="2014-05-17T04:18:10.020" UserId="70" Text="This isn't a full solution, but you may want to look into [OrientDB][1] as part of your stack. Orient is a Graph-Document database server written entirely in Java. &#xD;&#xA;&#xD;&#xA;In graph databases, relationships are considered first class citizens and therefore traversing those relationships can be done pretty quickly. Orient is also a document database which would allow you the kind of schema-free architecture it sounds like you would need. The real reason I suggest Orient, however, is because of its extensiblity. It supports streaming via sockets, and the entire database can be embedded into another application. Finally, it can be scaled efficiently and/or can work entirely through memory. So, with some Java expertise, you can actually run your preset queries against the database in memory.&#xD;&#xA;&#xD;&#xA;We are doing something similar. In creating an app/site for social science research collaboration, we found ourselves with immensely complex data models. We ended up writing several of the queries using the Gremlin Traversal Language (a subset of Groovy, which is, of course, Java at its heart), and then exposing those queries through the binary connection server of the OrientDB. So, the client opens a TCP socket, sends a short binary message, and the query is executing in Java directly against the in-memory database.&#xD;&#xA;&#xD;&#xA;OrientDB also supports writing function queries in Javascript, and you can use Node.js to interact directly with an Orient instance.&#xD;&#xA;&#xD;&#xA;For something of this size, I would want to use Orient in conjunction with Hadoop or something like that. You can also use Orient in conjunction with esper.&#xD;&#xA;&#xD;&#xA;Consider:&#xD;&#xA;An introduction to orient: http://www.sitepoint.com/a-look-at-orientdb-the-graph-document-nosql/&#xD;&#xA;&#xD;&#xA;Complex, real-time queries: http://www.gft-blog.com/business-trends/leveraging-real-time-scoring-through-bigdata-to-detect-insurance-fraud/&#xD;&#xA;&#xD;&#xA;A discussion about streaming options with java and orient: https://github.com/orientechnologies/orientdb/issues/1227&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;  [1]: http://www.orientechnologies.com/" />
  <row Id="306" PostHistoryTypeId="2" PostId="113" RevisionGUID="0f316ff9-666b-4b3e-83c6-6b6cfe494f76" CreationDate="2014-05-17T04:53:03.913" UserId="199" Text="When a relational database like mySQL has better performance than a no relational, like mongo?&#xD;&#xA;&#xD;&#xA;I saw a question on Quora other day, about why Quora still uses mySQL as their backend. And how their performance is still good." />
  <row Id="307" PostHistoryTypeId="1" PostId="113" RevisionGUID="0f316ff9-666b-4b3e-83c6-6b6cfe494f76" CreationDate="2014-05-17T04:53:03.913" UserId="199" Text="When a relational database has better performance than a no relational" />
  <row Id="308" PostHistoryTypeId="3" PostId="113" RevisionGUID="0f316ff9-666b-4b3e-83c6-6b6cfe494f76" CreationDate="2014-05-17T04:53:03.913" UserId="199" Text="&lt;bigdata&gt;&lt;nosql&gt;&lt;databases&gt;" />
  <row Id="312" PostHistoryTypeId="5" PostId="107" RevisionGUID="2d70edb1-2ea1-47a0-8680-c47bc36b432b" CreationDate="2014-05-17T08:40:59.320" UserId="53" Comment="Corrected spelling and grammar. Added link for usage of a possibly diificult term." Text="Consider a stream containing [tuples][tuples] `(user, new_score)` representing users' scores in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players. &#xD;&#xA;&#xD;&#xA;I would like to have some standing queries like: &#xD;&#xA;&#xD;&#xA;1. Which players posted more than x scores in a sliding window of one hour&#xD;&#xA;2. Which players gained x% score in a sliding window of one hour&#xD;&#xA;&#xD;&#xA;My question is which open source tools can I employ to jumpstart this project? I am considering [Esper][1] at the moment. &#xD;&#xA;&#xD;&#xA;Note: I have just completed reading &quot;Mining Data Streams&quot; (chapter 4 of [Mining of Massive Datasets][2]) and I am quite new to mining data streams.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://esper.codehaus.org/&#xD;&#xA;  [2]: http://infolab.stanford.edu/~ullman/mmds.html&#xD;&#xA;  [tuples]: http://en.m.wikipedia.org/wiki/Tuple" />
  <row Id="313" PostHistoryTypeId="24" PostId="107" RevisionGUID="2d70edb1-2ea1-47a0-8680-c47bc36b432b" CreationDate="2014-05-17T08:40:59.320" Comment="Proposed by 53 approved by 200 edit id of 23" />
  <row Id="314" PostHistoryTypeId="2" PostId="115" RevisionGUID="86da5fdb-3dfb-4960-b07e-00fbe5bf3a8f" CreationDate="2014-05-17T08:45:08.420" UserId="212" Text="If I have a very long list of paper names, how could I get abstract of these papers from internet or any database?&#xD;&#xA;&#xD;&#xA;The paper names are like &quot;Assessment of Utility in Web Mining for the Domain of Public Health&quot;.&#xD;&#xA;&#xD;&#xA;Does any one know any API that can give me a solution? I tried to crawl google scholar, however, google blocked my crawler." />
  <row Id="315" PostHistoryTypeId="1" PostId="115" RevisionGUID="86da5fdb-3dfb-4960-b07e-00fbe5bf3a8f" CreationDate="2014-05-17T08:45:08.420" UserId="212" Text="Is there any APIs for crawling abstract of paper?" />
  <row Id="316" PostHistoryTypeId="3" PostId="115" RevisionGUID="86da5fdb-3dfb-4960-b07e-00fbe5bf3a8f" CreationDate="2014-05-17T08:45:08.420" UserId="212" Text="&lt;data-mining&gt;&lt;machine-learning&gt;" />
  <row Id="318" PostHistoryTypeId="2" PostId="116" RevisionGUID="867300c0-8817-4870-ade0-25429746b9bf" CreationDate="2014-05-17T09:16:18.823" UserId="173" Text="I have a database from my Facebook application and I am trying to use machine learning to estimate users' age based on what Facebook sites they like.&#xD;&#xA;&#xD;&#xA;There are two crucial characteristics of my database:&#xD;&#xA;&#xD;&#xA; - the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);&#xD;&#xA;&#xD;&#xA; - many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).&#xD;&#xA;&#xD;&#xA; - there's many more features than samples.&#xD;&#xA;&#xD;&#xA;So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?&#xD;&#xA;&#xD;&#xA;I mainly use Python, so Python-specific hints would be greatly appreciated." />
  <row Id="319" PostHistoryTypeId="1" PostId="116" RevisionGUID="867300c0-8817-4870-ade0-25429746b9bf" CreationDate="2014-05-17T09:16:18.823" UserId="173" Text="Machine learning techniques for estimating users' age based on Facebook sites they like" />
  <row Id="320" PostHistoryTypeId="3" PostId="116" RevisionGUID="867300c0-8817-4870-ade0-25429746b9bf" CreationDate="2014-05-17T09:16:18.823" UserId="173" Text="&lt;machine-learning&gt;&lt;dimensionality-reduction&gt;&lt;python&gt;" />
  <row Id="324" PostHistoryTypeId="2" PostId="118" RevisionGUID="d2c32564-4819-4fa3-a494-edc96100fdbf" CreationDate="2014-05-17T13:41:20.283" UserId="-1" Text="" />
  <row Id="325" PostHistoryTypeId="2" PostId="119" RevisionGUID="70af58e8-dd3f-4dd1-9d4c-dccaef07d5b0" CreationDate="2014-05-17T13:41:20.283" UserId="-1" Text="" />
  <row Id="327" PostHistoryTypeId="5" PostId="16" RevisionGUID="977b326c-9676-4b70-b3aa-3f75ad9d6c49" CreationDate="2014-05-17T16:24:14.523" UserId="84" Comment="Fixed grammar, and improving formatting." Text="I use [Libsvm][1] to train data and predict classification on **semantic analysis** problem. But it has a **performance** issue on large-scale data, because semantic analysis concerns ***n-dimension*** problem.&#xD;&#xA;&#xD;&#xA;Last year, [Liblinear][2] was release, and it can solve performance bottleneck.&#xD;&#xA;But it cost too much **memory**. Is **MapReduce** the only way to solve semantic analysis problem on big data? Or are there any other methods that can improve memory bottleneck on **Liblinear**?&#xD;&#xA;&#xD;&#xA;  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/&#xD;&#xA;  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/" />
  <row Id="328" PostHistoryTypeId="24" PostId="16" RevisionGUID="977b326c-9676-4b70-b3aa-3f75ad9d6c49" CreationDate="2014-05-17T16:24:14.523" Comment="Proposed by 84 approved by 63 edit id of 24" />
  <row Id="329" PostHistoryTypeId="2" PostId="120" RevisionGUID="c92d8bc9-2952-437c-bd41-ce67a2ecfd8b" CreationDate="2014-05-17T18:15:11.937" UserId="92" Text="arXiv has an [API and bulk download](http://arxiv.org/help/bulk_data) but if you want something for paid journals it will be hard to come by without paying an indexer like pubmed or elsevier or the like." />
  <row Id="330" PostHistoryTypeId="2" PostId="121" RevisionGUID="7470dd79-cb71-48de-852c-bd9a2d41be07" CreationDate="2014-05-17T18:53:30.123" UserId="92" Text="One thing to start off with would be k-NN.  The idea here is that you have a user/item matrix and for some of the users you have a reported age.  The age for a person in the user item matrix might be well determined by something like the mean or median age of some nearest neighbors in the item space.&#xD;&#xA;&#xD;&#xA;So you have each user expressed as a vector in item space, find the k nearest neighbors and assign the vector in question some summary stat of the nearest neighbor ages.  You can choose k on a distance cutoff or more realistically by iteratively assigning ages to a train hold out and choosing the k that minimizes the error in that assignment.&#xD;&#xA;&#xD;&#xA;If the dimensionality is a problem you can easily perform reduction in this setup by single value decomposition choosing the m vectors that capture the most variance across the group.&#xD;&#xA;&#xD;&#xA;In all cases since each feature is binary it seems that cosine similarity would be your go to distance metric.&#xD;&#xA;&#xD;&#xA;I need to think a bit more about other approaches (regression, rf, etc...) given the narrow focus of your feature space (all variants of the same action, liking) I think the user/item approach might be the best.&#xD;&#xA;&#xD;&#xA;One note of caution, if the ages you have for train are self reported you might need to correct some of them.  People on facebook tend to report ages in the decade they were born.  Plot a histogram of the birth dates (derived from ages) and see if you have spikes at decades like 70s, 80s, 90s." />
  <row Id="331" PostHistoryTypeId="5" PostId="116" RevisionGUID="dd47aa8d-791d-4338-a66f-c8393c3ab2ca" CreationDate="2014-05-17T19:26:53.783" UserId="173" Comment="added 2 characters in body" Text="I have a database from my Facebook application and I am trying to use machine learning to estimate users' age based on what Facebook sites they like.&#xD;&#xA;&#xD;&#xA;There are three crucial characteristics of my database:&#xD;&#xA;&#xD;&#xA; - the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);&#xD;&#xA;&#xD;&#xA; - many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).&#xD;&#xA;&#xD;&#xA; - there's many more features than samples.&#xD;&#xA;&#xD;&#xA;So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?&#xD;&#xA;&#xD;&#xA;I mainly use Python, so Python-specific hints would be greatly appreciated." />
  <row Id="332" PostHistoryTypeId="2" PostId="122" RevisionGUID="e1bfdaca-8468-4108-a9c2-042d02b85790" CreationDate="2014-05-17T20:56:15.577" UserId="180" Text="It depends on your data and what you're doing with it. For example, if the processing you have to do requires transactions to synchronize across nodes, it will likely be faster to use transactions implemented in an RDBMS rather than implementing it yourself on top of NoSQL databases which don't support it natively. " />
  <row Id="333" PostHistoryTypeId="2" PostId="123" RevisionGUID="269ed444-397c-4b6e-b3df-f6e1b1cbb076" CreationDate="2014-05-17T21:10:41.990" UserId="-1" Text="" />
  <row Id="334" PostHistoryTypeId="2" PostId="124" RevisionGUID="3e5293b2-8ca5-48fb-acd7-36525d159223" CreationDate="2014-05-17T21:10:41.990" UserId="-1" Text="" />
  <row Id="336" PostHistoryTypeId="2" PostId="125" RevisionGUID="aa1b2818-db3c-4213-8ca2-daaf0e7c7283" CreationDate="2014-05-17T21:52:34.563" UserId="109" Text="I want learn about NoSQL and when is better SQL or NoSQL, I know that this question depend of case, because it I ask about a good documentation about NoSQL and that explain when is better a SQL and when is better a NoSQL, cases of example,etc; and also your opinions on NoSQL databases and use cases, and any recommendations for learning about this topic.&#xD;&#xA;&#xD;&#xA;Thanks for your help." />
  <row Id="337" PostHistoryTypeId="1" PostId="125" RevisionGUID="aa1b2818-db3c-4213-8ca2-daaf0e7c7283" CreationDate="2014-05-17T21:52:34.563" UserId="109" Text="how learn noSQL databases and how know if is better SQL or noSQL" />
  <row Id="338" PostHistoryTypeId="3" PostId="125" RevisionGUID="aa1b2818-db3c-4213-8ca2-daaf0e7c7283" CreationDate="2014-05-17T21:52:34.563" UserId="109" Text="&lt;nosql&gt;" />
  <row Id="339" PostHistoryTypeId="2" PostId="126" RevisionGUID="aecbc0ea-764b-4b8f-92d9-7abe3cfb6610" CreationDate="2014-05-17T23:53:42.700" UserId="26" Text="Please have a look at my answer here: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767&#xD;&#xA;&#xD;&#xA;Short version:&#xD;&#xA;&#xD;&#xA;- Use NoSQL is data size and number of transactions per second forces it, which typically happens above a few tens of TB and millions of transactions per second (db in memory, running on cluster), *or* at hundreds of TB and thousands of transactions per second (traditional db on disk, transactions per second is *highly* dependent on the usage pattern). Traditional SQL scales up to that point just fine.&#xD;&#xA;&#xD;&#xA;- NoSQL is well suited for some problems (data has a natural sharding, schema is flexible, eventual consistency is ok).  You can use there even if scaling doesn't force you to.&#xD;&#xA;&#xD;&#xA;- Developer familiarity with tools and ops ease of deployment are *major* factors, don't overlook those.  A solution may be technically better but you may have a hard time using it, make sure you need it and make sure you budget for the learning curve." />
  <row Id="340" PostHistoryTypeId="5" PostId="126" RevisionGUID="b334174e-2f12-4abf-9939-8dc1ff1ca1fc" CreationDate="2014-05-17T23:59:59.580" UserId="26" Comment="added 334 characters in body" Text="Please have a look at my answer here: &#xD;&#xA;&#xD;&#xA;**[Motivations for using relational database / ORM or document database / ODM][1]**&#xD;&#xA;&#xD;&#xA;Short version:&#xD;&#xA;&#xD;&#xA;- Use NoSQL is data size and number of transactions per second forces it, which typically happens above a few tens of TB and millions of transactions per second (db in memory, running on cluster), *or* at hundreds of TB and thousands of transactions per second (traditional db on disk, transactions per second is *highly* dependent on the usage pattern). Traditional SQL scales up to that point just fine.&#xD;&#xA;&#xD;&#xA;- NoSQL is well suited for some problems (data has a natural sharding, schema is flexible, eventual consistency is ok).  You can use there even if scaling doesn't force you to.&#xD;&#xA;&#xD;&#xA;- Developer familiarity with tools and ops ease of deployment are *major* factors, don't overlook those.  A solution may be technically better but you may have a hard time using it, make sure you need it and make sure you budget for the learning curve.&#xD;&#xA;&#xD;&#xA;As to how to learn it: fire up a MongoDB image on AWS, or DynamoDB, and have fun! &#xD;&#xA;http://docs.mongodb.org/ecosystem/platforms/amazon-ec2/&#xD;&#xA;http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStartedDynamoDB.html&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767" />
  <row Id="344" PostHistoryTypeId="2" PostId="128" RevisionGUID="33a17081-4e0b-485f-9510-524e2d23a7df" CreationDate="2014-05-18T06:10:52.543" UserId="122" Text="Latent Dirichlet Allocation (LDA) is a topic modeling process (http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). &#xD;&#xA;&#xD;&#xA;And Hierarchical Dirichlet Process (HDP) is also a topic modeling process (http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process). &#xD;&#xA;&#xD;&#xA;**The major difference is LDA requires the specification of the number of topics but HDP doesn't. Why is that so?**&#xD;&#xA;&#xD;&#xA;**What are the differences and pros and cons of both topic modelling methods?**&#xD;&#xA;&#xD;&#xA;" />
  <row Id="345" PostHistoryTypeId="1" PostId="128" RevisionGUID="33a17081-4e0b-485f-9510-524e2d23a7df" CreationDate="2014-05-18T06:10:52.543" UserId="122" Text="Latent Dirichlet Allocation vs Hierarchical Dirichlet Process" />
  <row Id="346" PostHistoryTypeId="3" PostId="128" RevisionGUID="33a17081-4e0b-485f-9510-524e2d23a7df" CreationDate="2014-05-18T06:10:52.543" UserId="122" Text="&lt;nlp&gt;&lt;topic-model&gt;&lt;lda&gt;&lt;hdp&gt;" />
  <row Id="347" PostHistoryTypeId="2" PostId="129" RevisionGUID="8fb34339-3d0f-469b-b7b5-3e9b5ba175d0" CreationDate="2014-05-18T06:17:37.587" UserId="122" Text="This question asked about generative vs discriminative algorithm, http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm but **can someone give an example of the difference when applied to Natural Language Processing?** &#xD;&#xA;&#xD;&#xA;**How are generative and discriminative models use in NLP?**" />
  <row Id="348" PostHistoryTypeId="1" PostId="129" RevisionGUID="8fb34339-3d0f-469b-b7b5-3e9b5ba175d0" CreationDate="2014-05-18T06:17:37.587" UserId="122" Text="What is generative and discriminative model? How are they used in Natural Language Processing?" />
  <row Id="349" PostHistoryTypeId="3" PostId="129" RevisionGUID="8fb34339-3d0f-469b-b7b5-3e9b5ba175d0" CreationDate="2014-05-18T06:17:37.587" UserId="122" Text="&lt;nlp&gt;&lt;language-model&gt;&lt;generative-model&gt;" />
  <row Id="350" PostHistoryTypeId="2" PostId="130" RevisionGUID="92bf1d2c-0435-4eda-aae3-264ba816940a" CreationDate="2014-05-18T06:26:15.673" UserId="122" Text="From wikipedia, &#xD;&#xA;&#xD;&#xA;&gt; dimensionality reduction or dimension reduction is the process of&#xD;&#xA;&gt; reducing the number of random variables under consideration, and&#xD;&#xA;&gt; can be divided into feature selection and feature extraction.&#xD;&#xA;&#xD;&#xA;**What is the difference between feature selection and feature extraction?**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**What is an example of dimensionality reduction in a Natural Language Processing task?**&#xD;&#xA;&#xD;&#xA;" />
  <row Id="351" PostHistoryTypeId="1" PostId="130" RevisionGUID="92bf1d2c-0435-4eda-aae3-264ba816940a" CreationDate="2014-05-18T06:26:15.673" UserId="122" Text="What is dimensionality reduction? What is the difference between feature selection and extraction?" />
  <row Id="352" PostHistoryTypeId="3" PostId="130" RevisionGUID="92bf1d2c-0435-4eda-aae3-264ba816940a" CreationDate="2014-05-18T06:26:15.673" UserId="122" Text="&lt;nlp&gt;&lt;dimensionality-reduction&gt;&lt;feature-selection&gt;" />
  <row Id="353" PostHistoryTypeId="5" PostId="111" RevisionGUID="1a6ac64c-4db4-4743-b4ee-f2d08b70e94e" CreationDate="2014-05-18T06:43:52.453" UserId="26" Comment="added 1 character in body" Text="Some factors you might consider:&#xD;&#xA;&#xD;&#xA;Developer familiarity: go with whatever you or your developers are familiar with.  Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfamiliar solution that might be slightly better go for familiar and save a bunch of development time.&#xD;&#xA;&#xD;&#xA;Ease of cloud deployment:  for example, if you are using Amazon AWS, then DynamoDB is likely an excellent choice.  Sure, you could use Mongo on AWS, but why bother?  Other cloud providers have their own preferred db, for example if you are using Google AppEngine, it makes sense to use BigTable or Cloud Datastore. &#xD;&#xA;&#xD;&#xA;Your use case seems both well suited to NoSQL and not very challenging since your data has a natural partition by user.  I think you'd be technically ok with anything, which is why I'm mainly covering other factors." />
  <row Id="354" PostHistoryTypeId="2" PostId="131" RevisionGUID="02d793c6-1cf6-4823-99bc-98b734e550c9" CreationDate="2014-05-18T06:54:08.560" UserId="26" Text="Look it up on:&#xD;&#xA;&#xD;&#xA;* Google Scholar [link][1]&#xD;&#xA;* Citeseer [link][2]&#xD;&#xA;&#xD;&#xA;If you get a single exact title match then you have probably found the right article, and can fill in the rest of the info from there.  Both give you download links and bibtex-style output.  What you would likely want to do though to get perfect metadata is download and parse the pdf (if any) and look for DOI-style identifier.&#xD;&#xA;&#xD;&#xA;Please be nice and rate-limit your requests if you do this.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scholar.google.com/scholar?hl=en&amp;q=Assessment%20of%20Utility%20in%20Web%20Mining%20for%20the%20Domain%20of%20Public%20Health&amp;btnG=&amp;as_sdt=1,5&amp;as_sdtp=&#xD;&#xA;  [2]: http://citeseerx.ist.psu.edu/search?q=Assessment%20of%20Utility%20in%20Web%20Mining%20for%20the%20Domain%20of%20Public%20Health&amp;submit=Search&amp;sort=rlv&amp;t=doc" />
  <row Id="355" PostHistoryTypeId="2" PostId="132" RevisionGUID="54d1fcb3-a466-40e9-a508-22cb01d2003c" CreationDate="2014-05-18T07:53:58.203" UserId="172" Text="Simply put: &#xD;&#xA;&#xD;&#xA;- feature selection: you select a subset of the original feature set; while&#xD;&#xA;- feature extraction: you build a new set of features from the original feature set. &#xD;&#xA;&#xD;&#xA;Examples of feature extraction: extraction of contours in images, extraction of digrams from a text, extraction of phonemes from recording of spoken text, etc.&#xD;&#xA;&#xD;&#xA;Feature extraction involves a transformation of the features, which often is not reversible because some information is lost in the process of dimensionality reduction." />
  <row Id="356" PostHistoryTypeId="2" PostId="133" RevisionGUID="c7af93ab-550a-4356-a365-1d6eb1ef452e" CreationDate="2014-05-18T09:09:47.780" UserId="26" Text="1. I think a number of clustering algorithms that normally use a metric, do not actually rely on the metric properties (other than commutativity, but I think you'd have that here).  For example, DBSCAN uses epsilon-neighborhoods around a point; there is nothing in there that specifically says the triangle inequality matters.  So you can probably use DBSCAN, although you may have to do some kind of nonstandard spatial index to do efficient lookups in your case.  Your version of epsilon-neighborhood will likely be sim &gt; 1/epsilon rather than the other way around.  Same story with k-means and related algorithms.&#xD;&#xA;&#xD;&#xA;2. Can you construct a metric from your similarity?  One possibility: dist(ei, ej) = min( sim(ei, ek) + sim(ek, ej) ) for all k ...  Alternately, can you provide an upper  bound such that sim(ei, ej) &lt; sim(ei, ek) + sim(ek, ej) + d, for all k and some positive constant d?  Intuitively, large sim means closer together: is 1/sim metric-like?  What about 1/(sim + constant)?&#xD;&#xA;&#xD;&#xA;3. An alternate construction of a metric is to do an embedding.  As a first step, you can try to map your points ei -&gt; xi, such that xi minimize sum( abs( sim(ei, ej) - f( dist(xi, xj) ) ), for some suitable function f and metric dist.  The function f converts distance in the embedding to a similarity-like value; you'd have to experiment a bit, but 1/dist or exp^-dist are good starting points.  You'd also have to experiment on the best dimension for xi.  From there, you can use conventional clustering on xi.  The idea here is that you can almost (in a best fit sense) convert your distances in the embedding to similarity values, so they would cluster correctly.&#xD;&#xA;&#xD;&#xA;4. On the use of predefined parameters, all algorithms have some tuning.  DBSCAN can find the number of clusters, but you still need to give it some parameters.  In general, tuning requires multiple runs of the algorithm with different values for the tunable parameters, together with some function that evaluates goodness-of-clustering (either calculated separately, provided by the clustering algorithm itself, or just eyeballed :)  If the character of your data doesn't change, you can tune once and then use those fixed parameters; if it changes then you have to tune for each run.  You can find that out by tuning for each run and then comparing how well the parameters from one run work on another, compared to the parameters specifically tuned for that.&#xD;&#xA;" />
  <row Id="357" PostHistoryTypeId="5" PostId="133" RevisionGUID="b196fe2b-d065-4ad0-8a76-d2e354302de2" CreationDate="2014-05-18T09:17:15.557" UserId="26" Comment="added 113 characters in body" Text="1. I think a number of clustering algorithms that normally use a metric, do not actually rely on the metric properties (other than commutativity, but I think you'd have that here).  For example, DBSCAN uses epsilon-neighborhoods around a point; there is nothing in there that specifically says the triangle inequality matters.  So you can probably use DBSCAN, although you may have to do some kind of nonstandard spatial index to do efficient lookups in your case.  Your version of epsilon-neighborhood will likely be sim &gt; 1/epsilon rather than the other way around.  Same story with k-means and related algorithms.&#xD;&#xA;&#xD;&#xA;2. Can you construct a metric from your similarity?  One possibility: dist(ei, ej) = min( sim(ei, ek) + sim(ek, ej) ) for all k ...  Alternately, can you provide an upper  bound such that sim(ei, ej) &lt; sim(ei, ek) + sim(ek, ej) + d, for all k and some positive constant d?  Intuitively, large sim values means closer together: is 1/sim metric-like?  What about 1/(sim + constant)?  What about min( 1/sim(ei, ek) + 1/sim(ek, ej) ) for all k? (that last is guaranteed to be a metric, btw)&#xD;&#xA;&#xD;&#xA;3. An alternate construction of a metric is to do an embedding.  As a first step, you can try to map your points ei -&gt; xi, such that xi minimize sum( abs( sim(ei, ej) - f( dist(xi, xj) ) ), for some suitable function f and metric dist.  The function f converts distance in the embedding to a similarity-like value; you'd have to experiment a bit, but 1/dist or exp^-dist are good starting points.  You'd also have to experiment on the best dimension for xi.  From there, you can use conventional clustering on xi.  The idea here is that you can almost (in a best fit sense) convert your distances in the embedding to similarity values, so they would cluster correctly.&#xD;&#xA;&#xD;&#xA;4. On the use of predefined parameters, all algorithms have some tuning.  DBSCAN can find the number of clusters, but you still need to give it some parameters.  In general, tuning requires multiple runs of the algorithm with different values for the tunable parameters, together with some function that evaluates goodness-of-clustering (either calculated separately, provided by the clustering algorithm itself, or just eyeballed :)  If the character of your data doesn't change, you can tune once and then use those fixed parameters; if it changes then you have to tune for each run.  You can find that out by tuning for each run and then comparing how well the parameters from one run work on another, compared to the parameters specifically tuned for that.&#xD;&#xA;" />
  <row Id="358" PostHistoryTypeId="2" PostId="134" RevisionGUID="9b507165-272e-4eb6-aa7c-bae5c8394746" CreationDate="2014-05-18T12:03:21.650" UserId="227" Text="In our company, we have a MongoDB database containing a lot of unstructured data, on which we need to run map-reduce algorithms to generate reports and other analyses. We have two approaches to select from for implementing the required analyses:&#xD;&#xA;&#xD;&#xA; 1. One approach is to extract the data from MongoDB to a Hadoop cluster and do the analysis completely in Hadoop platform. However, this requires considerable investment on preparing the platform (software and hardware) and educating the team to work with Hadoop and write map-reduce tasks for it.&#xD;&#xA;&#xD;&#xA; 2. Another approach is to just put our effort on designing the map-reduce algorithms, and run the algorithms on MongoDB map-reduce functionalities. This way, we can create an initial prototype of final system that can generate the reports. I know that the MongoDB's map-reduce functionalities are much slower compared to Hadoop, but currently the data is not that big that makes this a bottleneck yet, at least not for the next six months.&#xD;&#xA;&#xD;&#xA;The question is, using the second approach and writing the algorithms for MongoDB, can them be later ported to Hadoop with little needed modification and algorithm redesign? MongoDB just supports JavaScript but programming language differences are easy to handle. However, is there any fundamental differences in the map-reduce model of MongoDB and Hadoop that may force us to redesign algorithms substantially for porting to Hadoop?" />
  <row Id="359" PostHistoryTypeId="1" PostId="134" RevisionGUID="9b507165-272e-4eb6-aa7c-bae5c8394746" CreationDate="2014-05-18T12:03:21.650" UserId="227" Text="Can map-reduce algorithms written for MongoDB be ported to Hadoop later?" />
  <row Id="360" PostHistoryTypeId="3" PostId="134" RevisionGUID="9b507165-272e-4eb6-aa7c-bae5c8394746" CreationDate="2014-05-18T12:03:21.650" UserId="227" Text="&lt;scalability&gt;&lt;hadoop&gt;&lt;map-reduce&gt;&lt;mongodb&gt;" />
  <row Id="361" PostHistoryTypeId="2" PostId="135" RevisionGUID="014f125f-755e-46c0-8e6f-bb777726e0ee" CreationDate="2014-05-18T12:30:06.853" UserId="227" Text="Considering another criteria, I think that in some cases using Python may be much superior to R for Big Data. I know the wide-spread use of R in data science educational materials and the good data analysis libraries available for it, but sometimes it just depend on the team.&#xD;&#xA;&#xD;&#xA;In my experience, for people already familiar with programming, using Python provides much more flexibility and productivity boost compared to a language like R, which is not as well-designed and powerful compared to Python in terms of a programming language. As an evidence, in a data mining course in my university, the best final project was written in Python, although the others has access to R's rich data analysis library. That is, sometimes the overall productivity (considering learning materials, documentation, etc.) for Python may be better than R even in the lack of special-purpose data analysis libraries for Python. Also, there are some good articles explaining the fast pace of Python in data science: [Python Displacing R][1] and [Rich Scientific Data Structures in Python][2] that may soon fill the gap of available libraries for R.&#xD;&#xA;&#xD;&#xA;Another important reason for not using R is when working with real world Big Data problems, contrary to academical only problems, there is much need for other tools and techniques, like data parsing, cleaning, visualization, web scrapping, and a lot of others that are much easier using a general purpose programming language. This may be why the default language used in many Hadoop courses (including the Udacity's [online course][3]) is Python.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://readwrite.com/2013/11/25/python-displacing-r-as-the-programming-language-for-data-science&#xD;&#xA;  [2]: http://wesmckinney.com/blog/?p=77&#xD;&#xA;  [3]: https://www.udacity.com/course/ud617" />
  <row Id="362" PostHistoryTypeId="2" PostId="136" RevisionGUID="76e5eeeb-f4b1-4d62-b4f9-5926d07e0f9d" CreationDate="2014-05-18T12:55:39.657" UserId="-1" Text="" />
  <row Id="363" PostHistoryTypeId="2" PostId="137" RevisionGUID="10239549-8176-469a-a3af-ca219aecf11c" CreationDate="2014-05-18T12:55:39.657" UserId="-1" Text="" />
  <row Id="365" PostHistoryTypeId="6" PostId="130" RevisionGUID="eda67480-4c12-4dbc-9db0-5dc9d422975d" CreationDate="2014-05-18T13:32:26.123" UserId="22" Comment="This question doesn't actually involve natural language processing." Text="&lt;dimensionality-reduction&gt;&lt;feature-selection&gt;" />
  <row Id="366" PostHistoryTypeId="24" PostId="130" RevisionGUID="eda67480-4c12-4dbc-9db0-5dc9d422975d" CreationDate="2014-05-18T13:32:26.123" Comment="Proposed by 22 approved by 122 edit id of 34" />
  <row Id="367" PostHistoryTypeId="2" PostId="138" RevisionGUID="ef42bc20-caf8-4c8f-b4c9-dbf915ca6aee" CreationDate="2014-05-18T14:02:51.350" UserId="84" Text="Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, everyone reaches out for C/C++. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).&#xD;&#xA;&#xD;&#xA;Of course such set of benefits would not come without a cost: writing the code, and sometimes even rewriting things that dozens of libraries have already implemented, can be quite expensive/tiresome. Although there are lots of implementations available, people are inclined to write the code by themselves whenever they need to *grant* performance. What *disables* performance assertions from using libraries while processing large databases?" />
  <row Id="368" PostHistoryTypeId="1" PostId="138" RevisionGUID="ef42bc20-caf8-4c8f-b4c9-dbf915ca6aee" CreationDate="2014-05-18T14:02:51.350" UserId="84" Text="Granting efficiency on BigData by rewriting C++ code" />
  <row Id="369" PostHistoryTypeId="3" PostId="138" RevisionGUID="ef42bc20-caf8-4c8f-b4c9-dbf915ca6aee" CreationDate="2014-05-18T14:02:51.350" UserId="84" Text="&lt;bigdata&gt;&lt;efficiency&gt;&lt;performance&gt;&lt;c++&gt;&lt;c&gt;" />
  <row Id="370" PostHistoryTypeId="2" PostId="139" RevisionGUID="3441d74f-46c8-4df4-a3ae-b7427bd72a75" CreationDate="2014-05-18T14:04:37.870" UserId="118" Text="I'd suggest [Apache Kafka][1] as message store and any stream processing solution of your choice like [Apache Camel][2] or [Twitter Storm][3]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://kafka.apache.org&#xD;&#xA;  [2]: https://camel.apache.org&#xD;&#xA;  [3]: https://github.com/apache/incubator-storm" />
  <row Id="371" PostHistoryTypeId="2" PostId="140" RevisionGUID="61a0191e-f112-4499-a474-3f023a96be0a" CreationDate="2014-05-18T14:30:10.553" UserId="118" Text="I've read very good [article][1] recently that suggests using [Twitter storm][2] for a task that looks pretty similar to yours.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/&#xD;&#xA;  [2]: https://github.com/nathanmarz/storm" />
  <row Id="372" PostHistoryTypeId="2" PostId="141" RevisionGUID="7cfc55a3-b990-4462-90fc-532252ce80c9" CreationDate="2014-05-18T14:36:16.350" UserId="-1" Text="" />
  <row Id="373" PostHistoryTypeId="2" PostId="142" RevisionGUID="b198f2ed-40b4-4942-b7c7-3adbabb32730" CreationDate="2014-05-18T14:36:16.350" UserId="-1" Text="" />
  <row Id="374" PostHistoryTypeId="2" PostId="143" RevisionGUID="39a2d7a7-772d-409d-8f68-36ef1dcb80ef" CreationDate="2014-05-18T14:37:20.477" UserId="229" Text="As All we know There are some data indexing techniques  using by well-known indexing apps like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc.&#xD;&#xA;&#xD;&#xA;For a No-Sql / OO Database (which i try to write/play a little around with C#) which technique you suggest ?&#xD;&#xA;&#xD;&#xA;I read about MurMurhash-2 and specially v3 comments say Murmur is very fast, also Lucene.Net has good comments..&#xD;&#xA;&#xD;&#xA;But what about their memory footprints in general?&#xD;&#xA;&#xD;&#xA;Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur?&#xD;&#xA;&#xD;&#xA;Or should i write a special index structure to get the best results?&#xD;&#xA;&#xD;&#xA;If i try to write my own, then is there any accepted scale for a good indexing some like 1% of data-node or 5% of data-node?&#xD;&#xA;&#xD;&#xA;Any useful Hint will be appreciated.&#xD;&#xA;&#xD;&#xA;Thanks from now..&#xD;&#xA;" />
  <row Id="375" PostHistoryTypeId="1" PostId="143" RevisionGUID="39a2d7a7-772d-409d-8f68-36ef1dcb80ef" CreationDate="2014-05-18T14:37:20.477" UserId="229" Text="What is the most efficient data indexing technique" />
  <row Id="376" PostHistoryTypeId="3" PostId="143" RevisionGUID="39a2d7a7-772d-409d-8f68-36ef1dcb80ef" CreationDate="2014-05-18T14:37:20.477" UserId="229" Text="&lt;nosql&gt;&lt;indexing&gt;&lt;data-indexing-techniques&gt;" />
  <row Id="377" PostHistoryTypeId="6" PostId="143" RevisionGUID="2018fad7-5c35-42a4-babd-4905526530b4" CreationDate="2014-05-18T14:58:05.690" UserId="229" Comment="edited tags" Text="&lt;bigdata&gt;&lt;nosql&gt;&lt;indexing&gt;&lt;data-indexing-techniques&gt;" />
  <row Id="378" PostHistoryTypeId="2" PostId="144" RevisionGUID="a6f4f52a-033d-4ac6-9874-9e2eec1e92f9" CreationDate="2014-05-18T14:58:34.853" UserId="-1" Text="" />
  <row Id="379" PostHistoryTypeId="2" PostId="145" RevisionGUID="6446ad63-f799-4559-be4a-bd3f10f3cb3e" CreationDate="2014-05-18T14:58:34.853" UserId="-1" Text="" />
  <row Id="380" PostHistoryTypeId="2" PostId="146" RevisionGUID="f6c5b35f-2b77-467d-a254-1149f8dbff4c" CreationDate="2014-05-18T15:01:24.080" UserId="-1" Text="" />
  <row Id="381" PostHistoryTypeId="2" PostId="147" RevisionGUID="f61d922c-2429-4143-a17a-84b70211fc74" CreationDate="2014-05-18T15:01:24.080" UserId="-1" Text="" />
  <row Id="382" PostHistoryTypeId="2" PostId="148" RevisionGUID="5f872d67-c07b-4130-9110-e9cd1a184583" CreationDate="2014-05-18T15:08:08.913" UserId="-1" Text="" />
  <row Id="383" PostHistoryTypeId="2" PostId="149" RevisionGUID="7fcde074-4b93-4867-99d1-9ac84d2e876f" CreationDate="2014-05-18T15:08:08.913" UserId="-1" Text="" />
  <row Id="384" PostHistoryTypeId="5" PostId="143" RevisionGUID="25f983f3-920b-4ad9-867f-43a6678822e4" CreationDate="2014-05-18T15:11:00.360" UserId="84" Comment="Fixed grammar, and improving formatting." Text="As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql/OO Database (which I try to write/play a little around with C#), which technique you suggest?&#xD;&#xA;&#xD;&#xA;I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?&#xD;&#xA;&#xD;&#xA;If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated." />
  <row Id="385" PostHistoryTypeId="6" PostId="143" RevisionGUID="25f983f3-920b-4ad9-867f-43a6678822e4" CreationDate="2014-05-18T15:11:00.360" UserId="84" Comment="Fixed grammar, and improving formatting." Text="&lt;bigdata&gt;&lt;nosql&gt;&lt;efficiency&gt;&lt;indexing&gt;" />
  <row Id="386" PostHistoryTypeId="24" PostId="143" RevisionGUID="25f983f3-920b-4ad9-867f-43a6678822e4" CreationDate="2014-05-18T15:11:00.360" Comment="Proposed by 84 approved by 229 edit id of 41" />
  <row Id="387" PostHistoryTypeId="6" PostId="143" RevisionGUID="9a7b9b70-e8b8-42bb-88f0-624b3f9e67ad" CreationDate="2014-05-18T15:17:37.167" UserId="229" Comment="edited tags" Text="&lt;bigdata&gt;&lt;nosql&gt;&lt;efficiency&gt;&lt;indexing&gt;&lt;data-indexing-techniques&gt;" />
  <row Id="388" PostHistoryTypeId="6" PostId="76" RevisionGUID="2cfaeaf8-6e0b-4b19-b54c-6c847299f712" CreationDate="2014-05-18T15:18:08.050" UserId="118" Comment="retagged post" Text="&lt;bigdata&gt;&lt;tools&gt;&lt;data-stream-mining&gt;" />
  <row Id="389" PostHistoryTypeId="24" PostId="76" RevisionGUID="2cfaeaf8-6e0b-4b19-b54c-6c847299f712" CreationDate="2014-05-18T15:18:08.050" Comment="Proposed by 118 approved by 158 edit id of 37" />
  <row Id="393" PostHistoryTypeId="2" PostId="151" RevisionGUID="8d5c0b8c-d82f-454e-b048-afa247a64b3a" CreationDate="2014-05-18T15:34:16.437" UserId="-1" Text="" />
  <row Id="394" PostHistoryTypeId="2" PostId="152" RevisionGUID="8c55f680-9d4a-47c6-8b8e-8885f5cbc541" CreationDate="2014-05-18T15:34:16.437" UserId="-1" Text="" />
  <row Id="395" PostHistoryTypeId="5" PostId="143" RevisionGUID="153292cc-3e2c-4760-af2b-4d4ec6c1f59d" CreationDate="2014-05-18T15:36:24.210" UserId="229" Comment="added 15 characters in body" Text="As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql / Object Oriented Database (which I try to write/play a little around with C#), which technique you suggest?&#xD;&#xA;&#xD;&#xA;I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?&#xD;&#xA;&#xD;&#xA;If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated." />
  <row Id="396" PostHistoryTypeId="5" PostId="129" RevisionGUID="4ad8fdfc-eb51-44bf-9711-c707da2467f6" CreationDate="2014-05-18T15:40:10.727" UserId="84" Comment="Improving formatting." Text="[This question](http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm) asks about generative vs. discriminative algorithm, but can someone give an example of the difference between these forms when applied to Natural Language Processing? **How are generative and discriminative models used in NLP?**" />
  <row Id="397" PostHistoryTypeId="24" PostId="129" RevisionGUID="4ad8fdfc-eb51-44bf-9711-c707da2467f6" CreationDate="2014-05-18T15:40:10.727" Comment="Proposed by 84 approved by 122 edit id of 46" />
  <row Id="399" PostHistoryTypeId="5" PostId="125" RevisionGUID="cea45259-df79-4e98-ae79-5427ae6d468b" CreationDate="2014-05-18T16:10:50.820" UserId="84" Comment="Fixed grammar, and improving formatting." Text="I want learn about NoSQL and when is better to use SQL or NoSQL. I know that this question depends on the case, but I'm asking for a good documentation on NoSQL, and some explanation of when is better to use SQL or NoSQL (use cases, etc). Also, your opinions on NoSQL databases, and any recommendations for learning about this topic are welcome." />
  <row Id="400" PostHistoryTypeId="4" PostId="125" RevisionGUID="cea45259-df79-4e98-ae79-5427ae6d468b" CreationDate="2014-05-18T16:10:50.820" UserId="84" Comment="Fixed grammar, and improving formatting." Text="How to learn noSQL databases and how to know when SQL or noSQL is better" />
  <row Id="401" PostHistoryTypeId="24" PostId="125" RevisionGUID="cea45259-df79-4e98-ae79-5427ae6d468b" CreationDate="2014-05-18T16:10:50.820" Comment="Proposed by 84 approved by 26, 109 edit id of 31" />
  <row Id="403" PostHistoryTypeId="2" PostId="153" RevisionGUID="55e55168-ffe9-4529-91e6-292f5bf3b276" CreationDate="2014-05-18T17:38:01.383" UserId="84" Text="The answers presented so far are very nice, but I was also expecting an emphasis on a particular difference between parallel and distributed processing: the code executed. Considering parallel processes, the code executed is the same, regardless of the level of parallelism (instruction, data, task). You write a *single code*, and it will be executed by different threads/processors, e.g., while computing matrices products, or generating permutations.&#xD;&#xA;&#xD;&#xA;On the other hand, distributed computing involves the execution of different algorithms/programs at the same time in different processors (from one or more machines). Such computations are later merged into a intermediate/final results by using the available means of data communication/synchronization (shared memory, network). Further, distributed computing is very appealing for BigData processing, as it allows for exploiting disk parallelism (usually the bottleneck for large databases).&#xD;&#xA;&#xD;&#xA;Finally, for the level of parallelism, it may be taken rather as a constraint on the synchronization. For example, in GPGPU, which is single-instruction multiple-data (SIMD), the parallelism occurs by having different inputs for a single instruction, each pair *(data_i, instruction)* being executed by a different thread. Such is the restraint that, in case of divergent branches, it is necessary to discard lots of unnecessary computations, until the threads reconverge. For CPU threads, though, they commonly diverge; yet, one may use synchronization structures to grant concurrent execution of specific sections of the code." />
  <row Id="405" PostHistoryTypeId="2" PostId="154" RevisionGUID="a61ca912-f857-4806-9576-5b766d47d6c4" CreationDate="2014-05-18T17:53:37.750" UserId="229" Text="Check [Martin Fowler's Personal website][1] He write good, and specially answer of your question One of his book : &quot;NoSQL Distilled&quot; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.martinfowler.com" />
  <row Id="406" PostHistoryTypeId="2" PostId="155" RevisionGUID="00d62c57-1a00-4d59-b4fb-d764f8f8ad27" CreationDate="2014-05-18T18:45:38.957" UserId="227" Text="One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metric from various sources for making higher level analysis. Looking at the other people's effort, especially other questions on this site, it appears that many people in this field are doing somehow repetitive works. For example, analyzing tweets, Facebook posts, Wikipedia articles, etc. is somehow part lots of Big Data problems.&#xD;&#xA;&#xD;&#xA;Some of these data sets are accessible using public APIs provided by the provider site, but usually some valuable calculated values are missing from those API outputs, and everyone has to do somehow same analyses over and over again. For example, although clustering users may depend on different use casesa and selection of features, but having a base clustering of twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API, nor available publicly in independent data sets.&#xD;&#xA;&#xD;&#xA;Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other Big Data problems? I mean something like GitHub (or a group of sites/public data sets or at least a comprehensive listing) for the data science. If not, what are the reasons of not having such a platform for data science? Commercial values of data, need to frequent update data sets, ...? Can not a open-source model for sharing data sets devised for data scientists?" />
  <row Id="407" PostHistoryTypeId="1" PostId="155" RevisionGUID="00d62c57-1a00-4d59-b4fb-d764f8f8ad27" CreationDate="2014-05-18T18:45:38.957" UserId="227" Text="Publicly available datasets" />
  <row Id="408" PostHistoryTypeId="3" PostId="155" RevisionGUID="00d62c57-1a00-4d59-b4fb-d764f8f8ad27" CreationDate="2014-05-18T18:45:38.957" UserId="227" Text="&lt;open-source&gt;&lt;dataset&gt;" />
  <row Id="411" PostHistoryTypeId="5" PostId="155" RevisionGUID="ea894b83-4a12-42c1-afc1-b72597a2320d" CreationDate="2014-05-18T19:12:38.843" UserId="227" Comment="edited body" Text="One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metric from various sources for making higher level analysis. Looking at the other people's effort, especially other questions on this site, it appears that many people in this field are doing somehow repetitive works. For example, analyzing tweets, Facebook posts, Wikipedia articles, etc. is somehow part lots of Big Data problems.&#xD;&#xA;&#xD;&#xA;Some of these data sets are accessible using public APIs provided by the provider site, but usually some valuable calculated values are missing from those API outputs, and everyone has to do somehow same analyses over and over again. For example, although clustering users may depend on different use cases and selection of features, but having a base clustering of Twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API, nor available publicly in independent data sets.&#xD;&#xA;&#xD;&#xA;Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other Big Data problems? I mean something like GitHub (or a group of sites/public data sets or at least a comprehensive listing) for the data science. If not, what are the reasons of not having such a platform for data science? Commercial value of data, need to frequently update data sets, ...? Can not a open-source model for sharing data sets devised for data scientists?" />
  <row Id="412" PostHistoryTypeId="2" PostId="156" RevisionGUID="f7a4357c-8d2f-4c77-b676-b707d58da289" CreationDate="2014-05-18T19:19:44.240" UserId="118" Text="[Freebase][1] is a free community driven database that spans many interesting topics and contains about 2,5 billion facts in machine readable format. It is also have good API to perform data queries.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.freebase.com" />
  <row Id="413" PostHistoryTypeId="2" PostId="157" RevisionGUID="cda8d654-a386-4a69-bc89-324580315738" CreationDate="2014-05-18T19:22:05.160" UserId="36" Text="R is great for &quot;big data&quot;! However, you need a workflow since R is limited (with some simplification) by the amount of RAM in the operating system. The approach I take is to interact with a relational database (see the `RSQLite` package for creating and interacting with a SQLite databse), run SQL-style queries to understand the structure of the data, and then extract particular subsets of the data for computationally-intensive statistical analysis.&#xD;&#xA;&#xD;&#xA;This just one approach, however: there are packages that allow you to interact with other databases (e.g., Monet) or run analyses in R with fewer memory limitations (e.g., see `pbdR`)." />
  <row Id="414" PostHistoryTypeId="2" PostId="158" RevisionGUID="8bf75579-956c-4bce-acbd-eb2eb5b07a48" CreationDate="2014-05-18T19:29:53.530" UserId="84" Text="There is, in fact, a very reasonable list of publicly-available datasets, supported by different enterprises/sources. Here are some of them:&#xD;&#xA;&#xD;&#xA;- [Public Datasets on Amazon WebServices](http://aws.amazon.com/publicdatasets/);&#xD;&#xA;- [Frequent Itemset Mining Implementation Repository](http://fimi.ua.ac.be/data/);&#xD;&#xA;- [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.html);&#xD;&#xA;- [KDnuggets](http://www.kdnuggets.com/datasets/index.html) -- big list of lots of public repositories.&#xD;&#xA;&#xD;&#xA;Now, two considerations on your question. First one, regarding policies of database sharing. From personal experience, there are some databases that can't be made publicly available, either for involving privacy restraints (as for some social network informations), or for concerning government information (like health system databases).&#xD;&#xA;&#xD;&#xA;Another point concerns the usage/application of the dataset. Although some bases can be reprocessed to suit the needs of the application, it would be great to have some *nice organization* of the datasets by purpose. The *taxonomy* should involve social graph analysis, itemset mining, classification, and lots of other reasearch areas there may be." />
  <row Id="415" PostHistoryTypeId="5" PostId="102" RevisionGUID="29eced39-f07a-41a4-8ce7-b94f6c2b06c9" CreationDate="2014-05-18T19:41:19.157" UserId="229" Comment="correct spellings, added a new thought-related tag" Text="What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users' historical records (like app purchasing) and analytics of usage behavior." />
  <row Id="416" PostHistoryTypeId="4" PostId="102" RevisionGUID="29eced39-f07a-41a4-8ce7-b94f6c2b06c9" CreationDate="2014-05-18T19:41:19.157" UserId="229" Comment="correct spellings, added a new thought-related tag" Text="What is the Best NoSQL backend for a mobile game" />
  <row Id="417" PostHistoryTypeId="6" PostId="102" RevisionGUID="29eced39-f07a-41a4-8ce7-b94f6c2b06c9" CreationDate="2014-05-18T19:41:19.157" UserId="229" Comment="correct spellings, added a new thought-related tag" Text="&lt;nosql&gt;&lt;performance&gt;" />
  <row Id="418" PostHistoryTypeId="24" PostId="102" RevisionGUID="29eced39-f07a-41a4-8ce7-b94f6c2b06c9" CreationDate="2014-05-18T19:41:19.157" Comment="Proposed by 229 approved by 199 edit id of 47" />
  <row Id="419" PostHistoryTypeId="5" PostId="138" RevisionGUID="536423ab-0931-4b92-800a-8dafa6cbe438" CreationDate="2014-05-18T19:44:39.420" UserId="84" Comment="added 253 characters in body" Text="Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, everyone reaches out for C/C++. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).&#xD;&#xA;&#xD;&#xA;Of course such set of benefits would not come without a cost: writing the code, and sometimes even rewriting things that dozens of libraries have already implemented, can be quite expensive/tiresome. Although there are lots of implementations available, people are inclined to write the code by themselves whenever they need to *grant* performance. What *disables* performance assertions from using libraries while processing large databases?&#xD;&#xA;&#xD;&#xA;**Edit**: In an attempt to clarify the post, here is a more succint/direct question:&#xD;&#xA;&#xD;&#xA;- What makes writing the code by oneself a *guarantee* of performance?&#xD;&#xA;- Why is it *risky* to count on a library implementation when you must **assure** high performance?" />
  <row Id="420" PostHistoryTypeId="2" PostId="159" RevisionGUID="c5683904-1635-432d-abd6-73f1bfccb73a" CreationDate="2014-05-18T19:46:44.653" UserId="199" Text="I see a lot of courses in Data Science emerging in the last 2 years. Even big universities like Stanford and Columbia offers MS specifically in Data Science. But as long as I see, it looks like data science is just a mix of computer science and statistics techniques.&#xD;&#xA;So I always think about this. If it is just a trend and if in 10 years from now, someone will still mention Data Science as an entire field or just a subject/topic inside CS or stats.&#xD;&#xA;What do you think?" />
  <row Id="421" PostHistoryTypeId="1" PostId="159" RevisionGUID="c5683904-1635-432d-abd6-73f1bfccb73a" CreationDate="2014-05-18T19:46:44.653" UserId="199" Text="Is Data Science just a trend or is a long term concept?" />
  <row Id="422" PostHistoryTypeId="3" PostId="159" RevisionGUID="c5683904-1635-432d-abd6-73f1bfccb73a" CreationDate="2014-05-18T19:46:44.653" UserId="199" Text="&lt;bigdata&gt;&lt;machine-learning&gt;&lt;databases&gt;&lt;statistics&gt;&lt;education&gt;" />
  <row Id="423" PostHistoryTypeId="2" PostId="160" RevisionGUID="384cfbcf-0580-4bd6-88fe-8f596503483e" CreationDate="2014-05-18T21:05:28.990" UserId="156" Text="The one thing that you can say for sure is: Nobody can say this for sure. And it might indeed be opinion-based to some extent. The introduction of terms like &quot;Big Data&quot; that some people consider as &quot;hypes&quot; or &quot;buzzwords&quot; don't make it easier to flesh out an appropriate answer here. But I'll try. &#xD;&#xA;&#xD;&#xA;In general, interdisciplinary fields often seem to have the problem of not being taken serious by either of the fields they are spanning. However, the more research is invested into a particular field, the greater is the urge to split this field into several sub-topics. And these sub-topics sonner of later have to be re-combined in new ways, in order to prevent an overspecialization, and to increase and broaden the applicability of techniques that are developed by the (over?)specialized experts in the different fields. &#xD;&#xA;&#xD;&#xA;And I consider &quot;Data Science&quot; as such an approach to combine the expertise and findings from different fields. You described it as&#xD;&#xA;&gt; ...a mix of computer science and statistics techniques&#xD;&#xA;&#xD;&#xA;And indeed, several questions here aim at the differentiation between data science and statistics. But a pure statistician will most likely not be able to set up a Hadoop cluster and show the results of his analysis in an interactive HTML5 dashboard. And someone who can implement a nice HTML5 dashboard might not be so familiar with the mathematical background of a Chi-Squared-Test. &#xD;&#xA;&#xD;&#xA;It is reasonable to assume that giving students enough knowledge to *apply* the most important techniques from the different fields that are covered by data science will lead to new applications of these techniques, and be beneficial - also for the &quot;purists&quot; in these fields. The combination of these techniques is not straightforward in many cases, and can justify an own branch of research. &#xD;&#xA;&#xD;&#xA;You also asked whether in 10 years, data science will be considered as &quot;just a topic inside computer science&quot;. Again: Nobody can say for sure. But I wonder at which point people stopped asking the question whether &quot;Computer Science&quot; will one day only be considered only as a mix of (or a subject of) Electrical Engineering and Mathematics...&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="424" PostHistoryTypeId="2" PostId="161" RevisionGUID="b6b733bd-f282-4e1e-a8bb-1a517d2c4e42" CreationDate="2014-05-18T21:25:29.040" UserId="21" Text="I don't think that everyone reaches for C/C++ when performance is an issue.&#xD;&#xA;&#xD;&#xA;The advantage to writing low-level code is using fewer CPU cycles, or sometimes, less memory. But I'd note that higher-level languages can call down to lower-level languages, and do, to get some of this value. Python and JVM languages can do this.&#xD;&#xA;&#xD;&#xA;The data scientist using, for example, scikit-learn on her desktop is already calling heavily optimized native routines to do the number crunching. There is no point in writing new code for speed.&#xD;&#xA;&#xD;&#xA;In the distributed &quot;big data&quot; context, you are more typically bottleneck on data movement: network transfer and I/O. Native code does not help. What helps is not writing the same code to run faster, but writing smarter code.&#xD;&#xA;&#xD;&#xA;Higher-level languages are going to let you implement more sophisticated distributed algorithms in a given amount of developer time than C/C++. At scale, the smarter algorithm with better data movement will beat dumb native code.&#xD;&#xA;&#xD;&#xA;It's also usually true that developer time, and bugs, cost loads more than new hardware. A year of a senior developer's time might be $200K fully loaded; over a year that also rents hundreds of servers worth of computation time. It may just not make sense in most cases to bother optimizing over throwing more hardware at it.&#xD;&#xA;&#xD;&#xA;_I don't understand the follow up about &quot;grant&quot; and &quot;disable&quot; and &quot;assert&quot;?_" />
  <row Id="425" PostHistoryTypeId="2" PostId="162" RevisionGUID="8fe23961-09ef-4cff-9f16-325e6280354e" CreationDate="2014-05-18T22:16:19.300" UserId="59" Text="There are many openly available data sets, one many people often overlook is [data.gov][1]. As mentioned previously Freebase is great, so are all the examples posted by @Rubens&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.data.gov/" />
  <row Id="426" PostHistoryTypeId="2" PostId="163" RevisionGUID="e57d83d8-1c28-4c49-950c-6c4abb32a966" CreationDate="2014-05-18T23:21:07.220" UserId="229" Text="As all we know, in Digital world there are many ways to do the same work / get expected results..&#xD;&#xA;&#xD;&#xA;And responsibilities / risks which comes from the code are on developers' shoulders..&#xD;&#xA;&#xD;&#xA;This is small but i guess a very useful example from .NET world..&#xD;&#xA;&#xD;&#xA;So Many .NET developers use the built-in BinaryReader - BinaryWriter on their data serialization for performance / get control over the process..&#xD;&#xA;&#xD;&#xA;This is CSharp source code of the FrameWork's built in BinaryWriter class' one of the overloaded Write Methods :&#xD;&#xA;&#xD;&#xA;   &#xD;&#xA;&#xD;&#xA;    // Writes a boolean to this stream. A single byte is written to the stream&#xD;&#xA;    // with the value 0 representing false or the value 1 representing true.&#xD;&#xA;    // &#xD;&#xA;    public virtual void Write(bool value) &#xD;&#xA;    {&#xD;&#xA;         //_buffer is a byte array which declared in ctor / init codes of the class&#xD;&#xA;        _buffer = ((byte) (value? 1:0));&#xD;&#xA;&#xD;&#xA;        //OutStream is the stream instance which BinaryWriter Writes the value(s) into it.&#xD;&#xA;        OutStream.WriteByte(_buffer[0]);&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;As you see, this method could written without no extra assigning :&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    public virtual void Write(bool value) &#xD;&#xA;    {&#xD;&#xA;        OutStream.WriteByte((byte) (value ? 1 : 0));&#xD;&#xA;    }&#xD;&#xA;        &#xD;&#xA;Without assigning we could gain few milliseconds..This few milliseconds can accept as &quot;almost nothing&quot; but what if there are multi-thousands of writing (i.e. in a server process)?&#xD;&#xA;&#xD;&#xA;Lets suppose that &quot;few&quot; is 2 (milliseconds) and multi-Thousands instances are only 2.000..&#xD;&#xA;This means 4 seconds more process time..4 seconds later returning..&#xD;&#xA;&#xD;&#xA;If we continue to subject from .NET and if you can check the source codes of BCL - .NET Base Class Library- from MSDN you can see a lot of performance losts from the developer decides..&#xD;&#xA;&#xD;&#xA;Any of the point from BCL source It's normal that you see developer decided to use while() or foreach() loops which could implement a faster for() loop in their code.&#xD;&#xA;&#xD;&#xA;This small gains give us the total performance..&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;And if we return to the BinaryWriter.Write() Method..&#xD;&#xA;&#xD;&#xA;Actually extra assigning to a _buffer implementation is not a developer fault..This is exactly decide to &quot;stay in safe&quot; !&#xD;&#xA;&#xD;&#xA;Suppose that we decide to not use _buffer and decided to implement the second method..If we try to send multi-thousands bytes over a wire (i.e. upload / download a BLOB or CLOB data) with the second method, it can fail commonly because of connection lost..Cause we try to send all data without any checks and controlling mechanism.When connection lost, Both the server and Client never know the sent data completed or not.&#xD;&#xA;&#xD;&#xA;If the developer decides &quot;stay in safe&quot; then normally it means performance costs depends to implemented &quot;stay in safe&quot; mechanism(s).&#xD;&#xA;&#xD;&#xA;But if the developer decides &quot;get risky, gain performance&quot; this is not a fault also..Till there are some discussions about &quot;risky&quot; coding.&#xD;&#xA;&#xD;&#xA;And as a small note : Commercial library developers always try to stay in safe because they can't know where their code will use.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="427" PostHistoryTypeId="5" PostId="163" RevisionGUID="b7ef4714-758f-41b1-a87b-316c90af77a6" CreationDate="2014-05-18T23:29:39.227" UserId="229" Comment="added 20 characters in body" Text="As all we know, in Digital world there are many ways to do the same work / get expected results..&#xD;&#xA;&#xD;&#xA;And responsibilities / risks which comes from the code are on developers' shoulders..&#xD;&#xA;&#xD;&#xA;This is small but i guess a very useful example from .NET world..&#xD;&#xA;&#xD;&#xA;So Many .NET developers use the built-in BinaryReader - BinaryWriter on their data serialization for performance / get control over the process..&#xD;&#xA;&#xD;&#xA;This is CSharp source code of the FrameWork's built in BinaryWriter class' one of the overloaded Write Methods :&#xD;&#xA;&#xD;&#xA;   &#xD;&#xA;&#xD;&#xA;    // Writes a boolean to this stream. A single byte is written to the stream&#xD;&#xA;    // with the value 0 representing false or the value 1 representing true.&#xD;&#xA;    // &#xD;&#xA;    public virtual void Write(bool value) &#xD;&#xA;    {&#xD;&#xA;         //_buffer is a byte array which declared in ctor / init codes of the class&#xD;&#xA;        _buffer = ((byte) (value? 1:0));&#xD;&#xA;&#xD;&#xA;        //OutStream is the stream instance which BinaryWriter Writes the value(s) into it.&#xD;&#xA;        OutStream.WriteByte(_buffer[0]);&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;As you see, this method could written without the extra assigning to _buffer variable:&#xD;&#xA;&#xD;&#xA;    &#xD;&#xA;&#xD;&#xA;    public virtual void Write(bool value) &#xD;&#xA;    {&#xD;&#xA;        OutStream.WriteByte((byte) (value ? 1 : 0));&#xD;&#xA;    }&#xD;&#xA;        &#xD;&#xA;Without assigning we could gain few milliseconds..This few milliseconds can accept as &quot;almost nothing&quot; but what if there are multi-thousands of writing (i.e. in a server process)?&#xD;&#xA;&#xD;&#xA;Lets suppose that &quot;few&quot; is 2 (milliseconds) and multi-Thousands instances are only 2.000..&#xD;&#xA;This means 4 seconds more process time..4 seconds later returning..&#xD;&#xA;&#xD;&#xA;If we continue to subject from .NET and if you can check the source codes of BCL - .NET Base Class Library- from MSDN you can see a lot of performance losts from the developer decides..&#xD;&#xA;&#xD;&#xA;Any of the point from BCL source It's normal that you see developer decided to use while() or foreach() loops which could implement a faster for() loop in their code.&#xD;&#xA;&#xD;&#xA;This small gains give us the total performance..&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;And if we return to the BinaryWriter.Write() Method..&#xD;&#xA;&#xD;&#xA;Actually extra assigning to a _buffer implementation is not a developer fault..This is exactly decide to &quot;stay in safe&quot; !&#xD;&#xA;&#xD;&#xA;Suppose that we decide to not use _buffer and decided to implement the second method..If we try to send multi-thousands bytes over a wire (i.e. upload / download a BLOB or CLOB data) with the second method, it can fail commonly because of connection lost..Cause we try to send all data without any checks and controlling mechanism.When connection lost, Both the server and Client never know the sent data completed or not.&#xD;&#xA;&#xD;&#xA;If the developer decides &quot;stay in safe&quot; then normally it means performance costs depends to implemented &quot;stay in safe&quot; mechanism(s).&#xD;&#xA;&#xD;&#xA;But if the developer decides &quot;get risky, gain performance&quot; this is not a fault also..Till there are some discussions about &quot;risky&quot; coding.&#xD;&#xA;&#xD;&#xA;And as a small note : Commercial library developers always try to stay in safe because they can't know where their code will use.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="428" PostHistoryTypeId="2" PostId="164" RevisionGUID="14034850-865c-45c4-8ab0-3dd82d45855f" CreationDate="2014-05-19T00:09:17.900" UserId="208" Text="What @Clayton posted seems about right to me, for those terms, and for &quot;data mining&quot; being one tool of the data scientist. However, I haven't really used the term &quot;data collection,&quot; and it doesn't strike me as synonymous with &quot;data mining.&quot;&#xD;&#xA;&#xD;&#xA;My own answer to your question: **no**, the terms aren't the same. Definitions may be loose in this field, but I haven't seen those terms used interchangeably. In my work, we sometimes use them to differentiate between goals, or methodologies. For us, [tag:data-science] is more about testing a hypothesis, and typically the data have been collected just for that purpose. [tag:data-mining] is more about sifting through existing data, looking for structure, and perhaps generating hypotheses. Data mining can start with a hypothesis, but it's often very weak or general, and can be difficult to resolve with confidence. (Dig long enough and you'll find *something*, though it may turn out to be pyrite.)&#xD;&#xA;&#xD;&#xA;However, we also have used &quot;data science&quot; as a wider term, to include &quot;data mining.&quot; We also talk about &quot;data modeling,&quot; which for us is about finding a model for a system of interest, based on data as well as other knowledge and objectives. Sometimes that means trying to find the math that explains the real system, and sometimes it means finding a predictive model that is good enough for a purpose." />
  <row Id="429" PostHistoryTypeId="6" PostId="107" RevisionGUID="0b5a80cf-c9d0-4ae4-b424-917a23944beb" CreationDate="2014-05-19T07:33:50.080" UserId="118" Comment="retagged post" Text="&lt;tools&gt;&lt;data-stream-mining&gt;" />
  <row Id="430" PostHistoryTypeId="24" PostId="107" RevisionGUID="0b5a80cf-c9d0-4ae4-b424-917a23944beb" CreationDate="2014-05-19T07:33:50.080" Comment="Proposed by 118 approved by 26, 200 edit id of 38" />
  <row Id="431" PostHistoryTypeId="5" PostId="135" RevisionGUID="3f021d76-d7ca-4a16-bf87-5b105e201c3e" CreationDate="2014-05-19T08:13:05.037" UserId="227" Comment="added 314 characters in body" Text="Considering another criteria, I think that in some cases using Python may be much superior to R for Big Data. I know the wide-spread use of R in data science educational materials and the good data analysis libraries available for it, but sometimes it just depend on the team.&#xD;&#xA;&#xD;&#xA;In my experience, for people already familiar with programming, using Python provides much more flexibility and productivity boost compared to a language like R, which is not as well-designed and powerful compared to Python in terms of a programming language. As an evidence, in a data mining course in my university, the best final project was written in Python, although the others has access to R's rich data analysis library. That is, sometimes the overall productivity (considering learning materials, documentation, etc.) for Python may be better than R even in the lack of special-purpose data analysis libraries for Python. Also, there are some good articles explaining the fast pace of Python in data science: [Python Displacing R][1] and [Rich Scientific Data Structures in Python][2] that may soon fill the gap of available libraries for R.&#xD;&#xA;&#xD;&#xA;Another important reason for not using R is when working with real world Big Data problems, contrary to academical only problems, there is much need for other tools and techniques, like data parsing, cleaning, visualization, web scrapping, and a lot of others that are much easier using a general purpose programming language. This may be why the default language used in many Hadoop courses (including the Udacity's [online course][3]) is Python.&#xD;&#xA;&#xD;&#xA;**Edit:**&#xD;&#xA;&#xD;&#xA;Recently DARPA has also invested $3 million to help fund Python's data processing and visualization capabilities for big data jobs, which is clearly a sign of Python's future in Big Data. ([details][4])&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://readwrite.com/2013/11/25/python-displacing-r-as-the-programming-language-for-data-science&#xD;&#xA;  [2]: http://wesmckinney.com/blog/?p=77&#xD;&#xA;  [3]: https://www.udacity.com/course/ud617&#xD;&#xA;  [4]: http://www.computerworld.com/s/article/9236558/Python_gets_a_big_data_boost_from_DARPA" />
  <row Id="432" PostHistoryTypeId="2" PostId="165" RevisionGUID="6af5a465-e3b6-4d8b-bf0e-2a4eb8fee574" CreationDate="2014-05-19T08:46:47.683" UserId="108" Text="I think you messed up some things in your question. Lucene (I know nothing about Lucene,NET, but I suppose is the same) is a library used to analyze, split in tokens, and store documents in order to be able to query and retrieve them later. Lucene has a pretty old but effective model, it uses inverted trees to find and retrieve documents. Without further details, all documents are split in tokens (terms), and for each term is maintained a data structure, which stores all the documents which contains the given term. As a data structure could be used a BTree, a hash table and in the latest major revisions you can even plug in your own data structures.&#xD;&#xA;&#xD;&#xA;A BTree (see [Wikipedia page][1] for further details), is a kind of a tree data structure, which is appropriate for working with big chunks of data and is often used for storing tree-like ordered structures on disk. For in-memory other trees performs better.&#xD;&#xA;&#xD;&#xA;Murmur hash (see [Wikipedia page][2] for further details), is a family of hash functions used in hash table. The implementation of the hash table is not important, it could be a standard chained implementation or more advanced open hash addressing scheme. The idea is that the hash tables allows one to get fast a key, from an unordered set of keys, and can answer to tasks like: is this key part of this set of keys? which is the value associated with this key? &#xD;&#xA;&#xD;&#xA;Now back to your main problem. You have one library (Lucene) and to data structures, both data structures are used in Lucene. Now you see that it is not possible to answer your question in these terms since they are not comparable.&#xD;&#xA;&#xD;&#xA;However, regarding you footprint and performance part of the question. First of all you have to know which kind of operations you need to implement. &#xD;&#xA;&#xD;&#xA;*Do you need only get value for key, or do you need to find all elements in a range? In other words do you need order or not?* If you do, than a tree can help. If you do not, than a hash table, which is faster could be used instead. &#xD;&#xA;&#xD;&#xA;*Do you have a lot of data which does not fit the memory?* If yes than a disk-based solution would help (like BTree). If your data fit the memory, than use the fastest in-memory solution and use disk only as a storage (with a different structure, much simpler).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/B-tree&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/MurmurHash" />
  <row Id="433" PostHistoryTypeId="10" PostId="159" RevisionGUID="2baf6166-9101-45e9-b28c-2ecfec1e3b6e" CreationDate="2014-05-19T08:54:23.303" UserId="-1" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:84,&quot;DisplayName&quot;:&quot;Rubens&quot;},{&quot;Id&quot;:59,&quot;DisplayName&quot;:&quot;MCP_infiltrator&quot;},{&quot;Id&quot;:63,&quot;DisplayName&quot;:&quot;Puffin GDI&quot;},{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;},{&quot;Id&quot;:108,&quot;DisplayName&quot;:&quot;rapaio&quot;}]}" />
  <row Id="434" PostHistoryTypeId="2" PostId="166" RevisionGUID="b04f82bd-73bd-41a8-ba33-2de13597e991" CreationDate="2014-05-19T11:13:48.067" UserId="21" Text="Let's say you are predicting the topic of a document given its words.&#xD;&#xA;&#xD;&#xA;A generative model describes how likely each topic is, and how likely words are given the topic. This is how it says documents are actually &quot;generated&quot; by the world -- a topic arises according to some distribution, words arise because of the topic, you have a document. Classifying documents of words W into topic T is a matter of maximizing the joint likelihood: P(T,W) = P(W|T)P(T)&#xD;&#xA;&#xD;&#xA;A discriminative model operates by only describing how likely a topic is given the words. It says nothing about how likely the words or topic are by themselves. The task is to model P(T|W) directly and find the T that maximizes this. These approaches do not care about P(T) or P(W) directly." />
  <row Id="435" PostHistoryTypeId="6" PostId="143" RevisionGUID="74927709-0a38-49be-a78c-8b81c9a82134" CreationDate="2014-05-19T12:05:13.513" UserId="229" Comment="edited tags" Text="&lt;nosql&gt;&lt;efficiency&gt;&lt;indexing&gt;&lt;data-indexing-techniques&gt;&lt;.net&gt;" />
  <row Id="436" PostHistoryTypeId="2" PostId="167" RevisionGUID="1d4cf600-9143-4e0d-82c4-c8c0fe2cb89b" CreationDate="2014-05-19T12:17:45.960" UserId="-1" Text="" />
  <row Id="437" PostHistoryTypeId="2" PostId="168" RevisionGUID="f3625077-630c-48cd-a3d2-200272e60ffb" CreationDate="2014-05-19T12:17:45.960" UserId="-1" Text="" />
  <row Id="438" PostHistoryTypeId="2" PostId="169" RevisionGUID="bb30a00b-b3f7-4658-85af-53562b266c8b" CreationDate="2014-05-19T15:55:24.983" UserId="113" Text="Assume a set of loosely structured data (e.g. Web tables/Linked Open Data), composed of many data sources. There is no common schema followed by the data and each source can use synonym attributes to describe the values (e.g. &quot;nationality&quot; vs &quot;bornIn&quot;). &#xD;&#xA;&#xD;&#xA;My goal is to find some &quot;important&quot; attributes that somehow &quot;define&quot; the entities that they describe. So, when I find the same value for such an attribute, I will know that the two descriptions are most likely about the same entity (e.g. the same person).&#xD;&#xA;&#xD;&#xA;For example, the attribute &quot;lastName&quot; is more discriminative than the attribute &quot;nationality&quot;. &#xD;&#xA;&#xD;&#xA;**How could I (statistically) find such attributes that are more important than others?**&#xD;&#xA;&#xD;&#xA;A naive solution would be to take the average IDF of the values of each attribute and make this the &quot;importance&quot; factor of the attribute. A similar approach would be to count how many distinct values appear for each attribute.&#xD;&#xA;&#xD;&#xA;I have seen the term feature, or attribute selection in machine learning, but I don't want to discard the remaining attributes, I just want to put higher weights to the most important ones. I am sure the machine learning community has much to offer to me in this problem :)" />
  <row Id="439" PostHistoryTypeId="1" PostId="169" RevisionGUID="bb30a00b-b3f7-4658-85af-53562b266c8b" CreationDate="2014-05-19T15:55:24.983" UserId="113" Text="How to specify &quot;discriminative&quot; attributes?" />
  <row Id="440" PostHistoryTypeId="3" PostId="169" RevisionGUID="bb30a00b-b3f7-4658-85af-53562b266c8b" CreationDate="2014-05-19T15:55:24.983" UserId="113" Text="&lt;machine-learning&gt;&lt;statistics&gt;&lt;feature-selection&gt;&lt;information-theory&gt;" />
  <row Id="441" PostHistoryTypeId="4" PostId="169" RevisionGUID="65e2d125-bfda-4e3f-8b73-bb0497e468f7" CreationDate="2014-05-19T16:20:14.243" UserId="113" Comment="edited title" Text="How to specify important attributes?" />
  <row Id="442" PostHistoryTypeId="2" PostId="170" RevisionGUID="1188eb0a-063b-40b9-96b8-8f1346a62b9b" CreationDate="2014-05-19T18:08:32.327" UserId="84" Text="A possible solution is to calculate the [information gain](http://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain) associated to each attribute:&#xD;&#xA;&#xD;&#xA;![Information Gain][1]&#xD;&#xA;&#xD;&#xA;Initially you have the whole dataset, and compute the information gain of each item. The one item with the best information gain, you use to partition the dataset, using the values of such item. Then, you perform the same computations for each item (but the ones selected), and always choose the one which best *describes/differentiates* the entries from your dataset.&#xD;&#xA;&#xD;&#xA;There are implementations available for such computations. [Decision trees](http://en.wikipedia.org/wiki/Decision_tree_learning) usually base their feature selection on the features with best information gain. You may use the resulting tree structure to find these *important* items.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/sUheW.png" />
  <row Id="443" PostHistoryTypeId="5" PostId="170" RevisionGUID="de17a888-1bc1-4911-a69c-d0b26f601fc7" CreationDate="2014-05-19T18:19:08.730" UserId="84" Comment="added 10 characters in body" Text="A possible solution is to calculate the [information gain](http://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain) associated to each attribute:&#xD;&#xA;&#xD;&#xA;![Information Gain][1]&#xD;&#xA;&#xD;&#xA;Initially you have the whole dataset, and compute the information gain of each item. The item with the best information gain is the one you should use to partition the dataset (considering the item's values). Then, perform the same computations for each item (but the ones selected), and always choose the one which best *describes/differentiates* the entries from your dataset.&#xD;&#xA;&#xD;&#xA;There are implementations available for such computations. [Decision trees](http://en.wikipedia.org/wiki/Decision_tree_learning) usually base their feature selection on the features with best information gain. You may use the resulting tree structure to find these *important* items.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/sUheW.png" />
  <row Id="444" PostHistoryTypeId="2" PostId="171" RevisionGUID="0bbbb106-477c-46a2-8a95-b7cad3067802" CreationDate="2014-05-19T19:44:48.500" UserId="172" Text="Two things you might find useful:&#xD;&#xA;&#xD;&#xA;1. [meta-learning][1] to speedup the search for the right model and the optimal parameters.&#xD;&#xA;Meta learning consists in applying machine learning tools to the problem of finding the right machine learning tool/parameters for the problem at hand. This for instance [this paper][2] for a practical example;&#xD;&#xA;&#xD;&#xA;2. [gpucomputing][3] to speedup the algorithm on larger datasets. For instance, [OpenCV can use GPUs][4], which are very effective at processing images/videos and can bring 10 to 100 speedups with respect to CPUs. As your computer most probably has a gpucomputing-able GPU, you could gain lots of time using it.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Meta_learning_%28computer_science%29&#xD;&#xA;  [2]: http://link.springer.com/chapter/10.1007/978-3-642-14464-6_11&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units&#xD;&#xA;  [4]: http://docs.opencv.org/modules/gpu/doc/introduction.html" />
  <row Id="445" PostHistoryTypeId="2" PostId="172" RevisionGUID="b77136c3-041a-4466-87e1-3a4034105a58" CreationDate="2014-05-19T23:59:58.070" UserId="250" Text="I have a modeling and scoring program that makes heavy use of the DataFrame.isin function of pandas, searching through lists of facebook &quot;like&quot; records of individual users for each of a few thousand specific pages. This is the most time-consuming part of the program, more so than the modeling or scoring pieces, simply because it only runs on one core while the rest runs on a few dozen simultaneously.&#xD;&#xA;&#xD;&#xA;Though I know I could manually break up the dataframe into chunks and run the operation in parallel, is there any straightforward way to do that automatically? In other words---is there any kind of package out there that will recognize I'm running an easily-delegated operation and automatically distribute it? Perhaps that's asking for too much, but I've been surprised enough in the past by what's already available in Python, so I figure it's worth asking.&#xD;&#xA;&#xD;&#xA;Any other suggestions about how this might be accomplished (even if not by some magic unicorn package!) would also be appreciated. Mainly, just trying to find a way to shave off 15-20 minutes per run without spending an equal amount of time coding the solution.&#xD;&#xA;&#xD;&#xA;Thanks!&#xD;&#xA;-Andrew" />
  <row Id="446" PostHistoryTypeId="1" PostId="172" RevisionGUID="b77136c3-041a-4466-87e1-3a4034105a58" CreationDate="2014-05-19T23:59:58.070" UserId="250" Text="Is there a straightforward way to run pandas.DataFrame.isin in parallel?" />
  <row Id="447" PostHistoryTypeId="3" PostId="172" RevisionGUID="b77136c3-041a-4466-87e1-3a4034105a58" CreationDate="2014-05-19T23:59:58.070" UserId="250" Text="&lt;performance&gt;&lt;python&gt;&lt;pandas&gt;&lt;parallel&gt;" />
  <row Id="448" PostHistoryTypeId="2" PostId="173" RevisionGUID="0100aba1-5752-449b-a7a1-2e48d5914f7c" CreationDate="2014-05-20T03:56:43.147" UserId="158" Text="Guessing it's likely you've seen this [YouTube demo][1] and the related [Google Tech Talk][2], which is related to these papers: &#xD;&#xA;&#xD;&#xA; - [P-N Learning: Bootstrapping Binary Classifiers by Structural Constraints][3]&#xD;&#xA; - [Tracking-Learning-Detection][4]&#xD;&#xA;&#xD;&#xA;And this set of code on GitHub for [OpenTLD][5]. If you check the &quot;read me&quot; on GitHub here, you'll notice that [author's email (Zdenek Kalal)][6] is listed, so it might be worth sending him an email about your questions, or even inviting him to reply to this question too.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.youtube.com/watch?v=1GhNXHCQGsM&#xD;&#xA;  [2]: http://www.youtube.com/watch?v=lmG_FjG4Dy8&#xD;&#xA;  [3]: http://eprints.pascal-network.org/archive/00006951/01/cvpr2010.pdf&#xD;&#xA;  [4]: http://epubs.surrey.ac.uk/713800/1/Kalal-PAMI-2011%281%29.pdf&#xD;&#xA;  [5]: https://github.com/zk00006/OpenTLD/&#xD;&#xA;  [6]: https://github.com/zk00006/OpenTLD/blob/master/README" />
  <row Id="449" PostHistoryTypeId="5" PostId="172" RevisionGUID="a750e541-13d8-4dac-9a9e-473bb2b903fb" CreationDate="2014-05-20T04:47:25.207" UserId="84" Comment="Improving formatting." Text="I have a modeling and scoring program that makes heavy use of the `DataFrame.isin` function of pandas, searching through lists of facebook &quot;like&quot; records of individual users for each of a few thousand specific pages. This is the most time-consuming part of the program, more so than the modeling or scoring pieces, simply because it only runs on one core while the rest runs on a few dozen simultaneously.&#xD;&#xA;&#xD;&#xA;Though I know I could manually break up the dataframe into chunks and run the operation in parallel, is there any straightforward way to do that automatically? In other words, is there any kind of package out there that will recognize I'm running an easily-delegated operation and automatically distribute it? Perhaps that's asking for too much, but I've been surprised enough in the past by what's already available in Python, so I figure it's worth asking.&#xD;&#xA;&#xD;&#xA;Any other suggestions about how this might be accomplished (even if not by some magic unicorn package!) would also be appreciated. Mainly, just trying to find a way to shave off 15-20 minutes per run without spending an equal amount of time coding the solution." />
  <row Id="450" PostHistoryTypeId="24" PostId="172" RevisionGUID="a750e541-13d8-4dac-9a9e-473bb2b903fb" CreationDate="2014-05-20T04:47:25.207" Comment="Proposed by 84 approved by 250 edit id of 50" />
  <row Id="452" PostHistoryTypeId="2" PostId="174" RevisionGUID="447f1bc4-e685-4ec0-91e3-83476d63cb44" CreationDate="2014-05-20T09:24:30.697" UserId="172" Text="Another suggestion is to test the [logistic regression][1]. As an added bonus, the  weights (coefficients) of the model will give you an idea of which sites are age-distriminant.&#xD;&#xA;&#xD;&#xA;Sklearn offers the [sklearn.linear_model.LogisticRegression][2] package that is designed to handle sparse data as well.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Logistic_regression&#xD;&#xA;  [2]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" />
  <row Id="453" PostHistoryTypeId="5" PostId="128" RevisionGUID="32fff684-b72d-404a-9a62-56f4b176ded6" CreationDate="2014-05-20T13:45:59.373" UserId="84" Comment="Improving formatting." Text="[Latent Dirichlet Allocation (LDA)](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) and [Hierarchical Dirichlet Process (HDP)](http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process) are both topic modeling processes. The major difference is LDA requires the specification of the number of topics, and HDP doesn't. Why is that so? And what are the differences, pros, and cons of both topic modelling methods?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="454" PostHistoryTypeId="5" PostId="151" RevisionGUID="2859b19f-57ec-4866-861f-538ab4b4f4e1" CreationDate="2014-05-20T13:48:12.163" UserId="229" Comment="added 735 characters in body" Text="Indexing is the almost most important part of data to get an efficient, properly storing and retrieval data from mediums&#xD;&#xA;&#xD;&#xA;In different Programming Languages, there are different indexing algorithms and structures can be found.&#xD;&#xA;&#xD;&#xA;As an Example, in Java Language the Apache Foundation's Lucene is very popular.&#xD;&#xA;&#xD;&#xA;In years there are very efficient, fast, scalable Indexing Algorithms projectioned to the computer-world such as MurMurHashing Algorithm (using by some NoSQL Databases), B+Tree Algorithm (using by versions of Windows OS itself), or Lucene (very popular in web technologies) and its variations.&#xD;&#xA;&#xD;&#xA;Also there can be found problem-specific Indexing within Chemicals or Medical Data Representations in Digital World&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="455" PostHistoryTypeId="24" PostId="151" RevisionGUID="2859b19f-57ec-4866-861f-538ab4b4f4e1" CreationDate="2014-05-20T13:48:12.163" Comment="Proposed by 229 approved by 50 edit id of 44" />
  <row Id="456" PostHistoryTypeId="5" PostId="48" RevisionGUID="6f0b10e3-bfd2-4f6c-b98a-fd52f81e0460" CreationDate="2014-05-20T13:49:04.060" UserId="201" Comment="added 1317 characters in body" Text="R is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&amp;T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.&#xD;&#xA;&#xD;&#xA;R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, ...) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.&#xD;&#xA;&#xD;&#xA;One of R's strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.&#xD;&#xA;&#xD;&#xA;R is available as Free Software under the terms of the Free Software Foundation's GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS." />
  <row Id="457" PostHistoryTypeId="24" PostId="48" RevisionGUID="6f0b10e3-bfd2-4f6c-b98a-fd52f81e0460" CreationDate="2014-05-20T13:49:04.060" Comment="Proposed by 201 approved by 50 edit id of 27" />
  <row Id="458" PostHistoryTypeId="5" PostId="79" RevisionGUID="f3ffb6a7-0916-432e-9e8f-e2dd1ca40a96" CreationDate="2014-05-20T13:49:35.103" UserId="53" Comment="linkify user statsRus, attempt 4" Text="Conceptually speaking, *data-mining* can be thought of as one item (or set of skills and applications) in the toolkit of the data scientist.&#xD;&#xA;&#xD;&#xA;More specifically, data-mining is an activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal.&#xD;&#xA;&#xD;&#xA;In US-English colloquial speech, data-mining and data-collection are often used interchangeably.&#xD;&#xA;&#xD;&#xA;However, a main difference between these two related activities is *intentionality*. &#xD;&#xA;&#xD;&#xA;*Definition inspired mostly by the contributions of [@statsRus](http://datascience.stackexchange.com/users/36/statsrus) to Data Science.SE*" />
  <row Id="459" PostHistoryTypeId="24" PostId="79" RevisionGUID="f3ffb6a7-0916-432e-9e8f-e2dd1ca40a96" CreationDate="2014-05-20T13:49:35.103" Comment="Proposed by 53 approved by 50 edit id of 20" />
  <row Id="460" PostHistoryTypeId="5" PostId="124" RevisionGUID="7eee99fd-bc9d-4246-8b8b-f8d569e2706f" CreationDate="2014-05-20T13:50:19.543" UserId="53" Comment="linked to describe relational independence" Text="A statistics term used to describe a type of dependence between variables (or data sets). Correlations are often used as an indicator of predictability. However, correlation does NOT imply causation. Different methods of calculating correlation exist to capture more complicated relationships between the variables being studied." />
  <row Id="461" PostHistoryTypeId="24" PostId="124" RevisionGUID="7eee99fd-bc9d-4246-8b8b-f8d569e2706f" CreationDate="2014-05-20T13:50:19.543" Comment="Proposed by 53 approved by 50 edit id of 30" />
  <row Id="462" PostHistoryTypeId="5" PostId="123" RevisionGUID="31f6d5eb-7a3b-46d2-a9ea-2252a56cc3d1" CreationDate="2014-05-20T13:50:21.763" UserId="53" Comment="linked to describe relational independence" Text="The most basic relationship to describe is a **linear relationship** between variables, *x* and *y*, such that they can be said to be highly-correlated when every increase in *x* results in a proportional increase in *y*. They can also be said to be *inversely proportional* so that when *x* increases, *y* decreases. And finally, the two variables can be said to be [independent](http://en.wikipedia.org/wiki/Independence_%28probability_theory%29) in the event that there is no linear relationship between the two (they are uncorrelated, or have a **Pearson correlation coefficient** of 0. [LaTeX support would be highly desirable at this point.]&#xD;&#xA;&#xD;&#xA;Different correlation coefficients and their uses:&#xD;&#xA;--------------------------------------------------&#xD;&#xA;&#xD;&#xA;[Pearson correlation coefficient](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) is useful.....&#xD;&#xA;[draft]&#xD;&#xA;" />
  <row Id="463" PostHistoryTypeId="24" PostId="123" RevisionGUID="31f6d5eb-7a3b-46d2-a9ea-2252a56cc3d1" CreationDate="2014-05-20T13:50:21.763" Comment="Proposed by 53 approved by 50 edit id of 29" />
  <row Id="464" PostHistoryTypeId="5" PostId="168" RevisionGUID="c528dfb3-f3ea-4ac2-bf64-4aef4fd48c98" CreationDate="2014-05-20T13:50:32.440" UserId="229" Comment="added 455 characters in body" Text=".NET is a very popular Object Oriented Programming Language Family which includes members such as C# (pronounced CSharp), VB.NET, F# (pronounced FSharp), J# (pronounced JSharp) and much more. The .NET Family offers programming with small effort with well-known high speed of compiled languages such as C and C++&#xD;&#xA;&#xD;&#xA;This Tag aims to group Data Science Questions and Answers which users want to operate their processes under .NET Programming Language Family" />
  <row Id="465" PostHistoryTypeId="24" PostId="168" RevisionGUID="c528dfb3-f3ea-4ac2-bf64-4aef4fd48c98" CreationDate="2014-05-20T13:50:32.440" Comment="Proposed by 229 approved by 50 edit id of 49" />
  <row Id="466" PostHistoryTypeId="5" PostId="104" RevisionGUID="9dc0253f-e204-4c0d-a1ef-28095641b189" CreationDate="2014-05-20T13:50:52.447" UserId="53" Comment="added 286 characters in body" Text="Use the [tag:definitions] tag when:&#xD;&#xA;-----------------------------------&#xD;&#xA;&#xD;&#xA;You think we should create an official definition.&#xD;&#xA;&#xD;&#xA;An existing Tag Wiki needs a more precise definition to avoid confusion and we need to create consensus before an edit.&#xD;&#xA;&#xD;&#xA;(rough draft - needs filling out)" />
  <row Id="467" PostHistoryTypeId="24" PostId="104" RevisionGUID="9dc0253f-e204-4c0d-a1ef-28095641b189" CreationDate="2014-05-20T13:50:52.447" Comment="Proposed by 53 approved by 50 edit id of 18" />
  <row Id="468" PostHistoryTypeId="5" PostId="118" RevisionGUID="6785e6ff-09bc-4af3-bfbd-872505aa6715" CreationDate="2014-05-20T13:51:16.240" UserId="201" Comment="added 1870 characters in body" Text="**NoSQL** (sometimes expanded to &quot;not only [tag:sql]&quot;) is a broad class of database management systems that differ from the classic model of the relational database management system ([tag:rdbms]) in some significant ways.&#xD;&#xA;&#xD;&#xA;###NoSQL systems:&#xD;&#xA;&#xD;&#xA;- Specifically designed for high load&#xD;&#xA;- Natively support horizontal scalability&#xD;&#xA;- Fault tolerant&#xD;&#xA;- Store data in denormalised manner&#xD;&#xA;- Do not usually enforce strict database schema&#xD;&#xA;- Do not usually store data in a table&#xD;&#xA;- Sometimes provide eventual consistency instead of ACID transactions&#xD;&#xA;&#xD;&#xA;###In contrast to RDBMS, NoSQL systems:&#xD;&#xA;&#xD;&#xA;- Do not guarantee data consistency&#xD;&#xA;- Usually support a limited query language (subset of SQL or another custom query language)&#xD;&#xA;- May not provide support for transactions/distributed transactions&#xD;&#xA;- Do not usually use some advanced concepts of RDBMS, such as triggers, views, stored procedures&#xD;&#xA;&#xD;&#xA;###NoSQL implementations can be categorised by their manner of implementation:&#xD;&#xA;&#xD;&#xA;- [Column-oriented][1]&#xD;&#xA;- [Document store][2]&#xD;&#xA;- [Graph][3]&#xD;&#xA;- [Key-value store][4]&#xD;&#xA;- [Multivalue databases][5]&#xD;&#xA;- [Object databases][6]&#xD;&#xA;- [Tripplestore](https://en.wikipedia.org/wiki/Triplestore)&#xD;&#xA;- [Tuple store](http://en.wikipedia.org/wiki/NoSQL#Tuple_store)&#xD;&#xA;&#xD;&#xA;### Free NoSQL Books&#xD;&#xA;&#xD;&#xA;*   [CouchDB: The Definitive Guide](http://books.couchdb.org/relax/)&#xD;&#xA;*   [The Little MongoDB Book](http://openmymind.net/2011/3/28/The-Little-MongoDB-Book)&#xD;&#xA;*   [The Little Redis Book](http://openmymind.net/2012/1/23/The-Little-Redis-Book/)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Column-oriented_DBMS&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Document-oriented_database&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Graph_database&#xD;&#xA;  [4]: http://dba.stackexchange.com/questions/607/what-is-a-key-value-store-database&#xD;&#xA;  [5]: https://en.wikipedia.org/wiki/MultiValue&#xD;&#xA;  [6]: https://en.wikipedia.org/wiki/Object_database" />
  <row Id="469" PostHistoryTypeId="24" PostId="118" RevisionGUID="6785e6ff-09bc-4af3-bfbd-872505aa6715" CreationDate="2014-05-20T13:51:16.240" Comment="Proposed by 201 approved by 50 edit id of 25" />
  <row Id="470" PostHistoryTypeId="5" PostId="142" RevisionGUID="b005e96e-b45e-4206-a7fb-f0b235980da7" CreationDate="2014-05-20T13:51:49.240" UserId="84" Comment="added 246 characters in body" Text="Efficiency, in algorithmic processing, is usually associated to resource usage. The metrics to evaluate the efficiency of a process are commonly account for execution time, memory/disk or storage requirements, network usage and power consumption." />
  <row Id="471" PostHistoryTypeId="24" PostId="142" RevisionGUID="b005e96e-b45e-4206-a7fb-f0b235980da7" CreationDate="2014-05-20T13:51:49.240" Comment="Proposed by 84 approved by 50 edit id of 36" />
  <row Id="472" PostHistoryTypeId="5" PostId="109" RevisionGUID="57345a56-1b97-42b2-b3f1-41d3e1ee5746" CreationDate="2014-05-20T13:52:00.620" UserId="200" Comment="added 126 characters in body" Text="An activity that seeks patterns in a continuous stream of data elements, usually involving summarizing the stream in some way." />
  <row Id="473" PostHistoryTypeId="24" PostId="109" RevisionGUID="57345a56-1b97-42b2-b3f1-41d3e1ee5746" CreationDate="2014-05-20T13:52:00.620" Comment="Proposed by 200 approved by 50 edit id of 22" />
  <row Id="474" PostHistoryTypeId="5" PostId="149" RevisionGUID="c386cfb8-c04f-4a0b-95ec-52ff58ffe13e" CreationDate="2014-05-20T13:52:19.333" UserId="229" Comment="added 325 characters in body" Text="In Computer science and Technologies The Data is the most important part.&#xD;&#xA;Since to work with data in an efficient manner to store the data in mediums and reuse, we use some techniques which named in general indexing.&#xD;&#xA;&#xD;&#xA;This tag interests with these techniques, efficiency, and anything about using stored data with indexing" />
  <row Id="475" PostHistoryTypeId="24" PostId="149" RevisionGUID="c386cfb8-c04f-4a0b-95ec-52ff58ffe13e" CreationDate="2014-05-20T13:52:19.333" Comment="Proposed by 229 approved by 50 edit id of 43" />
  <row Id="476" PostHistoryTypeId="5" PostId="167" RevisionGUID="2abe3856-c55b-40fa-a69a-642132b257a2" CreationDate="2014-05-20T13:52:50.373" UserId="229" Comment="added 517 characters in body" Text="**.NET** is a very popular Object Oriented Programming Language Family.This Family includes members such as **C#** (pronounced **CSharp**), **VB.NET**, **F#** (pronounced **Fsharp**), **J#** (pronounced **JSharp**) and much more. The **.NET** Family offers programming with small effort with well-known high speed of compiled languages such as **C** and **C++**&#xD;&#xA;&#xD;&#xA;This Tag **aims** to group **Data Science Questions and Answers** which users want to operate their processes **under .NET** Programming Language Family" />
  <row Id="477" PostHistoryTypeId="24" PostId="167" RevisionGUID="2abe3856-c55b-40fa-a69a-642132b257a2" CreationDate="2014-05-20T13:52:50.373" Comment="Proposed by 229 approved by 50 edit id of 48" />
  <row Id="478" PostHistoryTypeId="5" PostId="147" RevisionGUID="ef541769-5807-421c-a994-36f865339f34" CreationDate="2014-05-20T13:52:59.427" UserId="118" Comment="added 449 characters in body" Text="Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human–computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation." />
  <row Id="479" PostHistoryTypeId="24" PostId="147" RevisionGUID="ef541769-5807-421c-a994-36f865339f34" CreationDate="2014-05-20T13:52:59.427" Comment="Proposed by 118 approved by 50 edit id of 40" />
  <row Id="480" PostHistoryTypeId="5" PostId="105" RevisionGUID="837b4c2b-8756-4461-b392-f5d101010c18" CreationDate="2014-05-20T13:53:05.697" UserId="53" Comment="added 124 characters in body" Text="a discussion (meta) tag used when there exists *disagreement* or *confusion* about the everyday meaning of a term or phrase." />
  <row Id="481" PostHistoryTypeId="24" PostId="105" RevisionGUID="837b4c2b-8756-4461-b392-f5d101010c18" CreationDate="2014-05-20T13:53:05.697" Comment="Proposed by 53 approved by 50 edit id of 19" />
  <row Id="482" PostHistoryTypeId="5" PostId="136" RevisionGUID="6c9c9fd8-1a2c-40e6-86df-3ca0560af181" CreationDate="2014-05-20T13:53:08.790" UserId="227" Comment="added 651 characters in body" Text="MapReduce is a framework for processing parallelizable problems across huge datasets using a large number of computers (nodes), collectively referred to as a cluster (if all nodes are on the same local network and use similar hardware) or a grid (if the nodes are shared across geographically and administratively distributed systems, and use more heterogenous hardware). Computational processing can occur on data stored either in a filesystem (unstructured) or in a database (structured). MapReduce can take advantage of locality of data, processing it on or near the storage assets in order to reduce the distance over which it must be transmitted." />
  <row Id="483" PostHistoryTypeId="24" PostId="136" RevisionGUID="6c9c9fd8-1a2c-40e6-86df-3ca0560af181" CreationDate="2014-05-20T13:53:08.790" Comment="Proposed by 227 approved by 50 edit id of 32" />
  <row Id="484" PostHistoryTypeId="5" PostId="152" RevisionGUID="9c94ad1d-90cf-43eb-a153-47c96d593671" CreationDate="2014-05-20T13:53:17.567" UserId="229" Comment="added 120 characters in body" Text="Indexing is the almost most important part of data to get an efficient, properly storing and retrieval data from mediums" />
  <row Id="485" PostHistoryTypeId="24" PostId="152" RevisionGUID="9c94ad1d-90cf-43eb-a153-47c96d593671" CreationDate="2014-05-20T13:53:17.567" Comment="Proposed by 229 approved by 50 edit id of 45" />
  <row Id="486" PostHistoryTypeId="5" PostId="145" RevisionGUID="ac8729f8-7707-4822-8e53-8b0ee6f752c6" CreationDate="2014-05-20T13:53:26.907" UserId="118" Comment="added 448 characters in body" Text="Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval etc." />
  <row Id="487" PostHistoryTypeId="24" PostId="145" RevisionGUID="ac8729f8-7707-4822-8e53-8b0ee6f752c6" CreationDate="2014-05-20T13:53:26.907" Comment="Proposed by 118 approved by 50 edit id of 39" />
  <row Id="488" PostHistoryTypeId="5" PostId="148" RevisionGUID="9090b1e1-06af-45b9-a7ac-1d591b294aeb" CreationDate="2014-05-20T13:53:31.717" UserId="229" Comment="added 325 characters in body" Text="In Computer science and Technologies The Data is the most important part.&#xD;&#xA;Since to work with data in an efficient manner to store the data in mediums and reuse, we use some techniques which named in general indexing.&#xD;&#xA;&#xD;&#xA;This tag interests with these techniques, efficiency, and anything about using stored data with indexing" />
  <row Id="489" PostHistoryTypeId="24" PostId="148" RevisionGUID="9090b1e1-06af-45b9-a7ac-1d591b294aeb" CreationDate="2014-05-20T13:53:31.717" Comment="Proposed by 229 approved by 50 edit id of 42" />
  <row Id="490" PostHistoryTypeId="5" PostId="137" RevisionGUID="8af71482-ba04-46ee-839e-55d3350b5c15" CreationDate="2014-05-20T13:53:39.727" UserId="227" Comment="added 116 characters in body" Text="MapReduce is a programming model for processing large data sets with a parallel, distributed algorithm on a cluster." />
  <row Id="491" PostHistoryTypeId="24" PostId="137" RevisionGUID="8af71482-ba04-46ee-839e-55d3350b5c15" CreationDate="2014-05-20T13:53:39.727" Comment="Proposed by 227 approved by 50 edit id of 33" />
  <row Id="492" PostHistoryTypeId="5" PostId="174" RevisionGUID="55c7716a-3a6d-4012-b3f1-f28963abf736" CreationDate="2014-05-20T19:36:16.283" UserId="172" Comment="added 213 characters in body" Text="Another suggestion is to test the [logistic regression][1]. As an added bonus, the  weights (coefficients) of the model will give you an idea of which sites are age-distriminant.  &#xD;&#xA;&#xD;&#xA;Sklearn offers the [sklearn.linear_model.LogisticRegression][2] package that is designed to handle sparse data as well.&#xD;&#xA;&#xD;&#xA;As mentionned in the comments, in the present case, with more input variables than samples, you need to regularize the model (with [sklearn.linear_model.LogisticRegression][2] use the `penalty='l1'` argument).&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Logistic_regression&#xD;&#xA;  [2]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" />
  <row Id="493" PostHistoryTypeId="2" PostId="175" RevisionGUID="bd8bddfc-182b-4b88-8915-0d5dc7721f63" CreationDate="2014-05-20T22:14:02.927" UserId="249" Text="I have data coming from a source system that is pipe delimited. Pipe was selected over comma since it was believed no pipes appeared in field, while it was known that commas do occur. After ingesting this data into Hive however it has been discovered that rarely a field does in fact contain a pipe character.&#xD;&#xA;&#xD;&#xA;Due to a constraint we are unable to regenerate from source to escape the delimiter or change delimiters in the usual way. However we have the metadata used to create the Hive table. Could we use knowledge of the fields around the problem field to reprocess the file on our side to escape it or to change the file delimiter prior to reloading the data into Hive?" />
  <row Id="494" PostHistoryTypeId="1" PostId="175" RevisionGUID="bd8bddfc-182b-4b88-8915-0d5dc7721f63" CreationDate="2014-05-20T22:14:02.927" UserId="249" Text="Can metadata be used to adapt parsing for an unescaped in field use of the delimiter?" />
  <row Id="495" PostHistoryTypeId="3" PostId="175" RevisionGUID="bd8bddfc-182b-4b88-8915-0d5dc7721f63" CreationDate="2014-05-20T22:14:02.927" UserId="249" Text="&lt;metadata&gt;&lt;parsing&gt;&lt;munging&gt;&lt;wrangling&gt;&lt;data-quality&gt;" />
  <row Id="496" PostHistoryTypeId="2" PostId="176" RevisionGUID="0b5b7037-4ec7-4086-bcee-c0298feaa588" CreationDate="2014-05-21T05:29:36.787" UserId="173" Text="I am seeking for a library/tool to visualize how social network changes when new nodes/edges are added to it.&#xD;&#xA;&#xD;&#xA;One of the existing solutions is [SoNIA: Social Network Image Animator][1]. It let's you make movies like [this one][2]. &#xD;&#xA;&#xD;&#xA;SoNIA's documentation says that it's broken at the moment, and besides this I would prefer JavaScript-based solution instead. So, my question is: are you familiar with any tools or are you able to point me to some libraries which would make this task as easy as possible?&#xD;&#xA;&#xD;&#xA;Right after posting this question I'll dig into [sigma.js][3], so please consider this library covered.&#xD;&#xA;&#xD;&#xA;In general, my input data would be something like this:&#xD;&#xA;&#xD;&#xA;time_elapsed; node1; node2&#xD;&#xA;1; A; B&#xD;&#xA;2; A; C&#xD;&#xA;3; B; C&#xD;&#xA;&#xD;&#xA;So, here we have three points in time (1, 2, 3), three nodes (A, B, C), and three edges, which represent a triadic closure between the three considered nodes.&#xD;&#xA;&#xD;&#xA;Moreover, every node will have two attributes (age and gender), so I would like to be able to change the shape/colour of the nodes.&#xD;&#xA;&#xD;&#xA;Also, after adding a new node, it would be perfect to have some ForceAtlas2 or similar algorithm to adjust the layout of the graph.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.stanford.edu/group/sonia/&#xD;&#xA;  [2]: https://www.youtube.com/watch?v=yGSNCED6mDc&#xD;&#xA;  [3]: http://sigmajs.org/" />
  <row Id="497" PostHistoryTypeId="1" PostId="176" RevisionGUID="0b5b7037-4ec7-4086-bcee-c0298feaa588" CreationDate="2014-05-21T05:29:36.787" UserId="173" Text="How to animate growth of a social network?" />
  <row Id="498" PostHistoryTypeId="3" PostId="176" RevisionGUID="0b5b7037-4ec7-4086-bcee-c0298feaa588" CreationDate="2014-05-21T05:29:36.787" UserId="173" Text="&lt;social-network-analysis&gt;&lt;time-series&gt;&lt;javascript&gt;&lt;visualization&gt;" />
  <row Id="499" PostHistoryTypeId="5" PostId="176" RevisionGUID="4de5dbb4-a293-4603-9e76-edb7768422f3" CreationDate="2014-05-21T05:51:58.330" UserId="173" Comment="added 16 characters in body" Text="I am seeking for a library/tool to visualize how social network changes when new nodes/edges are added to it.&#xD;&#xA;&#xD;&#xA;One of the existing solutions is [SoNIA: Social Network Image Animator][1]. It let's you make movies like [this one][2]. &#xD;&#xA;&#xD;&#xA;SoNIA's documentation says that it's broken at the moment, and besides this I would prefer JavaScript-based solution instead. So, my question is: are you familiar with any tools or are you able to point me to some libraries which would make this task as easy as possible?&#xD;&#xA;&#xD;&#xA;Right after posting this question I'll dig into [sigma.js][3], so please consider this library covered.&#xD;&#xA;&#xD;&#xA;In general, my input data would be something like this:&#xD;&#xA;&#xD;&#xA;    time_elapsed; node1; node2&#xD;&#xA;    1; A; B&#xD;&#xA;    2; A; C&#xD;&#xA;    3; B; C&#xD;&#xA;&#xD;&#xA;So, here we have three points in time (1, 2, 3), three nodes (A, B, C), and three edges, which represent a triadic closure between the three considered nodes.&#xD;&#xA;&#xD;&#xA;Moreover, every node will have two attributes (age and gender), so I would like to be able to change the shape/colour of the nodes.&#xD;&#xA;&#xD;&#xA;Also, after adding a new node, it would be perfect to have some ForceAtlas2 or similar algorithm to adjust the layout of the graph.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.stanford.edu/group/sonia/&#xD;&#xA;  [2]: https://www.youtube.com/watch?v=yGSNCED6mDc&#xD;&#xA;  [3]: http://sigmajs.org/" />
  <row Id="500" PostHistoryTypeId="2" PostId="177" RevisionGUID="bba8d982-9b8b-4a4f-b452-e86f2518ad52" CreationDate="2014-05-21T07:05:44.780" UserId="-1" Text="" />
  <row Id="501" PostHistoryTypeId="2" PostId="178" RevisionGUID="7126fa44-3de9-4899-8a52-34104fc704fb" CreationDate="2014-05-21T07:05:44.780" UserId="-1" Text="" />
  <row Id="502" PostHistoryTypeId="2" PostId="179" RevisionGUID="f28d2d21-10bf-42bf-9183-e665d6af7aa5" CreationDate="2014-05-21T07:09:20.093" UserId="97" Text="My first guess is to [visualize social network in Tableau][1].&#xD;&#xA;&#xD;&#xA;And particularly: [building network graphs in Tableau][2].&#xD;&#xA;&#xD;&#xA;What you need is to add time dimension to the &quot;Pages&quot; section to be able to see network change dynamics.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.google.com/search?q=visualize%20social%20network%20in%20tableau&#xD;&#xA;  [2]: http://www.clearlyandsimply.com/clearly_and_simply/2012/12/build-network-graphs-in-tableau.html" />
  <row Id="503" PostHistoryTypeId="2" PostId="180" RevisionGUID="ebe743f1-292b-40a5-aaf7-0c01bc011237" CreationDate="2014-05-21T07:10:45.827" UserId="-1" Text="" />
  <row Id="504" PostHistoryTypeId="2" PostId="181" RevisionGUID="3c14a563-0fb7-41f3-be1c-cf5fb01d86aa" CreationDate="2014-05-21T07:10:45.827" UserId="-1" Text="" />
  <row Id="505" PostHistoryTypeId="5" PostId="179" RevisionGUID="4ea482c2-11b7-4c82-a47d-9af31a79e9d7" CreationDate="2014-05-21T07:18:48.453" UserId="97" Comment="added screen shot" Text="My first guess is to [visualize social network in Tableau][1].&#xD;&#xA;&#xD;&#xA;And particularly: [building network graphs in Tableau][2].&#xD;&#xA;&#xD;&#xA;What you need is to add time dimension to the &quot;Pages&quot; section to be able to see network change dynamics.&#xD;&#xA;&#xD;&#xA;This is screen from the link above.&#xD;&#xA;![enter image description here][3]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.google.com/search?q=visualize%20social%20network%20in%20tableau&#xD;&#xA;  [2]: http://www.clearlyandsimply.com/clearly_and_simply/2012/12/build-network-graphs-in-tableau.html&#xD;&#xA;  [3]: http://i.stack.imgur.com/Quq8G.png" />
  <row Id="506" PostHistoryTypeId="2" PostId="182" RevisionGUID="1d2b696f-9008-43b3-bf06-bcd45e477411" CreationDate="2014-05-21T07:19:32.297" UserId="21" Text="So, a few of your rows will have too many columns by one or more as a result. That's easy to detect, but harder to infer where the error was -- which two columns are actually one? which delimiter is not a delimiter?&#xD;&#xA;&#xD;&#xA;In some cases, you can use the metadata, because it helps you know when an interpretation of the columns can't be right. For example, if just the one column can have a text value, and all the others must be numeric, it's unambiguous where the error is. Any additional columns created by this error occur right after the text column.&#xD;&#xA;&#xD;&#xA;If they're all text, this doesn't work of course.&#xD;&#xA;&#xD;&#xA;You might be able to leverage more than the metadata's column type. For example you may know that some fields are from an enumerated set of values, and use that to determine when a column assignment is wrong." />
  <row Id="507" PostHistoryTypeId="2" PostId="183" RevisionGUID="fe43efff-9ac9-49d8-b1fc-c6d2a139e519" CreationDate="2014-05-21T10:53:26.870" UserId="262" Text="I was very impressed when I saw [this animation][1] of the [discourse][2] git repository. They used [Gourse][3] which is specifically for git. But it may give ideas about how to represent the dynamics of growth.&#xD;&#xA;&#xD;&#xA;Also, [this stackoverflow answer][4] seems to point at a python/networkx/matplotlib solution.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://youtu.be/T7Ncmq6scck&#xD;&#xA;  [2]: http://www.discourse.org&#xD;&#xA;  [3]: https://code.google.com/p/gource/&#xD;&#xA;  [4]: http://stackoverflow.com/a/13571425/1083707" />
  <row Id="508" PostHistoryTypeId="2" PostId="184" RevisionGUID="0e66aa84-4327-4d0a-aa77-7de5db8e4d49" CreationDate="2014-05-21T11:22:34.657" UserId="200" Text="The details of the Google Prediction API are on this [page][1], but I am not able to find any details about the prediction algorithms running behind the API. &#xD;&#xA;&#xD;&#xA;So far I have gathered that they let you provide your preprocessing steps in PMML format.&#xD;&#xA;&#xD;&#xA;  [1]: https://developers.google.com/prediction/" />
  <row Id="509" PostHistoryTypeId="1" PostId="184" RevisionGUID="0e66aa84-4327-4d0a-aa77-7de5db8e4d49" CreationDate="2014-05-21T11:22:34.657" UserId="200" Text="Google prediction API: What training/prediction methods Google Prediction API employs?" />
  <row Id="510" PostHistoryTypeId="3" PostId="184" RevisionGUID="0e66aa84-4327-4d0a-aa77-7de5db8e4d49" CreationDate="2014-05-21T11:22:34.657" UserId="200" Text="&lt;tools&gt;&lt;google-prediction-api&gt;" />
  <row Id="512" PostHistoryTypeId="10" PostId="125" RevisionGUID="96e2e71b-110f-42cb-9c68-e4fd049a43f5" CreationDate="2014-05-21T14:00:22.100" UserId="-1" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:227,&quot;DisplayName&quot;:&quot;Amir Ali Akbari&quot;},{&quot;Id&quot;:53,&quot;DisplayName&quot;:&quot;Clayton&quot;},{&quot;Id&quot;:115,&quot;DisplayName&quot;:&quot;Johnny000&quot;},{&quot;Id&quot;:113,&quot;DisplayName&quot;:&quot;vefthym&quot;},{&quot;Id&quot;:148,&quot;DisplayName&quot;:&quot;ProgramFOX&quot;}]}" />
  <row Id="513" PostHistoryTypeId="2" PostId="185" RevisionGUID="880f344b-4e2c-4b54-b9c2-d548a131d898" CreationDate="2014-05-21T14:14:38.797" UserId="108" Text="If you take a look over the specifications of PMML which you can find [here][1] you can see on the left menu what options you have (like ModelTree, NaiveBayes, Neural Nets and so on).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.dmg.org/v3-0/GeneralStructure.html" />
  <row Id="514" PostHistoryTypeId="5" PostId="183" RevisionGUID="838c0638-a986-4ff1-88cc-753c87e10e07" CreationDate="2014-05-21T14:41:35.207" UserId="262" Comment="added 284 characters in body" Text="I was very impressed when I saw [this animation][1] of the [discourse][2] git repository. They used [Gourse][3] which is specifically for git. But it may give ideas about how to represent the dynamics of growth.&#xD;&#xA;&#xD;&#xA;Also, [this stackoverflow answer][4] seems to point at a python/networkx/matplotlib solution.&#xD;&#xA;&#xD;&#xA;If you're looking for a web-based solution then d3.js is excellent. See [this][5], [this][6] and [this][7] for example.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://youtu.be/T7Ncmq6scck&#xD;&#xA;  [2]: http://www.discourse.org&#xD;&#xA;  [3]: https://code.google.com/p/gource/&#xD;&#xA;  [4]: http://stackoverflow.com/a/13571425/1083707&#xD;&#xA;  [5]: http://bl.ocks.org/mbostock/4062045&#xD;&#xA;  [6]: http://mbostock.github.io/d3/talk/20111116/force-collapsible.html&#xD;&#xA;  [7]: http://bl.ocks.org/mbostock/929623" />
  <row Id="515" PostHistoryTypeId="2" PostId="186" RevisionGUID="17ec82b3-c836-4db2-96aa-7cc9db0a51f2" CreationDate="2014-05-21T15:12:18.980" UserId="133" Text="I am learning [Support Vector Machines][1] and I am unable to understand how a class label is chosen for a data point in a binary classifier.&#xD;&#xA;&#xD;&#xA;Is it chosen by consensus with respect to the classification in each dimension of the separating hyperplane?&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Support_vector_machine" />
  <row Id="516" PostHistoryTypeId="1" PostId="186" RevisionGUID="17ec82b3-c836-4db2-96aa-7cc9db0a51f2" CreationDate="2014-05-21T15:12:18.980" UserId="133" Text="Using SVM as a binary classifier, is the label for a data point chosen by consensus?" />
  <row Id="517" PostHistoryTypeId="3" PostId="186" RevisionGUID="17ec82b3-c836-4db2-96aa-7cc9db0a51f2" CreationDate="2014-05-21T15:12:18.980" UserId="133" Text="&lt;svm&gt;&lt;classification&gt;&lt;binary&gt;" />
  <row Id="518" PostHistoryTypeId="5" PostId="186" RevisionGUID="33b2155d-2002-402e-be45-bc26303f4014" CreationDate="2014-05-21T15:26:02.533" UserId="84" Comment="Improving formatting." Text="I'm learning [Support Vector Machines][1], and I'm unable to understand how a class label is chosen for a data point in a binary classifier. Is it chosen by consensus with respect to the classification in each dimension of the separating hyperplane?&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Support_vector_machine" />
  <row Id="519" PostHistoryTypeId="5" PostId="183" RevisionGUID="5d946f7a-2906-4fe7-938f-a7a064f3cfd2" CreationDate="2014-05-21T15:29:00.877" UserId="262" Comment="added 172 characters in body" Text="I was very impressed when I saw [this animation][1] of the [discourse][2] git repository. They used [Gourse][3] which is specifically for git. But it may give ideas about how to represent the dynamics of growth.&#xD;&#xA;&#xD;&#xA;Also, [this stackoverflow answer][4] seems to point at a python/networkx/matplotlib solution.&#xD;&#xA;&#xD;&#xA;If you're looking for a web-based solution then d3.js is excellent. See [this][5], [this][6] and [this][7] for example.&#xD;&#xA;&#xD;&#xA;See also [this stackoverflow question][8], the accepted answer points to D3.js again.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://youtu.be/T7Ncmq6scck&#xD;&#xA;  [2]: http://www.discourse.org&#xD;&#xA;  [3]: https://code.google.com/p/gource/&#xD;&#xA;  [4]: http://stackoverflow.com/a/13571425/1083707&#xD;&#xA;  [5]: http://bl.ocks.org/mbostock/4062045&#xD;&#xA;  [6]: http://mbostock.github.io/d3/talk/20111116/force-collapsible.html&#xD;&#xA;  [7]: http://bl.ocks.org/mbostock/929623&#xD;&#xA;  [8]: http://stackoverflow.com/questions/7416659/interactive-graph-visualisation" />
  <row Id="520" PostHistoryTypeId="2" PostId="187" RevisionGUID="949e7b0a-d163-4f8c-91ea-da8ab8f99467" CreationDate="2014-05-21T15:39:54.830" UserId="84" Text="The term *consensus*, as far as I'm concerned, is used rather for cases when you have more a than one source of metric/measure/choice from which to make a decision. And, in order to choose a possible result, you perform some *average evaluation/consensus* over the values available.&#xD;&#xA;&#xD;&#xA;This is not the case for SVM. The algorithm is based on a [quadratic optimization](http://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png), that maximizes the distance from the closest documents of two different classes, using a hyperplane to make the split.&#xD;&#xA;&#xD;&#xA;![Hyperplane separating two different classes][1]&#xD;&#xA;&#xD;&#xA;So, the only *consensus* here is the resulting hyperplane, computed from the closest documents of each class. In other words, the classes are attributed to each point by calculating the distance from the point to the hyperplane derived. If the distance is positive, it belongs to a certain class, otherwise, it belongs to the other one.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/CCO7Z.png" />
  <row Id="521" PostHistoryTypeId="5" PostId="183" RevisionGUID="ee5e8890-91e5-4954-b3f5-c23195c0d238" CreationDate="2014-05-21T15:39:57.380" UserId="262" Comment="more structure and ipython video" Text="###Fancy animations are cool&#xD;&#xA;I was very impressed when I saw [this animation][1] of the [discourse][2] git repository. They used [Gourse][3] which is specifically for git. But it may give ideas about how to represent the dynamics of growth.&#xD;&#xA;&#xD;&#xA;###You can create animations with matplotlib&#xD;&#xA;[This stackoverflow answer][4] seems to point at a python/networkx/matplotlib solution.&#xD;&#xA;&#xD;&#xA;###But D3.js provides interaction&#xD;&#xA;If you're looking for a web-based solution then d3.js is excellent. See [this][5], [this][6] and [this][7] for example.&#xD;&#xA;See also [this stackoverflow question][8], the accepted answer points to D3.js again.&#xD;&#xA;&#xD;&#xA;##Conclusion&#xD;&#xA;&#xD;&#xA;I would be drawn towards the python/networkx options for network analysis (possibly to add attributes to your raw data file for example). Then, for visualisation and dissemination D3.js is perfect. You might be surprised how easy it can be to write d3.js once you get into it. I believe [it even works within an ipython notebook!][9]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://youtu.be/T7Ncmq6scck&#xD;&#xA;  [2]: http://www.discourse.org&#xD;&#xA;  [3]: https://code.google.com/p/gource/&#xD;&#xA;  [4]: http://stackoverflow.com/a/13571425/1083707&#xD;&#xA;  [5]: http://bl.ocks.org/mbostock/4062045&#xD;&#xA;  [6]: http://mbostock.github.io/d3/talk/20111116/force-collapsible.html&#xD;&#xA;  [7]: http://bl.ocks.org/mbostock/929623&#xD;&#xA;  [8]: http://stackoverflow.com/questions/7416659/interactive-graph-visualisation&#xD;&#xA;  [9]: https://www.youtube.com/watch?v=8UtoIR2IEkI" />
  <row Id="522" PostHistoryTypeId="2" PostId="188" RevisionGUID="1bd20c98-88ff-4f8e-bb99-d7d66b2af0d1" CreationDate="2014-05-21T16:13:25.590" UserId="82" Text="You can use map reduce algorithms in Hadoop without programming them in Java. It is called streaming and works like Linux piping. If you believe that you can port your functions to read and write to terminal, it should work nicely. [Here][1] is example blog post which shows how to use map reduce functions written in Python in Hadoop.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/" />
  <row Id="523" PostHistoryTypeId="2" PostId="189" RevisionGUID="f0909f06-5b4e-4e92-ba38-acb871f48488" CreationDate="2014-05-21T19:41:19.857" UserId="151" Text="I'm currently using GAMS (and more specifically CPLEX within GAMS) to solve a very large mixed integer programming problem. This allows me to parallelize the process over 4 cores (although I have more, CPLEX utilizes a maximum of 4 cores) and it finds an optimal solution in a relatively short amount of time. Is there an open source mixed integer programming tool that I could use as an alternative to GAMS and CPLEX? It must be comparable in speed or faster for me to consider it. I have a preference for R based solutions, but I am open to suggestions of all kinds and other users may be interested in different solutions." />
  <row Id="524" PostHistoryTypeId="1" PostId="189" RevisionGUID="f0909f06-5b4e-4e92-ba38-acb871f48488" CreationDate="2014-05-21T19:41:19.857" UserId="151" Text="Open source solver for large mixed integer programming task?" />
  <row Id="525" PostHistoryTypeId="3" PostId="189" RevisionGUID="f0909f06-5b4e-4e92-ba38-acb871f48488" CreationDate="2014-05-21T19:41:19.857" UserId="151" Text="&lt;r&gt;&lt;open-source&gt;&lt;parallelism&gt;&lt;mixed-integer&gt;&lt;optimization&gt;" />
  <row Id="526" PostHistoryTypeId="2" PostId="190" RevisionGUID="987d895e-0679-40f8-a686-a0598bd73a86" CreationDate="2014-05-22T12:14:49.727" UserId="173" Text="It turned out that this task was quite easy to accomplish using [vis.js][1]. [This][2] was the best example code which I have found.&#xD;&#xA;&#xD;&#xA;The example of what I have built upon this is [here][3] (scroll to the bottom of this post). This graph represents the growth of a subnetwork of Facebook friends. Green dots are females, blue ones are males. The darker the colour, the older the user. By clicking &quot;Dodaj węzły&quot; you can add more nodes and edges to the graph.&#xD;&#xA;&#xD;&#xA;Anyway, I am still interested in other ways to accomplish this task, so I won't accept any answer as for now.&#xD;&#xA;&#xD;&#xA;Thanks for your contributions!&#xD;&#xA;&#xD;&#xA;  [1]: http://visjs.org/&#xD;&#xA;  [2]: http://visjs.org/examples/graph/20_navigation.html&#xD;&#xA;  [3]: http://laboratoriumdanych.pl/jak-powstaje-siec/" />
  <row Id="527" PostHistoryTypeId="2" PostId="191" RevisionGUID="e9259adf-4713-43db-8f7f-a98ccd4011b9" CreationDate="2014-05-22T13:36:24.120" UserId="273" Text="Can someone explain me, how to classify a data like MNIST with MLBP-Neural network if I make more than one output (e.g 8), I mean if I just use one output I can easily classify the data, but if I use more than one, which output should I choose ?" />
  <row Id="528" PostHistoryTypeId="1" PostId="191" RevisionGUID="e9259adf-4713-43db-8f7f-a98ccd4011b9" CreationDate="2014-05-22T13:36:24.120" UserId="273" Text="Multi layer back propagation Neural network for classification" />
  <row Id="529" PostHistoryTypeId="3" PostId="191" RevisionGUID="e9259adf-4713-43db-8f7f-a98ccd4011b9" CreationDate="2014-05-22T13:36:24.120" UserId="273" Text="&lt;neuralnetwork&gt;" />
  <row Id="530" PostHistoryTypeId="2" PostId="192" RevisionGUID="d63c3ff3-97d1-40a7-bd6c-279a42e07b53" CreationDate="2014-05-22T15:15:41.133" UserId="88" Text="The most popular use case seem to be recommender systems of different kinds (such as recommending shopping items, users in social networks etc.).&#xD;&#xA;&#xD;&#xA;But what are other typical data science applications, which may be used in a different verticals?&#xD;&#xA;&#xD;&#xA;For example: customer churn prediction with machine learning, evaluating customer lifetime value, sales forecasting.&#xD;&#xA;" />
  <row Id="531" PostHistoryTypeId="1" PostId="192" RevisionGUID="d63c3ff3-97d1-40a7-bd6c-279a42e07b53" CreationDate="2014-05-22T15:15:41.133" UserId="88" Text="What are the most popular data science application use cases for consumer web companies" />
  <row Id="532" PostHistoryTypeId="3" PostId="192" RevisionGUID="d63c3ff3-97d1-40a7-bd6c-279a42e07b53" CreationDate="2014-05-22T15:15:41.133" UserId="88" Text="&lt;usecase&gt;&lt;consumerweb&gt;" />
  <row Id="533" PostHistoryTypeId="2" PostId="193" RevisionGUID="d6cee457-0656-4c40-9dd1-fa87f20129d5" CreationDate="2014-05-22T15:43:57.160" UserId="178" Text="It depends, of course, on the focus of the company: commerce, service, etc.  In adition to the use cases you suggested, some other use cases would be:&#xD;&#xA;&#xD;&#xA; - Funnel analysis: Analyzing the way in which consumers use a website and complete a sale may include data science techniques, especially if the company operates at a large scale.&#xD;&#xA; - Advertising: Companies that place ads use *a lot* of machine learning techniques to analyze and predict which ads would be most effective or most remunerative give the user's demographics that would view them." />
  <row Id="534" PostHistoryTypeId="2" PostId="194" RevisionGUID="a18690d0-d38f-4e7b-acee-6fc346a3c0ab" CreationDate="2014-05-22T19:20:14.130" UserId="108" Text="Suppose that you need to classify something in K classes, where K &gt; 2. In this case the most often setup I use is one hot encoding. You will have K output columns, and in the training set you will set all values to 0, except the one which has the category index, which could have value 1. Thus, for each training data set instance you will have all outputs with values 0 or 1, all outputs sum to 1 for each instance.&#xD;&#xA;&#xD;&#xA;This looks like a probability, which reminds me of a technique used often to connect some outputs which are modeled as probability. This is called softmax function, more details [on Wikipedia][1]. This will allow you to put some constraints on the output values (it is basically a logistic function generalization) so that the output values will be modeled as probabilities. &#xD;&#xA;&#xD;&#xA;Finally, with or without softmax you can use the output as a discriminant function to select the proper category.&#xD;&#xA;&#xD;&#xA;Another final thought would be to avoid to encode you variables in a connected way. For example you can have the binary representation of the category index. This would induce to the learner an artificial connection between some outputs which are arbitrary. The one hot encoding has the advantage that is neutral to how labels are indexed.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Softmax_activation_function" />
  <row Id="535" PostHistoryTypeId="2" PostId="195" RevisionGUID="f926bd04-c9c7-4d20-b0a8-f3a294ad3d0b" CreationDate="2014-05-22T20:48:55.297" UserId="92" Text="Satisfaction is a huge one that I run into a lot.&#xD;&#xA;&#xD;&#xA;The bottom line is that for very large services (search engines, facebook, linkedin, etc...) your users are simply a collection of log lines.  You have little ability to solicit feed back from them (not a hard and fast rule necessarily).  So you have to infer their positive or negative feedback most of the time.&#xD;&#xA;&#xD;&#xA;This means finding ways, even outside of predictive modelling, to truly tell, from a collection of log lines, whether or not someone actually liked something they experienced.  This simple act is even more fundamental (in my biased opinion) than a/b testing since you're talking about metrics you will eventually track on a test scorecard.&#xD;&#xA;&#xD;&#xA;Once you have a handle on good SAT metrics then you can start making predictive models and experimenting.  But even deciding what piece of log instrumentation can tell you about SAT is non-trivial (and often changes)." />
  <row Id="536" PostHistoryTypeId="2" PostId="196" RevisionGUID="24c0794d-2091-455b-87d3-8a465f0f091a" CreationDate="2014-05-22T21:47:26.980" UserId="275" Text="So we have potential for a machine learning application that fits fairly neatly into the traditional problem domain solved by classifiers, i.e., we have a set of attributes describing an item and a &quot;bucket&quot; that they end up in. However, rather than create models of probabilities like in Naive Bayes or similar classifiers, we want our output to be a set of roughly human-readable rules that can be reviewed and modified by an end user.&#xD;&#xA;&#xD;&#xA;Association rule learning looks like the family of algorithms that solves this type of problem, but these algorithms seem to focus on identifying common combinations of features and don't include the concept of a final bucket that those features might point to. For example, our data set looks something like this:&#xD;&#xA;&#xD;&#xA;    Item A { 4-door, small, steel } =&gt; { sedan }&#xD;&#xA;    Item B { 2-door, big,   steel } =&gt; { truck }&#xD;&#xA;    Item C { 2-door, small, steel } =&gt; { coupe }&#xD;&#xA;&#xD;&#xA;I just want the rules that say &quot;if it's big and a 2-door, it's a truck,&quot; not the rules that say &quot;if it's a 4-door it's also small.&quot; &#xD;&#xA;&#xD;&#xA;One workaround I can think of is to simply use association rule learning algorithms and ignore the rules that don't involve an end bucket, but that seems a bit hacky. Have I missed some family of algorithms out there? Or perhaps I'm approaching the problem incorrectly to begin with?&#xD;&#xA;&#xD;&#xA;Thanks in advance!" />
  <row Id="537" PostHistoryTypeId="1" PostId="196" RevisionGUID="24c0794d-2091-455b-87d3-8a465f0f091a" CreationDate="2014-05-22T21:47:26.980" UserId="275" Text="Algorithm for generating classification rules" />
  <row Id="538" PostHistoryTypeId="3" PostId="196" RevisionGUID="24c0794d-2091-455b-87d3-8a465f0f091a" CreationDate="2014-05-22T21:47:26.980" UserId="275" Text="&lt;machine-learning&gt;&lt;classification&gt;" />
  <row Id="539" PostHistoryTypeId="2" PostId="197" RevisionGUID="1ed7d7d3-a296-431c-95b0-a2f341c79c26" CreationDate="2014-05-22T21:54:05.660" UserId="108" Text="C45 made by Quinlan is able to produce rule for prediction. Check this [Wikipedia][1] page. I know that in [Weka][2] its name is J48. I have no idea which are implementations in R or Python. Anyway, from this kind of decision tree  you should be able to infer rules for prediction.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/C4.5_algorithm&#xD;&#xA;  [2]: http://www.cs.waikato.ac.nz/~ml/weka/" />
  <row Id="540" PostHistoryTypeId="5" PostId="197" RevisionGUID="1f78b08a-3fcd-41bd-b491-8cb9d48331fa" CreationDate="2014-05-22T21:59:22.117" UserId="108" Comment="added 508 characters in body" Text="C45 made by Quinlan is able to produce rule for prediction. Check this [Wikipedia][1] page. I know that in [Weka][2] its name is J48. I have no idea which are implementations in R or Python. Anyway, from this kind of decision tree  you should be able to infer rules for prediction.&#xD;&#xA;&#xD;&#xA;*Later edit*&#xD;&#xA;&#xD;&#xA;Also you might be interested in algorithms for directly inferring rules for classification. RIPPER is one, which again in Weka it received a different name JRip. See the original paper for RIPPER: [Fast Effective Rule Induction, W.W. Cohen 1995][3] &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/C4.5_algorithm&#xD;&#xA;  [2]: http://www.cs.waikato.ac.nz/~ml/weka/&#xD;&#xA;  [3]: http://www.google.ro/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCYQFjAA&amp;url=http://www.cs.utsa.edu/~bylander/cs6243/cohen95ripper.pdf&amp;ei=-XJ-U-7pGoqtyAOej4Ag&amp;usg=AFQjCNFqLnuJWi3gGXVCrugmv3NTRhHHLA&amp;bvm=bv.67229260,d.bGQ&amp;cad=rja" />
  <row Id="541" PostHistoryTypeId="2" PostId="198" RevisionGUID="942cf03f-a80c-443f-a00e-f18f48d99cbb" CreationDate="2014-05-23T03:05:57.990" UserId="88" Text="Also, there seem to be a very comprehensive list of data science use cases by function and by vertical on Kaggle - [&quot;Data Science Use Cases&quot;][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.kaggle.com/wiki/DataScienceUseCases" />
  <row Id="542" PostHistoryTypeId="5" PostId="196" RevisionGUID="ec581e41-ca26-422e-9889-beda67bf3a87" CreationDate="2014-05-23T03:27:20.630" UserId="84" Comment="deleted 22 characters in body" Text="So we have potential for a machine learning application that fits fairly neatly into the traditional problem domain solved by classifiers, i.e., we have a set of attributes describing an item and a &quot;bucket&quot; that they end up in. However, rather than create models of probabilities like in Naive Bayes or similar classifiers, we want our output to be a set of roughly human-readable rules that can be reviewed and modified by an end user.&#xD;&#xA;&#xD;&#xA;Association rule learning looks like the family of algorithms that solves this type of problem, but these algorithms seem to focus on identifying common combinations of features and don't include the concept of a final bucket that those features might point to. For example, our data set looks something like this:&#xD;&#xA;&#xD;&#xA;    Item A { 4-door, small, steel } =&gt; { sedan }&#xD;&#xA;    Item B { 2-door, big,   steel } =&gt; { truck }&#xD;&#xA;    Item C { 2-door, small, steel } =&gt; { coupe }&#xD;&#xA;&#xD;&#xA;I just want the rules that say &quot;if it's big and a 2-door, it's a truck,&quot; not the rules that say &quot;if it's a 4-door it's also small.&quot; &#xD;&#xA;&#xD;&#xA;One workaround I can think of is to simply use association rule learning algorithms and ignore the rules that don't involve an end bucket, but that seems a bit hacky. Have I missed some family of algorithms out there? Or perhaps I'm approaching the problem incorrectly to begin with?" />
  <row Id="543" PostHistoryTypeId="5" PostId="195" RevisionGUID="da925617-0fa9-4ae2-b06f-cbcc65d2d583" CreationDate="2014-05-23T06:03:40.577" UserId="92" Comment="added 53 characters in body" Text="Satisfaction is a huge one that I run into a lot.  Huge referring to importance/difficulty/complexity.&#xD;&#xA;&#xD;&#xA;The bottom line is that for very large services (search engines, facebook, linkedin, etc...) your users are simply a collection of log lines.  You have little ability to solicit feed back from them (not a hard and fast rule necessarily).  So you have to infer their positive or negative feedback most of the time.&#xD;&#xA;&#xD;&#xA;This means finding ways, even outside of predictive modelling, to truly tell, from a collection of log lines, whether or not someone actually liked something they experienced.  This simple act is even more fundamental (in my biased opinion) than a/b testing since you're talking about metrics you will eventually track on a test scorecard.&#xD;&#xA;&#xD;&#xA;Once you have a handle on good SAT metrics then you can start making predictive models and experimenting.  But even deciding what piece of log instrumentation can tell you about SAT is non-trivial (and often changes)." />
  <row Id="544" PostHistoryTypeId="33" PostId="128" RevisionGUID="231bdeaa-b248-4fba-9ccd-95e9ea7bafeb" CreationDate="2014-05-23T06:20:39.747" UserId="122" Comment="1" />
  <row Id="545" PostHistoryTypeId="33" PostId="130" RevisionGUID="80281580-45cb-434f-b8a2-bb4a6d4fd231" CreationDate="2014-05-23T06:20:55.193" UserId="122" Comment="2" />
  <row Id="546" PostHistoryTypeId="2" PostId="199" RevisionGUID="2b5594be-56eb-4cd4-aa79-455ae9ae1233" CreationDate="2014-05-23T06:25:50.480" UserId="122" Text="LDA has two hyperparameters, tuning them changes the induced topics. &#xD;&#xA;&#xD;&#xA;What does the alpha and beta hyperparameters contribute to LDA? &#xD;&#xA;&#xD;&#xA;How does the topic change if one or the other hyperparameters increase or decrease? &#xD;&#xA;&#xD;&#xA;Why are they hyperparamters and not just parameters?" />
  <row Id="547" PostHistoryTypeId="1" PostId="199" RevisionGUID="2b5594be-56eb-4cd4-aa79-455ae9ae1233" CreationDate="2014-05-23T06:25:50.480" UserId="122" Text="What does the alpha and beta hyperparameters contribute to in Latent Dirichlet allocation?" />
  <row Id="548" PostHistoryTypeId="3" PostId="199" RevisionGUID="2b5594be-56eb-4cd4-aa79-455ae9ae1233" CreationDate="2014-05-23T06:25:50.480" UserId="122" Text="&lt;topic-model&gt;&lt;lda&gt;&lt;parameter&gt;&lt;hyperparameter&gt;" />
  <row Id="549" PostHistoryTypeId="2" PostId="200" RevisionGUID="559420b4-78d1-4a89-9897-6da93f4f010f" CreationDate="2014-05-23T08:34:38.900" UserId="278" Text="You also can create a MongoDB-Hadoop [connection][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://docs.mongodb.org/ecosystem/tutorial/getting-started-with-hadoop/" />
  <row Id="550" PostHistoryTypeId="2" PostId="201" RevisionGUID="5209e052-c2f4-442e-b31f-d6a7582b8514" CreationDate="2014-05-23T09:09:44.490" UserId="97" Text=" In addition to the listed sources: some social network data sets.&#xD;&#xA;&#xD;&#xA; - [Stanford University large network dataset collection (SNAP)][1]&#xD;&#xA; - [A huge twitter dataset that includes followers][2] + [large collection of twitter datasets here][3]&#xD;&#xA; - [LastFM data set][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://snap.stanford.edu/data/&#xD;&#xA;  [2]: http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/&#xD;&#xA;  [3]: http://www.infochimps.com/collections/twitter-census&#xD;&#xA;  [4]: http://mtg.upf.edu/node/1671" />
  <row Id="551" PostHistoryTypeId="5" PostId="201" RevisionGUID="139ebe29-5d52-4bde-b875-7567db44d747" CreationDate="2014-05-23T09:19:41.627" UserId="97" Comment="added more sources" Text="In addition to the listed sources.&#xD;&#xA;&#xD;&#xA;Some social network data sets:&#xD;&#xA;&#xD;&#xA; - [Stanford University large network dataset collection (SNAP)][1]&#xD;&#xA; - [A huge twitter dataset that includes followers][2] + [large collection of twitter datasets here][3]&#xD;&#xA; - [LastFM data set][4]&#xD;&#xA;&#xD;&#xA;There are plenty of sources listed at Stats SE:&#xD;&#xA;&#xD;&#xA; - [Locating freely available data samples][5]&#xD;&#xA; - [Data APIs/feeds available as packages in R][6]&#xD;&#xA; - [Free data set for very high dimensional classification][7]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://snap.stanford.edu/data/&#xD;&#xA;  [2]: http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/&#xD;&#xA;  [3]: http://www.infochimps.com/collections/twitter-census&#xD;&#xA;  [4]: http://mtg.upf.edu/node/1671&#xD;&#xA;  [5]: http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/&#xD;&#xA;  [6]: http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r&#xD;&#xA;  [7]: http://stats.stackexchange.com/questions/973/free-data-set-for-very-high-dimensional-classification" />
  <row Id="552" PostHistoryTypeId="2" PostId="202" RevisionGUID="9d319c11-fd98-4196-be06-15be8d76f19a" CreationDate="2014-05-23T13:47:54.603" UserId="108" Text="The Dirichlet distribution is a multivariate distribution. We can denote the parameters of the Dirichlet as a vector of size K of the form ~ 1/B(a) * Product(x_i ^ (a_i-1)), where a is the vector of size K of the parameters, and sum of x_i = 1.&#xD;&#xA;&#xD;&#xA;Now the LDA uses some constructs like:&#xD;&#xA;- a document can have multiple topics (because of this multiplicity, we need the Dirichlet distribution); and there is a Dirichlet distribution which models this relation&#xD;&#xA;- words can also belong to multiple topics, when you consider them outside of a document; so here we need another Dirichlet to model this&#xD;&#xA;&#xD;&#xA;The previous two are distributions which you do not really see from data, this is why is called latent, or hidden.&#xD;&#xA;&#xD;&#xA;Now, in Bayesian inference you use the Bayes rule to infer the posterior probability. For simplicity, let's say you have data *x* and you have a model for this data governed by some parameters theta. In order to infer values for this parameters, in full Bayesian inference you will infer the posterior probability of these parameters using Bayes' rule with *p(theta|x) = p(x|theta)p(theta|alpha)/p(x|alpha)*. In plain words is *posterior probability = likelihood x prior probability / marginal likelihood*. Note that here comes an *alpha*. This is your initial belief about this distribution, and is the parameter of the prior distribution. Usually this is chosen in such a way that will have a conjugate prior (so the distribution of the posterior is the same with the distribution of the prior) and often to encode some knowledge if you have one or to have maximum entropy if you know nothing.&#xD;&#xA;&#xD;&#xA;The parameters of the prior are called *hyperparameters*. So, in LDA, both topic distributions, over documents and over words have also correspondent priors, which are denoted usually with alpha and beta, and because are the parameters of the prior distributions are called hyperparameters. &#xD;&#xA;&#xD;&#xA;Now about choosing priors. If you plot some Dirichlet distributions you will note that if the individual parameters *alpha_k* have the same value, the pdf is symmetric in the simplex defined by the *x* values, which is the minimum or maximum for pdf is at the center. &#xD;&#xA;&#xD;&#xA;If all the alpha_k have values lower than unit the maximum is found at corners&#xD;&#xA;&#xD;&#xA;&lt;img src=&quot;http://i.stack.imgur.com/5khZE.png&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;&#xD;&#xA;&#xD;&#xA;or can if all values alpha_k are the same and greater than 1 the maximum will be found in center like &#xD;&#xA;&#xD;&#xA;&lt;img src=&quot;http://research.microsoft.com/en-us/um/people/cmbishop/prml/prmlfigs-png/Figure2.5c.png&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;&#xD;&#xA;&#xD;&#xA;It is easy to see that if values for alpha_k are not equal the symmetry is broken and the maximum will be found near bigger values. &#xD;&#xA;&#xD;&#xA;Additional, please note that values for priors parameters produce smooth pdfs of the distribution as the values of the parameters are near 1. So if you have great confidence that something is clearly distributed in a way you know, with a high degree of confidence, than values far from 1 in absolute value are to be used, if you do not have such kind of knowledge than values near 1 would be encode this lack of knowledge. It is easy to see why 1 plays such a role in Dirichlet distribution from the formula of the distribution itself.&#xD;&#xA;&#xD;&#xA;Another way to understand this is to see that prior encode prior-knowledge. In the same time you might think that prior encode some prior seen data. This data was not saw by the algorithm itself, it was saw by you, you learned something, and you can model prior according to what you know (learned). So in the prior parameters (hyperparameters) you encode also how big this data set you apriori saw, because the sum of alpha_k can be that also as the size of this more or less imaginary data set. So the bigger the prior data set, the bigger is the confidence, the bigger the values of alpha_k you can choose, the sharper the surface near maximum value, which means also less doubts. &#xD;&#xA;&#xD;&#xA;Hope it helped.&#xD;&#xA;&#xD;&#xA;PS: It's a hell to write something without LaTeX notation. I home moderators/administrators will do something." />
  <row Id="553" PostHistoryTypeId="2" PostId="203" RevisionGUID="b0f5b49e-8b99-4284-91ff-34b0e69802e8" CreationDate="2014-05-23T14:28:41.563" UserId="265" Text="Never done stuff **on that scale**, but as no-one else has jumped in yet have you seen these two papers that discuss non-commercial solutions?  Symphony and COIN-OR seem to be the dominant suggestions.&#xD;&#xA;&#xD;&#xA;Linderoth, Jeffrey T., and Andrea Lodi. &quot;MILP software.&quot; Wiley encyclopedia of operations research and management science (2010). [PDF version][1]&#xD;&#xA;&#xD;&#xA;Linderoth, Jeffrey T., and Ted K. Ralphs. &quot;Noncommercial software for mixed-integer linear programming.&quot; Integer programming: theory and practice 3 (2005): 253-303. [Compares performance][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://homepages.cae.wisc.edu/~Linderot/papers/Linderoth-Lodi-10.pdf&#xD;&#xA;  [2]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.5872" />
  <row Id="554" PostHistoryTypeId="2" PostId="204" RevisionGUID="b2bee134-b885-4070-ac32-b419a2afe85e" CreationDate="2014-05-23T19:28:01.903" UserId="250" Text="I recently did a similar project in Python (predicting opinions using FB like data), and had good results with the following basic process:&#xD;&#xA;&#xD;&#xA; 1. Read in the training set (n = N) by iterating over comma-delimited like records line-by-line and use a counter to identify the most popular pages&#xD;&#xA; 2. For each of the K most popular pages (I used about 5000, but you can play around with different values), use pandas.DataFrame.isin to test whether each individual in the training set likes each page, then make a N x K dataframe of the results (I'll call it xdata_train)&#xD;&#xA; 3. Create a series (I'll call it ydata_train) containing all of the outcome variables (in my case opinions, in yours age) with the same index as xdata_train&#xD;&#xA; 4. Set up a random forest classifier through scikit-learn to predict&#xD;&#xA;    ydata_train based on xdata_train&#xD;&#xA; 5. Use scikit-learn's cross-validation testing to tweak parameters and&#xD;&#xA;    refine accuracy (tweaking number of popular pages, number of trees,&#xD;&#xA;    min leaf size, etc.)&#xD;&#xA; 6. Output random forest classifier and list of most popular pages with pickle (or keep in memory if you are doing everything at once)&#xD;&#xA; 7. Load in the rest of your data, load the list of popular pages (if necessary), and repeat step 2 to produce xdata_new&#xD;&#xA; 8. Load the random forest classifier (if necessary) and use it to predict values for the xdata_new data&#xD;&#xA; 9. Output the predicted scores to a new CSV or other output format of your choosing&#xD;&#xA;&#xD;&#xA;In your case, you'd need to swap out the classifier for a regressor (so see here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) but otherwise the same process should work without much trouble. &#xD;&#xA;&#xD;&#xA;Also, you should be aware of the most amazing feature of random forests in Python: instant parallelization! Those of us who started out doing this in R and then moved over are always amazed, especially when you get to work on a machine with a few dozen cores (see here: http://blog.yhathq.com/posts/comparing-random-forests-in-python-and-r.html).&#xD;&#xA;&#xD;&#xA;Finally, note that this would be a perfect application for network analysis if you have the data on friends as well as the individuals themselves. If you can analyze the ages of a user's friends, the age of the user will almost certainly be within a year or two of the median among his or her friends, particularly if the users are young enough to have built their friend networks while still in school (since most will be classmates). That prediction would likely trump any you would get from modeling---this is a textbook example of a problem where the right data &gt; the right model every time. &#xD;&#xA;&#xD;&#xA;Good luck!&#xD;&#xA;" />
  <row Id="555" PostHistoryTypeId="2" PostId="205" RevisionGUID="23adfc19-f8ac-4163-b0bf-e5501f9176d0" CreationDate="2014-05-23T19:45:54.283" UserId="250" Text="Working on what could often be called &quot;medium data&quot; projects, I've been able to parallelize my code (mostly for modeling and prediction in Python) on a single system across anywhere from 4 to 32 cores. Now I'm looking at scaling up to clusters on EC2 (probably with StarCluster/IPython, but open to other suggestions as well), and have been puzzled by how to reconcile distributing work across cores on an instance vs. instances on a cluster.&#xD;&#xA;&#xD;&#xA;Is it even practical to parallelize across instances as well as across cores on each instance? If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance? &#xD;&#xA;&#xD;&#xA;Bandwidth and RAM are non-trivial concerns in my projects, but it's easy to spot when those are the bottlenecks and readjust. It's much harder, I'd imagine, to benchmark the right mix of cores to instances without repeated testing, and my projects vary too much for any single test to apply to all circumstances. Thanks in advance, and if I've just failed to google this one properly, feel free to point me to the right answer somewhere else!&#xD;&#xA;&#xD;&#xA;" />
  <row Id="556" PostHistoryTypeId="1" PostId="205" RevisionGUID="23adfc19-f8ac-4163-b0bf-e5501f9176d0" CreationDate="2014-05-23T19:45:54.283" UserId="250" Text="Instances vs. cores when using EC2" />
  <row Id="557" PostHistoryTypeId="3" PostId="205" RevisionGUID="23adfc19-f8ac-4163-b0bf-e5501f9176d0" CreationDate="2014-05-23T19:45:54.283" UserId="250" Text="&lt;parallel&gt;&lt;clusters&gt;&lt;aws&gt;" />
  <row Id="558" PostHistoryTypeId="2" PostId="206" RevisionGUID="ae763f36-1ff4-4f38-8c28-8d7072134a8e" CreationDate="2014-05-23T21:01:18.630" UserId="172" Text="All things considered equal (cost, CPU perf, etc.) you could choose the smallest instance that can hold all of my dataset in memory and scale out. That way &#xD;&#xA;&#xD;&#xA;- you make sure not to induce unnecessary latencies due to network communications, and&#xD;&#xA;- you tend to maximize the overall available memory bandwidth for your processes.&#xD;&#xA;&#xD;&#xA;Assuming you are running some sort of [cross-validation scheme][1] to optimize some [meta parameter][2] of your model, assign each core a value to test and choose an many instances as needed to cover all the parameter space in as few rounds as you see fit.&#xD;&#xA;&#xD;&#xA;If your data does not fit in the memory of one system, of course you'll need to distribute across instances. Then it is a matter of balancing memory latency (better with many instances) with network latency (better with fewer instances) but given the nature of EC2 I'd bet you'll often prefer to work with few fat instances.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Meta-optimization" />
  <row Id="559" PostHistoryTypeId="2" PostId="207" RevisionGUID="156b946a-e3c1-4a59-93e7-ae69ebebfab9" CreationDate="2014-05-24T10:36:58.987" UserId="21" Text="A general rule of thumb is to not distribute until you have to. It's usually more efficient to have N servers of a certain capacity than 2N servers of half that capacity. More of the data access will be local, and therefore fast in memory versus slow across the network.&#xD;&#xA;&#xD;&#xA;At a certain point, scaling up one machine becomes uneconomical because the cost of additional resource scales more than linearly. However this point is still amazingly high.&#xD;&#xA;&#xD;&#xA;On Amazon in particular though, the economics of each instance type can vary a lot if you are using spot market instances. The default pricing more or less means that the same amount of resource costs about the same regardless of the instance type, that can vary a lot; large instances can be cheaper than small ones, or N small instances can be much cheaper than one large machine with equivalent resources.&#xD;&#xA;&#xD;&#xA;One massive consideration here is that the computation paradigm can change quite a lot when you move from one machine to multiple machines. The tradeoffs that the communication overhead induce may force you to, for example, adopt a data-parallel paradigm to scale. That means a different choice of tools and algorithm. For example, SGD looks quite different in-memory and in Python than on MapReduce. So you would have to consider this before parallelizing.&#xD;&#xA;&#xD;&#xA;You may choose to distribute work across a cluster, even if a single node and non-distributed paradigms work for you, for reliability. If a single node fails, you lose all of the computation; a distributed computation can potentially recover and complete just the part of the computation that was lost." />
  <row Id="560" PostHistoryTypeId="2" PostId="208" RevisionGUID="c677f842-6942-44bf-b85f-446860ed5658" CreationDate="2014-05-24T11:18:26.497" UserId="26" Text="When using IPython, you very nearly don't have to worry about it (at the expense of some loss of efficiency/greater communication overhead).  The parallel IPython plugin in StarCluster will by default start one engine per physical core on each node (I believe this is configurable but not sure where).  You just run whatever you want across all engines by using the DirectView api (map_sync, apply_sync, ...) or the %px magic commands.   If you are already using IPython in parallel on one machine, using it on a cluster is no different.&#xD;&#xA;&#xD;&#xA;Addressing some of your specific questions:&#xD;&#xA;&#xD;&#xA;&quot;how to reconcile distributing work across cores on an instance vs. instances on a cluster&quot; - You get one engine per core (at least); work is automatically distributed across all cores and across all instances.&#xD;&#xA;&#xD;&#xA;&quot;Is it even practical to parallelize across instances as well as across cores on each instance?&quot; - Yes :)  If the code you are running is embarrassingly parallel (exact same algo on multiple data sets) then you can mostly ignore where a particular engine is running.  If the core requires a lot of communication between engines, then of course you need to structure it so that engines primarily communicate with other engines on the same physical machine; but that kind of problem is not ideally suited for IPython, I think.&#xD;&#xA;&#xD;&#xA;&quot;If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance?&quot; - Use the largest c3 instances for compute-bound, and the smallest for memory-bandwidth-bound problems (or small enough that the problem almost stops being memory-bandwidth-bound); for message-passing-bound problems, also use the largest instances but try to partition the problem so that each partition runs on one physical machine and most message passing is within the same partition.  Problems which run slower on n quadruple c3 than on 2n double c3 are rare (an artificial example may be running multiple simple filters on a large number of images, where you go through all images for each filter rather than all filters for the same image).  Using largest instances is a good rule of thumb.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="561" PostHistoryTypeId="5" PostId="208" RevisionGUID="0751f7cf-edc8-4185-9d29-24865ef39d12" CreationDate="2014-05-24T12:12:31.857" UserId="26" Comment="added 14 characters in body" Text="When using IPython, you very nearly don't have to worry about it (at the expense of some loss of efficiency/greater communication overhead).  The parallel IPython plugin in StarCluster will by default start one engine per physical core on each node (I believe this is configurable but not sure where).  You just run whatever you want across all engines by using the DirectView api (map_sync, apply_sync, ...) or the %px magic commands.   If you are already using IPython in parallel on one machine, using it on a cluster is no different.&#xD;&#xA;&#xD;&#xA;Addressing some of your specific questions:&#xD;&#xA;&#xD;&#xA;&quot;how to reconcile distributing work across cores on an instance vs. instances on a cluster&quot; - You get one engine per core (at least); work is automatically distributed across all cores and across all instances.&#xD;&#xA;&#xD;&#xA;&quot;Is it even practical to parallelize across instances as well as across cores on each instance?&quot; - Yes :)  If the code you are running is embarrassingly parallel (exact same algo on multiple data sets) then you can mostly ignore where a particular engine is running.  If the core requires a lot of communication between engines, then of course you need to structure it so that engines primarily communicate with other engines on the same physical machine; but that kind of problem is not ideally suited for IPython, I think.&#xD;&#xA;&#xD;&#xA;&quot;If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance?&quot; - Use the largest c3 instances for compute-bound, and the smallest for memory-bandwidth-bound problems (or small enough that the problem almost stops being memory-bandwidth-bound); for message-passing-bound problems, also use the largest instances but try to partition the problem so that each partition runs on one physical machine and most message passing is within the same partition.  Problems which run significantly slower on N quadruple c3 than on 2N double c3 are rare (an artificial example may be running multiple simple filters on a large number of images, where you go through all images for each filter rather than all filters for the same image).  Using largest instances is a good rule of thumb.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="562" PostHistoryTypeId="2" PostId="209" RevisionGUID="5c239016-baf9-4c0b-86e0-1613cff2356a" CreationDate="2014-05-25T13:57:52.657" UserId="71" Text="A recommendation system keeps a log of what recommendations have been made to a particular user and whether that user accepts the recommendation. It's like&#xD;&#xA;&#xD;&#xA;    user_id item_id result&#xD;&#xA;    1       4       1&#xD;&#xA;    1       7       -1&#xD;&#xA;    5       19      1&#xD;&#xA;    5       80      1&#xD;&#xA;&#xD;&#xA;where 1 means the user accepted the recommendation while -1 means the user did not respond to the recommendation. &#xD;&#xA;&#xD;&#xA;**Question:** If I am going to make recommendations to a bunch of users based on the kind of log described above, and I want to maximize MAP@3 scores, how should I deal with the implicit data (1 or -1)?&#xD;&#xA;&#xD;&#xA;My idea is to treat 1 and -1 as ratings, and predict the rating using factorization machines-type algorithms. But this does not seem right, given the asymmetry of the implicit data (-1 does not mean the user does not like the recommendation)." />
  <row Id="563" PostHistoryTypeId="1" PostId="209" RevisionGUID="5c239016-baf9-4c0b-86e0-1613cff2356a" CreationDate="2014-05-25T13:57:52.657" UserId="71" Text="How should one deal with implicit data in recommendation" />
  <row Id="564" PostHistoryTypeId="3" PostId="209" RevisionGUID="5c239016-baf9-4c0b-86e0-1613cff2356a" CreationDate="2014-05-25T13:57:52.657" UserId="71" Text="&lt;recommendation&gt;" />
  <row Id="565" PostHistoryTypeId="2" PostId="210" RevisionGUID="5e1b3be6-9cd8-4d0d-98b0-36e7b4634125" CreationDate="2014-05-26T04:07:32.390" UserId="283" Text="Assuming symmetric Dirichlet distributions (for simplicity), a low alpha value places more weight on having each document composed of only a few dominant topics (whereas a high value will return many more relatively dominant topics). Similarly, a low beta value places more weight on having each topic composed of only a few dominant words." />
  <row Id="566" PostHistoryTypeId="5" PostId="209" RevisionGUID="782e8050-c987-4a5f-b91c-c6fcdeca23c3" CreationDate="2014-05-26T05:12:32.653" UserId="71" Comment="added 682 characters in body" Text="A recommendation system keeps a log of what recommendations have been made to a particular user and whether that user accepts the recommendation. It's like&#xD;&#xA;&#xD;&#xA;    user_id item_id result&#xD;&#xA;    1       4       1&#xD;&#xA;    1       7       -1&#xD;&#xA;    5       19      1&#xD;&#xA;    5       80      1&#xD;&#xA;&#xD;&#xA;where 1 means the user accepted the recommendation while -1 means the user did not respond to the recommendation. &#xD;&#xA;&#xD;&#xA;**Question:** If I am going to make recommendations to a bunch of users based on the kind of log described above, and I want to maximize MAP@3 scores, how should I deal with the implicit data (1 or -1)?&#xD;&#xA;&#xD;&#xA;My idea is to treat 1 and -1 as ratings, and predict the rating using factorization machines-type algorithms. But this does not seem right, given the asymmetry of the implicit data (-1 does not mean the user does not like the recommendation).&#xD;&#xA;&#xD;&#xA;**Edit 1**&#xD;&#xA;Let us think about it in the context of a matrix factorization approach. If we treat -1 and 1 as ratings, there will be some problem. For example, user 1 likes movie A which scores high in one factor (e.g. having glorious background music) in the latent factor space. The system recommends movie B which also scores high in &quot;glorious background music&quot;, but for some reason user 1 is too busy to look into the recommendation, and we have a -1 rating movie B. If we just treat 1 or -1 equally, then the system might be discouraged to recommend movie with glorious BGM to user 1 while user 1 still loves movie with glorious BGM. I think this situation is to be avoided." />
  <row Id="567" PostHistoryTypeId="2" PostId="211" RevisionGUID="0e20cd8f-34d0-44ca-9656-fb127e4cbf2e" CreationDate="2014-05-27T10:41:33.220" UserId="295" Text="I'm new to this community and hopefully my question will well fit in here.&#xD;&#xA;As part of my undergraduate data analytics course I have choose to do the project on human activity recognition using smartphone data sets. As far as I'm concern this topic relates to Machine Learning and Support Vector Machines. I'm not well familiar with this technologies yet so I will need some help. &#xD;&#xA;&#xD;&#xA;I have decided to follow this project idea http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html (first project on the top)&#xD;&#xA;The project goal is determine what activity a person is engaging in (e.g., WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subject's waist. Using its embedded accelerometer and gyroscope, the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz.&#xD;&#xA;&#xD;&#xA;All the data set is given in one folder with some description and feature labels. The data is divided for 'test' and 'train' files in which data is represented in this format:&#xD;&#xA;&#xD;&#xA;      2.5717778e-001 -2.3285230e-002 -1.4653762e-002 -9.3840400e-001 -9.2009078e-001 -6.6768331e-001 -9.5250112e-001 -9.2524867e-001 -6.7430222e-001 -8.9408755e-001 -5.5457721e-001 -4.6622295e-001  7.1720847e-001  6.3550240e-001  7.8949666e-001 -8.7776423e-001 -9.9776606e-001 -9.9841381e-001 -9.3434525e-001 -9.7566897e-001 -9.4982365e-001 -8.3047780e-001 -1.6808416e-001 -3.7899553e-001  2.4621698e-001  5.2120364e-001 -4.8779311e-001  4.8228047e-001 -4.5462113e-002  2.1195505e-001 -1.3489443e-001  1.3085848e-001 -1.4176313e-002 -1.0597085e-001  7.3544013e-002 -1.7151642e-001  4.0062978e-002  7.6988933e-002 -4.9054573e-001 -7.0900265e-001&#xD;&#xA;&#xD;&#xA;And that's only a vary small sample of what the file contain. &#xD;&#xA;&#xD;&#xA;I don't really know what this data represents and how can be interpreted. Also for analyzing, classification and clustering of the data, what tools will I need to use? &#xD;&#xA;Is there any way I can put this data into excel with labels included and for example use R or python to extract sample data and work on this?&#xD;&#xA;&#xD;&#xA;Any hints/tips would be much appreciated. &#xD;&#xA;&#xD;&#xA;Thank you!" />
  <row Id="568" PostHistoryTypeId="1" PostId="211" RevisionGUID="0e20cd8f-34d0-44ca-9656-fb127e4cbf2e" CreationDate="2014-05-27T10:41:33.220" UserId="295" Text="Human activity recognition using smartphone data set problem" />
  <row Id="569" PostHistoryTypeId="3" PostId="211" RevisionGUID="0e20cd8f-34d0-44ca-9656-fb127e4cbf2e" CreationDate="2014-05-27T10:41:33.220" UserId="295" Text="&lt;bigdata&gt;&lt;machine-learning&gt;&lt;databases&gt;&lt;clustering&gt;&lt;data-mining&gt;" />
  <row Id="570" PostHistoryTypeId="2" PostId="212" RevisionGUID="e2633786-6b37-4632-933c-6e981de9961c" CreationDate="2014-05-27T10:58:00.620" UserId="21" Text="Your system isn't just trained on items that are recommended right? if so you have a big feedback loop here. You want to learn from all clicks/views, I hope.&#xD;&#xA;&#xD;&#xA;You suggest that not-looking at an item is a negative signal. I strongly suggest you do not treat it that way. Not interacting with something is almost always best treated as no information. If you have an explicit signal that indicates a dislike, like a down vote (or, maybe watched 10 seconds of a video and stopped), maybe that's valid.&#xD;&#xA;&#xD;&#xA;I would not construe this input as rating-like data. (Although in your case, you may get away with it.) Instead think of them as weights, which is exactly the treatment in the Hu Koren Volinsky paper on ALS that @Trey mentions in a comment. This lets you record relative strength of positive/negative interactions.&#xD;&#xA;&#xD;&#xA;Finally I would note that this paper, while is very likely to be what you're looking for, does not provide for negative weights. It is simple to extend in this way. If you get that far I can point you to the easy extension, which exists already in two implementations that I know of, in [Spark][1] and [Oryx][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala&#xD;&#xA;  [2]: https://github.com/cloudera/oryx/blob/master/als-common/src/main/java/com/cloudera/oryx/als/common/factorizer/als/AlternatingLeastSquares.java" />
  <row Id="571" PostHistoryTypeId="2" PostId="213" RevisionGUID="e0ca2a3a-da57-4e61-b42c-e405ea2039f0" CreationDate="2014-05-27T12:07:45.920" UserId="59" Text="The data set definitions are on the page here:&#xD;&#xA;&#xD;&#xA;[Attribute Information at the bottom][1]&#xD;&#xA;&#xD;&#xA;or you can see inside the ZIP folder the file named activity_labels, that has your column headings inside of it, make sure you read the README carefully, it has some good info in it. You can easily bring in a `.csv` file in R using the `read.csv` command.&#xD;&#xA;&#xD;&#xA;For example if you name you file `samsungdata` you can open R and run this command:&#xD;&#xA;&#xD;&#xA;    data &lt;- read.csv(&quot;directory/where/file/is/located/samsungdata.csv&quot;, header = TRUE)&#xD;&#xA;&#xD;&#xA;Or if you are already inside of the working directory in R you can just run the following&#xD;&#xA;&#xD;&#xA;    data &lt;- read.csv(&quot;samsungdata.csv&quot;, header = TRUE)&#xD;&#xA;&#xD;&#xA;Where the name `data` can be changed to whatever you want to call your data set.&#xD;&#xA;&#xD;&#xA;  [1]: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones#" />
  <row Id="574" PostHistoryTypeId="5" PostId="211" RevisionGUID="7ce1dc45-e93b-4713-95c0-5647f929b02f" CreationDate="2014-05-27T14:57:34.150" UserId="84" Comment="Fixed grammar, and improving formatting." Text="I'm new to this community and hopefully my question will well fit in here.&#xD;&#xA;As part of my undergraduate data analytics course I have choose to do the project on human activity recognition using smartphone data sets. As far as I'm concern this topic relates to Machine Learning and Support Vector Machines. I'm not well familiar with this technologies yet so I will need some help. &#xD;&#xA;&#xD;&#xA;I have decided to follow this project idea http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html (first project on the top)&#xD;&#xA;The project goal is determine what activity a person is engaging in (e.g., WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subject's waist. Using its embedded accelerometer and gyroscope, the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz.&#xD;&#xA;&#xD;&#xA;All the data set is given in one folder with some description and feature labels. The data is divided for 'test' and 'train' files in which data is represented in this format:&#xD;&#xA;&#xD;&#xA;      2.5717778e-001 -2.3285230e-002 -1.4653762e-002 -9.3840400e-001 -9.2009078e-001 -6.6768331e-001 -9.5250112e-001 -9.2524867e-001 -6.7430222e-001 -8.9408755e-001 -5.5457721e-001 -4.6622295e-001  7.1720847e-001  6.3550240e-001  7.8949666e-001 -8.7776423e-001 -9.9776606e-001 -9.9841381e-001 -9.3434525e-001 -9.7566897e-001 -9.4982365e-001 -8.3047780e-001 -1.6808416e-001 -3.7899553e-001  2.4621698e-001  5.2120364e-001 -4.8779311e-001  4.8228047e-001 -4.5462113e-002  2.1195505e-001 -1.3489443e-001  1.3085848e-001 -1.4176313e-002 -1.0597085e-001  7.3544013e-002 -1.7151642e-001  4.0062978e-002  7.6988933e-002 -4.9054573e-001 -7.0900265e-001&#xD;&#xA;&#xD;&#xA;And that's only a very small sample of what the file contain. &#xD;&#xA;&#xD;&#xA;I don't really know what this data represents and how can be interpreted. Also for analyzing, classification and clustering of the data, what tools will I need to use? &#xD;&#xA;Is there any way I can put this data into excel with labels included and for example use R or python to extract sample data and work on this?&#xD;&#xA;&#xD;&#xA;Any hints/tips would be much appreciated." />
  <row Id="575" PostHistoryTypeId="2" PostId="214" RevisionGUID="af834d70-2dbe-42eb-a6e8-9bbdacbcdb40" CreationDate="2014-05-27T16:05:02.883" UserId="295" Text="Public Data Sets&#xD;&#xA;&#xD;&#xA; - &#xD;&#xA;&#xD;&#xA;https://www.opensciencedatacloud.org/publicdata/&#xD;&#xA;&#xD;&#xA;Google Public Data Sets&#xD;&#xA;&#xD;&#xA; - &#xD;&#xA;http://www.google.com/publicdata/directory&#xD;&#xA;&#xD;&#xA;Amazon Web Services&#xD;&#xA;&#xD;&#xA; - &#xD;&#xA;&#xD;&#xA;https://aws.amazon.com/publicdatasets/&#xD;&#xA;&#xD;&#xA;Finding Data on the Internet&#xD;&#xA;&#xD;&#xA; - &#xD;&#xA;&#xD;&#xA;http://www.inside-r.org/howto/finding-data-internet" />
  <row Id="576" PostHistoryTypeId="2" PostId="215" RevisionGUID="7435ff22-915f-4106-b77c-58bab667b4f8" CreationDate="2014-05-27T21:07:48.973" UserId="250" Text="I'm building a workflow for creating machine learning models (in my case, using Python's `pandas` and `sklearn` packages) from data pulled from a very large database (here, Vertica by way of SQL and `pyodbc`), and a critical step in that process involves imputing missing values of the predictors. This is straightforward within a single analytics or stats platform---be it Python, R, Stata, etc.---but I'm curious where best to locate this step in a multi-platform workflow.&#xD;&#xA;&#xD;&#xA;It's simple enough to do this in Python, either with the &lt;a href=&quot;http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values&quot;&gt;`sklearn.preprocessing.Imputer`&lt;/a&gt; class, using the &lt;a href=&quot;http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.fillna.html&quot;&gt;`pandas.DataFrame.fillna`&lt;/a&gt; method, or by hand (depending upon the complexity of the imputation method used). But since I'm going to be using this for dozens or hundreds of columns across hundreds of millions of records, I wonder if there's a more efficient way to do this directly through SQL ahead of time. Aside from the potential efficiencies of doing this in a distributed platform like Vertica, this would have the added benefit of allowing us to create an automated pipeline for building &quot;complete&quot; versions of tables, so we don't need to fill in a new set of missing values from scratch every time we want to run a model.&#xD;&#xA;&#xD;&#xA;I haven't been able to find much guidance about this, but I imagine that we could:&#xD;&#xA;&#xD;&#xA; 1. create a table of substitute values (e.g., mean/median/mode, either overall or by group) for each incomplete column&#xD;&#xA; 2. join the substitute value table with the original table to assign a substitute value for each row and incomplete column&#xD;&#xA; 3. use a series of case statements to take the original value if available and the substitute value otherwise&#xD;&#xA;&#xD;&#xA;Is this a reasonable thing to do in Vertica/SQL, or is there a good reason not to bother and just handle it in Python instead? And if the latter, is there a strong case for doing this in pandas rather than sklearn or vice-versa? Thanks!" />
  <row Id="577" PostHistoryTypeId="1" PostId="215" RevisionGUID="7435ff22-915f-4106-b77c-58bab667b4f8" CreationDate="2014-05-27T21:07:48.973" UserId="250" Text="Where in the workflow should we deal with missing data?" />
  <row Id="578" PostHistoryTypeId="3" PostId="215" RevisionGUID="7435ff22-915f-4106-b77c-58bab667b4f8" CreationDate="2014-05-27T21:07:48.973" UserId="250" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;pandas&gt;&lt;sklearn&gt;&lt;vertica&gt;" />
  <row Id="580" PostHistoryTypeId="2" PostId="216" RevisionGUID="04cffb36-5fb5-45d9-a1f5-0e1777c646ec" CreationDate="2014-05-28T07:08:05.393" UserId="108" Text="My strong opinion regarding automated tasks like imputation (but, here I can include also scaling, centering, feature selection, etc) is to avoid in any way do such things without carefully inspecting your data. &#xD;&#xA;&#xD;&#xA;Of course, after deciding what kind of imputation to apply it can be automated (under the assumption that the new data has the same shape/problems).&#xD;&#xA;&#xD;&#xA;So, before anything, take a wise decision. I often wasted time trying to automate this things, destroying my data. I will give you some examples:&#xD;&#xA;- a marketplace encoded as N/A, which I missed and considered to be North/America&#xD;&#xA;- numbers like -999.0, because the data producer could not find a better replacement for missing data&#xD;&#xA;- number like 0 for blood pressure or body temperature, instead of missing data (it is hard to imagine a living human with 0 blood pressure)&#xD;&#xA;- multiple placeholders for missing data, due to the fact that the data was collected from various sources&#xD;&#xA;&#xD;&#xA;After that you need to understand what kind of imputation would resemble better the information from your data for a given task. This is often much harder to do it right than it seems.&#xD;&#xA;&#xD;&#xA;After all those things, my advice is to delay your imputation task to an upper layer where you have tools to reproduce on new data and to inspect if the assumptions for the new data are not violated (if it is possible)." />
  <row Id="581" PostHistoryTypeId="2" PostId="217" RevisionGUID="1637bff0-8316-4b9e-ba2c-d541508da5a0" CreationDate="2014-05-28T09:43:54.197" UserId="82" Text="It looks like this (or very similar data set) is used for Coursera courses. Cleaning this dataset is task for [Getting and Cleaning Data][1], but it is also used for case study for [Exploratory Data analysis][2]. Video from this case study is available in videos for week 4 of EDA course-ware. It might help you with starting with this data.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.coursera.org/course/getdata&#xD;&#xA;  [2]: https://class.coursera.org/exdata-002" />
  <row Id="582" PostHistoryTypeId="2" PostId="218" RevisionGUID="7bd05da6-4c4b-4639-a077-436919934475" CreationDate="2014-05-29T13:08:09.060" UserId="273" Text="So, I have a dataset with 39.949 variables and 180 rows. dataset is successfully &#xD;&#xA;saved in DataFrame but when I try to find cov() it result an error.&#xD;&#xA;here is the code&#xD;&#xA;&#xD;&#xA;      import pandas as pd&#xD;&#xA;      cov_data=pd.DataFrame(dataset).cov()&#xD;&#xA;  &#xD;&#xA;Here is the error&#xD;&#xA;&#xD;&#xA;    File &quot;/home/syahdeini/Desktop/FP/pca_2.py&quot;, line 44, in find_eagen&#xD;&#xA;    cov_data=pd.DataFrame(data_mat).cov()&#xD;&#xA;    File &quot;/usr/lib/python2.7/dist-packages/pandas/core/frame.py&quot;, line 3716, in cov&#xD;&#xA;    baseCov = np.cov(mat.T)&#xD;&#xA;    File &quot;/usr/lib/python2.7/dist-packages/numpy/lib/function_base.py&quot;, line 1766, in cov&#xD;&#xA;    return (dot(X, X.T.conj()) / fact).squeeze()&#xD;&#xA;    ValueError: array is too big.&#xD;&#xA;" />
  <row Id="583" PostHistoryTypeId="1" PostId="218" RevisionGUID="7bd05da6-4c4b-4639-a077-436919934475" CreationDate="2014-05-29T13:08:09.060" UserId="273" Text="built-in cov in pandas DataFrame results ValueError array is too big" />
  <row Id="584" PostHistoryTypeId="3" PostId="218" RevisionGUID="7bd05da6-4c4b-4639-a077-436919934475" CreationDate="2014-05-29T13:08:09.060" UserId="273" Text="&lt;python&gt;&lt;pandas&gt;" />
  <row Id="585" PostHistoryTypeId="2" PostId="219" RevisionGUID="39774ccc-d4d0-43f0-8929-ab52b3ebdad9" CreationDate="2014-05-29T13:51:48.820" UserId="178" Text="Since you have 39,949 variables, the covariance matrix would have about 1.6 billion elements (39,949 * 39,949 = 1,595,922,601).  That is likely why you are getting that error." />
  <row Id="586" PostHistoryTypeId="2" PostId="220" RevisionGUID="9882f334-d988-4b68-958f-193aaec026ee" CreationDate="2014-05-29T14:13:00.317" UserId="250" Text="Agreed. To be simplistic about it, if this translates to 1.6B floats, at 16 bytes per float (32-bit version; 64-bit is bigger), then you're trying to create an array of about 26 GB. Even if you have the RAM for that, I'd imagine that it's probably going to overload something else along the way. &#xD;&#xA;&#xD;&#xA;(Maybe not, but generally speaking, any operations that are that computationally intensive should first raise the question of whether you are doing the right calculation in the first place. And if you do need to do something of that magnitude, you should then try to break it down into more manageable chunks that can be run in parallel or distributed across machines.)&#xD;&#xA;&#xD;&#xA;But given that you are describing a very, very wide dataset (~40k columns x 180 rows), I wonder whether you really want to take the covariance matrix of the transposed dataset (so 180x180 = 32,400 covariances)? That would be a far more tractable problem, and it's easier to see how it might be useful.&#xD;&#xA;&#xD;&#xA;In any case, you're probably far better off calculating each pairwise covariance (or at least, the vector of cov(x_i,x_k) for all x_k != x_i) at the point where you'll actually use it, rather than calculating a giant matrix initially then referring back to it later. Memory issues aside, it'll make your life much easier if you start running things in parallel, and will help ensure you don't waste resources on unnecessary calculations." />
  <row Id="587" PostHistoryTypeId="2" PostId="221" RevisionGUID="56420626-767c-4446-8037-dea50b7530be" CreationDate="2014-05-29T14:30:21.357" UserId="250" Text="It's actually even simpler than that, from what you describe---you're just looking for a basic classification tree algorithm (so no need for slightly more complex variants like C4.5 which are optimized for prediction accuracy). The canonical text is:&#xD;&#xA;&#xD;&#xA;http://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418&#xD;&#xA;&#xD;&#xA;This is readily implemented in R: &#xD;&#xA;&#xD;&#xA;http://cran.r-project.org/web/packages/tree/tree.pdf&#xD;&#xA;&#xD;&#xA;and Python:&#xD;&#xA;&#xD;&#xA;http://scikit-learn.org/stable/modules/tree.html" />
  <row Id="588" PostHistoryTypeId="5" PostId="220" RevisionGUID="07f04cd9-29af-43dc-a72c-b22c44b1b8ee" CreationDate="2014-05-29T14:31:51.913" UserId="250" Comment="Clarified what I'm agreeing with at the outset." Text="Christopher is right about the size of the array. To be simplistic about it, if this translates to 1.6B floats, at 16 bytes per float (32-bit version; 64-bit is bigger), then you're trying to create an array of about 26 GB. Even if you have the RAM for that, I'd imagine that it's probably going to overload something else along the way. &#xD;&#xA;&#xD;&#xA;(Maybe not, but generally speaking, any operations that are that computationally intensive should first raise the question of whether you are doing the right calculation in the first place. And if you do need to do something of that magnitude, you should then try to break it down into more manageable chunks that can be run in parallel or distributed across machines.)&#xD;&#xA;&#xD;&#xA;But given that you are describing a very, very wide dataset (~40k columns x 180 rows), I wonder whether you really want to take the covariance matrix of the transposed dataset (so 180x180 = 32,400 covariances)? That would be a far more tractable problem, and it's easier to see how it might be useful.&#xD;&#xA;&#xD;&#xA;In any case, you're probably far better off calculating each pairwise covariance (or at least, the vector of cov(x_i,x_k) for all x_k != x_i) at the point where you'll actually use it, rather than calculating a giant matrix initially then referring back to it later. Memory issues aside, it'll make your life much easier if you start running things in parallel, and will help ensure you don't waste resources on unnecessary calculations." />
  <row Id="589" PostHistoryTypeId="5" PostId="138" RevisionGUID="8dc644fb-677f-4b51-beb1-7fc1959f5e90" CreationDate="2014-05-29T15:01:17.840" UserId="84" Comment="Improving question." Text="Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, people tend to reach out for C/C++/low-level languages. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).&#xD;&#xA;&#xD;&#xA;Of course such set of benefits would not come without a cost: writing the code, and sometimes even *reinventing the wheel*, can be quite expensive/tiresome. Although there are lots of libraries available, people are inclined to write the code by themselves whenever they need to *grant* performance. What *disables* performance assertions from using libraries while processing large databases?&#xD;&#xA;&#xD;&#xA;For example, consider an entreprise that continuously crawls webpages and parses the data collected. For each sliding-window, different data mining algorithms are run upon the data extracted. Why would the developers ditch off using available libraries/frameworks (be it for crawling, text processing, and data mining)? Using stuff already implemented would not only ease the burden of coding the whole process, but also would save a lot of time.&#xD;&#xA;&#xD;&#xA;**In a single shot**:&#xD;&#xA;&#xD;&#xA;- what makes writing the code by oneself a *guarantee* of performance?&#xD;&#xA;- why is it *risky* to rely on a frameworks/libraries when you must **assure** high performance?" />
  <row Id="590" PostHistoryTypeId="4" PostId="138" RevisionGUID="8dc644fb-677f-4b51-beb1-7fc1959f5e90" CreationDate="2014-05-29T15:01:17.840" UserId="84" Comment="Improving question." Text="Why is it hard to grant efficiency while using libraries?" />
  <row Id="591" PostHistoryTypeId="2" PostId="222" RevisionGUID="5d6279bf-6fac-47e9-8ad7-527ecbad9cb2" CreationDate="2014-05-29T19:02:13.210" UserId="43" Text="[Enigma][1] is a repository of public available datasets. Its free plan offers public data search, with 10k API calls per month. Not all public databases are listed, but the list is enough for common cases.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I used it for academic research and it saved me a lot of time.&#xD;&#xA;&#xD;&#xA;  [1]: http://enigma.io" />
  <row Id="592" PostHistoryTypeId="2" PostId="223" RevisionGUID="92a3929a-0987-4d88-b32d-55e344e971d5" CreationDate="2014-05-29T20:11:16.327" UserId="227" Text="Having a lot of text documents (in natural language, unstructured), what are the possible ways of annotating them with some semantic meta-data? For example, consider a short document:&#xD;&#xA;&#xD;&#xA;    I saw the company's manager last day.&#xD;&#xA;&#xD;&#xA;To be able to extract information from it, it must be annotated with additional data to be less ambiguous. The process of finding such meta-data is not in question, so assume it is done manually. The question is how to store these data in a way that further analysis on it can be done more conveniently/efficiently?&#xD;&#xA;&#xD;&#xA;A possible approach is to use XML tags (see below), but it seems too verbose, and maybe there are better approaches/guidelines for storing such meta-data on text documents.&#xD;&#xA;&#xD;&#xA;    &lt;Person name=&quot;John&quot;&gt;I&lt;/Person&gt; saw the &lt;Organization name=&quot;ACME&quot;&gt;company&lt;/Organization&gt;'s&#xD;&#xA;    manager &lt;Time value=&quot;2014-5-29&quot;&gt;last day&lt;/Time&gt;.&#xD;&#xA;" />
  <row Id="593" PostHistoryTypeId="1" PostId="223" RevisionGUID="92a3929a-0987-4d88-b32d-55e344e971d5" CreationDate="2014-05-29T20:11:16.327" UserId="227" Text="How to annotate text documents with meta-data?" />
  <row Id="594" PostHistoryTypeId="3" PostId="223" RevisionGUID="92a3929a-0987-4d88-b32d-55e344e971d5" CreationDate="2014-05-29T20:11:16.327" UserId="227" Text="&lt;nlp&gt;&lt;metadata&gt;&lt;data-cleaning&gt;&lt;text-mining&gt;" />
  <row Id="595" PostHistoryTypeId="5" PostId="222" RevisionGUID="d3f78d63-1ae1-4cb2-8138-47cc87ed443b" CreationDate="2014-05-30T21:05:52.147" UserId="43" Comment="Add another source" Text="[Enigma][1] is a repository of public available datasets. Its free plan offers public data search, with 10k API calls per month. Not all public databases are listed, but the list is enough for common cases.&#xD;&#xA;&#xD;&#xA;I used it for academic research and it saved me a lot of time.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;Another interesting source of data is the [@unitedstates project][2], containing data and tools to collect them, about the United States (members of Congress, geographic shapes…).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://enigma.io&#xD;&#xA;  [2]: http://theunitedstates.io/" />
  <row Id="596" PostHistoryTypeId="34" PostId="128" RevisionGUID="735dd6b5-5aca-41ef-80b5-11734be9a0fc" CreationDate="2014-05-31T07:53:35.590" UserId="-1" Comment="1" />
  <row Id="597" PostHistoryTypeId="34" PostId="130" RevisionGUID="14a2110d-0337-45ea-828e-444e7c15bdfa" CreationDate="2014-05-31T07:53:35.590" UserId="-1" Comment="2" />
  <row Id="598" PostHistoryTypeId="2" PostId="224" RevisionGUID="1f331690-cb36-41f6-87d9-bc9ff26f567f" CreationDate="2014-05-31T14:28:42.317" UserId="122" Text="The output of my word alignment file looks as such:&#xD;&#xA;&#xD;&#xA;    I wish to say with regard to the initiative of the Portuguese Presidency that we support the spirit and the political intention behind it .	In bezug auf die Initiative der portugiesischen Präsidentschaft möchte ich zum Ausdruck bringen , daß wir den Geist und die politische Absicht , die dahinter stehen , unterstützen .	0-0 5-1 5-2 2-3 8-4 7-5 11-6 12-7 1-8 0-9 9-10 3-11 10-12 13-13 13-14 14-15 16-16 17-17 18-18 16-19 20-20 21-21 19-22 19-23 22-24 22-25 23-26 15-27 24-28&#xD;&#xA;    It may not be an ideal initiative in terms of its structure but we accept Mr President-in-Office , that it is rooted in idealism and for that reason we are inclined to support it .	Von der Struktur her ist es vielleicht keine ideale Initiative , aber , Herr amtierender Ratspräsident , wir akzeptieren , daß sie auf Idealismus fußt , und sind deshalb geneigt , sie mitzutragen .	0-0 11-2 8-3 0-4 3-5 1-6 2-7 5-8 6-9 12-11 17-12 15-13 16-14 16-15 17-16 13-17 14-18 17-19 18-20 19-21 21-22 23-23 21-24 26-25 24-26 29-27 27-28 30-29 31-30 33-31 32-32 34-33&#xD;&#xA;&#xD;&#xA;What should i do to get from the output above to get the phrase tables that are used by MOSES?&#xD;&#xA;" />
  <row Id="599" PostHistoryTypeId="1" PostId="224" RevisionGUID="1f331690-cb36-41f6-87d9-bc9ff26f567f" CreationDate="2014-05-31T14:28:42.317" UserId="122" Text="How to get phrase tables from word alignments? - Machine Translation" />
  <row Id="600" PostHistoryTypeId="3" PostId="224" RevisionGUID="1f331690-cb36-41f6-87d9-bc9ff26f567f" CreationDate="2014-05-31T14:28:42.317" UserId="122" Text="&lt;machine-trasnlation&gt;&lt;moses&gt;&lt;alignment&gt;" />
  <row Id="604" PostHistoryTypeId="2" PostId="226" RevisionGUID="77d3d243-a7a2-4c5d-b8d5-e89a4d3deab9" CreationDate="2014-05-31T19:59:15.563" UserId="290" Text="I've found this link in Data Science Central with a list of free datasets: [Big data sets available for free][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.datasciencecentral.com/profiles/blogs/big-data-sets-available-for-free" />
  <row Id="605" PostHistoryTypeId="2" PostId="227" RevisionGUID="e04c1976-9113-442d-b2c2-1eaf8f9adae5" CreationDate="2014-06-01T10:25:51.163" UserId="339" Text="Can someone kindly tell me about the trade-offs involved when selected Storm or Hadoop for data processing? Aside from the obvious one that Hadoop is a batch processing system and Storm is a real-time processing system. I have worked a bit with Hadoop but I haven't worked with Storm. After looking through a lot of presentations and articles, I still haven't been able to find a satisfactory and comprehensive answer. " />
  <row Id="606" PostHistoryTypeId="1" PostId="227" RevisionGUID="e04c1976-9113-442d-b2c2-1eaf8f9adae5" CreationDate="2014-06-01T10:25:51.163" UserId="339" Text="Tradeoffs between Storm and Hadoop?" />
  <row Id="607" PostHistoryTypeId="3" PostId="227" RevisionGUID="e04c1976-9113-442d-b2c2-1eaf8f9adae5" CreationDate="2014-06-01T10:25:51.163" UserId="339" Text="&lt;bigdata&gt;&lt;hadoop&gt;" />
  <row Id="608" PostHistoryTypeId="5" PostId="227" RevisionGUID="5e7b9b36-6d68-43db-8945-ea424609365a" CreationDate="2014-06-01T11:45:40.060" UserId="84" Comment="Improving formatting." Text="Can someone kindly tell me about the trade-offs involved when choosing between Storm and Hadoop for data processing? Of course, aside from the obvious one, that Hadoop is a batch processing system, and Storm is a real-time processing system.&#xD;&#xA;&#xD;&#xA;I have worked a bit with Hadoop, but I haven't worked with Storm. After looking through a lot of presentations and articles, I still haven't been able to find a satisfactory and comprehensive answer." />
  <row Id="609" PostHistoryTypeId="4" PostId="227" RevisionGUID="5e7b9b36-6d68-43db-8945-ea424609365a" CreationDate="2014-06-01T11:45:40.060" UserId="84" Comment="Improving formatting." Text="Tradeoffs between Storm and Hadoop" />
  <row Id="610" PostHistoryTypeId="6" PostId="227" RevisionGUID="5e7b9b36-6d68-43db-8945-ea424609365a" CreationDate="2014-06-01T11:45:40.060" UserId="84" Comment="Improving formatting." Text="&lt;bigdata&gt;&lt;efficiency&gt;&lt;hadoop&gt;&lt;distributed&gt;" />
  <row Id="611" PostHistoryTypeId="2" PostId="228" RevisionGUID="42eb4fc6-2a18-4596-bf43-9b3cee753664" CreationDate="2014-06-01T12:51:25.040" UserId="339" Text="Going through the presentation and material of Summingbird by Twitter, one of the reasons that is mentioned for using Storm and Hadoop clusters together in Summingbird is that processing through Storm results in cascading of error. In order to avoid this cascading of error and accumulation of it, Hadoop cluster is used to batch process the data and discard the Storm results after the same data is processed by Hadoop. &#xD;&#xA;&#xD;&#xA;What is the reasons for generation of this accumulation of error? and why is it not present in Hadoop? Since I have not worked with Storm, I do not know the reasons for it. Is it because Storm uses some approximate algorithm to process the data in order to process them in real time? or is the cause something else?" />
  <row Id="612" PostHistoryTypeId="1" PostId="228" RevisionGUID="42eb4fc6-2a18-4596-bf43-9b3cee753664" CreationDate="2014-06-01T12:51:25.040" UserId="339" Text="Cascaded Error in Apache Storm" />
  <row Id="613" PostHistoryTypeId="3" PostId="228" RevisionGUID="42eb4fc6-2a18-4596-bf43-9b3cee753664" CreationDate="2014-06-01T12:51:25.040" UserId="339" Text="&lt;bigdata&gt;&lt;hadoop&gt;&lt;apache-storm&gt;&lt;summingbird&gt;" />
  <row Id="614" PostHistoryTypeId="2" PostId="229" RevisionGUID="424ff205-e32a-4d32-8bbd-2981c2ae1aa8" CreationDate="2014-06-01T19:48:41.693" UserId="21" Text="This is kind of like asking about the tradeoffs between frying pan and your drawer of silverware. They are not two things you compare, really. You might use them together as part of a larger project. &#xD;&#xA;&#xD;&#xA;Hadoop itself is not one thing, but a name for a federation of services, like HDFS, Hive, HBase, MapReduce, etc. Storm is something you use with some of these services, like HDFS or HBase. It is a stream-processing framework. There are others within the extended Hadoop ecosystem, like Spark Streaming.&#xD;&#xA;&#xD;&#xA;When would you choose a stream-processing framework? when you need to react to new data in near-real-time. If you need this kind of tool, you deploy this kind of tool, too." />
  <row Id="615" PostHistoryTypeId="5" PostId="227" RevisionGUID="b2164607-f475-473b-9edc-d0cb8b58c698" CreationDate="2014-06-01T21:52:00.540" UserId="339" Comment="Improved the question to reduce ambiguity " Text="Can someone kindly tell me about the trade-offs involved when choosing between Storm and MapReduce in Hadoop Cluster for data processing? Of course, aside from the obvious one, that Hadoop (processing via MapReduce in a Hadoop Cluster) is a batch processing system, and Storm is a real-time processing system.&#xD;&#xA;&#xD;&#xA;I have worked a bit with Hadoop Eco System, but I haven't worked with Storm. After looking through a lot of presentations and articles, I still haven't been able to find a satisfactory and comprehensive answer." />
  <row Id="616" PostHistoryTypeId="4" PostId="227" RevisionGUID="b2164607-f475-473b-9edc-d0cb8b58c698" CreationDate="2014-06-01T21:52:00.540" UserId="339" Comment="Improved the question to reduce ambiguity " Text="Tradeoffs between Storm and Hadoop (MapReduce)" />
  <row Id="617" PostHistoryTypeId="5" PostId="227" RevisionGUID="cddc982f-015b-4ec8-b11c-eea07693bd82" CreationDate="2014-06-01T21:59:02.347" UserId="339" Comment="Improved the question to reduce ambiguity" Text="Can someone kindly tell me about the trade-offs involved when choosing between Storm and MapReduce in Hadoop Cluster for data processing? Of course, aside from the obvious one, that Hadoop (processing via MapReduce in a Hadoop Cluster) is a batch processing system, and Storm is a real-time processing system.&#xD;&#xA;&#xD;&#xA;I have worked a bit with Hadoop Eco System, but I haven't worked with Storm. After looking through a lot of presentations and articles, I still haven't been able to find a satisfactory and comprehensive answer.&#xD;&#xA;&#xD;&#xA;Note: The term tradeoff here is not meant to compare to similar things. It is meant to represent the consequences of getting results real-time that are absent from a batch processing system. " />
  <row Id="618" PostHistoryTypeId="2" PostId="230" RevisionGUID="f445fb8a-553e-4211-917f-847579d43dc7" CreationDate="2014-06-02T15:03:35.940" UserId="178" Text="In general, you don't want to use XML tags to tag documents in this way because tags may overlap.&#xD;&#xA;&#xD;&#xA;[UIMA](http://uima.apache.org/), [GATE](http://gate.ac.uk/) and similar NLP frameworks denote the tags separate from the text.  Each tag, such as `Person`, `ACME`, `John` etc. is stored as the position that the tag begins and the position that it ends.  So, for the tag `ACME`, it would be stored as starting a position 11 and ending at position 17. &#xD;&#xA;&#xD;&#xA;" />
  <row Id="620" PostHistoryTypeId="2" PostId="231" RevisionGUID="31363516-cd4e-4936-a095-bc9ff2fa67e9" CreationDate="2014-06-05T09:00:27.950" UserId="133" Text="I want to test the accuracy of a methodology. I ran it ~400 times and I got a different classification for each run. I also have the ground truth, i.e. the real classification to test against.&#xD;&#xA;&#xD;&#xA;For each classification I computed a confusion matrix. Now I want to aggregate these results in order to get the overall confusion matrix. How can I achieve it?&#xD;&#xA;&#xD;&#xA;May I sum all confusion matrices in order to obtain the overall one?&#xD;&#xA;&#xD;&#xA;Thanks" />
  <row Id="621" PostHistoryTypeId="1" PostId="231" RevisionGUID="31363516-cd4e-4936-a095-bc9ff2fa67e9" CreationDate="2014-06-05T09:00:27.950" UserId="133" Text="How to get an aggregate confusion matrix from n different classifications" />
  <row Id="622" PostHistoryTypeId="3" PostId="231" RevisionGUID="31363516-cd4e-4936-a095-bc9ff2fa67e9" CreationDate="2014-06-05T09:00:27.950" UserId="133" Text="&lt;classification&gt;&lt;confusion-matrix&gt;&lt;accuracy&gt;" />
  <row Id="623" PostHistoryTypeId="5" PostId="231" RevisionGUID="7a4551d8-3620-4b74-8d8d-1f70f95629fb" CreationDate="2014-06-05T15:21:40.640" UserId="84" Comment="Minor corrections." Text="I want to test the accuracy of a methodology. I ran it ~400 times, and I got a different classification for each run. I also have the ground truth, i.e., the real classification to test against.&#xD;&#xA;&#xD;&#xA;For each classification I computed a confusion matrix. Now I want to aggregate these results in order to get the overall confusion matrix. How can I achieve it?&#xD;&#xA;&#xD;&#xA;May I sum all confusion matrices in order to obtain the overall one?" />
  <row Id="624" PostHistoryTypeId="2" PostId="232" RevisionGUID="7cdf03ac-cdf8-4a8c-b6a7-76f601367c67" CreationDate="2014-06-06T14:55:18.867" UserId="375" Text="There are a few ways to achieve your &quot;master confusion matrix&quot;.&#xD;&#xA;&#xD;&#xA; 1. Sum all the confusion matrices together:  Like you suggested, summing this results in a confusion matrix.  The problem with this is you can not interpret totals.&#xD;&#xA;&#xD;&#xA; 2. Average the entries.  This method is the same as number one, but you divide each entry by the number of trials (~400 in your case).  This would be my preferred method because then you can actually translate each category to a (mean) +- (an error measurement) and actually see which categories are the most volatile or stable.  Careful with interpreting this 'error measurement' though.&#xD;&#xA;&#xD;&#xA; 3.  Report a problem specific measurement of the confusion numbers.  For example, if your numbers have outliers, medians would preferred over means.&#xD;&#xA;&#xD;&#xA;There are other statistics that are possible to report as well.  You can redo the method to keep track of individual classifications.  Then we can say other important stats like '% of classifications that stay the same and are accurate', etc..." />
  <row Id="626" PostHistoryTypeId="33" PostId="224" RevisionGUID="4312f0e4-ac22-46dc-8451-4a7425918fbf" CreationDate="2014-06-07T20:37:03.950" UserId="122" Comment="3" />
  <row Id="627" PostHistoryTypeId="2" PostId="233" RevisionGUID="c7a83cbb-54a4-4d70-a66a-1bd10f30c3f6" CreationDate="2014-06-08T07:03:43.710" UserId="381" Text="To complete Damien's answer, an example of dimensionality reduction in NLP is a [topic model](http://en.wikipedia.org/wiki/Topic_model), where you represent the document by a vector indicating the weights of its constituent topics." />
  <row Id="628" PostHistoryTypeId="2" PostId="234" RevisionGUID="0d5dd006-8e5c-436b-804c-591312bbd968" CreationDate="2014-06-09T04:43:03.497" UserId="386" Text="As Yann LeCun [mentioned][1], a number of PhD programs in data science will be popping up in the next few years.&#xD;&#xA;&#xD;&#xA;[NYU][2] already have one, where Prof.LeCun is at right now.&#xD;&#xA;&#xD;&#xA;A statistics or cs PhD in machine learning is probably more rigorous than a data science one.  Is data science PhD for the less mathy people like myself? &#xD;&#xA;&#xD;&#xA;Are these cash cow programs?&#xD;&#xA;&#xD;&#xA;There is a huge industry demand for big data, but what is the academic value of these programs, as you probably can't be a professor or publish any paper.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/chisdw1&#xD;&#xA;  [2]: http://datascience.nyu.edu/academics/programs/" />
  <row Id="629" PostHistoryTypeId="1" PostId="234" RevisionGUID="0d5dd006-8e5c-436b-804c-591312bbd968" CreationDate="2014-06-09T04:43:03.497" UserId="386" Text="Data science Ph.D. program, what do you think?" />
  <row Id="630" PostHistoryTypeId="3" PostId="234" RevisionGUID="0d5dd006-8e5c-436b-804c-591312bbd968" CreationDate="2014-06-09T04:43:03.497" UserId="386" Text="&lt;knowledge-base&gt;" />
  <row Id="631" PostHistoryTypeId="2" PostId="235" RevisionGUID="c0d5a609-abdc-44a6-8f14-61cd2cd8421c" CreationDate="2014-06-09T08:34:29.337" UserId="122" Text="Data visualization is an important subfield in data science and python programmers would need to have available toolkits for them.&#xD;&#xA;&#xD;&#xA;**Is there a python API to tablaeu?**&#xD;&#xA;&#xD;&#xA;**Are there any python based data visualization toolkit?**" />
  <row Id="632" PostHistoryTypeId="1" PostId="235" RevisionGUID="c0d5a609-abdc-44a6-8f14-61cd2cd8421c" CreationDate="2014-06-09T08:34:29.337" UserId="122" Text="Are there any python based data visualization toolkit?" />
  <row Id="633" PostHistoryTypeId="3" PostId="235" RevisionGUID="c0d5a609-abdc-44a6-8f14-61cd2cd8421c" CreationDate="2014-06-09T08:34:29.337" UserId="122" Text="&lt;python&gt;&lt;visualization&gt;" />
  <row Id="634" PostHistoryTypeId="2" PostId="236" RevisionGUID="bfab3b37-d0ba-4e83-98fe-239cb27bc27a" CreationDate="2014-06-09T18:02:00.613" UserId="381" Text="No-one knows since no-one's completed one of these PhD programs yet! However, I would look at the syllabus and the teachers to base my decision. It all depends on what you want to do; industry or academia?" />
  <row Id="635" PostHistoryTypeId="2" PostId="237" RevisionGUID="87f549e9-5d23-4b2b-9f42-caf1587338e5" CreationDate="2014-06-09T19:52:41.847" UserId="59" Text="There is a Tablaeu API and you can use Python to use it, but maybe not in the sense that you think. There is a Data Extract API that you could use to import your data into Python and do your visualizations there, so I do not know if this is going to answer your question entirely.&#xD;&#xA;&#xD;&#xA;As in the first comment you can use Matplotlib from [Matplotlib website][1], or you could install Canopy from Enthought which has it available, there is also Pandas, which you could also use for data analysis and some visualizations. There is also a package called `ggplot` which is used in `R` alot, but is also made for Python, which you can find here [ggplot for python][2].&#xD;&#xA;&#xD;&#xA;The Tableau data extract API and some information about it can be found [at this link][3]. There are a few web sources that I found concerning it using duckduckgo [at this link][4].&#xD;&#xA;Here are some samples:&#xD;&#xA;&#xD;&#xA;[Link 1][5]&#xD;&#xA;&#xD;&#xA;[Link 2][6]&#xD;&#xA;&#xD;&#xA;[Link 3][7]&#xD;&#xA;&#xD;&#xA;As far as an API like matplotlib, I cannot say for certain that one exists. Hopefully this gives some sort of reference to help answer your question.&#xD;&#xA;&#xD;&#xA;Also to help avoid closure flags and downvotes you should try and show some of what you have tried to do or find, this makes for a better question and helps to illicit responses.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.matplotlib.org&#xD;&#xA;  [2]: https://pypi.python.org/pypi/ggplot&#xD;&#xA;  [3]: http://www.tableausoftware.com/new-features/data-engine-api-0&#xD;&#xA;  [4]: https://duckduckgo.com/?q=tableau%20PYTHON%20API&amp;kp=1&amp;kd=-1&#xD;&#xA;  [5]: https://www.interworks.com/blogs/bbickell/2012/12/06/introducing-python-tableau-data-extract-api-csv-extract-example&#xD;&#xA;  [6]: http://ryrobes.com/python/building-tableau-data-extract-files-with-python-in-tableau-8-sample-usage/&#xD;&#xA;  [7]: http://nbviewer.ipython.org/github/Btibert3/tableau-r/blob/master/Python-R-Tableau-Predictive-Modeling.ipynb" />
  <row Id="637" PostHistoryTypeId="2" PostId="238" RevisionGUID="2632501a-15ad-443d-ada3-52b3a04ae396" CreationDate="2014-06-09T21:36:44.297" UserId="395" Text="I think this question assumes a false premise. As a student at NYU, I only know of a Masters in Data Science. You linked to a page that confirms this.&#xD;&#xA;&#xD;&#xA;It's hard to gauge the benefit of a program that doesn't exist yet." />
  <row Id="640" PostHistoryTypeId="2" PostId="241" RevisionGUID="3f7678f2-b982-4e3e-97da-c1f35521b938" CreationDate="2014-06-09T21:51:53.793" UserId="403" Text="It seems to me that the premise of a PhD is to expand knowledge in some little slice of the world. Since a &quot;data scientist&quot; is by nature is somewhat of a jack-of-all-trades it does seem a little odd to me. A masters program seems much more appropriate.&#xD;&#xA;&#xD;&#xA;What do you hope to gain from a PhD? If the rigor scares (or bores) you, then what about a more applied area? Signal processing, robotics, applied physics, operations research, etc." />
  <row Id="641" PostHistoryTypeId="2" PostId="242" RevisionGUID="556ceec3-79a0-4e9e-8690-c1409604c283" CreationDate="2014-06-09T21:57:30.240" UserId="406" Text="**MapReduce**: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds)&#xD;&#xA;&#xD;&#xA;A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state.&#xD;&#xA;&#xD;&#xA;An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model.&#xD;&#xA;&#xD;&#xA;**Storm**: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard.&#xD;&#xA;&#xD;&#xA;Storm doesn't have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**If you want to know more...**&#xD;&#xA;If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way [here are slides for a talk I gave on building real-time recommendation engines on HBase.][1] &#xD;&#xA;&#xD;&#xA;An excellent paper that marries real-time counting and persistence in an interesting way is [Google News Personalization: Scalable Online Collaborative Filtering][2] Another interesting marriage of MR and Storm is [SummingBird][3]. &#xD;&#xA;&#xD;&#xA;Summingbird allows you to define data analysis operations that can be applied via Storm or MR.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.slideshare.net/cloudera/hbasecon-2013-24063525&#xD;&#xA;  [2]: http://dl.acm.org/citation.cfm?id=1242610" />
  <row Id="642" PostHistoryTypeId="5" PostId="242" RevisionGUID="03079e95-d516-46dc-85b7-3c9dd8abc03d" CreationDate="2014-06-09T22:03:08.983" UserId="406" Comment="deleted 5 characters in body" Text="**MapReduce**: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds)&#xD;&#xA;&#xD;&#xA;A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state.&#xD;&#xA;&#xD;&#xA;An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model.&#xD;&#xA;&#xD;&#xA;**Storm**: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard.&#xD;&#xA;&#xD;&#xA;Storm doesn't have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**If you want to know more...**&#xD;&#xA;If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way [here are slides for a talk I gave on building real-time recommendation engines on HBase.][1] &#xD;&#xA;&#xD;&#xA;An excellent paper that marries real-time counting and persistence in an interesting way is [Google News Personalization: Scalable Online Collaborative Filtering][2] &#xD;&#xA;&#xD;&#xA;Another interesting marriage of MR and Storm is SummingBird. Summingbird allows you to define data analysis operations that can be applied via Storm or MR.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.slideshare.net/cloudera/hbasecon-2013-24063525&#xD;&#xA;  [2]: http://dl.acm.org/citation.cfm?id=1242610" />
  <row Id="643" PostHistoryTypeId="2" PostId="243" RevisionGUID="1361689a-63bc-4d24-9ba1-3bb27e256825" CreationDate="2014-06-10T01:36:27.520" UserId="432" Text="A variety of methods are available to the user.   The support documentation gives walkthroughs and tips for when one or another model is most appropriate. &#xD;&#xA;&#xD;&#xA;[This page][1] shows the following learning methods:&#xD;&#xA;&#xD;&#xA;- &quot;AssociationModel&quot;&#xD;&#xA;- &quot;ClusteringModel&quot; &#xD;&#xA;- &quot;GeneralRegressionModel&quot;&#xD;&#xA;- &quot;MiningModel&quot; &#xD;&#xA;- &quot;NaiveBayesModel&quot; &#xD;&#xA;- &quot;NeuralNetwork&quot; &#xD;&#xA;- &quot;RegressionModel&quot;&#xD;&#xA;- &quot;RuleSetModel&quot; &#xD;&#xA;- &quot;SequenceModel&quot; &#xD;&#xA;- &quot;SupportVectorMachineModel&quot;&#xD;&#xA;- &quot;TextModel&quot; &#xD;&#xA;- &quot;TimeSeriesModel&quot; &#xD;&#xA;- &quot;TreeModel&quot;&#xD;&#xA;&#xD;&#xA;  [1]: https://developers.google.com/prediction/docs/pmml-schema" />
  <row Id="644" PostHistoryTypeId="2" PostId="244" RevisionGUID="0032c28c-8054-43f8-abd1-2c77b4eaa42c" CreationDate="2014-06-10T01:40:23.263" UserId="434" Text="Coming from a programmers perspective, frameworks rarely target performance as the highest priority.  If your library is going to be widely leveraged the things people are likely to value most are ease of use, flexibility, and reliability.&#xD;&#xA;&#xD;&#xA;Performance is generally valued in secondary competitive libraries.  &quot;X library is better because it's faster.&quot;  Even then very frequently those libraries will trade off the most optimal solution for one that can be widely leveraged.  &#xD;&#xA;&#xD;&#xA;By using any framework you are inherently taking a risk that a faster solution exists.  I might go so far as to say that a faster solution almost always exists.&#xD;&#xA;&#xD;&#xA;Writing something yourself is not a guarantee of performance, but if you know what you are doing and have a fairly limited set of requirements it can help.&#xD;&#xA;&#xD;&#xA;An example might be JSON parsing.  There are a hundred libraries out there for a variety of languages that will turn JSON into a referable object and vice versa.  I know of one implementation that does it all in CPU registers.  It's measurably faster than all other parsers, but it is also very limited and that limitation will vary based on what CPU you are working with.&#xD;&#xA;&#xD;&#xA;Is the task of building a high-performant environment specific JSON parser a good idea?  I would leverage a respected library 99 times out of 100.  In that one separate instance a few extra CPU cycles multiplied by a million iterations would make the development time worth it.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="645" PostHistoryTypeId="5" PostId="243" RevisionGUID="a50d0c26-43b6-4369-80e3-3d6696cc11e1" CreationDate="2014-06-10T01:43:47.883" UserId="432" Comment="added 160 characters in body" Text="A variety of methods are available to the user.   The support documentation gives walkthroughs and tips for when one or another model is most appropriate. &#xD;&#xA;&#xD;&#xA;[This page][1] shows the following learning methods:&#xD;&#xA;&#xD;&#xA;- &quot;AssociationModel&quot;&#xD;&#xA;- &quot;ClusteringModel&quot; &#xD;&#xA;- &quot;GeneralRegressionModel&quot;&#xD;&#xA;- &quot;MiningModel&quot; &#xD;&#xA;- &quot;NaiveBayesModel&quot; &#xD;&#xA;- &quot;NeuralNetwork&quot; &#xD;&#xA;- &quot;RegressionModel&quot;&#xD;&#xA;- &quot;RuleSetModel&quot; &#xD;&#xA;- &quot;SequenceModel&quot; &#xD;&#xA;- &quot;SupportVectorMachineModel&quot;&#xD;&#xA;- &quot;TextModel&quot; &#xD;&#xA;- &quot;TimeSeriesModel&quot; &#xD;&#xA;- &quot;TreeModel&quot;&#xD;&#xA;&#xD;&#xA;EDIT: I don't see any specific information about the algorithms, though.  For example, does the tree model use information gain or gini index for splits? &#xD;&#xA;&#xD;&#xA;  [1]: https://developers.google.com/prediction/docs/pmml-schema&#xD;&#xA;" />
  <row Id="646" PostHistoryTypeId="2" PostId="245" RevisionGUID="afb53716-3c7d-4a8c-9f4f-463669179517" CreationDate="2014-06-10T01:54:40.647" UserId="432" Text="Computer Science is itself a multi-disciplinary field which has varying requirements among universities.  For example, Stockholm University does not require any math above algebra for its CS programs (some courses may have higher requirements, but not often).  &#xD;&#xA;&#xD;&#xA;I am not sure what you mean by a machine learning program being more rigorous.  They are just two different programs.  Data Science would likely take a broader view and focus on application and management (business courses are maybe on offer?).  The research could be rigorous in its own right, but it definitely won't be tailored to someone who wants to optimize new algorithms or solve the low-level problems of machine learning.  &#xD;&#xA;&#xD;&#xA;I don't see the Ph.D program listed yet in the link you provided.  Will you please follow up here if you get more specific information?" />
  <row Id="647" PostHistoryTypeId="2" PostId="246" RevisionGUID="5a681aba-8089-45d6-8074-74cc90ab57e9" CreationDate="2014-06-10T02:42:02.050" UserId="434" Text="There will definitely be a translation task at the end if you prototype using just mongo.&#xD;&#xA;&#xD;&#xA;When you run a MapReduce task on mongodb, it has the data source and structure built in.  When you eventually convert to hadoop, your data structures might not look the same.  You could leverage the mongodb-hadoop connector to access mongo data directly from within hadoop, but that won't be quite as straightforward as you might think.  The time to figure out how exactly to do the conversion most optimally will be easier to justify once you have a prototype in place, IMO.&#xD;&#xA;&#xD;&#xA;While you will need to translate mapreduce functions, the basic pseudocode should apply well to both systems.  You won't find anything that can be done in MongoDB that can't be done using Java or that is significantly more complex to do with Java." />
  <row Id="648" PostHistoryTypeId="2" PostId="247" RevisionGUID="cc53064d-d5be-43ca-9a15-4d7816a6658e" CreationDate="2014-06-10T02:50:51.153" UserId="434" Text="[Bokeh][1] is an excellent data visualization library for python.&#xD;&#xA;[NodeBox][2] is another that comes to mind.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://bokeh.pydata.org/&#xD;&#xA;  [2]: http://www.cityinabottle.org/nodebox/" />
  <row Id="649" PostHistoryTypeId="2" PostId="248" RevisionGUID="79dda3e4-aa17-4e6b-b996-2c9c526acb37" CreationDate="2014-06-10T03:21:56.473" UserId="434" Text="A cash cow program?  No.  PhD programs are never cash cows.&#xD;&#xA;&#xD;&#xA;I don't know why you couldn't be a professor with a PhD in data science.  Rarely does a professor of a given course have to have a specific degree in order to teach it.&#xD;&#xA;&#xD;&#xA;As far as publishing goes, there are any number of related journals that would accept papers from somebody on topics that would be covered by the topic of Data Science.&#xD;&#xA;&#xD;&#xA;When I went to college, MIS, Computer Engineering, and Computer Science were new subjects.  Most of the people in my graduating class for Computer Science couldn't program anything significant at graduation.  Within a few years, CS programs around the country matured significantly.&#xD;&#xA;&#xD;&#xA;When you are part of a new program, sometimes it's possible to help define what it is that's required for graduation.  Being a part of that puts you in rare company for that field.&#xD;&#xA;&#xD;&#xA;As far as mathematical rigor is concerned, I would expect Data Science to leverage a heavy dose of mathematically based material.  I wouldn't expect anything particularly new - statistics, calculus, etc. should have been covered in undergrad.  Masters and PhD programs should be more about applying that knowledge and not so much about learning it." />
  <row Id="650" PostHistoryTypeId="2" PostId="249" RevisionGUID="246fc0a8-fce9-4286-9c70-dd1e3d255d99" CreationDate="2014-06-10T03:42:51.637" UserId="434" Text="Twitter uses Storm for real-time processing of data.  Problems can happen with real-time data.  Systems might go down.  Data might be inadvertently processed twice.  Network connections can be lost.  A lot can happen in a real-time system.  &#xD;&#xA;&#xD;&#xA;They use hadoop to reliably process historical data.  I don't know specifics, but for instance, getting solid information from aggregated logs is probably more reliable than attaching to the stream.&#xD;&#xA;&#xD;&#xA;If they simply relied on Storm for everything - Storm would have problems due to the nature of providing real-time information at scale.  If they relied on hadoop for everything, there's a good deal of latency involved.  Combining the two with Summingbird is the next logical step." />
  <row Id="651" PostHistoryTypeId="2" PostId="250" RevisionGUID="aefd1807-10c5-47a0-91ed-20fca6655527" CreationDate="2014-06-10T04:47:13.040" UserId="434" Text="Google does not publish the models they use, but they specifically do not support models from the PMML specification.&#xD;&#xA;&#xD;&#xA;If you look closely at the documentation on [this page][1], you will notice that the model selection within the schema is greyed out indicating that it is an unsupported feature of the schema.&#xD;&#xA;&#xD;&#xA;The [documentation does spell out][2] that by default it will use a regression model for training data that has numeric answers, and an unspecified categorization model for training data that results in text based answers.&#xD;&#xA;&#xD;&#xA;The Google Prediction API also supports hosted models (although only a few demo models are currently available), and models specified with a PMML transform.  The documentation does contain an [example of a model defined by a PMML transform][3].  (There is also a note on that page stating that PMML ...Model elements are not supported).  &#xD;&#xA;&#xD;&#xA;The PMML standard that google partially supports is [version 4.0.1][4].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://developers.google.com/prediction/docs/pmml-schema&#xD;&#xA;  [2]: https://developers.google.com/prediction/docs/developer-guide#whatisprediction&#xD;&#xA;  [3]: https://developers.google.com/prediction/docs/pmml-schema&#xD;&#xA;  [4]: http://www.dmg.org//pmml-v4-0-1.html" />
  <row Id="652" PostHistoryTypeId="2" PostId="251" RevisionGUID="460d7015-658d-4f93-8c0c-b71c86c6be65" CreationDate="2014-06-10T05:57:13.897" UserId="451" Text="Having done the rewriting game over and over myself (and still doing it), my immediate reaction was *adaptability*.&#xD;&#xA;&#xD;&#xA;While frameworks and libraries have a huge arsenal of (possibly intertwinable) routines for standard tasks, their framework property often (always?) disallows shortcuts.  In fact, most frameworks have some sort of core infrastructure around which a core layer of basic functionality is implemented.  More specific functionality makes use of the basic layer and is placed in a second layer around the core.&#xD;&#xA;&#xD;&#xA;Now by shortcuts I mean going straight from a second layer routine to another second layer routine without using the core.  Typical example (from my domain) would be timestamps: You have a timestamped data source of some kind.  Thus far the job is simply to read the data off the wire and pass it to the core so your other code can feast on it.&#xD;&#xA;&#xD;&#xA;Now your industry changes the default timestamp format for a very good reason (in my case they went from unix time to GPS time).  Unless your framework is industry-specific it is very unlikely that they're willing to change the core representation of time, so you end up using a framework that *almost* does what you want.  Every time you access your data you have to convert it to industry-time-format first, and every time you want it modified you have to convert it back to whatever the core deems appropriate.  There is no way that you can hand over data straight from the source to a sink without double conversion.&#xD;&#xA;&#xD;&#xA;This is where your hand-crafted frameworks will shine, it's just a minor change and you're back modelling the real world whereas all other (non-industry-specific) frameworks will now have a performance disadvantage.&#xD;&#xA;&#xD;&#xA;Over time, the discrepancy between the real world and the model will add up.  With an off-the-shelf framework you'd soon be facing questions like: How can I represent `this` in `that` or how do make routine `X` accept/produce `Y`.&#xD;&#xA;&#xD;&#xA;So far this wasn't about C/C++.  But if, for some reason, you can't change the framework, i.e. you do have to put up with double conversion of data to go from one end to another, then you'd typically employ something that minimises the additional overhead.  In my case, a TAI-&gt;UTC or UTC-&gt;TAI converter is best left to raw C (or an FPGA).  There is no elegance possible, no profound smart data structure that makes the problem trivial.  It's just a boring switch statement, and why not use a language whose compilers are good at optimising exactly that?&#xD;&#xA;" />
  <row Id="653" PostHistoryTypeId="2" PostId="252" RevisionGUID="643061a8-f39a-483a-ae8d-ec9e8a616fff" CreationDate="2014-06-10T06:19:46.510" UserId="88" Text="There seem to be at least 2 ways to connect to HBase from external application, with language other then Java (i.e. Python):&#xD;&#xA;&#xD;&#xA; 1. HBase Thrift API&#xD;&#xA; 2. HBase Stargate (REST API) &#xD;&#xA;&#xD;&#xA;Does anyone know which one should be used in which circumstances?&#xD;&#xA;I.e. what are their main differences, and pros/cons?" />
  <row Id="654" PostHistoryTypeId="1" PostId="252" RevisionGUID="643061a8-f39a-483a-ae8d-ec9e8a616fff" CreationDate="2014-06-10T06:19:46.510" UserId="88" Text="HBase connector - Thrift or REST" />
  <row Id="655" PostHistoryTypeId="3" PostId="252" RevisionGUID="643061a8-f39a-483a-ae8d-ec9e8a616fff" CreationDate="2014-06-10T06:19:46.510" UserId="88" Text="&lt;bigdata&gt;&lt;hadoop&gt;" />
  <row Id="656" PostHistoryTypeId="2" PostId="253" RevisionGUID="2bf8b6b8-0228-46fc-8d17-8438827d209d" CreationDate="2014-06-10T06:20:20.817" UserId="456" Text="An aspiring data scientist here. I don't know anything about Hadoop, but as I have been reading about Data Science and Big Data, I see a lot of talk about Hadoop. Is it absolutely necessary to learn Hadoop to be a Data Scientist? " />
  <row Id="657" PostHistoryTypeId="1" PostId="253" RevisionGUID="2bf8b6b8-0228-46fc-8d17-8438827d209d" CreationDate="2014-06-10T06:20:20.817" UserId="456" Text="Do I need to learn Hadoop to be a Data Scientist?" />
  <row Id="658" PostHistoryTypeId="3" PostId="253" RevisionGUID="2bf8b6b8-0228-46fc-8d17-8438827d209d" CreationDate="2014-06-10T06:20:20.817" UserId="456" Text="&lt;bigdata&gt;&lt;hadoop&gt;" />
  <row Id="659" PostHistoryTypeId="2" PostId="254" RevisionGUID="92b4a6a2-d8fc-4856-976d-c6f37039051f" CreationDate="2014-06-10T07:10:45.057" UserId="463" Text="Actually there are more than one question to answer here:&#xD;&#xA;&#xD;&#xA; 1. How to work on schemaless/loose/missing data&#xD;&#xA; 2. How to label a person (from what I understand unsupervised) and create an identifier&#xD;&#xA; 3. How to train your system so that it can tell you which attributes you should use in order to identify the person &#xD;&#xA;&#xD;&#xA;As Rubens mentioned, you can use **decision tree** methods, specifically &lt;a href=&quot;http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm&quot;&gt;Random Forests&lt;/a&gt; for calculating the most important attributes based on information gain if you have already found a way to identify how to label a person.&#xD;&#xA;&#xD;&#xA;However, if you do not have any label information maybe you can use some expert view for preliminary attribute selection. After that you make **unsupervised classification** in order to retrieve your labels. Lastly, you can select the most important fields using **Random Forest** or other methods like **Bayesian Belief Networks**.&#xD;&#xA;&#xD;&#xA;In order to achieve all that, you also need complete data set. If your data set is loose you have to manually or heuristically find a way to couple attributes indicating same thing with different names. What is more, you can use *imputation* techniques such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;Expectation Maximization&lt;/a&gt; method and complete your data set. Or you can also work with Bayesian Networks and can leave missing fields as they are." />
  <row Id="660" PostHistoryTypeId="2" PostId="255" RevisionGUID="0ab22aaa-bcd9-4d73-bc74-286d285dee92" CreationDate="2014-06-10T07:28:49.127" UserId="466" Text="You should learn Hadoop if you want to be work as data scientist, but maybe before starting with Hadoop you should read something about ETL or Big Data... this book could be a good starting point: http://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343&#xD;&#xA;&#xD;&#xA;Hope it helps and good luck!" />
  <row Id="661" PostHistoryTypeId="2" PostId="256" RevisionGUID="02941d4a-b366-4c3e-b82c-36445aae27b2" CreationDate="2014-06-10T07:30:53.510" UserId="434" Text="Different people use different tools for different things.  Terms like Data Science are generic for a reason.  A data scientist could spend an entire career without having to learn a particular tool like hadoop.  Hadoop is widely used, but it is not the only platform that is capable of managing and manipulating data, even large scale data.&#xD;&#xA;&#xD;&#xA;I would say that a data scientist should be familiar with concepts like MapReduce, distributed systems, distributed file systems, and the like, but I wouldn't judge someone for not knowing about such things.  &#xD;&#xA;&#xD;&#xA;It's a big field.  There is a sea of knowledge and most people are capable of learning and being an expert in a single drop.  The key to being a scientist is having the desire to learn and the motivation to know that which you don't already know.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="662" PostHistoryTypeId="2" PostId="257" RevisionGUID="47a6a8bf-1c66-4261-b8a3-bab29aa6808c" CreationDate="2014-06-10T07:42:57.470" UserId="11" Text="Yes, you should learn a platform that is capable of dissecting your problem as a data parallel problem. Hadoop is one. For your simple needs (design patterns like counting, aggregation, filtering etc.) you need Hadoop and for more complex Machine Learning stuff like doing some Bayesian, SVM you need Mahout which in turn needs Hadoop (Now Apache Spark) to solve your problem using a data-parallel approach.&#xD;&#xA;&#xD;&#xA;So Hadoop is a good platform to learn and really important for your batch processing needs. Not only Hadoop but you also need to know Spark (Mahout runs it's algorithms utilizing Spark) &amp; Twitter Storm (for your real time analytics needs). This list will continue and evolve so if you are good with the building blocks (Distributed Computing, Data-Parallel Problems and so on) and know how one such platform (say Hadoop) operates you will fairly quickly be up to speed on others.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="664" PostHistoryTypeId="6" PostId="22" RevisionGUID="3073a77e-c4c3-4037-b200-abbcc996fa19" CreationDate="2014-06-10T07:53:48.253" UserId="97" Comment="Added 1 more relevant tag" Text="&lt;data-mining&gt;&lt;clustering&gt;&lt;octave&gt;&lt;k-means&gt;&lt;categorical-data&gt;" />
  <row Id="665" PostHistoryTypeId="2" PostId="258" RevisionGUID="dd65905f-a7fe-4907-87a7-88424248449b" CreationDate="2014-06-10T07:56:45.587" UserId="434" Text="Thrift is generally faster because the data being exchanged is smaller.  Stargate offers a web service which is an integration method that is widely supported, which is a concern when you are working with commercial products with limited integration possibilities.&#xD;&#xA;&#xD;&#xA;In a closed environment where everything is controlled, I would prefer Thrift.  If I'm exposing data to external teams or systems I would prefer Stargate." />
  <row Id="666" PostHistoryTypeId="2" PostId="259" RevisionGUID="2aacbbed-77d7-4625-841a-9c4743c8ec95" CreationDate="2014-06-10T08:04:20.400" UserId="454" Text="I would like to point to [The Open Data Census][1]. It is an initiative of the Open Knowledge Foundation based on contributions from open data advocates and experts around the world. &#xD;&#xA;&#xD;&#xA;The value of Open data Census is open, community driven, and systematic effort to collect and update the database of open datasets globally on country and, in some cases, [like U.S., on city level][2]. &#xD;&#xA;&#xD;&#xA;Also, it presents an opportunity to compare different countries and cities on in selected areas of interest.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://national.census.okfn.org/&#xD;&#xA;  [2]: http://us-city.census.okfn.org/" />
  <row Id="667" PostHistoryTypeId="5" PostId="256" RevisionGUID="8f4d5aee-bd08-4e7e-b0f7-d29ec2c3ea6b" CreationDate="2014-06-10T08:21:19.197" UserId="434" Comment="added an example" Text="Different people use different tools for different things.  Terms like Data Science are generic for a reason.  A data scientist could spend an entire career without having to learn a particular tool like hadoop.  Hadoop is widely used, but it is not the only platform that is capable of managing and manipulating data, even large scale data.&#xD;&#xA;&#xD;&#xA;I would say that a data scientist should be familiar with concepts like MapReduce, distributed systems, distributed file systems, and the like, but I wouldn't judge someone for not knowing about such things.  &#xD;&#xA;&#xD;&#xA;It's a big field.  There is a sea of knowledge and most people are capable of learning and being an expert in a single drop.  The key to being a scientist is having the desire to learn and the motivation to know that which you don't already know.&#xD;&#xA;&#xD;&#xA;As an example:  I could hand the right person a hundred structured CSV files containing information about classroom performance in one particular class over a decade.  A data scientist would be able to spend a year gleaning insights from the data without ever needing to spread computation across multiple machines.  You could apply machine learning algorithms, analyze it using visualizations, combine it with external data about the region, ethnic makeup, changes to environment over time, political information, weather patterns, etc.  All of that would be &quot;data science&quot; in my opinion.  It might take something like hadoop to test and apply anything you learned to data comprising an entire country of students rather than just a classroom, but that final step doesn't necessarily make someone a data scientist.  And not taking that final step doesn't necessarily disqualify someone from being a data scientist.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="668" PostHistoryTypeId="2" PostId="260" RevisionGUID="ddb2f077-0758-4861-a209-d05e3caacafe" CreationDate="2014-06-10T08:38:27.093" UserId="454" Text="The algorithm that is used in this case is called [one-vs-all classifier][1] or multiclass classifier. &#xD;&#xA;&#xD;&#xA;In your case you have to take one class, e. g. number 1 , mark it as positive and combine the rest seven classes in one negative class. The neural network will output the probability of this case being class number 1 vs the rest of the classes. &#xD;&#xA;&#xD;&#xA;Afterwords, you have to assign as positive another class, e.g. number 2, assign all other classes as one big negative class and get the predicted probability from the network again.&#xD;&#xA;&#xD;&#xA;After repeating this procedure for all eight classes, assign each case to the the class that had the maximum probability from all the classes outputted from the neural network. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://class.coursera.org/ml-003/lecture/38" />
  <row Id="669" PostHistoryTypeId="2" PostId="261" RevisionGUID="36e24cab-25ec-48c0-8e50-b5ccbb8f438c" CreationDate="2014-06-10T09:21:14.013" UserId="480" Text="You could take a look at CN2 rule learner in Orange http://orange.biolab.si/docs/latest/widgets/rst/classify/cn2/&#xD;&#xA;" />
  <row Id="670" PostHistoryTypeId="2" PostId="262" RevisionGUID="e14f5001-4e2e-4c4a-a094-140e20f364f1" CreationDate="2014-06-10T09:26:06.593" UserId="97" Text="What are the main benefits from storing data in HDF? And what are the main data science tasks where HDF is really suitable and useful?" />
  <row Id="671" PostHistoryTypeId="1" PostId="262" RevisionGUID="e14f5001-4e2e-4c4a-a094-140e20f364f1" CreationDate="2014-06-10T09:26:06.593" UserId="97" Text="Benefits from using Hierarchical Data Format" />
  <row Id="672" PostHistoryTypeId="3" PostId="262" RevisionGUID="e14f5001-4e2e-4c4a-a094-140e20f364f1" CreationDate="2014-06-10T09:26:06.593" UserId="97" Text="&lt;hdf&gt;&lt;data-formats&gt;&lt;hierarchical-data-format&gt;" />
  <row Id="673" PostHistoryTypeId="2" PostId="263" RevisionGUID="d66d4a07-b297-4d1b-ad3e-746d2e52d2ba" CreationDate="2014-06-10T09:41:34.697" UserId="478" Text="Unfortunately, parallelization is not yet implemented in pandas. You can join [this github issue](http://github.com/pydata/pandas/issues/5751) if you want to participate in the development of this feature.&#xD;&#xA;&#xD;&#xA;I don't know any &quot;magic unicorn package&quot; for this purposes, so the best thing will be write your own solution. But if you still don't want to spend time on that and want to learn something new – you can try the two methods built into MongoDB (map reduce and agg framework).  See [mongodb_agg_framework](http://docs.mongodb.org/manual/core/aggregation/)." />
  <row Id="674" PostHistoryTypeId="2" PostId="264" RevisionGUID="9bfc6315-6165-4905-bfbf-7952e652c277" CreationDate="2014-06-10T10:48:58.457" UserId="490" Text="You can also give the Expectation Maximization clustering algorithm a try.  It can work on categorical data and will give you a statistical likelihood of which categorical value (or values) a cluster is most likely to take on." />
  <row Id="675" PostHistoryTypeId="2" PostId="265" RevisionGUID="9f2bb3a5-a3ef-4757-904a-9faa34955818" CreationDate="2014-06-10T10:58:58.447" UserId="434" Text="I have a variety of NFL datasets that I think might make a good side-project, but I haven't done anything with them just yet.&#xD;&#xA;&#xD;&#xA;Coming to this site made me think of machine learning algorithms and I wondering how good they might be at either predicting the outcome of football games or even the next play.&#xD;&#xA;&#xD;&#xA;It seems to me that there would be some trends that could be identified - on 3rd down and 1, a team with a strong running back *theoretically should* have a tendency to run the ball in that situation.&#xD;&#xA;&#xD;&#xA;Scoring might be more difficult to predict, but the winning team might be.&#xD;&#xA;&#xD;&#xA;My question is whether these are good questions to throw at a machine learning algorithm.  It could be that a thousand people have tried it before, but the nature of sports makes it an unreliable topic." />
  <row Id="676" PostHistoryTypeId="1" PostId="265" RevisionGUID="9f2bb3a5-a3ef-4757-904a-9faa34955818" CreationDate="2014-06-10T10:58:58.447" UserId="434" Text="Can machine learning algorithms predict sports scores or plays?" />
  <row Id="677" PostHistoryTypeId="3" PostId="265" RevisionGUID="9f2bb3a5-a3ef-4757-904a-9faa34955818" CreationDate="2014-06-10T10:58:58.447" UserId="434" Text="&lt;machine-learning&gt;" />
  <row Id="678" PostHistoryTypeId="2" PostId="266" RevisionGUID="ae26a95e-89c6-404b-92b8-360c3f28a5ef" CreationDate="2014-06-10T11:05:47.273" UserId="434" Text="Being new to machine-learning in general, I'd like to start playing around and see what the possibilities are.&#xD;&#xA;&#xD;&#xA;I'm curious as to what applications you might recommend that would offer the fastest time from installation to producing a meaningful result.&#xD;&#xA;&#xD;&#xA;Also, any recommendations for good getting-started materials on the subject of machine-learning in general would be appreciated." />
  <row Id="679" PostHistoryTypeId="1" PostId="266" RevisionGUID="ae26a95e-89c6-404b-92b8-360c3f28a5ef" CreationDate="2014-06-10T11:05:47.273" UserId="434" Text="What are some easy to learn machine-learning applications?" />
  <row Id="680" PostHistoryTypeId="3" PostId="266" RevisionGUID="ae26a95e-89c6-404b-92b8-360c3f28a5ef" CreationDate="2014-06-10T11:05:47.273" UserId="434" Text="&lt;machine-learning&gt;" />
  <row Id="682" PostHistoryTypeId="2" PostId="268" RevisionGUID="554c6981-f1d6-4653-8df8-9d3e023240db" CreationDate="2014-06-10T11:36:19.287" UserId="418" Text="I think [Weka][1] is a good starting point. You can do a bunch of stuff like supervised learning or clustering and easily compare a large set of algorithms na methodologies.&#xD;&#xA;&#xD;&#xA;Weka's manual is actually a book on machine learning and data mining that can be used as introductory material. &#xD;&#xA;&#xD;&#xA;  [1]: http://www.cs.waikato.ac.nz/ml/weka/" />
  <row Id="683" PostHistoryTypeId="2" PostId="269" RevisionGUID="b5d68d14-89a3-4759-a24c-760171f93746" CreationDate="2014-06-10T11:37:28.293" UserId="97" Text="Definitely they can.&#xD;&#xA;I can target you to a **[nice paper][1]**. Once I used it for soccer league results prediction algorithm implementation, primarily aiming at having some value against bookmakers.&#xD;&#xA;&#xD;&#xA;From paper's abstract:&#xD;&#xA;&gt; a Bayesian dynamic generalized model to estimate the time dependent skills of all teams in a league, and to predict next weekend's soccer matches.&#xD;&#xA;&#xD;&#xA;Keywords:&#xD;&#xA;&#xD;&#xA;&gt; Dynamic Models, Generalized Linear Models, Graphical Models, Markov&#xD;&#xA;&gt; Chain Monte Carlo Methods, Prediction of Soccer Matches&#xD;&#xA;&#xD;&#xA;  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7448&amp;rep=rep1&amp;type=pdf" />
  <row Id="684" PostHistoryTypeId="2" PostId="270" RevisionGUID="08cac896-e8e9-41d8-b03f-241cb0623f8b" CreationDate="2014-06-10T11:39:19.603" UserId="478" Text="Machine learning and statistical techniques can improve the forecast, but nobody can predict the real result.&#xD;&#xA;&#xD;&#xA;There was a kaggle competition a few month ago about [predicting the 2014 NCAA Tournament][1]. You can read the Competition Forum to get a better idea on what people did and what results did they achieve.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.kaggle.com/c/march-machine-learning-mania" />
  <row Id="685" PostHistoryTypeId="2" PostId="271" RevisionGUID="39efabbe-79ea-4048-abb6-0af48316bcd8" CreationDate="2014-06-10T11:49:23.777" UserId="418" Text="It has been shown before that machine learning techniques can be applied for predicting sport results. Simple google search should give you a bunch of results.&#xD;&#xA;&#xD;&#xA;However, it has also been showed (for NFL btw) that very complex predictive models, simple predictive models, questioning people, or crowd knowledge by utilising betting info, they all perform more or less the same. Source: &quot;[Everything is obvious once you know the answer - How common sense Fails][1]&quot;, Chapter 7, by Duncan Watts. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://everythingisobvious.com/" />
  <row Id="686" PostHistoryTypeId="2" PostId="272" RevisionGUID="38346f6e-e3c5-437c-970a-ba743ca90609" CreationDate="2014-06-10T11:53:07.737" UserId="478" Text="I would recommend to start with some MOOC on machine learning. For example Andrew Ng's [course][1] at coursera.&#xD;&#xA;&#xD;&#xA;You should also take a look at [Orange][2] application. It has a graphical interface and probably it is easier to understand some ML techniques using it. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.coursera.org/course/ml&#xD;&#xA;  [2]: http://orange.biolab.si/" />
  <row Id="687" PostHistoryTypeId="2" PostId="273" RevisionGUID="d3f7ccd7-4a9a-44b9-80bd-efc401b14fa5" CreationDate="2014-06-10T12:10:28.713" UserId="498" Text="You can apply data science techniques to data on one machine so the answer to the question - is it absolutely necessary to learn Hadoop to be a data scientist is no." />
  <row Id="688" PostHistoryTypeId="2" PostId="274" RevisionGUID="f2c4f682-6daa-4bfd-8887-c8a17f76138f" CreationDate="2014-06-10T12:57:04.307" UserId="434" Text="One benefit is wide support - C, Java, Perl, Python, and R all have HDF5 bindings.&#xD;&#xA;&#xD;&#xA;Another benefit is speed.  I haven't ever seen it benchmarked, but HDF is supposed to be faster than SQL databases.&#xD;&#xA;&#xD;&#xA;I understand that it is very good when used with both large sets of scientific data and time series data - network monitoring, usage tracking, etc.&#xD;&#xA;&#xD;&#xA;I don't believe there is a size limitation for HDF files (although OS limits would still apply.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="689" PostHistoryTypeId="2" PostId="275" RevisionGUID="8572d413-8e41-494b-8f72-6a7fe86c01ae" CreationDate="2014-06-10T13:17:48.433" UserId="508" Text="For time series data in particular, [Quandl](http://www.quandl.com/) is an excellent resource -- an easily browsable directory of (mostly) clean time series.&#xD;&#xA;&#xD;&#xA;One of their coolest features is [open-data stock prices](http://blog.quandl.com/blog/quandl-open-data/) -- i.e. financial data that can be edited wiki-style, and isn't encumbered by licensing." />
  <row Id="691" PostHistoryTypeId="2" PostId="276" RevisionGUID="d2daea04-a0e1-498b-8796-478c42bf2a59" CreationDate="2014-06-10T13:38:31.207" UserId="434" Text="Not all government data is listed on data.gov - [Sunlight Foundation][1] put together a [set of spreadsheets][2] back in February describing sets of available data.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://sunlightfoundation.com/blog/2014/02/21/open-data-inventories-ready-for-human-consumption/&#xD;&#xA;  [2]: https://drive.google.com/folderview?id=0B4QuErjcV2a0WXVDOURwbzh6S2s&amp;usp=sharing" />
  <row Id="692" PostHistoryTypeId="2" PostId="277" RevisionGUID="92a9bbc5-a008-4264-9688-54aad5737932" CreationDate="2014-06-10T14:25:41.903" UserId="518" Text="To be honest, I think that doing some projects will teach you much more than doing a full course. One reason is that doing a project is more motivating and open-ended than doing assignments.&#xD;&#xA;&#xD;&#xA;A course, if you have the time AND motivation (real motivation), is better than doing a project. The other commentators have made good platform recommendations on tech. &#xD;&#xA;&#xD;&#xA;I think, from a fun project standpoint, you should ask a question and get a computer to learn to answer it. &#xD;&#xA;&#xD;&#xA;Some good classic questions that have good examples are:&#xD;&#xA;&#xD;&#xA; - Neural Networks for recognizing hand written digits&#xD;&#xA; - Spam email classification using logistic regression&#xD;&#xA; - Classification of objects using Gaussian Mixture models&#xD;&#xA; - Some use of linear regression, perhaps forecasting of grocery prices given neighborhoods&#xD;&#xA;&#xD;&#xA;These projects have the math done, code done, and can be found with Google readily.&#xD;&#xA;&#xD;&#xA;Other cool subjects can be done by you!&#xD;&#xA;&#xD;&#xA;Lastly, I research robotics, so for me the most FUN applications are behavioral.&#xD;&#xA;Examples can include (if you can play with an arduino)&#xD;&#xA;&#xD;&#xA;Create a application, that uses logistic regression perhaps, that learns when to turn the fan off and on given the inner temperature, and the status of the light in the room.&#xD;&#xA;&#xD;&#xA;Create an application that teaches a robot to move an actuator, perhaps a wheel, based on sensor input (perhaps a button press), using Gaussian Mixture Models (learning from demonstration).&#xD;&#xA;&#xD;&#xA;Anyway, those are pretty advanced. The point I'm making is that if you pick a project that you (really really) like, and spend a few week on it, you will learn a massive amount, and understand so much more than you will get doing a few assignments." />
  <row Id="693" PostHistoryTypeId="2" PostId="278" RevisionGUID="665f8d63-d86e-434b-b846-017d28c58cc1" CreationDate="2014-06-10T14:30:33.667" UserId="524" Text="Assuming you're familiar with programming I would recommend looking at [scikit-learn][1]. It has especially nice help pages that can serve as mini-tutorials/a quick tour through machine learning. Pick an area you find interesting and work through the examples. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/" />
  <row Id="694" PostHistoryTypeId="2" PostId="279" RevisionGUID="2f94890c-ef51-4c44-94fd-aebc193fd8da" CreationDate="2014-06-10T14:57:47.810" UserId="514" Text="There is also another resource provided by The Guardian, the British Daily on their website. The datasets published by the Guardian Datablog are all hosted. Datasets related to Football Premier League Clubs' accounts, Inflation and GDP details of UK, Grammy awards data etc.&#xD;&#xA;The datasets are available at &#xD;&#xA;&#xD;&#xA; - http://www.theguardian.com/news/datablog/interactive/2013/jan/14/all-our-datasets-index" />
  <row Id="695" PostHistoryTypeId="5" PostId="273" RevisionGUID="09123e26-7fe2-4fa6-8ee6-b5caf0a9d2af" CreationDate="2014-06-10T15:04:01.177" UserId="498" Comment="precised grammar" Text="You can apply data science techniques to data on one machine so the answer to the question as the OP phrased it, is no." />
  <row Id="696" PostHistoryTypeId="2" PostId="280" RevisionGUID="44af33ad-263a-4cdd-be41-6fb9a85efa83" CreationDate="2014-06-10T15:08:54.073" UserId="531" Text="I am developing a system that is intended to capture the &quot;context&quot; of user activity within an application; it is a framework that web applications can use to tag user activity based on requests made to the system.  It is hoped that this data can then power ML features such as context aware information retrieval.&#xD;&#xA;&#xD;&#xA;I'm having trouble deciding on what features to select in addition to these user tags - the URL being requested, approximate time spent with any given resource, estimating the current &quot;activity&quot; within the system.&#xD;&#xA;&#xD;&#xA;I am interested to know if there are good examples of this kind of technology or any prior research on the subject - a cursory search of the ACM DL revealed some related papers but nothing really spot-on." />
  <row Id="697" PostHistoryTypeId="1" PostId="280" RevisionGUID="44af33ad-263a-4cdd-be41-6fb9a85efa83" CreationDate="2014-06-10T15:08:54.073" UserId="531" Text="Feature selection for tracking user activity within an application" />
  <row Id="698" PostHistoryTypeId="3" PostId="280" RevisionGUID="44af33ad-263a-4cdd-be41-6fb9a85efa83" CreationDate="2014-06-10T15:08:54.073" UserId="531" Text="&lt;feature-selection&gt;" />
  <row Id="705" PostHistoryTypeId="2" PostId="282" RevisionGUID="f343fd8f-17fb-4799-9b28-ad10edbc7f7a" CreationDate="2014-06-10T16:25:24.223" UserId="514" Text="Yes. Why not?!&#xD;&#xA;With so much of data being recorded in each sport in each game, smart use of data could lead us in obtaining important insights regarding player performance.&#xD;&#xA;&#xD;&#xA;Some examples:&#xD;&#xA;&#xD;&#xA; - **Baseball**: In the movie Moneyball (which is an adaptation of the MoneyBall book), Brad Pitt plays a character who analyses player statistics to come up with a team that performs tremendously well! It was a depiction of the real life story of Oakland Athletics baseball team. For more info, http://www.theatlantic.com/entertainment/archive/2013/09/forget-2002-this-years-oakland-as-are-the-real-em-moneyball-em-team/279927/&#xD;&#xA; - **Cricket**: SAP Labs has come up with an auction analytics tool that has given insights about impact players to buy in the 2014 Indian Premier League auction for the Kolkata Knight Riders team, which eventually went on to win the 2014 IPL **Championship**. For more info, http://scn.sap.com/community/hana-in-memory/blog/2014/06/10/sap-hana-academy-cricket-demo--how-sap-hana-powered-the-kolkata-knight-riders-to-ipl-championship&#xD;&#xA;&#xD;&#xA;So, yes, statistical analysis of the player records can give us insights about **which players are more likely to perform but not which players will perform**. So, machine learning, a close cousin of statistical analysis will be proving to be a game changer." />
  <row Id="708" PostHistoryTypeId="2" PostId="284" RevisionGUID="e498aa53-308c-425d-9354-f0c77833a8f2" CreationDate="2014-06-10T17:06:54.950" UserId="84" Text="Well, this may not answer the question thoroughly, but since you're dealing with information retrieval, it may be of some use. [This page](http://moz.com/search-ranking-factors) mantains a set of features and associated *correlations* with page-ranking methods of search engines. As a disclaimer from the webpage itself:&#xD;&#xA;&#xD;&#xA;&gt; Note that these factors are not &quot;proof&quot; of what search engines use to rank websites, but simply show the characteristics of web pages that tend to rank higher.&#xD;&#xA;&#xD;&#xA;The list pointed may give you some insights on which features would be nice to select. For example, considering the second most correlated feature, # of google +1's, it may be possible to add some probability of a user making use of such service if he/she accesses many pages with high # of google +1 (infer &quot;user context&quot;). Thus, you could try to &quot;guess&quot; some other relations that may shed light on interesting features for your tracking app." />
  <row Id="709" PostHistoryTypeId="2" PostId="285" RevisionGUID="7780b9ba-7668-41a6-817c-d110a704ea87" CreationDate="2014-06-10T17:15:52.953" UserId="553" Text="There are a lot of good questions about Football (and sports, in general) that would be awesome to throw to an algorithm and see what comes out. The tricky part is to know *what* to throw to the algorithm.&#xD;&#xA;&#xD;&#xA;A team with a good RB could just pass on 3rd-and-short just because the opponents would probably expect run, for instance. So, in order to actually produce some worthy results, I'd break the problem in smaller pieces and analyse them statistically while throwing them to the machines!&#xD;&#xA;&#xD;&#xA;There are a few (good) websites that try to do the same, you should check'em out and use whatever they found to help you out:&#xD;&#xA;&#xD;&#xA;* [Football Outsiders](http://www.footballoutsiders.com/)&#xD;&#xA;* [Advanced Football Analytics](http://www.advancedfootballanalytics.com/)&#xD;&#xA;&#xD;&#xA;And if you truly want to explore Sports Data Analysis, you should definitely check the [Sloan Sports Conference](http://www.sloansportsconference.com/) videos. There's a lot of them spread on Youtube.&#xD;&#xA;" />
  <row Id="710" PostHistoryTypeId="2" PostId="286" RevisionGUID="995076c7-c53c-4812-bce6-63f2a5404526" CreationDate="2014-06-10T17:27:31.080" UserId="381" Text="It depends on your employer. Many stipulate that you know it, especially if the job involves &quot;big data&quot;, while others will let you learn on the job or not care. Take a look at the job boards and see for yourself!" />
  <row Id="711" PostHistoryTypeId="2" PostId="287" RevisionGUID="6b26c8be-1a42-452d-af5e-f5c21c0b56ca" CreationDate="2014-06-10T17:32:19.120" UserId="108" Text="I do not know a standard answer to this, but I thought about it some times ago and I have some ideas to share. &#xD;&#xA;&#xD;&#xA;When you have one confusion matrix, you have more or less a picture of how you classification model confuse (mis-classify) classes. When you repeat classification tests you will end up having multiple confusion matrices. The question is how to get a meaningful aggregate confusion matrix. The answer depends on what is the meaning of meaningful (pun intended). I think there is not a single version of meaningful.&#xD;&#xA;&#xD;&#xA;One way is to follow the rough idea of multiple testing. In general, you test something multiple times in order to get more accurate results. As a general principle one can reason that averaging on the results of the multiple tests reduces the variance of the estimates, so as a consequence, it increases the precision of the estimates. You can proceed in this way, of course, by summing position by position and then dividing by the number of tests. You can go further and instead of estimating only a value for each cell of the confusion matrix, you can also compute some confidence intervals, t-values and so on. This is OK from my point of view. But it tell only one side of the story.&#xD;&#xA;&#xD;&#xA;The other side of the story which might be investigated is how stable are the results for the same instances. To exemplify that I will take an extreme example. Suppose you have a classification model for 3 classes. Suppose that these classes are in the same proportion. If your model is able to predict one class perfectly and the other 2 classes with random like performance, you will end up having 0.33 + 0.166 + 0.166 = 0.66 misclassification ratio. This might seem good, but even if you take a look on a single confusion matrix you will not know that your performance on the last 2 classes varies wildly. Multiple tests can help. But averaging the confusion matrices would reveal this? My belief is not. The averaging will give the same result more or less, and doing multiple tests will only decrease the variance of the estimation. However it says nothing about the wild instability of prediction.&#xD;&#xA;&#xD;&#xA;So another way to do compose the confusion matrices would better involve a prediction density for each instance. One can build this density by counting for each instance, the number of times it was predicted a given class. After normalization, you will have for each instance a prediction density rather a single prediction label. You can see that a single prediction label is similar with a degenerated density where you have probability of 1 for the predicted class and 0 for the other classes for each separate instance. Now having this densities one can build a confusion matrix by adding the probabilities from each instance and predicted class to the corresponding cell of the aggregated confusion matrix. &#xD;&#xA;&#xD;&#xA;One can argue that this would give similar results like the previous method. However I think that this might be the case, the second method is less affected by how the samples from the tests are drawn, and thus more stable and closer to the reality. &#xD;&#xA;&#xD;&#xA;Also the second method might be altered in order to obtain a third method, where one can assign as prediction the label with highest density from the prediction of a given instance. &#xD;&#xA;&#xD;&#xA;I do not implemented those things but I plan to study further because I believe might worth spending some time. " />
  <row Id="712" PostHistoryTypeId="2" PostId="288" RevisionGUID="e040466e-33cd-4913-93cd-fdcbbb8e5c6d" CreationDate="2014-06-10T17:36:11.580" UserId="381" Text="The goal determines the features, so I would initially take as many as possible, then use cross validation to select the optimal subset.&#xD;&#xA;&#xD;&#xA;My educated guess is that a Markov model would work. If you discretize the action space (e.g., select this menu item, press that button, etc.), you can predict the next action based on the past ones. It's a _sequence_ or [structured prediction](http://en.wikipedia.org/wiki/Structured_prediction) problem." />
  <row Id="713" PostHistoryTypeId="2" PostId="289" RevisionGUID="bb8f16f3-de1d-419c-bfc6-9e4f361f10a8" CreationDate="2014-06-10T17:56:34.847" UserId="560" Text="Yann LeCun mentioned in his [AMA](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/) that he considers having a PhD very important in order to get a job at a top company.&#xD;&#xA;&#xD;&#xA;I have a masters in statistics and my undergrad was in economics(math intensive though), but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?  " />
  <row Id="714" PostHistoryTypeId="1" PostId="289" RevisionGUID="bb8f16f3-de1d-419c-bfc6-9e4f361f10a8" CreationDate="2014-06-10T17:56:34.847" UserId="560" Text="Qualifications for PhD Programs" />
  <row Id="715" PostHistoryTypeId="3" PostId="289" RevisionGUID="bb8f16f3-de1d-419c-bfc6-9e4f361f10a8" CreationDate="2014-06-10T17:56:34.847" UserId="560" Text="&lt;education&gt;" />
  <row Id="716" PostHistoryTypeId="2" PostId="290" RevisionGUID="b5c99728-676c-4c64-9a1b-83d073406798" CreationDate="2014-06-10T18:22:32.610" UserId="571" Text="I found the pluralsight course [Introduction to machine learning encog][1] a great resource so start with. It uses the [Encog library][2] to quickly explore different ml techniques. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pluralsight.com/training/courses/TableOfContents?courseName=introduction-to-machine-learning-encog&amp;highlight=abhishek-kumar_introduction-to-machine-learning-encog-m2-applications!abhishek-kumar_introduction-to-machine-learning-encog-m3-tasks!abhishek-kumar_introduction-to-machine-learning-encog-m1-intro*1#introduction-to-machine-learning-encog-m2-applications&#xD;&#xA;  [2]: http://www.heatonresearch.com/encog" />
  <row Id="717" PostHistoryTypeId="2" PostId="291" RevisionGUID="850631b4-f385-476e-8df3-d9e3a178d25e" CreationDate="2014-06-10T18:55:39.010" UserId="381" Text="If I were you I would take a MOOC or two (e.g., [Algorithms, Part I](https://www.coursera.org/course/algs4partI), [Algorithms, Part II](https://www.coursera.org/course/algs4partII), [Functional Programming Principles in Scala](https://www.coursera.org/course/progfun)), a good book on data structures and algorithms, then just code as much as possible. You could implement some statistics or ML algorithms, for example; that would be good practice for you and useful to the community.&#xD;&#xA;&#xD;&#xA;For a PhD program, however, I would also make sure I were familiar with the type of maths they use. If you want to see what it's like at the deep end, browse the papers at the [JMLR](http://jmlr.org/papers/). That will let you calibrate yourself in regards to theory; can you sort of follow the maths?&#xD;&#xA;&#xD;&#xA;Oh, and you don't need a PhD to work at top companies, unless you want to join research departments like his. But then you'll spend more time doing development, and you'll need good coding skills..." />
  <row Id="718" PostHistoryTypeId="5" PostId="289" RevisionGUID="c89593a0-9d56-4a48-aae4-7e2bfe8aa873" CreationDate="2014-06-10T19:02:52.720" UserId="560" Comment="added 275 characters in body" Text="Yann LeCun mentioned in his [AMA](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/) that he considers having a PhD very important in order to get a job at a top company.&#xD;&#xA;&#xD;&#xA;I have a masters in statistics and my undergrad was in economics(math intensive though), but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?  &#xD;&#xA;&#xD;&#xA;edit: I have taken a few MOOCs (Machine Learning, Recommender Systems, NLP) and code R/python on a daily basis. I have a lot of coding experience with statistical languages and implement ML algorithms daily. I am more concerned with things that I can put on applications." />
  <row Id="719" PostHistoryTypeId="5" PostId="289" RevisionGUID="21105974-4460-491e-8b66-c6e5acf12fed" CreationDate="2014-06-10T19:25:37.803" UserId="560" Comment="deleted 7 characters in body" Text="Yann LeCun mentioned in his [AMA](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/) that he considers having a PhD very important in order to get a job at a top company.&#xD;&#xA;&#xD;&#xA;I have a masters in statistics and my undergrad was in economics(math intensive), but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?  &#xD;&#xA;&#xD;&#xA;edit: I have taken a few MOOCs (Machine Learning, Recommender Systems, NLP) and code R/python on a daily basis. I have a lot of coding experience with statistical languages and implement ML algorithms daily. I am more concerned with things that I can put on applications." />
  <row Id="720" PostHistoryTypeId="2" PostId="292" RevisionGUID="4e414ea2-a09e-4091-9ac5-923dc94bc0b6" CreationDate="2014-06-10T19:43:11.860" UserId="587" Text="Your time would probably be better spent on Kaggle than in a PhD program. When you read the stories by winners ([Kaggle blog][1]) you'll see that it takes a large amount of practice and the winners are not just experts of one single method.&#xD;&#xA;&#xD;&#xA;On the other hand, being active and having a plan in a PhD program can get you connections that you otherwise would probably not get.&#xD;&#xA;&#xD;&#xA;I guess the real question is for you - what are the reasons for wanting a job at a top company?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://blog.kaggle.com/" />
  <row Id="725" PostHistoryTypeId="2" PostId="293" RevisionGUID="309378bd-a8b6-48d9-8007-719b38618893" CreationDate="2014-06-10T20:28:54.613" UserId="26" Text="Perhaps a good way to paraphrase the question is, what are the advantages compared to alternative formats?  &#xD;&#xA;&#xD;&#xA;The main alternatives are, I think: a database, text files, or another packed/binary format.&#xD;&#xA;&#xD;&#xA;The database options to consider are probably a columnar store or NoSQL, or for small self-contained datasets SQLite.  The main advantage of the database is the ability to work with data much larger than memory, to have random or indexed access, and to add/append/modify data quickly.  The main *dis*advantage is that it is much slower than HDF, for problems in which the entire dataset needs to be read in and processed.  Another disadvantage is that, with the exception of embedded-style databases like SQLite, a database is a system (requiring admnistration, setup, maintenance, etc) rather than a simple self-contained data store.  &#xD;&#xA;&#xD;&#xA;The text file format options are XML/JSON/CSV.  They are cross-platform/language/toolkit, and are a good archival format due to the ability to be self-describing (or obvious :).  If uncompressed, they are huge (10x-100x HDF), but if compressed, they can be fairly space-efficient (compressed XML is about the same as HDF).  The main disadvantage here is again speed: parsing text is much, much slower than HDF.&#xD;&#xA;&#xD;&#xA;The other binary formats (npy/npz numpy files, blz blaze files, protocol buffers, Avro, ...) have very similar properties to HDF, except they are less widely supported (may be limited to just one platform: numpy) and may have specific other limitations.  They typically do not offer a compelling advantage.&#xD;&#xA;&#xD;&#xA;HDF is a good complement to databases, it may make sense to run a query to produce a roughly memory-sized dataset and then cache it in HDF if the same data would be used more than once.  If you have a dataset which is fixed, and usually processed as a whole, storing it as a collection of appropriately sized HDF files is not a bad option.  If you have a dataset which is updated often, staging some of it as HDF files periodically might still be helpful.&#xD;&#xA;&#xD;&#xA;To summarize, HDF is a good format for data which is read (or written) typically as a whole; it is the lingua franca or common/preferred interchange format for many applications due to wide support and compatibility, decent as an archival format, and very fast.&#xD;&#xA;&#xD;&#xA;P.S. To give this some practical context, my most recent experience comparing HDF to alternatives, a certain small (much less than memory-sized) dataset took 2 seconds to read as HDF (and most of this is probably overhead from Pandas); ~1 minute to read from JSON; and 1 *hour* to write to database.  Certainly the database write could be sped up, but you'd better have a good DBA!  This is how it works out of the box." />
  <row Id="726" PostHistoryTypeId="2" PostId="294" RevisionGUID="51b0d4da-63b6-4b04-928c-815967371671" CreationDate="2014-06-10T20:40:25.623" UserId="602" Text="As a former Hadoop engineer, it is not needed but it helps. Hadoop is just one system - the most common system, based on Java, and a ecosystem of products, which apply a particular technique &quot;Map/Reduce&quot; to obtain results in a timely manner. Hadoop is not used at Google, though I assure you they use big data analytics. Google uses their own systems, developed in C++. In fact, Hadoop was created as a result of Google publishing their Map/Reduce and BigTable (HBase in Hadoop) white papers.&#xD;&#xA;&#xD;&#xA;Data scientists will interface with hadoop engineers, though at smaller places you may be required to wear both hats. If you are strictly a data scientist, then whatever you use for your analytics, R, Excel, Tableau, etc, will operate only on a small subset, then will need to be converted to run against the full data set involving hadoop. " />
  <row Id="727" PostHistoryTypeId="2" PostId="295" RevisionGUID="90d3f93e-a274-4e8a-9018-ca165c09fb5d" CreationDate="2014-06-10T20:43:28.533" UserId="386" Text="I am glad you also found Yann LeCun's AMA page, it's very useful.&#xD;&#xA;&#xD;&#xA;Here are my opinions  &#xD;&#xA;Q: Should I take some intro software engineering courses at my local University to make myself a stronger candidate?  &#xD;&#xA;A: No, you need to take more math courses.  It's not the applied stuff that's hard, it's the theory stuff.  I don't know what your school offers.  Take theoretical math courses, along with some computer science courses.  &#xD;&#xA;&#xD;&#xA;Q:What other advice you have for someone applying to PhD programs from outside the CS field?  &#xD;&#xA;A:  How closely related are you looking for.  Without a specific question, it's hard to give a specific answer.  &#xD;&#xA;" />
  <row Id="728" PostHistoryTypeId="2" PostId="296" RevisionGUID="0647bbf6-5143-4ef8-b46d-cce29b8f4712" CreationDate="2014-06-10T21:50:51.347" UserId="14" Text="HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of &quot;topics&quot; in document-modeling terms) is not known a priori.  So that's the reason why there's a difference.&#xD;&#xA;&#xD;&#xA;Using LDA for document modeling, one treats each &quot;topic&quot; as a distribution of words in some known vocabulary.  For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word).&#xD;&#xA;&#xD;&#xA;For HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics.  So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution.&#xD;&#xA;&#xD;&#xA;As far as pros and cons, HDP has the advantage that the maximum number of topics can be unbounded and learned from the data rather than specified in advance.  I suppose though it is more complicated to implement, and unnecessary in the case where a bounded number of topics is acceptable." />
  <row Id="729" PostHistoryTypeId="2" PostId="297" RevisionGUID="bc66cabf-290c-4c7a-b4cd-70bf460b95aa" CreationDate="2014-06-10T22:26:53.623" UserId="418" Text="As in @damienfrancois answer feature selection is about selecting a subset of features. So in NLP it would be selecting a set of specific words (the typical in NLP is that each word represents a feature with value equal to the frequency of the word or some other weight based on TF/IDF or similar).&#xD;&#xA;&#xD;&#xA;Dimensionality reduction is the introduction of new feature space where the original features are represented. The new space is of lower dimension that the original space. In case of text an example would be the [hashing trick][1] where a piece of text is reduced to a vector of few bits (say 16 or 32) or bytes. The amazing thing is that the geometry of the space in reserved (given enough bits), so relative distances between documents remain the same as in the original space, so you can deploy standard machine learning techniques without having to deal with unbound (and huge number of) dimensions found in text.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Feature_hashing" />
  <row Id="730" PostHistoryTypeId="2" PostId="298" RevisionGUID="49498d35-0e47-44c2-9677-acd16836cd81" CreationDate="2014-06-10T22:29:52.873" UserId="609" Text="You already have a Masters in Statistics, which is great! In general, I'd suggest to people to take as much statistics as they can, especially Bayesian Data Analysis. &#xD;&#xA;&#xD;&#xA;Depending on what you want to do with your PhD, you would benefit from foundational courses in the discipline(s) in your application area.  You already have Economics but if you want to do Data Science on social behavior, then courses in Sociology would be valuable.  If you want to work in fraud prevention, then a courses in banking and financial transactions would be good.  If you want to work in information security, then taking a few security courses would be good.&#xD;&#xA;&#xD;&#xA;There are people who argue that it's not valuable for Data Scientists to spend time on courses in sociology or other disciplines.  But consider the recent case of the Google Flu Trends project.  In [this article][1] their methods were strongly criticized for making avoidable mistakes.  The critics call it &quot;Big Data hubris&quot;.&#xD;&#xA;&#xD;&#xA;There's another reason for building strength in social science disciplines: personal competitive advantage.  With the rush of academic degree programs, certificate programs, and MOOCs, there is a mad rush of students into the Data Science field.  Most will come out with capabilities for core Machine Learning methods and tools.  PhD graduates will have more depth and more theoretical knowledge, but they are all competing for the same sorts of jobs, delivering the same sorts of value.  With this flood of graduates, I expect that they won't be able to command premium salaries.&#xD;&#xA;&#xD;&#xA;But if you can differentiate yourself with a combination of formal education and practical experience in a particular domain and application area, then you should be able to set yourself apart from the crowd.&#xD;&#xA;&#xD;&#xA;(Context: I'm in a PhD program in Computational Social Science, which has a heavy focus on modeling, evolutionary computation, and social science disciplines, and less emphasis on ML and other empirical data analysis topics).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.uvm.edu/~cdanfort/csc-reading-group/lazer-flu-science-2014.pdf" />
  <row Id="734" PostHistoryTypeId="2" PostId="300" RevisionGUID="4349ec34-4361-4ab9-8831-f0aeaa79fc9e" CreationDate="2014-06-10T23:20:16.670" UserId="609" Text="Topological Data Analysis is a method explicitly designed for the setting you describe. Rather than a global distance metric, it relies only on a local metric of proximity or neighborhood. See: [Topology and data][1] and [Extracting insights from the shape of complex data using topology][2]. You can find additional resources at the website for Ayasdi.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ams.org/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf&#xD;&#xA;  [2]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3566620/" />
  <row Id="735" PostHistoryTypeId="5" PostId="242" RevisionGUID="5b599d5d-e8e7-425c-a9fa-0598d9f56e37" CreationDate="2014-06-10T23:33:28.443" UserId="406" Comment="Gained enough reputation to add a 3rd link." Text="**MapReduce**: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds)&#xD;&#xA;&#xD;&#xA;A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state.&#xD;&#xA;&#xD;&#xA;An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model.&#xD;&#xA;&#xD;&#xA;**Storm**: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard.&#xD;&#xA;&#xD;&#xA;Storm doesn't have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**If you want to know more...**&#xD;&#xA;If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way [here are slides for a talk I gave on building real-time recommendation engines on HBase.][1] &#xD;&#xA;&#xD;&#xA;An excellent paper that marries real-time counting and persistence in an interesting way is [Google News Personalization: Scalable Online Collaborative Filtering][2] &#xD;&#xA;&#xD;&#xA;Another interesting marriage of MR and Storm is [SummingBird][3]. Summingbird allows you to define data analysis operations that can be applied via Storm or MR.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.slideshare.net/cloudera/hbasecon-2013-24063525&#xD;&#xA;  [2]: http://dl.acm.org/citation.cfm?id=1242610&#xD;&#xA;  [3]: https://github.com/twitter/summingbird/wiki" />
  <row Id="736" PostHistoryTypeId="2" PostId="301" RevisionGUID="66513a55-3165-482c-a9fe-77d64c5f1e8a" CreationDate="2014-06-11T00:32:54.887" UserId="609" Text="One reason that data cleaning is rarely fully automated is that there is so much judgment required to define what &quot;clean&quot; means given your particular problem, methods, and goals.&#xD;&#xA;&#xD;&#xA;It may be as simple as imputing values for any missing data, or it might be as complex as diagnosing data entry errors or data transformation errors from previous automated processes (e.g. coding, censoring, transforming).  In these last two cases, the data *looks good* by outward appearance but it's really erroneous.  Such diagnosis often requires manual analysis and inspection, and also out-of-band information such as information about the data sources and methods they used.&#xD;&#xA;&#xD;&#xA;Also, some data analysis methods work better when erroneous or missing data is left blank (or N/A) rather than imputed or given a default value.  This is true when there is explicit representations of uncertainty and ignorance, such as Dempster-Shafer Belief functions.&#xD;&#xA;&#xD;&#xA;Finally, it's useful to have specific diagnostics and metrics for the cleaning process.  Are missing or erroneous values randomly distributed or are they concentrated in any way that might affect the outcome of the analysis.  It's useful to test the effects of alternative cleaning strategies or algorithms to see if they affect the final results.&#xD;&#xA;&#xD;&#xA;Given these concerns, I'm very suspicious of any method or process that treats data cleaning in a superficial, cavalier or full-automated fashion.  There are many devils hiding in those details and it pays to give them serious attention." />
  <row Id="737" PostHistoryTypeId="2" PostId="302" RevisionGUID="37ff0e7f-5a6f-4524-8ab5-9ce4c8b39dcc" CreationDate="2014-06-11T00:44:20.517" UserId="434" Text="I've seen a few similar systems over the years.  I remember a company called ClickTrax which if I'm not mistaken got bought by Google and some of their features are now part of Google Analytics.&#xD;&#xA;&#xD;&#xA;Their purpose was marketing, but the same concept can be applied to user experience analytics.  The beauty of their system was that what was tracked was defined by the webmaster - in your case the application developer.&#xD;&#xA;&#xD;&#xA;I can imagine as an application developer I would want to be able to see statistical data on two things - task accomplishment, and general feature usage.&#xD;&#xA;&#xD;&#xA;As an example of task accomplishment, I might have 3 ways to print a page - Ctrl+P, File-&gt;Print, and a toolbar button.  I would want to be able to compare usage to see if the screenspace utilized by the toolbar button was actually worth it.&#xD;&#xA;&#xD;&#xA;As an example of general feature usage, I would want to define a set of features within my application and focus my development efforts on expanding the features used most by my end users.  Some features that take maybe 5 clicks and are popular, I might want to provide a hotkey for, or slim down the number of clicks to activate that feature.  There is also event timing.  Depending on the application, I might want to know the average amount of time spent on a particular feature.  &#xD;&#xA;&#xD;&#xA;Another thing I would want to look at are click streams.  How are people getting from point A to point B in my application?  What are the most popular point B's?  What are the most popular starting points?  " />
  <row Id="738" PostHistoryTypeId="2" PostId="303" RevisionGUID="5575a0ee-513a-43d9-b83b-ac6c1e85dac5" CreationDate="2014-06-11T01:09:06.100" UserId="609" Text="This isn't my area of specialty and I'm not familiar with Moses, but I found this after some searching.  &#xD;&#xA;&#xD;&#xA;I think you are looking for GIZA++.  You'll see GIZA++ listed in the &quot;Training&quot; section (left menu) on the Moses home page, as the second step. GIZA++ is briefly described in tutorial fashion [here][1]. Here are a few tutorial PowerPoint slides: http://www.tc.umn.edu/~bthomson/wordalignment/GIZA.ppt&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/questions/5752043/is-there-a-tutorial-about-giza" />
  <row Id="739" PostHistoryTypeId="5" PostId="288" RevisionGUID="cc0f0b78-def0-4e73-94a9-73eed650b2e3" CreationDate="2014-06-11T01:19:18.183" UserId="381" Comment="added 55 characters in body" Text="The goal determines the features, so I would initially take as many as possible, then use cross validation to select the optimal subset.&#xD;&#xA;&#xD;&#xA;My educated guess is that a Markov model would work. If you discretize the action space (e.g., select this menu item, press that button, etc.), you can predict the next action based on the past ones. It's a _sequence_ or [structured prediction](http://en.wikipedia.org/wiki/Structured_prediction) problem.&#xD;&#xA;&#xD;&#xA;For commercial offerings, search **app analytics**." />
  <row Id="744" PostHistoryTypeId="2" PostId="305" RevisionGUID="174562c9-20cd-4c05-bde3-eef18d377810" CreationDate="2014-06-11T04:24:04.183" UserId="534" Text="There is plenty of hype surrounding Hadoop and it's ego-system. However, in practice, where many data sets are in the tera-byte range, is it not more reasonable to use Amazon RedShift for querying large data sets, rather than spending time and effort building a Hadoop cluster? Also, how does Amazon Redshift compare with Hadoop with respect to setup complexity, cost, and performance?&#xD;&#xA;&#xD;&#xA; " />
  <row Id="745" PostHistoryTypeId="1" PostId="305" RevisionGUID="174562c9-20cd-4c05-bde3-eef18d377810" CreationDate="2014-06-11T04:24:04.183" UserId="534" Text="Does Amazon RedShift replace Hadoop for ~1XTB data?" />
  <row Id="746" PostHistoryTypeId="3" PostId="305" RevisionGUID="174562c9-20cd-4c05-bde3-eef18d377810" CreationDate="2014-06-11T04:24:04.183" UserId="534" Text="&lt;hadoop&gt;&lt;map-reduce&gt;&lt;aws&gt;" />
  <row Id="747" PostHistoryTypeId="2" PostId="306" RevisionGUID="a726728d-5eab-404a-8537-0be194b3e65b" CreationDate="2014-06-11T05:17:12.253" UserId="434" Text="Personally, I don't think it's all that difficult to set up a hadoop cluster, but I know that it is sometimes painful when you are getting started.&#xD;&#xA;&#xD;&#xA;HDFS size limitations well exceed a TB (or did you mean exabyte?).  If I'm not mistaken it scales to yottabytes or some other measurement that I don't even know the word for.  Whatever it is, it's really big.&#xD;&#xA;&#xD;&#xA;Tools like Redshift have their place, but I always worry about vendor specific solutions.  My main concern is always &quot;what do I do when I am dissatisfied with their service?&quot; - I can go to google and shift my analysis work into their paradigm or I can go to hadoop and shift that same work into that system.  Either way, I'm going to have to learn something new and do a lot of work translating things.&#xD;&#xA;&#xD;&#xA;That being said, it's nice to be able to upload a dataset and get to work quickly - especially if what I'm doing has a short lifecycle.  Amazon has done a good job of answering the data security problem.&#xD;&#xA;&#xD;&#xA;If you want to avoid hadoop, there will always be an alternative.  But it's not all that difficult to work with once you get going with it." />
  <row Id="748" PostHistoryTypeId="2" PostId="307" RevisionGUID="26bdcc75-4c32-4978-a700-e2d04d4e7700" CreationDate="2014-06-11T06:07:45.767" UserId="496" Text="I have read lot of blogs\article on how different type of industries are using Big Data Analytic. But most of these article fails to mention&#xD;&#xA;&#xD;&#xA; 1. What kinda data these companies used. What was the size of the data&#xD;&#xA; 2. What kinda of tools technologies they used to process the data&#xD;&#xA; 3. What was the problem they were facing and how the insight they got the data helped them to resolve the issue.&#xD;&#xA; 4. How they selected the tool\technology to suit their need.&#xD;&#xA; 5. What kinda pattern they identified from the data &amp; what kind of patterns they were looking from the data.&#xD;&#xA;&#xD;&#xA;I wonder if someone can provide me answer to all these questions or a link which at-least answer some of the the questions.&#xD;&#xA;&#xD;&#xA;It would be great if someone share how finance industry is making use of Big Data Analytic." />
  <row Id="749" PostHistoryTypeId="1" PostId="307" RevisionGUID="26bdcc75-4c32-4978-a700-e2d04d4e7700" CreationDate="2014-06-11T06:07:45.767" UserId="496" Text="Big data case study or use case example" />
  <row Id="750" PostHistoryTypeId="3" PostId="307" RevisionGUID="26bdcc75-4c32-4978-a700-e2d04d4e7700" CreationDate="2014-06-11T06:07:45.767" UserId="496" Text="&lt;bigdata&gt;&lt;hadoop&gt;&lt;data-mining&gt;&lt;social-network-analysis&gt;&lt;usecase&gt;" />
  <row Id="751" PostHistoryTypeId="2" PostId="308" RevisionGUID="4ab8f87e-161d-4c90-92d8-ad182ae6f251" CreationDate="2014-06-11T06:49:04.070" UserId="434" Text="News outlets tend to use &quot;Big Data&quot; pretty loosely.  Vendors usually provide case studies surrounding their specific products.  There aren't a lot out there for open source implementations, but they do get mentioned.  For instance, Apache isn't going to spend a lot of time building a case study on hadoop, but vendors like Cloudera and Hortonworks probably will.&#xD;&#xA;&#xD;&#xA;Here's an [example case study from Cloudera][1] in the finance sector.&#xD;&#xA;&#xD;&#xA;Quoting the study:&#xD;&#xA;&#xD;&#xA;&gt; One major global financial services conglomerate uses Cloudera and Datameer to help&#xD;&#xA;identify rogue trading activity. Teams within the firm’s asset management group are&#xD;&#xA;performing ad hoc analysis on daily feeds of price, position, and order information. Having&#xD;&#xA;ad hoc analysis to all of the detailed data allows the group to detect anomalies across&#xD;&#xA;certain asset classes and identify suspicious behavior. Users previously relied solely on&#xD;&#xA;desktop spreadsheet tools. Now, with Datameer and Cloudera, users have a powerful&#xD;&#xA;platform that allows them to sift through more data more quickly and avert potential&#xD;&#xA;losses before they begin.&#xD;&#xA;&#xD;&#xA;.&#xD;&#xA;&#xD;&#xA;&gt; A leading retail bank is using Cloudera and Datameer to validate data accuracy and quality&#xD;&#xA;as required by the Dodd-Frank Act and other regulations. Integrating loan and branch data&#xD;&#xA;as well as wealth management data, the bank’s data quality initiative is responsible for&#xD;&#xA;ensuring that every record is accurate. The process includes subjectingthe data to over&#xD;&#xA;50 data sanity and quality checks. The results of those checks are trended over time to&#xD;&#xA;ensure that the tolerances for data corruption and data domains aren’t changing adversely&#xD;&#xA;and that the risk profiles being reported to investors and regulatory agencies are prudent&#xD;&#xA;and in compliance with regulatory requirements. The results are reported through a data&#xD;&#xA;quality dashboard to the Chief Risk Officer and Chief Financial Officer, who are ultimately&#xD;&#xA;responsible for ensuring the accuracy of regulatory compliance reporting as well as&#xD;&#xA;earnings forecasts to investors&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I didn't see any other finance related studies at Cloudera, but I didn't search very hard.  You can have a look at [their library][2] here.&#xD;&#xA;&#xD;&#xA;Also, Hortonworks has a [case study on Trading Strategies][3] where they saw a 20% decrease in the time it took to develop a strategy by leveraging K-means, Hadoop, and R.&#xD;&#xA;&#xD;&#xA;![Each color indicates a group of strategies with similar probability of a profit and loss][4]&#xD;&#xA;&#xD;&#xA;![how the trading system was improved by using Hadoop (Hortonworks Data Platform), and the k-means algorithm][5]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cloudera.com/content/cloudera/en/resources/library/casestudy/joint-success-story-major-retail-bank-case-study-datameer.html&#xD;&#xA;  [2]: http://www.cloudera.com/content/cloudera/en/resources/library.html?q=bank&#xD;&#xA;  [3]: http://hortonworks.com/blog/building-stock-trading-strategies-20-faster-with-hadoop/&#xD;&#xA;  [4]: http://i.stack.imgur.com/qX3Qy.png&#xD;&#xA;  [5]: http://i.stack.imgur.com/Sj8U9.png" />
  <row Id="752" PostHistoryTypeId="2" PostId="309" RevisionGUID="b7ac9fe9-8655-4e73-b609-bf0e679102aa" CreationDate="2014-06-11T06:51:19.143" UserId="638" Text="**tl;dr:** They markedly differ in many aspects and I can't think Redshift will replace Hadoop anytime soon.  &#xD;&#xA;&#xD;&#xA;-Function  &#xD;&#xA;You can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-Performance Profile  &#xD;&#xA;Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term `indexing` very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.&#xD;&#xA;&#xD;&#xA;-Cost Profile  &#xD;&#xA;Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  &#xD;&#xA;&#xD;&#xA;Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly because it was way easier to develop because of Redshift's maturity, it's ACID characteristics and quicker response time. It's a great tool to have, but it won't replace Hadoop." />
  <row Id="753" PostHistoryTypeId="5" PostId="308" RevisionGUID="ea24869f-e170-4b7c-bb8c-426e07b558c0" CreationDate="2014-06-11T06:54:42.593" UserId="434" Comment="added 347 characters in body" Text="News outlets tend to use &quot;Big Data&quot; pretty loosely.  Vendors usually provide case studies surrounding their specific products.  There aren't a lot out there for open source implementations, but they do get mentioned.  For instance, Apache isn't going to spend a lot of time building a case study on hadoop, but vendors like Cloudera and Hortonworks probably will.&#xD;&#xA;&#xD;&#xA;Here's an [example case study from Cloudera][1] in the finance sector.&#xD;&#xA;&#xD;&#xA;Quoting the study:&#xD;&#xA;&#xD;&#xA;&gt; One major global financial services conglomerate uses Cloudera and Datameer to help&#xD;&#xA;identify rogue trading activity. Teams within the firm’s asset management group are&#xD;&#xA;performing ad hoc analysis on daily feeds of price, position, and order information. Having&#xD;&#xA;ad hoc analysis to all of the detailed data allows the group to detect anomalies across&#xD;&#xA;certain asset classes and identify suspicious behavior. Users previously relied solely on&#xD;&#xA;desktop spreadsheet tools. Now, with Datameer and Cloudera, users have a powerful&#xD;&#xA;platform that allows them to sift through more data more quickly and avert potential&#xD;&#xA;losses before they begin.&#xD;&#xA;&#xD;&#xA;.&#xD;&#xA;&#xD;&#xA;&gt; A leading retail bank is using Cloudera and Datameer to validate data accuracy and quality&#xD;&#xA;as required by the Dodd-Frank Act and other regulations. Integrating loan and branch data&#xD;&#xA;as well as wealth management data, the bank’s data quality initiative is responsible for&#xD;&#xA;ensuring that every record is accurate. The process includes subjectingthe data to over&#xD;&#xA;50 data sanity and quality checks. The results of those checks are trended over time to&#xD;&#xA;ensure that the tolerances for data corruption and data domains aren’t changing adversely&#xD;&#xA;and that the risk profiles being reported to investors and regulatory agencies are prudent&#xD;&#xA;and in compliance with regulatory requirements. The results are reported through a data&#xD;&#xA;quality dashboard to the Chief Risk Officer and Chief Financial Officer, who are ultimately&#xD;&#xA;responsible for ensuring the accuracy of regulatory compliance reporting as well as&#xD;&#xA;earnings forecasts to investors&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I didn't see any other finance related studies at Cloudera, but I didn't search very hard.  You can have a look at [their library][2] here.&#xD;&#xA;&#xD;&#xA;Also, Hortonworks has a [case study on Trading Strategies][3] where they saw a 20% decrease in the time it took to develop a strategy by leveraging K-means, Hadoop, and R.&#xD;&#xA;&#xD;&#xA;![Each color indicates a group of strategies with similar probability of a profit and loss][4]&#xD;&#xA;&#xD;&#xA;![how the trading system was improved by using Hadoop (Hortonworks Data Platform), and the k-means algorithm][5]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;These don't answer all of your questions.  I'm pretty sure both of these studies covered most of them. I don't see anything about tool selection specifically.  I imagine sales reps had a lot to do with getting the overall product in the door, but the data scientists themselves leveraged the tools they were most comfortable with.  I don't have a lot of insight into that area in the big data space.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cloudera.com/content/cloudera/en/resources/library/casestudy/joint-success-story-major-retail-bank-case-study-datameer.html&#xD;&#xA;  [2]: http://www.cloudera.com/content/cloudera/en/resources/library.html?q=bank&#xD;&#xA;  [3]: http://hortonworks.com/blog/building-stock-trading-strategies-20-faster-with-hadoop/&#xD;&#xA;  [4]: http://i.stack.imgur.com/qX3Qy.png&#xD;&#xA;  [5]: http://i.stack.imgur.com/Sj8U9.png" />
  <row Id="754" PostHistoryTypeId="5" PostId="309" RevisionGUID="e87f0010-4f77-4b26-b340-e954c1b7cb5a" CreationDate="2014-06-11T06:56:32.073" UserId="638" Comment="added 320 characters in body" Text="**tl;dr:** They markedly differ in many aspects and I can't think Redshift will replace Hadoop anytime soon.  &#xD;&#xA;&#xD;&#xA;-Function  &#xD;&#xA;You can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-Performance Profile  &#xD;&#xA;Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term `indexing` very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.&#xD;&#xA;&#xD;&#xA;-Cost Profile  &#xD;&#xA;Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  &#xD;&#xA;&#xD;&#xA;Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly because it was way easier to develop because of Redshift's maturity, it's ACID characteristics and quicker response time. It's a great tool to have, but it won't replace Hadoop.  &#xD;&#xA;&#xD;&#xA;**EDIT**:  As for setup complexity, I'd even say it's easier with Hadoop if you use AWS's EMR. Their tools are so mature that it's ridiculously easy to have your Hadoop job running. Redshift isn't that mature yet and you have to be careful with how you load/delete data, which can add some complexity to your ETL. " />
  <row Id="755" PostHistoryTypeId="5" PostId="309" RevisionGUID="b47db0fb-d593-4ec8-b9b2-f109cc13b515" CreationDate="2014-06-11T07:50:57.727" UserId="638" Comment="deleted 13 characters in body" Text="**tl;dr:** They markedly differ in many aspects and I can't think Redshift will replace Hadoop.  &#xD;&#xA;&#xD;&#xA;-Function  &#xD;&#xA;You can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-Performance Profile  &#xD;&#xA;Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term `indexing` very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.&#xD;&#xA;&#xD;&#xA;-Cost Profile  &#xD;&#xA;Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  &#xD;&#xA;&#xD;&#xA;Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly because it was way easier to develop because of Redshift's maturity, it's ACID characteristics and quicker response time. It's a great tool to have, but it won't replace Hadoop.  &#xD;&#xA;&#xD;&#xA;**EDIT**:  As for setup complexity, I'd even say it's easier with Hadoop if you use AWS's EMR. Their tools are so mature that it's ridiculously easy to have your Hadoop job running. Redshift isn't that mature yet and you have to be careful with how you load/delete data, which can add some complexity to your ETL. " />
  <row Id="756" PostHistoryTypeId="5" PostId="309" RevisionGUID="9dfe6d7a-d375-41af-b920-b229b8b430ce" CreationDate="2014-06-11T08:55:46.877" UserId="638" Comment="added 48 characters in body" Text="**tl;dr:** They markedly differ in many aspects and I can't think Redshift will replace Hadoop.  &#xD;&#xA;&#xD;&#xA;-Function  &#xD;&#xA;You can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-Performance Profile  &#xD;&#xA;Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term `indexing` very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.&#xD;&#xA;&#xD;&#xA;-Cost Profile  &#xD;&#xA;Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  &#xD;&#xA;&#xD;&#xA;Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly because it was way easier to develop because of Redshift's Query Engine's maturity versus Hive's, it's ACID characteristics and quicker response time. It's a great tool to have, but it won't replace Hadoop.  &#xD;&#xA;&#xD;&#xA;**EDIT**:  As for setup complexity, I'd even say it's easier with Hadoop if you use AWS's EMR. Their tools are so mature that it's ridiculously easy to have your Hadoop job running. Redshift's operation tools aren't that mature yet and you have to be careful with how you load/delete data, which can add some complexity to your ETL. " />
  <row Id="757" PostHistoryTypeId="5" PostId="309" RevisionGUID="ccf9cd79-e725-4b16-b0b1-9a137f44d9cc" CreationDate="2014-06-11T09:07:33.570" UserId="638" Comment="added 48 characters in body" Text="**tl;dr:** They markedly differ in many aspects and I can't think Redshift will replace Hadoop.  &#xD;&#xA;&#xD;&#xA;-Function  &#xD;&#xA;You can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-Performance Profile  &#xD;&#xA;Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term `indexing` very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.&#xD;&#xA;&#xD;&#xA;-Cost Profile  &#xD;&#xA;Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  &#xD;&#xA;&#xD;&#xA;Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly for the ease of development. Redshift's Query Engine is based on PostgreSQL and is very mature, compared to Hive's. Its ACID characteristics make it easier to reason about it, and the quicker response time allows more testing to be done. It's a great tool to have, but it won't replace Hadoop.  &#xD;&#xA;&#xD;&#xA;**EDIT**:  As for setup complexity, I'd even say it's easier with Hadoop if you use AWS's EMR. Their tools are so mature that it's ridiculously easy to have your Hadoop job running. Tools and mechanisms surrounding Redshift's operation aren't that mature yet. For example, Redshift can't handle trickle loading and thus you have to come up with something that turns that into a batched load, which can add some complexity to your ETL. " />
  <row Id="758" PostHistoryTypeId="5" PostId="287" RevisionGUID="d145096a-0a12-4a57-a5a4-a5341767f585" CreationDate="2014-06-11T09:39:34.373" UserId="108" Comment="added 49 characters in body" Text="I do not know a standard answer to this, but I thought about it some times ago and I have some ideas to share. &#xD;&#xA;&#xD;&#xA;When you have one confusion matrix, you have more or less a picture of how you classification model confuse (mis-classify) classes. When you repeat classification tests you will end up having multiple confusion matrices. The question is how to get a meaningful aggregate confusion matrix. The answer depends on what is the meaning of meaningful (pun intended). I think there is not a single version of meaningful.&#xD;&#xA;&#xD;&#xA;One way is to follow the rough idea of multiple testing. In general, you test something multiple times in order to get more accurate results. As a general principle one can reason that averaging on the results of the multiple tests reduces the variance of the estimates, so as a consequence, it increases the precision of the estimates. You can proceed in this way, of course, by summing position by position and then dividing by the number of tests. You can go further and instead of estimating only a value for each cell of the confusion matrix, you can also compute some confidence intervals, t-values and so on. This is OK from my point of view. But it tell only one side of the story.&#xD;&#xA;&#xD;&#xA;The other side of the story which might be investigated is how stable are the results for the same instances. To exemplify that I will take an extreme example. Suppose you have a classification model for 3 classes. Suppose that these classes are in the same proportion. If your model is able to predict one class perfectly and the other 2 classes with random like performance, you will end up having 0.33 + 0.166 + 0.166 = 0.66 misclassification ratio. This might seem good, but even if you take a look on a single confusion matrix you will not know that your performance on the last 2 classes varies wildly. Multiple tests can help. But averaging the confusion matrices would reveal this? My belief is not. The averaging will give the same result more or less, and doing multiple tests will only decrease the variance of the estimation. However it says nothing about the wild instability of prediction.&#xD;&#xA;&#xD;&#xA;So another way to do compose the confusion matrices would better involve a prediction density for each instance. One can build this density by counting for each instance, the number of times it was predicted a given class. After normalization, you will have for each instance a prediction density rather a single prediction label. You can see that a single prediction label is similar with a degenerated density where you have probability of 1 for the predicted class and 0 for the other classes for each separate instance. Now having this densities one can build a confusion matrix by adding the probabilities from each instance and predicted class to the corresponding cell of the aggregated confusion matrix. &#xD;&#xA;&#xD;&#xA;One can argue that this would give similar results like the previous method. However I think that this might be the case sometimes, often when the model has low variance, the second method is less affected by how the samples from the tests are drawn, and thus more stable and closer to the reality. &#xD;&#xA;&#xD;&#xA;Also the second method might be altered in order to obtain a third method, where one can assign as prediction the label with highest density from the prediction of a given instance. &#xD;&#xA;&#xD;&#xA;I do not implemented those things but I plan to study further because I believe might worth spending some time. " />
  <row Id="759" PostHistoryTypeId="2" PostId="310" RevisionGUID="85cf4006-f529-4e37-a2c3-abc9e7f6b75e" CreationDate="2014-06-11T10:11:59.397" UserId="555" Text="I'm working on improving an existing supervised classifier, for classifying {protein} sequences as belonging to a specific class (Neuropeptide hormone precursors), or not.&#xD;&#xA;&#xD;&#xA;There are about 1,150 known &quot;positives&quot;, against a background of about 13 million protein sequences (&quot;Unknown/poorly annotated background&quot;), or about 100,000 reviewed, relevant proteins, annotated with a variety of properties (but very few annotated in an explicitly &quot;negative&quot; way). &#xD;&#xA; &#xD;&#xA;My previous implementation looked at this as a binary classification problem: &#xD;&#xA;Positive set = Proteins marked as Neuropeptides.&#xD;&#xA;Negative set: Random sampling of 1,300 samples (total) from among the remaining proteins of a roughly similar length-wise distribution. &#xD;&#xA;&#xD;&#xA;That worked, but I want to greatly improve the machines discriminatory abilities (Currently, it's at about 83-86% in terms of accuracy, AUC, F1, measured by CV, on multiple randomly sampled negative sets).&#xD;&#xA;&#xD;&#xA;My thoughts were to:&#xD;&#xA;1) Make this a multiclass problem, choosing 2-3 different classes of protein that will definetly be negatives, by their properties/functional class, along with (maybe) another randomly sampled set.&#xD;&#xA; (Priority here would be negative sets that are similar in their characteristics/features to the positive set, while still having defining characteristics) . &#xD;&#xA;2) One class learning - Would be nice, but as I understand it, it's meant just for anomaly detection, and has poorer performance than discriminatory approaches.&#xD;&#xA;&#xD;&#xA;*) I've heard of P-U learning, which sounds neat, but I'm a programming N00b, and I don't know of any existing implementations for it. (In Python/sci-kit learn).&#xD;&#xA;&#xD;&#xA;So, does approach 1 make sense in a theoretical POV? Is there a best way to make multiple negative sets? (I could also simply use a massive [50K] pick of the &quot;negative&quot; proteins, but they're all very very different from each other, so I don't know how well the classifier would handle them as one big , unbalanced mix).&#xD;&#xA;Thanks!&#xD;&#xA;&#xD;&#xA;" />
  <row Id="760" PostHistoryTypeId="1" PostId="310" RevisionGUID="85cf4006-f529-4e37-a2c3-abc9e7f6b75e" CreationDate="2014-06-11T10:11:59.397" UserId="555" Text="One-Class discriminatory classification with imbalanced, heterogenous Negative background?" />
  <row Id="761" PostHistoryTypeId="3" PostId="310" RevisionGUID="85cf4006-f529-4e37-a2c3-abc9e7f6b75e" CreationDate="2014-06-11T10:11:59.397" UserId="555" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;python&gt;&lt;classification&gt;" />
  <row Id="762" PostHistoryTypeId="2" PostId="311" RevisionGUID="7ff7f460-4ee9-45ab-a10b-8eb238358666" CreationDate="2014-06-11T11:24:19.963" UserId="434" Text="The way I would attack the problem, in general, is to leverage statistical analysis like Principal Component Analysis or Ordinary Least Squares to help determine what attributes within these protein sequences are best suited to classify proteins as Neuropeptide hormone precursors.  &#xD;&#xA;&#xD;&#xA;In order to do that, you'll have to convert the protein sequences into numeric data, but I believe some work has already been done in that regard using formulas leveraged in Amino Acid PCA.&#xD;&#xA;&#xD;&#xA;See these two links:&#xD;&#xA;http://www.ncbi.nlm.nih.gov/pubmed/24496727&#xD;&#xA;&#xD;&#xA;http://www.ncbi.nlm.nih.gov/pubmed/16615809&#xD;&#xA;&#xD;&#xA;Once that work has been done, I would attempt to classify using the entire dataset and a reinforcement learning algorithm, like [Naive Bayes][1] while slimming down the data into that which PCA has identified as important.  &#xD;&#xA;&#xD;&#xA;The reason I would try to use Bayes is because it has proven to be one of the best methods for determining spam vs. regular email, which has a similarly skewed dataset.&#xD;&#xA;&#xD;&#xA;Having said all of that...&#xD;&#xA;&#xD;&#xA;Slimming down the number or type of negative classifications might skew your results a few points one way or the other, but I don't think you'll see the long term effectiveness change substantially until you do the leg work of determining how to best remove the fuzziness from your training data.  That will either require a field expert or statistical analysis.&#xD;&#xA;&#xD;&#xA;I could be completely off base.  I am interested in seeing some other answers, but that is my 2 cents.&#xD;&#xA;&#xD;&#xA;  [1]: http://findingscience.com/ankusa/hbase/hadoop/ruby/2010/12/02/naive-bayes-classification-in-ruby-using-hadoop-and-hbase.html" />
  <row Id="763" PostHistoryTypeId="2" PostId="312" RevisionGUID="d0ef6d9b-404d-4298-bb93-1457a791076c" CreationDate="2014-06-11T11:32:13.433" UserId="655" Text="Therriault, really happy to hear you are using Vertica! Full disclosure, I am the chief data scientist there :) . The workflow you describe is exactly what I encounter quite frequently and I am a true believer in preprocessing those very large datasets in the database prior to any pyODBC and pandas work. I'd suggest creating a view or table via a file based query just to ensure reproducible work. Good Luck" />
  <row Id="764" PostHistoryTypeId="2" PostId="313" RevisionGUID="14e0c23d-ab11-4957-a0cb-dd110bb9c9c7" CreationDate="2014-06-11T13:28:35.980" UserId="663" Text="What are the good books about the science and mathematics behind data science? It feels like so many &quot;data science&quot; books are programming tutorials and don't touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.&#xD;&#xA;&#xD;&#xA;If I am ready to burn $1000 on books (so around 10 books... sigh), what should I buy?" />
  <row Id="765" PostHistoryTypeId="1" PostId="313" RevisionGUID="14e0c23d-ab11-4957-a0cb-dd110bb9c9c7" CreationDate="2014-06-11T13:28:35.980" UserId="663" Text="Books about the &quot;Science&quot; in Data Science?" />
  <row Id="766" PostHistoryTypeId="3" PostId="313" RevisionGUID="14e0c23d-ab11-4957-a0cb-dd110bb9c9c7" CreationDate="2014-06-11T13:28:35.980" UserId="663" Text="&lt;statistics&gt;" />
  <row Id="767" PostHistoryTypeId="5" PostId="313" RevisionGUID="33532ab6-7ffc-43c4-bc5a-e0e84075292c" CreationDate="2014-06-11T13:35:33.213" UserId="663" Comment="added 249 characters in body" Text="What are the good books about the science and mathematics behind data science? It feels like so many &quot;data science&quot; books are programming tutorials and don't touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.&#xD;&#xA;&#xD;&#xA;If I am ready to burn $1000 on books (so around 10 books... sigh), what should I buy?&#xD;&#xA;&#xD;&#xA;Examples: Agresti's [Categorical Data Analysis][1], [Linear Mixed Models for Longitudinal Data][2], etc... etc...&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.amazon.com/Categorical-Data-Analysis-Alan-Agresti/dp/0470463635&#xD;&#xA;  [2]: http://www.amazon.com/dp/1441902996/" />
  <row Id="768" PostHistoryTypeId="2" PostId="314" RevisionGUID="25626139-4af7-4b07-bb1b-e65aaa71f394" CreationDate="2014-06-11T13:49:08.970" UserId="178" Text="If I could only recomend one to you, it would be: [The Elements of Statistical Learning and Prediction](http://www.amazon.com/The-Elements-Statistical-Learning-Prediction/dp/0387848576) by Hastie, Tibshirani and Friedman.  It provides the math/statistics behind a lot of commonly used techniques in data science.&#xD;&#xA;&#xD;&#xA;For Bayesian Techniques, [Bayesian Data Analysis](http://www.amazon.com/Bayesian-Analysis-Edition-Chapman-Statistical/dp/1439840954/ref=pd_sim_b_6?ie=UTF8&amp;refRID=09QA1N2WPFHA4CTXJ8G4) by Gelman, Carlin, Stern, Dunson, Vehtari and Rubin is excellent.&#xD;&#xA;&#xD;&#xA;[Statistical Inference](http://www.amazon.com/Statistical-Inference-George-Casella/dp/0534243126) by Casella and Berger is a good graduate-level textbook on the theoretical foundation of statistics.  This book does require a pretty high level of comfort with math (probability theory is based on measure theory, which is not trivial to understand).&#xD;&#xA;&#xD;&#xA;With respect to data generating processes, I don't have a recommendation for a book.  What I can say is that a good understanding of the assumptions of the techniques used and ensuring that the data was collected or generated in a manner that does not violate those assumptions goes a long way towards a good analysis." />
  <row Id="769" PostHistoryTypeId="5" PostId="313" RevisionGUID="6f0aa961-96e5-4b92-8132-695afdbd8cc7" CreationDate="2014-06-11T14:33:57.470" UserId="663" Comment="deleted 5 characters in body" Text="What are the books about the science and mathematics behind data science? It feels like so many &quot;data science&quot; books are programming tutorials and don't touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.&#xD;&#xA;&#xD;&#xA;If I am ready to burn $1000 on books (so around 10 books... sigh), what could I buy?&#xD;&#xA;&#xD;&#xA;Examples: Agresti's [Categorical Data Analysis][1], [Linear Mixed Models for Longitudinal Data][2], etc... etc...&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.amazon.com/Categorical-Data-Analysis-Alan-Agresti/dp/0470463635&#xD;&#xA;  [2]: http://www.amazon.com/dp/1441902996/" />
  <row Id="770" PostHistoryTypeId="5" PostId="305" RevisionGUID="d38f2adb-1576-4cbb-bc25-15ba6a1ef01d" CreationDate="2014-06-11T15:02:46.890" UserId="434" Comment="minor typos" Text="There is plenty of hype surrounding Hadoop and its eco-system.  However, in practice, where many data sets are in the terabyte range, is it not more reasonable to use [Amazon RedShift][1] for querying large data sets, rather than spending time and effort building a Hadoop cluster? &#xD;&#xA;&#xD;&#xA;Also, how does Amazon Redshift compare with Hadoop with respect to setup complexity, cost, and performance?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://aws.amazon.com/redshift/" />
  <row Id="771" PostHistoryTypeId="24" PostId="305" RevisionGUID="d38f2adb-1576-4cbb-bc25-15ba6a1ef01d" CreationDate="2014-06-11T15:02:46.890" Comment="Proposed by 434 approved by 534 edit id of 53" />
  <row Id="773" PostHistoryTypeId="2" PostId="316" RevisionGUID="99138a94-bc13-450f-ae32-32e9c83c18f3" CreationDate="2014-06-11T16:25:34.747" UserId="514" Text="As Konstantin has pointed, R performs all its computation in the system's memory i.e. RAM. Hence, RAM capacity is a very important constraint for computation intensive operations in R. Overcoming this constraint, data is being stored these days in HDFS systems, where data isn't loaded onto memory and program is run instead, program goes to the data and performs the operations, thus overcoming the memory constraints.  RHadoop (https://github.com/RevolutionAnalytics/RHadoop/wiki) is the connector you are looking for.&#xD;&#xA;&#xD;&#xA;Coming to the impact on algorithms which are computation intensive, Random Forests/Decision Trees/Ensemble methods on a considerable amount of data (minimum 50,000 observations in my experience) take up a lot of memory and are considerably slow. To speed up the process, parallelization is the way to go and parallelization is inherently available in Hadoop! That's where, Hadoop is really efficient.&#xD;&#xA;&#xD;&#xA;So, if you are going for ensemble methods which are compute intensive and are slow, you would want to try out on the HDFS system which gives a considerable performance improvement." />
  <row Id="774" PostHistoryTypeId="5" PostId="279" RevisionGUID="2bc9baae-200b-4e18-af31-c8798b44bb57" CreationDate="2014-06-11T16:30:06.930" UserId="514" Comment="Added new links" Text="There is also another resource provided by The Guardian, the British Daily on their website. The datasets published by the Guardian Datablog are all hosted. Datasets related to Football Premier League Clubs' accounts, Inflation and GDP details of UK, Grammy awards data etc.&#xD;&#xA;The datasets are available at &#xD;&#xA;&#xD;&#xA; - http://www.theguardian.com/news/datablog/interactive/2013/jan/14/all-our-datasets-index&#xD;&#xA;&#xD;&#xA;Some more resources. Some of the datasets are in R format or R commads exist for directly importing data to R.&#xD;&#xA;&#xD;&#xA; - http://www.inside-r.org/howto/finding-data-internet" />
  <row Id="775" PostHistoryTypeId="2" PostId="317" RevisionGUID="24204f27-ae56-4560-939c-214a0bc162a0" CreationDate="2014-06-11T16:56:59.660" UserId="680" Text="If you already know R Studio, then the caret package is a good place to start. Here are some tutorials: &#xD;&#xA;&#xD;&#xA; 1. https://class.coursera.org/predmachlearn-002&#xD;&#xA; 2. http://caret.r-forge.r-project.org/index.html&#xD;&#xA;&#xD;&#xA;With R and caret you can easily load and splice data sets, feature reduction, principal component analysis, and train and predict using various algorithms." />
  <row Id="776" PostHistoryTypeId="2" PostId="318" RevisionGUID="66d361ac-7784-4649-8b04-353012bed1d8" CreationDate="2014-06-11T17:24:53.610" UserId="471" Text="If you can reproduce the 6x3 grid of graphs from the banner of the http://scikit-learn.org/ page then you will have learnt some ML and some Python. You didn't mention a language. Python is easy enough to learn very quickly, and scikit-learn has a wide range of algorithms implemented.&#xD;&#xA;&#xD;&#xA;Then try on your own data!&#xD;&#xA;" />
  <row Id="777" PostHistoryTypeId="6" PostId="59" RevisionGUID="3825b584-67cc-4d63-b107-6142f5d2a0e2" CreationDate="2014-06-11T17:35:51.877" UserId="158" Comment="edited tags" Text="&lt;hadoop&gt;&lt;r&gt;&lt;recommendation&gt;" />
  <row Id="778" PostHistoryTypeId="2" PostId="319" RevisionGUID="b6aa265c-7e15-48c3-b908-14cfd7a0a4a1" CreationDate="2014-06-11T18:22:36.267" UserId="691" Text="I've built an artificial neural network in python using the scipy.optimize.minimize (Conjugate gradient) optimization function.&#xD;&#xA;&#xD;&#xA;I've implemented gradient checking, double checked everything etc and I'm pretty certain it's working correctly.&#xD;&#xA;&#xD;&#xA;I've run it a few times and it reaches 'Optimization terminated successfully' however when I increase the number of hidden layers, the cost of the hypothesis increases (everything else is kept the same) after it has successfully terminated.&#xD;&#xA;&#xD;&#xA;Intuitively it feels as if the cost should decrease when the number of hidden layers is increased, as it is able to generate a more complex hypothesis which can fit the data better, however this appears not to be the case.&#xD;&#xA;&#xD;&#xA;I'd be interested to understand what's going on here, or if I've implemented neural net incorrectly?&#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="779" PostHistoryTypeId="1" PostId="319" RevisionGUID="b6aa265c-7e15-48c3-b908-14cfd7a0a4a1" CreationDate="2014-06-11T18:22:36.267" UserId="691" Text="Number of layers vs cost in a Neural Network" />
  <row Id="780" PostHistoryTypeId="3" PostId="319" RevisionGUID="b6aa265c-7e15-48c3-b908-14cfd7a0a4a1" CreationDate="2014-06-11T18:22:36.267" UserId="691" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;neuralnetwork&gt;" />
  <row Id="782" PostHistoryTypeId="5" PostId="319" RevisionGUID="0cd3d509-d654-478f-a056-c66bc165e565" CreationDate="2014-06-11T20:06:18.687" UserId="84" Comment="Improving formatting." Text="I've built an artificial neural network in python using the scipy.optimize.minimize (Conjugate gradient) optimization function.&#xD;&#xA;&#xD;&#xA;I've implemented gradient checking, double checked everything etc and I'm pretty certain it's working correctly.&#xD;&#xA;&#xD;&#xA;I've run it a few times and it reaches 'Optimization terminated successfully' however when I increase the number of hidden layers, the cost of the hypothesis increases (everything else is kept the same) after it has successfully terminated.&#xD;&#xA;&#xD;&#xA;Intuitively it feels as if the cost should decrease when the number of hidden layers is increased, as it is able to generate a more complex hypothesis which can fit the data better, however this appears not to be the case.&#xD;&#xA;&#xD;&#xA;I'd be interested to understand what's going on here, or if I've implemented neural net incorrectly?" />
  <row Id="783" PostHistoryTypeId="24" PostId="319" RevisionGUID="0cd3d509-d654-478f-a056-c66bc165e565" CreationDate="2014-06-11T20:06:18.687" Comment="Proposed by 84 approved by 691 edit id of 54" />
  <row Id="784" PostHistoryTypeId="2" PostId="320" RevisionGUID="b49f19a1-8b74-4ab6-9676-fb6aff9daf34" CreationDate="2014-06-11T20:34:51.873" UserId="574" Text="There are so many ways to go wrong with a neural net that it's going to be difficult to debug. Also, to address your intuition, each additional hidden layer makes learning much harder. With that said, here are some possibilities:&#xD;&#xA;&#xD;&#xA; 1. You have added weight decay. Adding more layers adds more weights which increases your regularization cost.&#xD;&#xA; 2. The problem is simple enough that a model with a single hidden layer is sufficient. Adding more hidden layers makes it harder for the network to learn (harder optimization problem).&#xD;&#xA; 3. The optimization method is not doing a great job (I prefer climin to scipy.optimize).&#xD;&#xA; 4. You are using the sigmoid/tanh activation function. The sigmoid function causes the vanishing gradient problem which makes learning hard with more layers. Try using the ReLu function.&#xD;&#xA;&#xD;&#xA;Training neural nets takes a lot of practice, luck, and patience. Good luck.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="789" PostHistoryTypeId="2" PostId="323" RevisionGUID="a73013b1-9653-4b66-a269-cc48dfed4056" CreationDate="2014-06-11T23:52:37.823" UserId="418" Text="The setup is simple: binary classification using a simple decision tree, each node of the tree has a single threshold applied on a single feature. In general, building a ROC curve requires moving a decision threshold over different values and computing the effect of that change on the true positive rate and the false positives rate of predictions. What's that decision threshold in the case of a simple fixed decision tree? &#xD;&#xA;" />
  <row Id="790" PostHistoryTypeId="1" PostId="323" RevisionGUID="a73013b1-9653-4b66-a269-cc48dfed4056" CreationDate="2014-06-11T23:52:37.823" UserId="418" Text="How can we calculate AUC for a simple decision tree?" />
  <row Id="791" PostHistoryTypeId="3" PostId="323" RevisionGUID="a73013b1-9653-4b66-a269-cc48dfed4056" CreationDate="2014-06-11T23:52:37.823" UserId="418" Text="&lt;machine-learning&gt;" />
  <row Id="793" PostHistoryTypeId="2" PostId="324" RevisionGUID="803f2281-2434-40ee-8154-9a5d02a111b7" CreationDate="2014-06-12T03:11:00.033" UserId="714" Text="I want to extract news about a company from online news by using RODBC package in R. And I want to use the extracted data for sentiment analysis. I want to accomplish this in such a way that the positive news is assigned a value of +1, the negative news is assigned a value of -1 and the neutral news is assigned a value of 0." />
  <row Id="794" PostHistoryTypeId="1" PostId="324" RevisionGUID="803f2281-2434-40ee-8154-9a5d02a111b7" CreationDate="2014-06-12T03:11:00.033" UserId="714" Text="How can I extract news about a particular company from various websites using RODBC package in R? And perform sentiment analysis on the data?" />
  <row Id="795" PostHistoryTypeId="3" PostId="324" RevisionGUID="803f2281-2434-40ee-8154-9a5d02a111b7" CreationDate="2014-06-12T03:11:00.033" UserId="714" Text="&lt;r&gt;&lt;text-mining&gt;" />
  <row Id="796" PostHistoryTypeId="2" PostId="325" RevisionGUID="80cefc65-2dd8-46b9-8794-196d30118dee" CreationDate="2014-06-12T05:17:46.453" UserId="434" Text="This isn't a question with a simple answer, so all I can really do is point you in the right direction.&#xD;&#xA;&#xD;&#xA;The RODBC package isn't meant to extract data online, it's meant to pull data from a database.  If you will be leveraging that package, it will be after you pull data down from the web.&#xD;&#xA;&#xD;&#xA;Jeffrey Bean put together a [slideshow tutorial][1] for doing sentiment analysis with Twitter data a few years back.  He used the Twitter stream as well as some data pulled in from web scraping.  It's a good starting point.&#xD;&#xA;&#xD;&#xA;There's also this site that discusses a few different approaches to this problem in detail, including Bean's, the sentiment package, and ViralHeat (which is a commercial sentiment analysis service who's data you can pull into R).  Sentiment has since been removed ([archived versions here][2]), but the [qdap package][3] is available and is designed for use in transcript analysis.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://jeffreybreen.wordpress.com/2011/07/04/twitter-text-mining-r-slides/&#xD;&#xA;  [2]: http://cran.us.r-project.org/src/contrib/Archive/sentiment/&#xD;&#xA;  [3]: http://cran.us.r-project.org/web/packages/qdap/index.html" />
  <row Id="797" PostHistoryTypeId="2" PostId="326" RevisionGUID="4ba2ec40-91ea-44ff-be0c-64e818fee364" CreationDate="2014-06-12T06:04:48.243" UserId="721" Text="I maybe asking a very basic Q. I am, just starting to develop ML application for academic purposes, but i am also interested in knowing what people are using both academia and Industries. A lot of places i saw people using python. what do you recommand? i am currently using R and train myself in it.&#xD;&#xA;&#xD;&#xA;Thanks , Ido" />
  <row Id="798" PostHistoryTypeId="1" PostId="326" RevisionGUID="4ba2ec40-91ea-44ff-be0c-64e818fee364" CreationDate="2014-06-12T06:04:48.243" UserId="721" Text="Python Vs R Machine learning" />
  <row Id="799" PostHistoryTypeId="3" PostId="326" RevisionGUID="4ba2ec40-91ea-44ff-be0c-64e818fee364" CreationDate="2014-06-12T06:04:48.243" UserId="721" Text="&lt;r&gt;&lt;python&gt;" />
  <row Id="800" PostHistoryTypeId="2" PostId="327" RevisionGUID="954384bc-c69c-46d9-a08b-834a0268871d" CreationDate="2014-06-12T07:05:05.653" UserId="115" Text="There is nothing like &quot;python is better&quot; or &quot;R is much better than x&quot;. &#xD;&#xA; &#xD;&#xA;The only fact I know is that in the industry allots of people stick to python because that is what they learned at the university. The python community is really active and have a few great frameworks for ML and data mining etc. &#xD;&#xA; &#xD;&#xA;But to be honest, if you get a good c programmer he can do the same as people do in python or r, if you got a good java programmer he can also do (near to) everything in java. &#xD;&#xA; &#xD;&#xA;So just stick with the language you are comfortable with." />
  <row Id="801" PostHistoryTypeId="2" PostId="328" RevisionGUID="04f68a22-0c75-4a8e-bfd7-04d898c2974f" CreationDate="2014-06-12T08:30:49.757" UserId="456" Text="There is no &quot;better&quot; language. I have tried both of them and I am comfortable with Python so I work with Python only. Though I am still learning stuff, but I haven't encounter any roadblock with Python till now. The good thing about Python is community is too good and you can get a lot of help on the Internet easily. Other than that, I would say go with the language you like not the one people recommend. " />
  <row Id="803" PostHistoryTypeId="2" PostId="330" RevisionGUID="e07216ec-c5d9-4c92-a833-f33010974ef2" CreationDate="2014-06-12T09:57:39.890" UserId="729" Text="You can also checkout the seaborn package for statistical charts." />
  <row Id="804" PostHistoryTypeId="2" PostId="331" RevisionGUID="efcbd099-d14c-4417-9171-dbf19497173e" CreationDate="2014-06-12T10:09:23.887" UserId="108" Text="Some additional thoughts.&#xD;&#xA;&#xD;&#xA;The programming language 'per se' is only a tool. All languages were designed to make some type of constructs more easy to build than others. And the knowledge and mastery of a programming language is more important and effective than the features of that language compared to others.   &#xD;&#xA;&#xD;&#xA;As far as I can see there are two dimensions of this question. The first dimension is the ability to explore, build proof of concepts or models at a fast pace, eventually having at hand enough tools to study what is going on (like statistical tests, graphics, measurement tools, etc). This kind of activity is usually preferred by researchers and data scientists (I always wonder what that means, but I use this term for its loose definition). They tend to rely on well-known and verified instruments, which can be used for proofs or arguments.&#xD;&#xA;&#xD;&#xA;The second dimension is the ability to extend, change, improve or even create tools, algorithms or models. In order to achieve that you need a proper programming language. Roughly all of them are the same. If you work for a company, than you depend a lot on the company's infrastructure, internal culture and your choices diminish significantly. Also, when you want to implement an algorithm for production use, you have to trust the implementation. And implementing in another language which you do not master will not help you much.&#xD;&#xA;&#xD;&#xA;I tend to favor for the first type of activity the R ecosystem. You have a great community, a huge set of tools, proofs that these tools works as expected. Also, you can consider Python, Octave (to name a few), which are reliable candidates.&#xD;&#xA;&#xD;&#xA;For the second task, you have to think before at what you really want. If you want robust production ready tools, than C/C++, Java, C# are great candidates. I consider Python as a second citizen in this category, together with Scala and friends. I do not want to flame a war, it's my opinion only. But after more than 17 years as a developer, I tend to prefer a strict contract and my knowledge, than the freedom to do whatever you might think of (like it happens with a lot of dynamic languages).&#xD;&#xA;&#xD;&#xA;Personally, I want to learn as much as possible. I decided that I have to choose the hard way, which means to implement myself everything from scratch. I use R as a model and inspiration. It has great treasures in libraries and a lot of experience distilled. However, as a programming language R, for me at least is a nightmare. So I decided to use Java, and use no additional library. That is only because of my experience, and nothing else.&#xD;&#xA;&#xD;&#xA;If you have time, the best thing you can do is to spend some time with all these things. In this way you will earn for yourself the best answer possible, fitted for you. Dijkstra said once that the tools influence the way you think, so it is advisable to know your tools before letting them to model how you think. You can read more about that in his famous paper called [The Humble Programmer][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html" />
  <row Id="805" PostHistoryTypeId="2" PostId="332" RevisionGUID="e8d0e839-9363-400c-a214-2cc47752ef79" CreationDate="2014-06-12T10:27:14.480" UserId="108" Text="In order to build the ROC curve and AUC (Area under curve) you have to have a binary classifier which provides you at classification time, the distribution (or at least a score), not the classification label. To give you an example, suppose you have a binary classification model, with classes c1 and c2. For a given instance, your classifier would have to return a score for c1 and another for c2. If this score is a probability-like (preferrable), than something like p(c1), p(c2) would work. In plain English is translated like &quot;I (the model) classify this instance as c1 with probability p(c1), and as c2 with probability  p(c2)=1-p(c1)&quot;. &#xD;&#xA;&#xD;&#xA;This applies for all type of classifiers, not only for decision trees. Having these scores you can than compute ROC or AUC by varying a threshold on p(c1) values, from the smallest to the greatest value.&#xD;&#xA;&#xD;&#xA;Now, if you have an implementation of a decision tree and you want to change that implementation to return scores instead of labels you have to compute those values. The most used way for decision trees is to use the proportion of classes from the leaf nodes. So, for example you have built a decision tree and when you predict the class for an instance you arrive at a leaf node which have (stored from the learning phase) 10 instances of class c1 and 15 instances of class c2, you can use the ratios as the scores. So, in our example, you would return p(c1) = 10 / (10+15) = 0.4 probability of class c1 and p(c2) = 15/(10+15)=0.6 probability of being class c2.&#xD;&#xA;&#xD;&#xA;For further reading on the ROC curves, the best and inspiring source of information I found to be the Tom Fawcett paper called [An Introduction to ROC Analysis][1], it's solid gold on this topic.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.google.ie/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCsQFjAA&amp;url=https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf&amp;ei=DYCZU96QGPOV7AaAx4CgBA&amp;usg=AFQjCNECLoecin6ieT-0ymLQ--FoMjkMZw&amp;sig2=3bGE2y1N3I2Re_YWn0EDFA" />
  <row Id="807" PostHistoryTypeId="2" PostId="334" RevisionGUID="681b6f02-4fb9-442f-9e11-7591a53ecda9" CreationDate="2014-06-12T10:52:03.410" UserId="434" Text="I've now seen two data science certification programs - the [John Hopkins one available at Coursera][1] and the [Cloudera one][2].&#xD;&#xA;&#xD;&#xA;I'm sure there are others out there.&#xD;&#xA;&#xD;&#xA;The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics:&#xD;&#xA;&#xD;&#xA;* R Programming&#xD;&#xA;* cleaning and obtaining data&#xD;&#xA;* Data Analysis&#xD;&#xA;* Reproducible Research&#xD;&#xA;* Statistical Inference&#xD;&#xA;* Regression Models&#xD;&#xA;* Machine Learning&#xD;&#xA;* Developing Data Products&#xD;&#xA;* And what looks to be a Project based completion task similar to Cloudera's Data Science Challenge&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The Cloudera program looks thin on the surface, but looks to answer the two important questions - &quot;Do you know the tools&quot;, &quot;Can you apply the tools in the real world&quot;.  Their program consists of:&#xD;&#xA;&#xD;&#xA;* Introduction to Data Science&#xD;&#xA;* Data Science Essentials Exam&#xD;&#xA;* Data Science Challenge (a real world data science project scenario)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I am not looking for a recommendation on a program or a quality comparison.&#xD;&#xA;&#xD;&#xA;I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage&#xD;&#xA;  [2]: http://cloudera.com/content/cloudera/en/training/certification/ccp-ds.html" />
  <row Id="808" PostHistoryTypeId="1" PostId="334" RevisionGUID="681b6f02-4fb9-442f-9e11-7591a53ecda9" CreationDate="2014-06-12T10:52:03.410" UserId="434" Text="What do you think of Data Science certifications?" />
  <row Id="809" PostHistoryTypeId="3" PostId="334" RevisionGUID="681b6f02-4fb9-442f-9e11-7591a53ecda9" CreationDate="2014-06-12T10:52:03.410" UserId="434" Text="&lt;education&gt;" />
  <row Id="810" PostHistoryTypeId="2" PostId="335" RevisionGUID="ef899587-672d-4263-830d-80b60022a69f" CreationDate="2014-06-12T11:11:35.600" UserId="478" Text="The certification programs you mentioned are really entry level courses. Personally, I think these certificates show only person's persistence and they can be only useful to those who is applying for internships, not the real data science jobs." />
  <row Id="811" PostHistoryTypeId="5" PostId="331" RevisionGUID="14535633-b53d-47f5-afd0-5ecb4e4c0e5e" CreationDate="2014-06-12T11:16:51.183" UserId="733" Comment="corrected spelling " Text="Some additional thoughts.&#xD;&#xA;&#xD;&#xA;The programming language 'per se' is only a tool. All languages were designed to make some type of constructs more easy to build than others. And the knowledge and mastery of a programming language is more important and effective than the features of that language compared to others.   &#xD;&#xA;&#xD;&#xA;As far as I can see there are two dimensions of this question. The first dimension is the ability to explore, build proof of concepts or models at a fast pace, eventually having at hand enough tools to study what is going on (like statistical tests, graphics, measurement tools, etc). This kind of activity is usually preferred by researchers and data scientists (I always wonder what that means, but I use this term for its loose definition). They tend to rely on well-known and verified instruments, which can be used for proofs or arguments.&#xD;&#xA;&#xD;&#xA;The second dimension is the ability to extend, change, improve or even create tools, algorithms or models. In order to achieve that you need a proper programming language. Roughly all of them are the same. If you work for a company, than you depend a lot on the company's infrastructure, internal culture and your choices diminish significantly. Also, when you want to implement an algorithm for production use, you have to trust the implementation. And implementing in another language which you do not master will not help you much.&#xD;&#xA;&#xD;&#xA;I tend to favor for the first type of activity the R ecosystem. You have a great community, a huge set of tools, proofs that these tools works as expected. Also, you can consider Python, Octave (to name a few), which are reliable candidates.&#xD;&#xA;&#xD;&#xA;For the second task, you have to think before at what you really want. If you want robust production ready tools, then C/C++, Java, C# are great candidates. I consider Python as a second citizen in this category, together with Scala and friends. I do not want to start a flame war, it's my opinion only. But after more than 17 years as a developer, I tend to prefer a strict contract and my knowledge, than the freedom to do whatever you might think of (like it happens with a lot of dynamic languages).&#xD;&#xA;&#xD;&#xA;Personally, I want to learn as much as possible. I decided that I have to choose the hard way, which means to implement myself everything from scratch. I use R as a model and inspiration. It has great treasures in libraries and a lot of experience distilled. However, as a programming language R, for me at least is a nightmare. So I decided to use Java, and use no additional library. That is only because of my experience, and nothing else.&#xD;&#xA;&#xD;&#xA;If you have time, the best thing you can do is to spend some time with all these things. In this way you will earn for yourself the best answer possible, fitted for you. Dijkstra said once that the tools influence the way you think, so it is advisable to know your tools before letting them to model how you think. You can read more about that in his famous paper called [The Humble Programmer][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html" />
  <row Id="812" PostHistoryTypeId="24" PostId="331" RevisionGUID="14535633-b53d-47f5-afd0-5ecb4e4c0e5e" CreationDate="2014-06-12T11:16:51.183" Comment="Proposed by 733 approved by 108 edit id of 55" />
  <row Id="813" PostHistoryTypeId="2" PostId="336" RevisionGUID="414f4c6e-75ec-4dd5-b0a8-e5c296e96eb1" CreationDate="2014-06-12T11:30:20.943" UserId="733" Text="There isn't a silver bullet language that can be used to solve each and every data related problem. The language choice depends on the context of the problem, size of data and if you are working at a workplace you have to stick to what they use.&#xD;&#xA;&#xD;&#xA;Personally I use R more often than Python due to its visualization libraries and interactive style. But if I need more performance or structured code I definitely use Python since it has some of the best libraries as SciKit-Learn, numpy, scipy etc. I use both R and Python in my projects interchangeably. &#xD;&#xA;&#xD;&#xA;So if you are starting on data science work I suggest you to learn both and it's not difficult since Python also provides a similar interface to R with [Pandas][1]. &#xD;&#xA;&#xD;&#xA;If you have to deal with much larger datasets, you can't escape eco-systems built with Java(Hadoop, Pig, Hbase etc).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pandas.pydata.org/" />
  <row Id="814" PostHistoryTypeId="2" PostId="337" RevisionGUID="b454d51c-11e7-4935-9ba8-559614e26c8d" CreationDate="2014-06-12T11:54:59.140" UserId="735" Text="I would add to what others have said till now. There is no single answer that one language is better than other.&#xD;&#xA;&#xD;&#xA;Having said that, R has a better community for data exploration and learning. It has extensive visualization capabilities. Python, on the other hand, has become better at data handling since introduction of pandas. Learning and development time is very less in Python, as compared to R (R being a low level language).&#xD;&#xA;&#xD;&#xA;I think it ultimately boils down to the eco-system you are in and personal preferences. For more details, you can look at this comparison [here][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.analyticsvidhya.com/blog/2014/03/sas-vs-vs-python-tool-learn/" />
  <row Id="815" PostHistoryTypeId="2" PostId="338" RevisionGUID="163da822-0c73-4694-9167-11ff57f6f41f" CreationDate="2014-06-12T12:13:26.940" UserId="737" Text="I did the first 2 courses and I'm planning to do all the others too.  If you don't know R, it's a really good program. There are assignments and quizzes every week. Many people find some courses very difficult. You are going to have hard time if you don't have any programming experience (even if they say it's not required). &#xD;&#xA;&#xD;&#xA;Just remember.. it's not because you can drive a car that you are a F1 pilot ;) " />
  <row Id="817" PostHistoryTypeId="2" PostId="339" RevisionGUID="dbd0ef4c-dd67-435e-8edb-22938d7529a0" CreationDate="2014-06-12T12:59:00.663" UserId="514" Text="Some real important differences to consider when you are choosing R or Python over one another:&#xD;&#xA;&#xD;&#xA; - **Machine Learning** has 2 phases. Model Building and Prediction phase. Typically, model building is performed as a batch process and **predictions are done realtime**. The model building process is a compute intensive process while the prediction happens in a jiffy. Therefore, performance of an algorithm in Python or R doesn't really affect the turn-around time of the user. Python 1, R 1.&#xD;&#xA; - **Production:** The real difference between Python and R comes in being production ready. Python, as such is a full fledged programming language and many organisations use it in their production systems. R is a statistical programming software favoured by many academia and due to the rise in data science and availability of libraries and being open source, the industry has started using R. Many of these organisations have their production systems either in Java, C++, C#, Python etc. So, ideally they would like to have the **prediction system** in the same language to reduce the latency and maintenance issues.&#xD;&#xA;Python 2, R 1.&#xD;&#xA; - **Libraries:** Both the languages have enormous and reliable libraries. R has over 5000 libraries catering to many domains while Python has some incredible packages like **Pandas, NumPy, SciPy, Scikit Learn, Matplotlib**. Python 3, R 2.&#xD;&#xA; - **Development:** Both the language are interpreted languages. Many say that python has a good learning curve, it's almost like reading english (to put it on a lighter note) but R has a reputation of having a steeper learning curve. Also, both of them have good IDEs (Spyder etc for Python and RStudio for R). Python 4, R 2.&#xD;&#xA; - **Speed:** R software initially had problems with large computations (say, like nxn matrix multiplications). But, this issue is addressed with the introduction of R by Revolution Analytics. They have re-written computation intensive operations in C which is blazingly fast. Python being a high level language is relatively slow. Python 4, R 3.&#xD;&#xA; - **Visualizations:** In data science, we frequently tend to plot data to showcase patterns to users. Therefore, visualisations become an important criteria in choosing a software and R completely kills Python in this regard. Thanks to Hadley Wickham for an incredible ggplot2 package. R wins hands down. Python 4, R 4.&#xD;&#xA; - **Dealing with Big Data:** One of the constraints of R is it stores the data in system memory (RAM). So, RAM capacity becomes a constraint when you are handling Big Data. Python does well, but I would say, as both R and Python have HDFS connectors, leveraging Hadoop infrastructure would give substantial performance improvement. So, Python 5, R 5.&#xD;&#xA;&#xD;&#xA;So, both the languages are equally good. Therefore, depending upon your domain and the place you work, you have got to smartly choose the right language. Technology world usually prefers single language. Business users (marketing analytics, retail analytics) usually go with statistical programming language R since they frequently do quick prototyping and build visualisations which is faster do in R." />
  <row Id="818" PostHistoryTypeId="2" PostId="340" RevisionGUID="9c1aca28-f253-4f56-ae10-cc5f60e1a06b" CreationDate="2014-06-12T13:38:10.877" UserId="471" Text="There's plenty. If you've ever used ggplot2 in R and want to do that in python:&#xD;&#xA;&#xD;&#xA;https://pypi.python.org/pypi/ggplot/0.5.9&#xD;&#xA;&#xD;&#xA;If you want to use a similar visualisation grammar (Vega) and go via D3 then:&#xD;&#xA;&#xD;&#xA;https://github.com/wrobstory/vincent&#xD;&#xA;&#xD;&#xA;Or if you want the full-on 3d shizzle:&#xD;&#xA;&#xD;&#xA;http://docs.enthought.com/mayavi/mayavi/&#xD;&#xA;&#xD;&#xA;" />
  <row Id="819" PostHistoryTypeId="2" PostId="341" RevisionGUID="1c7c326b-de34-4483-b8fd-285555534dbf" CreationDate="2014-06-12T13:42:05.383" UserId="743" Text="You have to first make it clear what do you mean by &quot;learn Hadoop&quot;. If you mean using Hadoop, such as learning to program in MapReduce, then most probably it is a good idea. But fundamental knowledge (database, machine learning, statistics) may play a bigger role as time goes on." />
  <row Id="822" PostHistoryTypeId="2" PostId="343" RevisionGUID="063f10f5-3726-47a7-9e98-77916a239149" CreationDate="2014-06-12T15:22:16.247" UserId="754" Text="Increasing the number of hidden layers for a standard neural network actually won't improve results in a majority of cases. Changing the _size_ of the hidden layer will.&#xD;&#xA;&#xD;&#xA;This fact has actually was noted historically and is the motivation behind the field of deep learning. Deep learning is effectively clever ways of training multilayer neural networks by, for example, isolating subsets of features when training different layers.&#xD;&#xA;&#xD;&#xA;Good introductory video on this topic on [YouTube](https://www.youtube.com/watch?v=vXMpKYRhpmI&amp;index=52&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)" />
  <row Id="824" PostHistoryTypeId="5" PostId="235" RevisionGUID="a63fd4eb-f1ee-4b89-8b32-dffc84ffd9cd" CreationDate="2014-06-12T16:32:27.490" UserId="471" Comment="fixed spelling 'Tableau'" Text="Data visualization is an important sub-field in data science and python programmers would need to have available toolkits for them.&#xD;&#xA;&#xD;&#xA;**Is there a Python API to Tableau?**&#xD;&#xA;&#xD;&#xA;**Are there any Python-based data visualization toolkits?**" />
  <row Id="825" PostHistoryTypeId="24" PostId="235" RevisionGUID="a63fd4eb-f1ee-4b89-8b32-dffc84ffd9cd" CreationDate="2014-06-12T16:32:27.490" Comment="Proposed by 471 approved by 50 edit id of 56" />
  <row Id="827" PostHistoryTypeId="4" PostId="319" RevisionGUID="df3f1c34-4a0f-45a3-9920-06b8ba9bc728" CreationDate="2014-06-12T16:43:59.513" UserId="381" Comment="edited title to match body" Text="Debugging Neural Networks" />
  <row Id="828" PostHistoryTypeId="24" PostId="319" RevisionGUID="df3f1c34-4a0f-45a3-9920-06b8ba9bc728" CreationDate="2014-06-12T16:43:59.513" Comment="Proposed by 381 approved by 50 edit id of 57" />
  <row Id="831" PostHistoryTypeId="2" PostId="345" RevisionGUID="c2b54984-0ce4-41fb-85f9-6dd65e9f891e" CreationDate="2014-06-12T16:52:46.557" UserId="554" Text="Introductory:&#xD;&#xA;&#xD;&#xA; - [Machine Learning: The Art and Science of Algorithms that Make Sense of Data (Flach)][1]&#xD;&#xA; - [Learning From Data (Abu-Mostafa et al.)][2]&#xD;&#xA; - [Introduction to Statistical Learning (James et al.)][3]&#xD;&#xA;&#xD;&#xA;Digging deeper:&#xD;&#xA;&#xD;&#xA; - [Elements of Statistical Learning (Hastie et al.)][4]&#xD;&#xA; - [Pattern Recognition and Machine Learning (Bishop)][5]&#xD;&#xA;&#xD;&#xA;Some special interest examples:&#xD;&#xA;&#xD;&#xA; - [Convex Optimization (Boyd)][6]&#xD;&#xA; - [Bayesian Reasoning and Machine Learning (Barber)][7]&#xD;&#xA; - [Probabilistic Graphical Models (Koller)][8]&#xD;&#xA; - [Neural Networks for Pattern Recognition (Bishop)][9]&#xD;&#xA;&#xD;&#xA;Broader reference works on machine learning (not really what you asked for, but for completeness):&#xD;&#xA;&#xD;&#xA; - [Machine Learning: A Probabilistic Perspective (Murphy)][10]&#xD;&#xA; - [Artificial Intelligence: A Modern Approach (Russell &amp; Norvig)][11]&#xD;&#xA;&#xD;&#xA;Bonus paper:&#xD;&#xA;&#xD;&#xA; - [Statistical Modeling: The Two Cultures (Breiman)][12]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.amazon.com/Machine-Learning-Science-Algorithms-Sense/dp/1107422221/&#xD;&#xA;  [2]: http://www.amazon.com/Learning-From-Data-Yaser-Abu-Mostafa/dp/1600490069/&#xD;&#xA;  [3]: http://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/&#xD;&#xA;  [4]: http://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/&#xD;&#xA;  [5]: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/&#xD;&#xA;  [6]: http://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787/&#xD;&#xA;  [7]: http://www.cs.ucl.ac.uk/staff/d.barber/brml/&#xD;&#xA;  [8]: http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193/&#xD;&#xA;  [9]: http://www.amazon.com/Networks-Pattern-Recognition-Advanced-Econometrics/dp/0198538642/&#xD;&#xA;  [10]: http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/&#xD;&#xA;  [11]: http://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/&#xD;&#xA;  [12]: http://projecteuclid.org/euclid.ss/1009213726" />
  <row Id="832" PostHistoryTypeId="12" PostId="203" RevisionGUID="0654da56-fea3-4cd9-915f-accdba1415e6" CreationDate="2014-06-12T16:56:27.573" UserId="50" Comment="via Vote" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:50,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="833" PostHistoryTypeId="13" PostId="203" RevisionGUID="9f85bcf8-226b-4993-bc17-9684e829d5e6" CreationDate="2014-06-12T16:57:44.967" UserId="50" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:50,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="835" PostHistoryTypeId="2" PostId="346" RevisionGUID="3dc0a14d-adb0-4b5b-8942-fc632b5abeb7" CreationDate="2014-06-12T17:03:15.733" UserId="554" Text="In addition to the courses and tutorials posted, I would suggest something a bit more 'hands on': [Kaggle][1] has some introductory competitions that might pique your interest (most people start with the Titanic competition). And there's a large variety of subjects to explore and compete in when you want to get more experience.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.kaggle.com/competitions" />
  <row Id="836" PostHistoryTypeId="2" PostId="347" RevisionGUID="affa3c12-4b7a-4bd9-bd67-60b29f10735c" CreationDate="2014-06-12T17:58:21.467" UserId="733" Text="As mentioned in above answers grasp the basics of ML by following MOOCs by Prof.Andrew Ng and ['Learning From Data'][1] by Prof. Yaser Abu-Mostafa.  &#xD;&#xA;&#xD;&#xA;R is the [clear winner][2] as the most used tool in Kaggle competitions. (Don't forget to check the resources on Kaggle wiki and forums)&#xD;&#xA;&#xD;&#xA;Learn basic R and Python. Coursera 'Data Science' track has an [introductory R course][3]. Almost all the algorithms can be found in Python and R libraries. Feel free to use the algorithms you learned in few kaggle competitions. As a starting point compare the performance of several algorithms on Titanic dataset and Digit recognizer dataset on [kaggle][4].&#xD;&#xA;&#xD;&#xA;And do continue practising on various datasets!&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://work.caltech.edu/telecourse.html&#xD;&#xA;  [2]: http://www.kaggle.com/wiki/Software&#xD;&#xA;  [3]: https://www.coursera.org/course/rprog&#xD;&#xA;  [4]: http://www.kaggle.com/" />
  <row Id="837" PostHistoryTypeId="5" PostId="343" RevisionGUID="84ed43ef-2706-4da1-b151-99a773978697" CreationDate="2014-06-12T18:08:07.507" UserId="754" Comment="added 52 characters in body" Text="Increasing the number of hidden layers for a standard neural network actually won't improve results in a majority of cases. Changing the _size_ of the hidden layer will.&#xD;&#xA;&#xD;&#xA;This fact (that the number of hidden layers does very little) has actually was noted historically and is the motivation behind the field of deep learning. Deep learning is effectively clever ways of training multilayer neural networks by, for example, isolating subsets of features when training different layers.&#xD;&#xA;&#xD;&#xA;Good introductory video on this topic on [YouTube](https://www.youtube.com/watch?v=vXMpKYRhpmI&amp;index=52&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)" />
  <row Id="838" PostHistoryTypeId="2" PostId="348" RevisionGUID="d3ef804e-0a3b-474d-8624-8c597b3b6422" CreationDate="2014-06-12T18:33:21.540" UserId="456" Text="Not sure about the cloud era one, but one of my friends joined the John Hopkins one and in his words it's &quot;brilliant to get you started&quot;. It has also been recommended by a lot of people. I am planning to join it in few weeks. As far as seriousness is concerned, I don't think these certifications are gonna help you land a job, but they sure will help you learn.  " />
  <row Id="842" PostHistoryTypeId="5" PostId="326" RevisionGUID="dac73363-fc90-418e-967e-752f70389528" CreationDate="2014-06-12T20:31:09.133" UserId="84" Comment="Fixed grammar, and improving formatting." Text="I maybe asking a very basic question. I'm just starting to develop a ML application for academic purposes, but I'm also interested in knowing what people are using both in academia and industry. A lot of places I saw people using Python. What do you recommend? I'm currently using R and training myself in it." />
  <row Id="843" PostHistoryTypeId="24" PostId="326" RevisionGUID="dac73363-fc90-418e-967e-752f70389528" CreationDate="2014-06-12T20:31:09.133" Comment="Proposed by 84 approved by 50 edit id of 58" />
  <row Id="844" PostHistoryTypeId="2" PostId="349" RevisionGUID="4c3a7f93-a9a3-4901-bdf5-d6855d99d839" CreationDate="2014-06-12T20:51:59.930" UserId="780" Text="As a former analytics manager and a current lead data scientist, I am very leery of the need for data science certificates.  The term data scientist is pretty vague and the field of data science is in it's infancy.  A certificates implies some sort of uniform standard which is just lacking in data science, it is still very much the wild west. &#xD;&#xA;&#xD;&#xA;While a certificate is probably not going to hurt you, I think your time would be better spent developing the experience know when to use a certain approach, and depth of understanding to be able to explain that approach to a non-technical audience." />
  <row Id="845" PostHistoryTypeId="2" PostId="350" RevisionGUID="a91422f7-8ff8-48b7-81f4-df159d20cfdb" CreationDate="2014-06-12T22:11:46.607" UserId="194" Text="Could you give some examples of typical tasks that a data scientist does in his daily job and the must know minimum for each of the levels (like junior, senior, etc. if there are any)? &#xD;&#xA;(if possible something like the following Programmer competency matrix http://www.starling-software.com/employment/programmer-competency-matrix.html)" />
  <row Id="846" PostHistoryTypeId="1" PostId="350" RevisionGUID="a91422f7-8ff8-48b7-81f4-df159d20cfdb" CreationDate="2014-06-12T22:11:46.607" UserId="194" Text="Example tasks of a data scientist and the necessary knowledge" />
  <row Id="847" PostHistoryTypeId="3" PostId="350" RevisionGUID="a91422f7-8ff8-48b7-81f4-df159d20cfdb" CreationDate="2014-06-12T22:11:46.607" UserId="194" Text="&lt;knowledge-base&gt;" />
  <row Id="848" PostHistoryTypeId="5" PostId="350" RevisionGUID="bcc02fb0-0390-4dd6-b0c0-da493ee4221e" CreationDate="2014-06-12T22:42:11.590" UserId="84" Comment="Improving formatting." Text="Could you give some examples of typical tasks that a data scientist does in his daily job, and the must-know minimum for each of the levels (like junior, senior, etc. if there are any)? If possible, something like a [Programmer competency matrix](http://www.starling-software.com/employment/programmer-competency-matrix.html)." />
  <row Id="849" PostHistoryTypeId="24" PostId="350" RevisionGUID="bcc02fb0-0390-4dd6-b0c0-da493ee4221e" CreationDate="2014-06-12T22:42:11.590" Comment="Proposed by 84 approved by 194 edit id of 60" />
  <row Id="850" PostHistoryTypeId="2" PostId="351" RevisionGUID="2a0a7cc8-7858-4a54-9615-d9fbcab4675f" CreationDate="2014-06-13T03:50:24.610" UserId="735" Text="There are multiple certifications going on, but they have different focus area and style of teaching.&#xD;&#xA;&#xD;&#xA;I prefer The Analytics Edge on eDX lot more over John Hopkins specialization, as it is more intensive and hands on. The expectation in John Hopkins specialization is to put in 3 - 4 hours a week vs. 11 - 12 hours a week on Analytics Edge.&#xD;&#xA;&#xD;&#xA;From an industry perspective, I take these certifications as a sign of interest and not level of knowledge a person possesses. There are too many dropouts in these MOOCs. I value other experience (like participating in Kaggle competitions) lot more than undergoing XYZ certification on MOOC." />
  <row Id="851" PostHistoryTypeId="2" PostId="352" RevisionGUID="820cc002-0290-4be8-9457-27766c0ebf1c" CreationDate="2014-06-13T05:40:39.360" UserId="62" Text="In some cases, [it may be impossible][1] to draw Euler diagrams with overlapping circles to represent all the overlapping subsets in the correct proportions. This type of data then requires using polygons or other figures to represent each set. When dealing with data that describes overlapping subsets, how can I figure out whether a simple Euler diagram is possible?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ncbi.nlm.nih.gov/pubmed/20975147" />
  <row Id="852" PostHistoryTypeId="1" PostId="352" RevisionGUID="820cc002-0290-4be8-9457-27766c0ebf1c" CreationDate="2014-06-13T05:40:39.360" UserId="62" Text="How do I figure out if subsets can be plotted in a normal Euler diagram?" />
  <row Id="853" PostHistoryTypeId="3" PostId="352" RevisionGUID="820cc002-0290-4be8-9457-27766c0ebf1c" CreationDate="2014-06-13T05:40:39.360" UserId="62" Text="&lt;visualization&gt;" />
  <row Id="855" PostHistoryTypeId="2" PostId="354" RevisionGUID="efb1b3b5-eeb6-474f-b3af-db977fcc930b" CreationDate="2014-06-13T07:28:37.763" UserId="791" Text="as I am very interested in programming and statistics, Data Science seems like a great career path to me - I like both fields and would like to combine them. Unfortunately, I have studied political science with a non-statistical sounding Master. I focused on statistics in this Master, visiting optional courses and writing a statistical thesis on a rather large dataset.&#xD;&#xA;&#xD;&#xA;Since almost all job adds are requiring  a degree in informatics, physics or some other techy-field, I am wondering if there is a chance to become a data scientist or if I should drop that idea.&#xD;&#xA;&#xD;&#xA;I am lacking knowledge in machine learning, sql and hadoop, while having a rather strong informatics and statistics background. &#xD;&#xA;&#xD;&#xA;So can somebody tell me how feasible my goal of becoming a data scientist is? " />
  <row Id="856" PostHistoryTypeId="1" PostId="354" RevisionGUID="efb1b3b5-eeb6-474f-b3af-db977fcc930b" CreationDate="2014-06-13T07:28:37.763" UserId="791" Text="Data Science as a Social Scientist?" />
  <row Id="857" PostHistoryTypeId="3" PostId="354" RevisionGUID="efb1b3b5-eeb6-474f-b3af-db977fcc930b" CreationDate="2014-06-13T07:28:37.763" UserId="791" Text="&lt;statistics&gt;" />
  <row Id="858" PostHistoryTypeId="2" PostId="355" RevisionGUID="db82f1f0-b62d-4ff7-842f-62503aae8af3" CreationDate="2014-06-13T10:08:14.087" UserId="434" Text="The downvotes are because of the topic, but I'll attempt to answer your question as best I can since it's here.&#xD;&#xA;&#xD;&#xA;Data science is a term that is thrown around as loosely as Big Data.  Everyone has a rough idea of what they mean by the term, but when you look at the actual work tasks, a data scientist's responsibilities will vary greatly from company to company.&#xD;&#xA;&#xD;&#xA;Statistical analysis could encompass the entirety of the workload in one job, and not even be a consideration for another.&#xD;&#xA;&#xD;&#xA;I wouldn't chase after a job title per se.  If you are interested in the field, network (like you are doing now) and find a good fit.  If you are perusing job ads, just look for the ones that stress statistical and informatics backgrounds.  Hadoop and SQL are both easy to become familiar with given the time and motivation, but I would stick with the areas you are strongest in and go from there." />
  <row Id="859" PostHistoryTypeId="2" PostId="356" RevisionGUID="dabc70e8-a683-4526-8894-13186ea578e2" CreationDate="2014-06-13T10:57:10.623" UserId="434" Text="I attack this problem frequently with inefficiency because it's always pretty low on the priority list and my clients are resistant to change until things break.  I would like some input on how to speed things up.  &#xD;&#xA;&#xD;&#xA;I have multiple datasets of information in a SQL database.  The database is vendor-designed, so I have little control over the structure.  It's a sql representation of a class-based structure.  It looks a little bit like this:&#xD;&#xA;&#xD;&#xA;    Main-class table&#xD;&#xA;     -sub-class table 1&#xD;&#xA;     -sub-class table 2&#xD;&#xA;      -sub-sub-class table&#xD;&#xA;     ...&#xD;&#xA;     -sub-class table n&#xD;&#xA;&#xD;&#xA;Each table contains fields for each attribute of the class.  A join exists which contains all of the fields for each of the sub-classes which contains all of the fields in the class table and all of the fields in each parent class' table, joined by a unique identifier.&#xD;&#xA;&#xD;&#xA;There are hundreds of classes. which means thousands of views and tens of thousands of columns.&#xD;&#xA;&#xD;&#xA;Beyond that, there are multiple datasets, indicated by a field value in the Main-class table.  There is the production dataset, visible to all end users, and there are several other datasets comprised of the most current version of the same data from various integration sources.  &#xD;&#xA;&#xD;&#xA;Daily, we run jobs that compare the production dataset to the live datasets and based on a set of rules we merge the data, purge the live datasets, then start all over again.  The rules are in place because we might trust one source of data more than another for a particular value of a particular class.&#xD;&#xA;&#xD;&#xA;The jobs are essentially a series of SQL statements that go row-by-row through each dataset, and field by field within each row.  The common changes are limited to a handful of fields in each row, but since anything can change we compare each value.&#xD;&#xA;&#xD;&#xA;There are 10s of millions of rows of data and in some environments the merge jobs can take longer than 24 hours.  We resolve that problem generally, by throwing more hardware at it, but this isn't a hadoop environment currently so there's a pretty finite limit to what can be done in that regard.&#xD;&#xA;&#xD;&#xA;How would you go about scaling a solution to this problem such that there were no limitations?  And how would you go about accomplishing the most efficient data-merge?  (currently it is field by field comparisons... painfully slow)." />
  <row Id="860" PostHistoryTypeId="1" PostId="356" RevisionGUID="dabc70e8-a683-4526-8894-13186ea578e2" CreationDate="2014-06-13T10:57:10.623" UserId="434" Text="How to best accomplish high speed comparison of like data?" />
  <row Id="861" PostHistoryTypeId="3" PostId="356" RevisionGUID="dabc70e8-a683-4526-8894-13186ea578e2" CreationDate="2014-06-13T10:57:10.623" UserId="434" Text="&lt;scaling&gt;" />
  <row Id="862" PostHistoryTypeId="6" PostId="356" RevisionGUID="5bbe468c-237e-46a8-b52b-a95d8d6576a9" CreationDate="2014-06-13T11:34:23.160" UserId="97" Comment="Adding relevant tags" Text="&lt;scaling&gt;&lt;etl&gt;&lt;sql&gt;" />
  <row Id="863" PostHistoryTypeId="24" PostId="356" RevisionGUID="5bbe468c-237e-46a8-b52b-a95d8d6576a9" CreationDate="2014-06-13T11:34:23.160" Comment="Proposed by 97 approved by 434 edit id of 61" />
  <row Id="864" PostHistoryTypeId="5" PostId="334" RevisionGUID="d40cb776-efca-408f-8b23-2e3719f2b50b" CreationDate="2014-06-13T11:35:51.697" UserId="434" Comment="added 79 characters in body" Text="I've now seen two data science certification programs - the [John Hopkins one available at Coursera][1] and the [Cloudera one][2].&#xD;&#xA;&#xD;&#xA;I'm sure there are others out there.&#xD;&#xA;&#xD;&#xA;The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics:&#xD;&#xA;&#xD;&#xA;* R Programming&#xD;&#xA;* cleaning and obtaining data&#xD;&#xA;* Data Analysis&#xD;&#xA;* Reproducible Research&#xD;&#xA;* Statistical Inference&#xD;&#xA;* Regression Models&#xD;&#xA;* Machine Learning&#xD;&#xA;* Developing Data Products&#xD;&#xA;* And what looks to be a Project based completion task similar to Cloudera's Data Science Challenge&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;The Cloudera program looks thin on the surface, but looks to answer the two important questions - &quot;Do you know the tools&quot;, &quot;Can you apply the tools in the real world&quot;.  Their program consists of:&#xD;&#xA;&#xD;&#xA;* Introduction to Data Science&#xD;&#xA;* Data Science Essentials Exam&#xD;&#xA;* Data Science Challenge (a real world data science project scenario)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I am not looking for a recommendation on a program or a quality comparison.&#xD;&#xA;&#xD;&#xA;I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community.&#xD;&#xA;&#xD;&#xA;EDIT: These are all great answers.  I'm choosing the correct answer by votes.&#xD;&#xA;&#xD;&#xA;  [1]: https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage&#xD;&#xA;  [2]: http://cloudera.com/content/cloudera/en/training/certification/ccp-ds.html" />
  <row Id="865" PostHistoryTypeId="2" PostId="357" RevisionGUID="c5248924-dad9-4af5-bc60-477bf131f529" CreationDate="2014-06-13T12:18:53.830" UserId="797" Text="I lead data science teams for a major Internet company and I have screened hundreds of profiles and interviewed dozens for our teams around the world. Many candidates have passed the aforementioned courses and programs or bring similar credentials. Personally, I have also taken the courses, some are good, others are disappointing but none of them makes you a &quot;data scientist&quot;.&#xD;&#xA;&#xD;&#xA;In general, I agree with the others here. A certificate from Coursera or Cloudera just signalizes an interest but it does not move the needle.  There is a lot more to consider and you can have a bigger impact by providing a comprehensive repository of your work (github profile for example) and by networking with other data scientists. Anyone hiring for a data science profile will always prefer to see your previous work and coding style/abilities.&#xD;&#xA;" />
  <row Id="866" PostHistoryTypeId="2" PostId="358" RevisionGUID="fa53a23b-d88c-43d8-a526-fab45912f857" CreationDate="2014-06-13T12:30:45.290" UserId="587" Text="Sounds interesting. Could the solution be to dump the data out, build a fast custom processing thingie to run it through and then import it back to the database? I've seen some blazing fast Java-based text processing tools for topic modeling that handle millions of lines of text per second.&#xD;&#xA;&#xD;&#xA;If it's an option then you can build a shell script to first dump the data in as good as format as possible. Then some pre-processing to separate the datasets, then real processing of comparison and changes. Lastly something that writes it back to a good format for input into database.&#xD;&#xA;&#xD;&#xA;Definately not a one-afternoon project, but you could probably get it to work in a couple of weeks." />
  <row Id="867" PostHistoryTypeId="2" PostId="359" RevisionGUID="2652b506-ceae-4d7d-8b44-c52d4da4d016" CreationDate="2014-06-13T13:49:35.777" UserId="743" Text="[Becoming a Data Scientist – Curriculum via Metromap][1] is a popular reference for this kind of question.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://nirvacana.com/thoughts/becoming-a-data-scientist/" />
  <row Id="868" PostHistoryTypeId="2" PostId="360" RevisionGUID="9822bfa6-f037-4407-abf6-0b13da70825b" CreationDate="2014-06-13T14:46:30.393" UserId="801" Text="That's because something called [bias-variance dilema][1]. The overfitted model means that we will have more complex decision boundary if we give more variance on model. The thing is, not only too simple models but also complex models are likely to have dis-classified result on unseen data. Consequently, over-fitted model is not good as under-fitted model. That's why overfitting is bad and we need to fit the model somewhere in the middle.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma" />
  <row Id="869" PostHistoryTypeId="5" PostId="349" RevisionGUID="20f414a8-53e6-4433-92c1-02ce3bf28f2c" CreationDate="2014-06-13T15:42:46.987" UserId="780" Comment="added a word" Text="As a former analytics manager and a current lead data scientist, I am very leery of the need for data science certificates.  The term data scientist is pretty vague and the field of data science is in it's infancy.  A certificates implies some sort of uniform standard which is just lacking in data science, it is still very much the wild west. &#xD;&#xA;&#xD;&#xA;While a certificate is probably not going to hurt you, I think your time would be better spent developing the experience to know when to use a certain approach, and depth of understanding to be able to explain that approach to a non-technical audience." />
  <row Id="870" PostHistoryTypeId="6" PostId="356" RevisionGUID="58793edf-97ac-473d-868f-90995dbf8b8f" CreationDate="2014-06-13T15:47:16.340" UserId="84" Comment="Changing tags." Text="&lt;efficiency&gt;&lt;scalability&gt;&lt;etl&gt;&lt;sql&gt;" />
  <row Id="871" PostHistoryTypeId="24" PostId="356" RevisionGUID="58793edf-97ac-473d-868f-90995dbf8b8f" CreationDate="2014-06-13T15:47:16.340" Comment="Proposed by 84 approved by 434 edit id of 62" />
  <row Id="872" PostHistoryTypeId="2" PostId="361" RevisionGUID="8b2f3983-9014-4e8b-a9ab-c44545765afd" CreationDate="2014-06-13T16:44:29.323" UserId="158" Text="Logic often states that by overfitting a model, it's capacity to generalize is increased. That said, clearly at some point underfitting a model cause models to become worse regardless of the complexity of data.&#xD;&#xA;&#xD;&#xA;How do you know when your model has struck the right balance and is not underfitting the data it seeks to model?&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;**Note:** This is a followup to my question, &quot;[Why Is Overfitting Bad?][1]&quot;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/61/why-is-overfitting-bad/" />
  <row Id="873" PostHistoryTypeId="1" PostId="361" RevisionGUID="8b2f3983-9014-4e8b-a9ab-c44545765afd" CreationDate="2014-06-13T16:44:29.323" UserId="158" Text="When is a Model Underfitted?" />
  <row Id="874" PostHistoryTypeId="3" PostId="361" RevisionGUID="8b2f3983-9014-4e8b-a9ab-c44545765afd" CreationDate="2014-06-13T16:44:29.323" UserId="158" Text="&lt;efficiency&gt;&lt;algorithms&gt;&lt;parameter&gt;" />
  <row Id="875" PostHistoryTypeId="5" PostId="361" RevisionGUID="a98a4f4d-8543-44b7-82f6-3989d4986287" CreationDate="2014-06-13T16:55:54.003" UserId="158" Comment="added 1 character in body" Text="Logic often states that by underfitting a model, it's capacity to generalize is increased. That said, clearly at some point underfitting a model cause models to become worse regardless of the complexity of data.&#xD;&#xA;&#xD;&#xA;How do you know when your model has struck the right balance and is not underfitting the data it seeks to model?&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;**Note:** This is a followup to my question, &quot;[Why Is Overfitting Bad?][1]&quot;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/61/why-is-overfitting-bad/" />
  <row Id="876" PostHistoryTypeId="5" PostId="61" RevisionGUID="1ac30de9-45cf-465a-b29d-9e26c6cde852" CreationDate="2014-06-13T16:58:23.247" UserId="158" Comment="added 191 characters in body" Text="Logic often states that by overfitting a model, it's capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;**Related:** Followup to the question above, &quot;[When is a Model Underfitted?][1]&quot;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted" />
  <row Id="879" PostHistoryTypeId="2" PostId="362" RevisionGUID="80daeffc-c924-47a3-8b66-7f2701575ba3" CreationDate="2014-06-13T17:14:26.260" UserId="737" Text="Can't you create a hash for each classes, and then merge rows by rows, field by field only the classes where the hash changed ? It should be faster if most of the classes don't change..&#xD;&#xA;&#xD;&#xA;Or a hash of each rows or maybe columns.. depending on how the data normally change.. " />
  <row Id="880" PostHistoryTypeId="2" PostId="363" RevisionGUID="7e0b96cb-cf8d-44c5-815b-999ba6f6aea4" CreationDate="2014-06-13T17:14:57.517" UserId="84" Text="Models are but abstractions of what is seen in real life. They are designed in order to abstract-away nitty-gritties of the real system in observation, while keeping sufficient information to support desired analysis.&#xD;&#xA;&#xD;&#xA;If a model is overfit, it takes into account too many details about what is being observed, and small changes on such object may cause the model to lose precision. On the other hand, if a model is underfit, it evaluates so few attributes that noteworthy changes on the object may be ignored.&#xD;&#xA;&#xD;&#xA;Note also that underfit may be seen as an *overfit*, depending on the dataset. If your input can be 99%-correctly-classified with a single attribute, you *overfit* the model to the data by simplifying the abstraction to a single characteristic. And, in this case, you'd be generalizing too much the 1% of the base into the 99%-class -- or also specifying the model so much that it can only *see* one class.&#xD;&#xA;&#xD;&#xA;A reasonable way to say that a model is neither over nor underfit is by performing cross-validations. You split your dataset into *k* parts, and say, pick one of them to perform your analysis, while using the other *k - 1* parts to train your model. Considering that the input itself is not biased, you should be able to have as much variance of data to train and evaluate as you'd have while using the model in real life processing." />
  <row Id="881" PostHistoryTypeId="2" PostId="364" RevisionGUID="81dc7276-200b-45cc-8a67-f9b768cf2ebf" CreationDate="2014-06-13T17:36:21.937" UserId="158" Text="The [Programmer Competency Matrix][1] is just a set of skills, which are more likely to occur when being a real programmer than other skills, they are not a checklist to being a programmer, or for that matter, required to be a programmer; most common way to know someone is a programmer is that they're paid to be a programmer, which honestly has nothing to do with programming skills.&#xD;&#xA;&#xD;&#xA;To be a data scientist, do data science.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://sijinjoseph.com/programmer-competency-matrix/" />
  <row Id="882" PostHistoryTypeId="2" PostId="365" RevisionGUID="b4e017e2-e660-4ba2-9307-ef603c40132a" CreationDate="2014-06-13T18:59:11.493" UserId="386" Text="@OP: Choosing answers by votes is the WORST idea.  &#xD;&#xA;&#xD;&#xA;Your question becomes a popularity contest.  You should seek the right answer, I doubt you know what you are asking, know what you are looking for.  &#xD;&#xA;&#xD;&#xA;To the your question:   &#xD;&#xA;Q: how seriously DS certifications are viewed at this point by the community.  &#xD;&#xA;&#xD;&#xA;A: What is your goal from taking these courses?  For work, for school, for self-improvement, etc?  Coursera classes are very applied, you will not learn much theory, those are intentionally reserved for classroom setting.  &#xD;&#xA;&#xD;&#xA;Nonetheless, Coursera classes is very useful.  I'd say it is equivalent to one year of stat grad class, out of a two year program.  &#xD;&#xA;&#xD;&#xA;I am not sure of its industry recognition yet, because the problem of how did you actually take the course?  How much time did you spend?  It's a lot easier to get A's in these courses than a classroom paper-pencil exam.  So, there is be a huge quality variability from person to person." />
  <row Id="883" PostHistoryTypeId="2" PostId="366" RevisionGUID="bf4f9970-7438-4d76-8f69-6dcb4d575c0f" CreationDate="2014-06-13T20:13:01.913" UserId="780" Text="To answer your question it is important to understand the frame of reference you are looking for, if you are looking for what philosophically you are trying to achieve in model fitting, check out Rubens answer he does a good job of explaining that context.&#xD;&#xA;&#xD;&#xA;However, in practice your question is almost entirely defined by business objectives.  &#xD;&#xA;&#xD;&#xA;To give a concrete example, lets say that you are a loan officer, you issued loans that are $3,000 and when people pay you back you make $50.  Naturally you are trying to build a model that predicts how if a person defaults on their loan.  Lets keep this simple and say that the outcomes are either full payment, or default.&#xD;&#xA;&#xD;&#xA;From a business perspective you can sum up a models performance with a contingency matrix:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;When the model predicts someone is going to default, do they?  To determining the downsides of over and under fitting I find it helpful to think of it as an optimization problem, because in each cross section of predicted verses actual model performance there is either a cost or profit to be made:&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;In this example predicting a default that is a default means avoiding any risk, and predicted a non-default which doesn't default will make $50 per loan issued.  Where things get dicey is when you are wrong, if you default when you predicted non-default you lose the entire loan principal and if you predict default when a customer actually would not have you suffer $50 of missed opportunity.  The numbers here are not important, just the approach.&#xD;&#xA;&#xD;&#xA;With this framework we can now begin to understand the difficulties associated with over and under fitting.&#xD;&#xA;&#xD;&#xA;Over fitting in this case would mean that your model works far better on you development/test data then it does in production.  Or to put it another way, your model in production will far underperform what you saw in development, this false confidence will probably cause you to take on far more risky loans then you otherwise would and leaves you very vulnerable to losing money.&#xD;&#xA;&#xD;&#xA;On the other hand, under fitting in this context will leave you with a model that just does a poor job of matching reality.  While the results of this can be wildly unpredictable, (the opposite word you want to describe your predictive models), commonly what happens is standards are tightened up to compensate for this, leading to less overall customers leading to lost good customers.  &#xD;&#xA;&#xD;&#xA;Under fitting suffers a kind of opposite difficulty that over fitting does, which is under fitting gives you lower confidence.  Insidiously, the lack of predictability still leads you to take on unexpected risk, all of which is bad news.&#xD;&#xA;&#xD;&#xA;In my experience the best way to avoid both of these situations is validating your model on data that is completely outside the scope of your training data, so you can have some confidence that you have a representative sample of what you will see &quot;in the wild&quot;.  &#xD;&#xA;&#xD;&#xA;Additionally, it is always a good practice to revalidate your models periodically, to determine how quickly your model is degrading, and if it is still accomplishing your objectives.&#xD;&#xA;&#xD;&#xA;Just to some things up, your model is under fitted when it does a poor job of predicting both the development and production data.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/RgWr9.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/78kH2.png" />
  <row Id="885" PostHistoryTypeId="5" PostId="365" RevisionGUID="12799921-e5a4-48e3-a3ba-01e903d8dcb1" CreationDate="2014-06-14T03:55:00.533" UserId="386" Comment="added 3 characters in body" Text="@OP: Choosing answers by votes is the WORST idea.  &#xD;&#xA;&#xD;&#xA;Your question becomes a popularity contest.  You should seek the right answer, I doubt you know what you are asking, know what you are looking for.  &#xD;&#xA;&#xD;&#xA;To answer your question:   &#xD;&#xA;Q: how seriously DS certifications are viewed at this point by the community.  &#xD;&#xA;&#xD;&#xA;A: What is your goal from taking these courses?  For work, for school, for self-improvement, etc?  Coursera classes are very applied, you will not learn much theory, those are intentionally reserved for classroom setting.  &#xD;&#xA;&#xD;&#xA;Nonetheless, Coursera classes is very useful.  I'd say it is equivalent to one year of stat grad class, out of a two year program.  &#xD;&#xA;&#xD;&#xA;I am not sure of its industry recognition yet, because the problem of how did you actually take the course?  How much time did you spend?  It's a lot easier to get A's in these courses than a classroom paper-pencil exam.  So, there is be a huge quality variability from person to person." />
  <row Id="886" PostHistoryTypeId="2" PostId="368" RevisionGUID="c718e853-3751-4740-9135-2ebc2b6dc84d" CreationDate="2014-06-14T07:34:37.643" UserId="791" Text="If you know R and it's ggplot library, you could try ggplot for python:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I like it, because I do work in R and python, and both are virtually identical.&#xD;&#xA;&#xD;&#xA;But if you are not familiar you have to deal with a very &quot;unpythonic&quot; syntax. But I think it's an easy library overall." />
  <row Id="889" PostHistoryTypeId="5" PostId="307" RevisionGUID="f7c34903-832d-4f98-8360-41a0052be101" CreationDate="2014-06-14T18:04:53.527" UserId="496" Comment="added 38 characters in body" Text="I have read lot of blogs\article on how different type of industries are using Big Data Analytic. But most of these article fails to mention&#xD;&#xA;&#xD;&#xA; 1. What kinda data these companies used. What was the size of the data&#xD;&#xA; 2. What kinda of tools technologies they used to process the data&#xD;&#xA; 3. What was the problem they were facing and how the insight they got the data helped them to resolve the issue.&#xD;&#xA; 4. How they selected the tool\technology to suit their need.&#xD;&#xA; 5. What kinda pattern they identified from the data &amp; what kind of patterns they were looking from the data.&#xD;&#xA;&#xD;&#xA;I wonder if someone can provide me answer to all these questions or a link which at-least answer some of the the questions. I am looking for real world example. &#xD;&#xA;&#xD;&#xA;It would be great if someone share how finance industry is making use of Big Data Analytic." />
  <row Id="890" PostHistoryTypeId="2" PostId="369" RevisionGUID="9ac75fbf-5efc-4efc-90f8-8992e644183c" CreationDate="2014-06-14T18:53:32.243" UserId="838" Text="What kind of error measures do each give and how do I know when to use one over the other? If you could give an example of when to use each, that would be great as well! Thanks so much!" />
  <row Id="891" PostHistoryTypeId="1" PostId="369" RevisionGUID="9ac75fbf-5efc-4efc-90f8-8992e644183c" CreationDate="2014-06-14T18:53:32.243" UserId="838" Text="Difference between using RMSE and nDCG to evaluate Recommender Systems?" />
  <row Id="892" PostHistoryTypeId="3" PostId="369" RevisionGUID="9ac75fbf-5efc-4efc-90f8-8992e644183c" CreationDate="2014-06-14T18:53:32.243" UserId="838" Text="&lt;machine-learning&gt;&lt;recommendation&gt;" />
  <row Id="893" PostHistoryTypeId="2" PostId="370" RevisionGUID="cea24788-07ef-4c46-ba61-13215c474ae4" CreationDate="2014-06-14T19:54:53.193" UserId="839" Text="I'd like to use my MSc thesis as an opportunity to explore 'data science'. Frankly the term seems a little vague to me (or at least, I've heard so many people apply it to so many situations that its become diluted), but I expect it requires a) machine learning (rather than traditional statistics) b) a large enough dataset that you have to run analyses on clusters. Anyway, we don't have any relevantly qualified professors at my college, so I'd like a dataset and problem that is accessible to a statistician, but could allow me to foray into this data science thing. Any suggestions? To keep this as narrow as possible, I'd ideally like links to open, well used datasets and example problems.&#xD;&#xA;&#xD;&#xA;Thanks!&#xD;&#xA;&#xD;&#xA;PS - engineering background, so I'm fairly comfy with math/programming" />
  <row Id="894" PostHistoryTypeId="1" PostId="370" RevisionGUID="cea24788-07ef-4c46-ba61-13215c474ae4" CreationDate="2014-06-14T19:54:53.193" UserId="839" Text="Data Science oriented dataset/research question for Statistics MSc thesis" />
  <row Id="895" PostHistoryTypeId="3" PostId="370" RevisionGUID="cea24788-07ef-4c46-ba61-13215c474ae4" CreationDate="2014-06-14T19:54:53.193" UserId="839" Text="&lt;statistics&gt;&lt;education&gt;&lt;knowledge-base&gt;&lt;definitions&gt;" />
  <row Id="896" PostHistoryTypeId="2" PostId="371" RevisionGUID="d86224c3-81f3-4b92-bfa3-4cc4d5abfb03" CreationDate="2014-06-14T20:32:06.143" UserId="434" Text="I'm curious about natural language querying.  Stanford has what looks to be a strong set of [software for processing natural language][1].  I've also seen the [Apache OpenNLP library][2], and the [General Architecture for Text Engineering][3].&#xD;&#xA;&#xD;&#xA;There are an incredible amount of uses for natural language processing and that makes the documentation of these projects difficult to quickly absorb.&#xD;&#xA;&#xD;&#xA;Can you simplify things for me a bit and at a high level outline the tasks necessary for performing a basic translation of simple questions into SQL?&#xD;&#xA;&#xD;&#xA;The first rectangle on my flow chart is a bit of a mystery.&#xD;&#xA;&#xD;&#xA;![enter image description here][4]  &#xD;&#xA;&#xD;&#xA;For example, I might want to know:&#xD;&#xA;&#xD;&#xA;How many books were sold last month?&#xD;&#xA;&#xD;&#xA;And I'd want that translated into&#xD;&#xA;&#xD;&#xA;    Select count(*) from sales where item_type='book' and &#xD;&#xA;      sales_date &gt;= '5/1/2014' and sales_date &lt;= '5/31/2014'&#xD;&#xA;&#xD;&#xA;  [1]: http://nlp.stanford.edu/software/index.shtml&#xD;&#xA;  [2]: http://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html&#xD;&#xA;  [3]: http://gate.ac.uk/science.html&#xD;&#xA;  [4]: http://i.stack.imgur.com/wJPx9.png" />
  <row Id="897" PostHistoryTypeId="1" PostId="371" RevisionGUID="d86224c3-81f3-4b92-bfa3-4cc4d5abfb03" CreationDate="2014-06-14T20:32:06.143" UserId="434" Text="How to process natural language queries?" />
  <row Id="898" PostHistoryTypeId="3" PostId="371" RevisionGUID="d86224c3-81f3-4b92-bfa3-4cc4d5abfb03" CreationDate="2014-06-14T20:32:06.143" UserId="434" Text="&lt;nlp&gt;" />
  <row Id="899" PostHistoryTypeId="5" PostId="371" RevisionGUID="a226f314-d88b-48da-8c29-26f5476fee0d" CreationDate="2014-06-14T20:39:25.657" UserId="434" Comment="added 39 characters in body" Text="I'm curious about natural language querying.  Stanford has what looks to be a strong set of [software for processing natural language][1].  I've also seen the [Apache OpenNLP library][2], and the [General Architecture for Text Engineering][3].&#xD;&#xA;&#xD;&#xA;There are an incredible amount of uses for natural language processing and that makes the documentation of these projects difficult to quickly absorb.&#xD;&#xA;&#xD;&#xA;Can you simplify things for me a bit and at a high level outline the tasks necessary for performing a basic translation of simple questions into SQL?&#xD;&#xA;&#xD;&#xA;The first rectangle on my flow chart is a bit of a mystery.&#xD;&#xA;&#xD;&#xA;![enter image description here][4]  &#xD;&#xA;&#xD;&#xA;For example, I might want to know:&#xD;&#xA;&#xD;&#xA;    How many books were sold last month?&#xD;&#xA;&#xD;&#xA;And I'd want that translated into&#xD;&#xA;&#xD;&#xA;    Select count(*) &#xD;&#xA;      from sales &#xD;&#xA;      where &#xD;&#xA;       item_type='book' and &#xD;&#xA;       sales_date &gt;= '5/1/2014' and &#xD;&#xA;       sales_date &lt;= '5/31/2014'&#xD;&#xA;&#xD;&#xA;  [1]: http://nlp.stanford.edu/software/index.shtml&#xD;&#xA;  [2]: http://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html&#xD;&#xA;  [3]: http://gate.ac.uk/science.html&#xD;&#xA;  [4]: http://i.stack.imgur.com/wJPx9.png" />
  <row Id="900" PostHistoryTypeId="2" PostId="372" RevisionGUID="47cb9752-ed35-4c77-b5ad-484a5aff71ac" CreationDate="2014-06-14T20:52:00.873" UserId="381" Text="Just head to kaggle.com; it'll keep you busy for a long time. For open data there's the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/). In fact, there's a whole [Stackexchange site](http://opendata.stackexchange.com/) devoted to this; look there." />
  <row Id="901" PostHistoryTypeId="2" PostId="373" RevisionGUID="290dda9d-c8be-419c-98db-53cbd6ce6bbf" CreationDate="2014-06-14T21:13:20.110" UserId="434" Text="The [Sunlight Foundation][1] is an organization that is focused on opening up and encouraging non-partisan analysis of government data.&#xD;&#xA;&#xD;&#xA;There is a ton of analysis out there in the wild that can be used for comparison, and a wide variety of topics.&#xD;&#xA;&#xD;&#xA;They provide [tools][2] and [apis][3] for accessing data, and have helped push to make data available in places like [data.gov][4].&#xD;&#xA;&#xD;&#xA;One interesting project is [Influence Explorer][5].  You can get [source data here][6] as well as access to real time data.&#xD;&#xA;&#xD;&#xA;You might also want to take a look at one of our more popular questions:&#xD;&#xA;&#xD;&#xA;[Publicly available datasets][7].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://sunlightfoundation.com/&#xD;&#xA;  [2]: http://sunlightfoundation.com/tools/&#xD;&#xA;  [3]: http://sunlightfoundation.com/api/&#xD;&#xA;  [4]: http://www.data.gov/&#xD;&#xA;  [5]: http://influenceexplorer.com/&#xD;&#xA;  [6]: http://data.influenceexplorer.com/&#xD;&#xA;  [7]: http://datascience.stackexchange.com/questions/155/publicly-available-datasets" />
  <row Id="902" PostHistoryTypeId="2" PostId="374" RevisionGUID="8553f00a-bb3a-4711-95ca-923d35691bb4" CreationDate="2014-06-14T21:39:15.833" UserId="780" Text="Natural language querying poses very many intricacies which can be very difficult to generalize.  From a high level, I would start with trying to think of things in terms of nouns and verbs.&#xD;&#xA;&#xD;&#xA;So for the sentence: How many books were sold last month?&#xD;&#xA;&#xD;&#xA;You would start by breaking the sentence down with a parser which will return a tree format similar to this:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;You can see that there is a subject books, a compound verbal phrase indicating the past action of sell, and then a noun phrase where you have the time focus of a month.&#xD;&#xA;&#xD;&#xA;We can further break down the subject for modifiers: &quot;how many&quot; for books, and &quot;last&quot; for month.&#xD;&#xA;&#xD;&#xA;Once you have broken the sentence down you need to map those elements to sql language e.g.: how many =&gt; count, books =&gt; book, sold =&gt; sales, month =&gt; sales_date (interval), and so on.  &#xD;&#xA;&#xD;&#xA;Finally, once you have the elements of the language you just need to come up with a set of rules for how different entities interact with each other, which leaves you with:&#xD;&#xA;&#xD;&#xA;Select count(*) &#xD;&#xA;  from sales &#xD;&#xA;  where &#xD;&#xA;   item_type='book' and &#xD;&#xA;   sales_date &gt;= '5/1/2014' and &#xD;&#xA;   sales_date &lt;= '5/31/2014'&#xD;&#xA;&#xD;&#xA;This is at a high level how I would begin, while almost every step I have mentioned is non-trivial and really the rabbit hole can be endless, this should give you many of the dots to connect.  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/ogoiY.png" />
  <row Id="903" PostHistoryTypeId="2" PostId="375" RevisionGUID="fb390798-6841-4d9b-acce-551187037c7d" CreationDate="2014-06-14T23:52:10.490" UserId="418" Text="The majority of people use S3. However, Google Drive seems a promising alternative solution for storing large amounts of data. Are there specific reasons why one is better than the other?" />
  <row Id="904" PostHistoryTypeId="1" PostId="375" RevisionGUID="fb390798-6841-4d9b-acce-551187037c7d" CreationDate="2014-06-14T23:52:10.490" UserId="418" Text="Amazon S3 vs Google Drive" />
  <row Id="905" PostHistoryTypeId="3" PostId="375" RevisionGUID="fb390798-6841-4d9b-acce-551187037c7d" CreationDate="2014-06-14T23:52:10.490" UserId="418" Text="&lt;bigdata&gt;" />
  <row Id="906" PostHistoryTypeId="2" PostId="376" RevisionGUID="70322382-dfd3-4fbe-84ae-c1a88bf70a7d" CreationDate="2014-06-15T00:00:51.657" UserId="84" Text="I'm developing a distributed algorithm, and to improve efficiency, it relies both on the number of disks (one per machine), and on an efficient load balance strategy. With more disks, we're able to reduce the time spent with I/O; and with an efficient load balance policy, we can distribute tasks without much data replication overhead.&#xD;&#xA;&#xD;&#xA;There are many studies on the literature that deal with the same problem, and each of them runs different experiments to evaluate their proposal. Some experiments are specific of the strategy presented, and some others, like weak scaling (scalability) and strong scaling (speedup), are common to all of the works.&#xD;&#xA;&#xD;&#xA;My question is, since the experiments are usually executed over entirely different infrastructures (disks, processors, # machines, network), how can one compare the results obtained and point out efficiency gains? For example, I may get 100% of speedup in my application running on 10 machines with Infiniband connection, whereas I could get the same or even worse results if my connection was Ethernet." />
  <row Id="907" PostHistoryTypeId="1" PostId="376" RevisionGUID="70322382-dfd3-4fbe-84ae-c1a88bf70a7d" CreationDate="2014-06-15T00:00:51.657" UserId="84" Text="How to compare experiments run over different infrastructures" />
  <row Id="908" PostHistoryTypeId="3" PostId="376" RevisionGUID="70322382-dfd3-4fbe-84ae-c1a88bf70a7d" CreationDate="2014-06-15T00:00:51.657" UserId="84" Text="&lt;bigdata&gt;&lt;efficiency&gt;&lt;performance&gt;&lt;scalability&gt;&lt;distributed&gt;" />
  <row Id="909" PostHistoryTypeId="5" PostId="376" RevisionGUID="b1fe9999-16c1-4fb9-90de-4ff1b36e29b0" CreationDate="2014-06-15T00:05:58.457" UserId="84" Comment="added 91 characters in body" Text="I'm developing a distributed algorithm, and to improve efficiency, it relies both on the number of disks (one per machine), and on an efficient load balance strategy. With more disks, we're able to reduce the time spent with I/O; and with an efficient load balance policy, we can distribute tasks without much data replication overhead.&#xD;&#xA;&#xD;&#xA;There are many studies on the literature that deal with the same problem, and each of them runs different experiments to evaluate their proposal. Some experiments are specific of the strategy presented, and some others, like weak scaling (scalability) and strong scaling (speedup), are common to all of the works.&#xD;&#xA;&#xD;&#xA;The problem is the experiments are usually executed over entirely different infrastructures (disks, processors, # machines, network), and depending on what is being evaluated, it may raise *false/unfair* comparisons. For example, I may get 100% of speedup in my application running on 10 machines with Infiniband connection, whereas I could get the same or even worse results if my connection was Ethernet.&#xD;&#xA;&#xD;&#xA;So, how can one honestly compare different experiments to point out efficiency gains? " />
  <row Id="910" PostHistoryTypeId="2" PostId="377" RevisionGUID="3ac4d239-c39d-4afc-bed7-c004df9a0959" CreationDate="2014-06-15T00:11:54.950" UserId="434" Text="From our perspective on here, the big benefit of S3 is the ease of accessing the data from within EC2.&#xD;&#xA;&#xD;&#xA;Google Drive is directly accessible from the Google Cloud platform.&#xD;&#xA;&#xD;&#xA;There are a host of other differences that might matter depending on your usage requirements, but that's the one that would matter most around here.&#xD;&#xA;&#xD;&#xA;The only other difference I can think of that would matter to the DS community is that when you are sharing something, you have no control of the address of a given file on google drive." />
  <row Id="914" PostHistoryTypeId="2" PostId="379" RevisionGUID="89d42e1b-0d40-4800-a642-d7b236580b71" CreationDate="2014-06-15T01:25:48.563" UserId="842" Text="Financial Services is a big user of Big Data, and innovator too.  One example is mortgage bond trading.  To answer your questions for it:&#xD;&#xA;&#xD;&#xA;&gt; What kinda data these companies used. What was the size of the data?&#xD;&#xA;&#xD;&#xA;- Long histories of each mortgage issued for the past many years, and payments by month against them.  (Billions of rows)&#xD;&#xA;- Long histories of credit histories.  (Billions of rows)&#xD;&#xA;- Home price indices.  (Not as big)&#xD;&#xA;&#xD;&#xA;&gt; What kinda of tools technologies they used to process the data?&#xD;&#xA;&#xD;&#xA;It varies.  Some use in-house solutions built on databases like Netezza or Teradata.  Others access the data via systems provided by the data providers.  (Corelogic, Experian, etc)  Some banks use columnal database technologies like KDB, or 1010data.&#xD;&#xA;&#xD;&#xA;&gt; What was the problem they were facing and how the insight they got the&#xD;&#xA;&gt; data helped them to resolve the issue.&#xD;&#xA;&#xD;&#xA;The key issue is determining when mortgage bonds (mortgage backed-securities) will prepay or default.  This is especially important for bonds that lack the government guarantee.  By digging into payment histories, credit files, and understanding the current value of the house, it's possible to predict the likelihood of a default.  Adding an interest rate model and prepayment model also helps predict the likelihood of a prepayment.&#xD;&#xA;&#xD;&#xA;&gt; How they selected the tool\technology to suit their need.&#xD;&#xA;&#xD;&#xA;If the project is driven by internal IT, usually it's based off of a large database vendor like Oracle, Teradata or Netezza.  If it's driven by the quants, then they are more likely to go straight to the data vendor, or a 3rd party &quot;All in&quot; system.&#xD;&#xA;&#xD;&#xA;&gt; What kinda pattern they identified from the data &amp; what kind of&#xD;&#xA;&gt; patterns they were looking from the data.&#xD;&#xA;&#xD;&#xA;Linking the data gives great insights into who is likely to default on their loans, and prepay them.  When you aggregated the loans into bonds, it can be the difference between a bond issued at $100,000,000 being worth that amount, or as little as $20,000,000." />
  <row Id="915" PostHistoryTypeId="2" PostId="380" RevisionGUID="7c185fa8-e4a1-465e-b03d-55148e457c50" CreationDate="2014-06-15T01:29:15.240" UserId="842" Text="I suspect this will get closed since it is very narrow, but my 2 cents...&#xD;&#xA;&#xD;&#xA;Data Science requires 3 skills:&#xD;&#xA;&#xD;&#xA;- Math/Stats&#xD;&#xA;- Programming&#xD;&#xA;- Domain Knowledge&#xD;&#xA;&#xD;&#xA;It can be very hard to show all three.  #1 and #2 can be signaled via degrees, but a hiring manager who may not have them doesn't want to trust a liberal arts degree.  If you're looking to get into Data Science, position yourself as a domain expert first.  Publish election predictions.  If you're correct, cite them.  That will get you noticed.&#xD;&#xA;&#xD;&#xA;If you're Domain knowledge is A+ level, you don't need A+ level programming skills, but learn programming enough so that you don't need someone else to fetch data for you." />
  <row Id="916" PostHistoryTypeId="2" PostId="381" RevisionGUID="d67fc9ab-aaf0-4b1f-8de4-0ddb6b47ba53" CreationDate="2014-06-15T01:36:51.693" UserId="842" Text="CAPM (Capital Asset Pricing Model) in Finance is a classic example of an underfit model.  It was built on the beautiful theory that &quot;Investors only pay for risk they can't diversify away&quot; so expected excess returns are equal to correlation to market returns.&#xD;&#xA;&#xD;&#xA;As a formula [0] Ra = Rf + B (Rm - Rf)&#xD;&#xA;where Ra is the expected return of the asset, Rf is the risk free rate, Rm is the market rate of return, and Beta is the correlation to the Equity premium (Rm - Rf)&#xD;&#xA;&#xD;&#xA;This is beautiful, elegant, and wrong.  Investors seem to require a higher rate of small stocks and value (defined by book to market, or dividend yield) stocks.  &#xD;&#xA;&#xD;&#xA;Fama and French [1] presented an update to the model, which adds additional Betas for Size and Value.  &#xD;&#xA;&#xD;&#xA;So how do you know in a general sense?  When the predictions you are making are wrong, and another variable with a logical explanation increases the prediction quality.  It's easy to understand why someone might think small stocks are risky, independent of non-diversifiable risk.  It's a good story, backed by the data.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[0]  http://www.investopedia.com/terms/c/capm.asp&#xD;&#xA;[1]  http://en.wikipedia.org/wiki/Fama%E2%80%93French_three-factor_model" />
  <row Id="918" PostHistoryTypeId="2" PostId="382" RevisionGUID="08fec671-6226-4508-96aa-3534c93f6e32" CreationDate="2014-06-15T12:26:50.060" UserId="846" Text="I've came across the following problem, that I recon is rather typical.&#xD;&#xA;&#xD;&#xA;I have some large data, say, a few million rows. I run some non-trivial analysis on it, e.g. an SQL query consisting of several sub-queries. I get some result, stating, for example, that property X is increasing over time.&#xD;&#xA;&#xD;&#xA;Now, there are two possible things that could lead to that:&#xD;&#xA;&#xD;&#xA; 1. X is indeed increasing over time&#xD;&#xA; 1. I have a bug in my analysis&#xD;&#xA;&#xD;&#xA;How can I test that the first happened, rather than the second? A step-wise debugger, even if one exists, won't help, since intermediate results can still consist of millions of lines.&#xD;&#xA;&#xD;&#xA;The only thing I could think of was to somehow generate a small, synthetic data set with the property that I want to test and run the analysis on it as a unit test. Are there tools to do this? Particularly, but not limited to, SQL." />
  <row Id="919" PostHistoryTypeId="1" PostId="382" RevisionGUID="08fec671-6226-4508-96aa-3534c93f6e32" CreationDate="2014-06-15T12:26:50.060" UserId="846" Text="How to debug data analysis?" />
  <row Id="920" PostHistoryTypeId="3" PostId="382" RevisionGUID="08fec671-6226-4508-96aa-3534c93f6e32" CreationDate="2014-06-15T12:26:50.060" UserId="846" Text="&lt;data-mining&gt;&lt;sql&gt;&lt;experiments&gt;" />
  <row Id="923" PostHistoryTypeId="2" PostId="384" RevisionGUID="f5a1b211-4925-470a-9ede-42735652cf94" CreationDate="2014-06-15T14:01:38.233" UserId="97" Text="I have a binary classification problem:&#xD;&#xA;&#xD;&#xA; - Approximately 1000 samples&#xD;&#xA; - 10 attributes, including binary, numeric and categorical&#xD;&#xA;&#xD;&#xA;Which algorithm is the best choice for this type of problem?&#xD;&#xA;&#xD;&#xA;By default I'm going to start with SVM, as it is considered the best for relatively clean and  not noisy data. " />
  <row Id="924" PostHistoryTypeId="1" PostId="384" RevisionGUID="f5a1b211-4925-470a-9ede-42735652cf94" CreationDate="2014-06-15T14:01:38.233" UserId="97" Text="Choose binary classification algorithm" />
  <row Id="925" PostHistoryTypeId="3" PostId="384" RevisionGUID="f5a1b211-4925-470a-9ede-42735652cf94" CreationDate="2014-06-15T14:01:38.233" UserId="97" Text="&lt;classification&gt;&lt;svm&gt;" />
  <row Id="928" PostHistoryTypeId="5" PostId="384" RevisionGUID="ea359b5b-dd65-4bb5-a036-0683ab6405ab" CreationDate="2014-06-15T14:11:08.710" UserId="97" Comment="Add tag, minor changes in text" Text="I have a binary classification problem:&#xD;&#xA;&#xD;&#xA; - Approximately 1000 samples in training set&#xD;&#xA; - 10 attributes, including binary, numeric and categorical&#xD;&#xA;&#xD;&#xA;Which algorithm is the best choice for this type of problem?&#xD;&#xA;&#xD;&#xA;By default I'm going to start with SVM, as it is considered the best for relatively clean and  not noisy data. " />
  <row Id="929" PostHistoryTypeId="6" PostId="384" RevisionGUID="ea359b5b-dd65-4bb5-a036-0683ab6405ab" CreationDate="2014-06-15T14:11:08.710" UserId="97" Comment="Add tag, minor changes in text" Text="&lt;classification&gt;&lt;svm&gt;&lt;binary&gt;" />
  <row Id="931" PostHistoryTypeId="2" PostId="386" RevisionGUID="db8edcfd-876b-403d-a703-153302ba19fc" CreationDate="2014-06-15T14:23:19.793" UserId="780" Text="For low parameters, pretty limited sample size,  and a binary classifier logistic regression should be plenty powerful enough.  You can use a more advanced algorithm but it's probably overkill. " />
  <row Id="932" PostHistoryTypeId="2" PostId="387" RevisionGUID="f88175ab-9c83-46cc-9d41-a1553b7db775" CreationDate="2014-06-15T15:11:29.970" UserId="84" Text="I once heard that filtering spam by using blacklists is not a good approach, since some user searching for entries in your dataset may be looking for particular information from the sources blocked. Also it'd become a burden to continuously validate the *current state* of each spammer blocked, checking if the site/domain still disseminate spam data.&#xD;&#xA;&#xD;&#xA;Considering that any approach must be efficient and scalable, so as to support filtering on very large datasets, what are the strategies available to get rid of spam in a non-biased manner?" />
  <row Id="933" PostHistoryTypeId="1" PostId="387" RevisionGUID="f88175ab-9c83-46cc-9d41-a1553b7db775" CreationDate="2014-06-15T15:11:29.970" UserId="84" Text="Filtering spam from retrieved data" />
  <row Id="934" PostHistoryTypeId="3" PostId="387" RevisionGUID="f88175ab-9c83-46cc-9d41-a1553b7db775" CreationDate="2014-06-15T15:11:29.970" UserId="84" Text="&lt;bigdata&gt;&lt;efficiency&gt;&lt;spam&gt;&lt;filtering&gt;" />
  <row Id="935" PostHistoryTypeId="2" PostId="388" RevisionGUID="c8521f86-033c-4cb3-900e-ddf4926c118d" CreationDate="2014-06-15T15:20:31.300" UserId="780" Text="Spam filtering, especially in email, has been revolutionized by neural networks, here are a couple papers that provide good reading on the subject:&#xD;&#xA;&#xD;&#xA;On Neural Networks And The Future Of Spam &#xD;&#xA;A. C. Cosoi, M. S. Vlad, V. Sgarciu &#xD;&#xA;http://ceai.srait.ro/index.php/ceai/article/viewFile/18/8&#xD;&#xA;&#xD;&#xA;Intelligent Word-Based Spam Filter Detection Using &#xD;&#xA;Multi-Neural Networks &#xD;&#xA;Ann Nosseir, Khaled Nagati and Islam Taj-Eddin&#xD;&#xA;http://www.ijcsi.org/papers/IJCSI-10-2-1-17-21.pdf&#xD;&#xA;&#xD;&#xA;Spam Detection using Adaptive Neural Networks: Adaptive Resonance Theory &#xD;&#xA;David Ndumiyana, Richard Gotora, and Tarisai Mupamombe&#xD;&#xA;http://onlineresearchjournals.org/JPESR/pdf/2013/apr/Ndumiyana%20et%20al.pdf" />
  <row Id="936" PostHistoryTypeId="5" PostId="387" RevisionGUID="6261454c-4775-4e16-9c81-f1ce1b0ed4b2" CreationDate="2014-06-15T15:23:01.007" UserId="84" Comment="added 134 characters in body" Text="I once heard that filtering spam by using blacklists is not a good approach, since some user searching for entries in your dataset may be looking for particular information from the sources blocked. Also it'd become a burden to continuously validate the *current state* of each spammer blocked, checking if the site/domain still disseminate spam data.&#xD;&#xA;&#xD;&#xA;Considering that any approach must be efficient and scalable, so as to support filtering on very large datasets, what are the strategies available to get rid of spam in a non-biased manner?&#xD;&#xA;&#xD;&#xA;**Edit**: if possible, any example of strategy, even if just the intuition behind it, would be very welcome along with the answer." />
  <row Id="937" PostHistoryTypeId="5" PostId="384" RevisionGUID="7a561167-a9e3-471d-8381-84238951bb76" CreationDate="2014-06-15T15:23:12.657" UserId="97" Comment="More details" Text="I have a binary classification problem:&#xD;&#xA;&#xD;&#xA; - Approximately 1000 samples in training set&#xD;&#xA; - 10 attributes, including binary, numeric and categorical&#xD;&#xA;&#xD;&#xA;Which algorithm is the best choice for this type of problem?&#xD;&#xA;&#xD;&#xA;By default I'm going to start with SVM (preliminary having nominal attributes values converted to binary features), as it is considered the best for relatively clean and  not noisy data. " />
  <row Id="938" PostHistoryTypeId="2" PostId="389" RevisionGUID="a44e941f-8cfd-4a96-b0fa-e5e09fb3ccd8" CreationDate="2014-06-15T15:49:12.907" UserId="515" Text="Here is a suggestion:&#xD;&#xA;&#xD;&#xA; - Code your analysis in such a way that it can be run on sub-samples.&#xD;&#xA; - Code a complementary routine which can sample, either randomly, or by time, or by region, or ...  This may be domain-specific. This is where your knowledge enters.&#xD;&#xA; - Combine the two and see if the results are stable across subsamples." />
  <row Id="939" PostHistoryTypeId="5" PostId="365" RevisionGUID="923303ba-3b1c-49bd-b3e0-01f7c16eb27b" CreationDate="2014-06-15T15:56:06.780" UserId="386" Comment="added 9 characters in body" Text="@OP: Choosing answers by votes is the WORST idea.  &#xD;&#xA;&#xD;&#xA;Your question becomes a popularity contest.  You should seek the right answer, I doubt you know what you are asking, know what you are looking for.  &#xD;&#xA;&#xD;&#xA;To answer your question:   &#xD;&#xA;Q: how seriously DS certifications are viewed at this point by the community.  &#xD;&#xA;&#xD;&#xA;A: What is your goal from taking these courses?  For work, for school, for self-improvement, etc?  Coursera classes are very applied, you will not learn much theory, they are intentionally reserved for classroom setting.  &#xD;&#xA;&#xD;&#xA;Nonetheless, Coursera classes are very useful.  I'd say it is equivalent to one year of stat grad class, out of a two year Master program.  &#xD;&#xA;&#xD;&#xA;I am not sure of its industry recognition yet, because the problem of how did you actually take the course?  How much time did you spend?  It's a lot easier to get A's in these courses than a classroom paper-pencil exam.  So, there is be a huge quality variation from person to person." />
  <row Id="940" PostHistoryTypeId="2" PostId="390" RevisionGUID="e61d4f45-b5e3-46df-af17-9e86e3ef3130" CreationDate="2014-06-15T16:07:35.543" UserId="21" Text="When categorical variables are in the mix, I reach for Random Decision Forests, as it handles categorical variables directly without the 1-of-n encoding transformation. This loses less information." />
  <row Id="941" PostHistoryTypeId="2" PostId="391" RevisionGUID="5bb6eaeb-cb21-4990-87ad-07917213250f" CreationDate="2014-06-15T16:59:50.190" UserId="735" Text="This is what I normally do - take up the most important variables (basis your business understanding and hypothesis - you can always revise it later), group by on these attributes to reduce the number of rows, which can then be imported into a Pivot. You should include the sum and count of the relevant metrics on each row.&#xD;&#xA;&#xD;&#xA;Make sure that you don't put any filters in the previous step. Once you have entire data at a summarized level, you can play around in Pivot tables and see what things are changing / increasing or decreasing.&#xD;&#xA;&#xD;&#xA;If the data is too big to be summarized even on important parameters, you need to partition it in 3 - 4 subsets and then do this again.&#xD;&#xA;&#xD;&#xA;Hope it helps." />
  <row Id="946" PostHistoryTypeId="5" PostId="388" RevisionGUID="18f28bf6-d8df-4265-a31d-66f88d46209c" CreationDate="2014-06-15T21:22:10.500" UserId="780" Comment="Added some intuition" Text="Spam filtering, especially in email, has been revolutionized by neural networks, here are a couple papers that provide good reading on the subject:&#xD;&#xA;&#xD;&#xA;On Neural Networks And The Future Of Spam &#xD;&#xA;A. C. Cosoi, M. S. Vlad, V. Sgarciu &#xD;&#xA;http://ceai.srait.ro/index.php/ceai/article/viewFile/18/8&#xD;&#xA;&#xD;&#xA;Intelligent Word-Based Spam Filter Detection Using &#xD;&#xA;Multi-Neural Networks &#xD;&#xA;Ann Nosseir, Khaled Nagati and Islam Taj-Eddin&#xD;&#xA;http://www.ijcsi.org/papers/IJCSI-10-2-1-17-21.pdf&#xD;&#xA;&#xD;&#xA;Spam Detection using Adaptive Neural Networks: Adaptive Resonance Theory &#xD;&#xA;David Ndumiyana, Richard Gotora, and Tarisai Mupamombe&#xD;&#xA;http://onlineresearchjournals.org/JPESR/pdf/2013/apr/Ndumiyana%20et%20al.pdf&#xD;&#xA;&#xD;&#xA;EDIT:&#xD;&#xA;The basic intuition behind using a neural network to help with spam filtering is by providing a weight to terms based on how often they are associated with spam.&#xD;&#xA;&#xD;&#xA;Neural networks can be trained most quickly in a supervised -- you explicitly provide the classification of the sentence in the training set -- environment.  Without going into the nitty gritty the basic idea can be illustrated with these sentences:&#xD;&#xA;&#xD;&#xA;Text = &quot;How is the loss of the Viagra patent going to affect Pfizer&quot;, Spam = false&#xD;&#xA;Text = &quot;Cheap Viagra Buy Now&quot;, Spam = true&#xD;&#xA;Text = &quot;Online pharmacy Viagra Cialis Lipitor&quot;, Spam = true&#xD;&#xA;&#xD;&#xA;For a two stage neural network, the first stage will calculate the likelihood of spam based off of if the word exists in the sentence.  So from our example:&#xD;&#xA;&#xD;&#xA;viagra =&gt; 66%&#xD;&#xA;buy =&gt; 100%&#xD;&#xA;Pfizer =&gt; 0%&#xD;&#xA;etc..&#xD;&#xA;&#xD;&#xA;Then for the second stage the results in the first stage are used as variables in the second stage:&#xD;&#xA;&#xD;&#xA;viagra &amp; buy =&gt; 100%&#xD;&#xA;Pfizer &amp; viagra=&gt; 0%&#xD;&#xA;&#xD;&#xA;This basic idea is run for many of the permutations of the all the words in your training data.  The end results once trained is basically just an equation that based of the context of the words in the sentence can assign a probability of being spam.  Set spamminess threshold, and filter out any data higher then said threshold." />
  <row Id="947" PostHistoryTypeId="2" PostId="393" RevisionGUID="dbdec824-642c-41f7-b4e9-7c6bdfb06307" CreationDate="2014-06-15T22:33:17.670" UserId="478" Text="Linear SVM should be a good starting point. Take a look at [this][1] guide to choose the right estimator.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" />
  <row Id="948" PostHistoryTypeId="34" PostId="224" RevisionGUID="9e8e614c-28a3-4acd-96ac-87442438ab74" CreationDate="2014-06-15T22:35:42.893" UserId="-1" Comment="3" />
  <row Id="949" PostHistoryTypeId="2" PostId="394" RevisionGUID="ee2dc973-a8c5-4ea3-8e85-bc24ef7c18c5" CreationDate="2014-06-16T04:21:36.340" UserId="548" Text="Personally, we use S3 on top of GCE and really love it. Depending on how much data you're dealing with, Google Drive just doesn't quite match the 5 TB max that S3 gives you. Also, if you're using python, `boto` does a pretty fantastic job of making most aws services pretty accessible regardless of what stack you're dealing with. Even if you're not using python, they've got a pretty straightforward API that generally is more accessible than Google Drive.&#xD;&#xA;&#xD;&#xA;Instead of google drive, though google did recently release a cloud storage service, apart from drive, that lets you more closely integrate your storage with any gce instance you've got, https://cloud.google.com/products/cloud-storage/&#xD;&#xA;&#xD;&#xA;They've got an API which seems to be pretty comparable to S3's, but I can't profess to having really played around with it much. Pricing-wise the two are identical, but I think that the large community and experience with aws in general still puts S3 squarely above both google's cloud storage and google drive." />
  <row Id="951" PostHistoryTypeId="2" PostId="395" RevisionGUID="60e9784f-2f61-41be-9dd2-ef822c1aebe7" CreationDate="2014-06-16T04:37:58.817" UserId="548" Text="It's hard to say without knowing a little more about your dataset, and how separable your dataset is based on your feature vector, but I would probably suggest using extreme random forest over standard random forests because of your relatively small sample set.&#xD;&#xA;&#xD;&#xA;Extreme random forests are pretty similar to standard random forests with the one exception that instead of optimizing splits on trees, extreme random forest makes splits at random. Initially this would seem like a negative, but it generally means that you have significantly better generalization and speed, though the AUC on your training set is likely to be a little worse.&#xD;&#xA;&#xD;&#xA;Logistic regression is also a pretty solid bet for these kinds of tasks, though with your relatively low dimensionality and small sample size I would be worried about overfitting. You might want to check out using K-Nearest Neighbors since it often performs very will with low dimensionalities, but it doesn't usually handle categorical variables very well.&#xD;&#xA;&#xD;&#xA;If I had to pick one without knowing more about the problem I would certainly place my bets on extreme random forest, as it's very likely to give you good generalization on this kind of dataset, and it also handles a mix of numerical and categorical data better than most other methods." />
  <row Id="952" PostHistoryTypeId="2" PostId="396" RevisionGUID="16326f5e-d2be-4ec3-aea1-f1c85d6a6ffe" CreationDate="2014-06-16T04:51:47.847" UserId="548" Text="Alex made a number of good points, though I might have to push back a bit on his implication that DBSCAN is the best clustering algorithm to use here. Depending on your implementation, and whether or not you're using accelerated indices (many implementations do not), your time and space complexity will both be `O(n2)`, which is far from ideal.&#xD;&#xA;&#xD;&#xA;Personally, my go-to clustering algorithms are OpenOrd for winner-takes-all clustering and FLAME for fuzzy clustering. Both methods are indifferent to whether the metrics used are similarity or distance (FLAME in particular is nearly identical in both constructions). The implementation of OpenOrd in Gephi is `O(nlogn)` and is known to be more scalable than any of the other clustering algorithms present in the Gephi package.&#xD;&#xA;&#xD;&#xA;FLAME on the other hand is great if you're looking for a fuzzy clustering method. While the complexity of FLAME is a little harder to determine since it's an iterative process, it has been shown to be sub-quadratic, and similar in run-speed to knn." />
  <row Id="953" PostHistoryTypeId="2" PostId="397" RevisionGUID="9dd5db07-7786-46d0-a160-927dc31db4af" CreationDate="2014-06-16T06:34:53.683" UserId="733" Text="First you need to verify that your implementation of the algorithm is accurate. For that use a small sample of data and check whether the result is correct. At this stage the sample doesn't need to be representative of the population. &#xD;&#xA;&#xD;&#xA;Once the implementation is verified, you need to verify that there is a significant relationship among the variables that you try to predict. To do that define null hypothesis and try to reject the null hypothesis with a significant confidence level. ([hypothesis testing for linear regression][1]) &#xD;&#xA;&#xD;&#xA;There might be unit test frameworks for your SQL distribution. But using a programming language like R will be more easier to implement.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stattrek.com/regression/slope-test.aspx" />
  <row Id="954" PostHistoryTypeId="2" PostId="398" RevisionGUID="8cde3abb-ecd1-4ead-8b55-624be9da6eaa" CreationDate="2014-06-16T07:32:29.137" UserId="872" Text="I'm currently in the very early stages of preparing a new research-project (still at the funding-application stage), and expect that data-analysis and especially visualisation tools will play a role in this project.&#xD;&#xA;&#xD;&#xA;In view of this I face the following dilemma: Should I learn Python to be able to use its extensive scientific libraries (Pandas, Numpy, Scipy, ...), or should I just dive into similar packages of a language I'm already acquainted with (Racket, or to a lesser extent Scala)?&#xD;&#xA;&#xD;&#xA;(Ideally I would learn Python in parallel with using statistical libraries in Racket, but I'm not sure I'll have time for both)&#xD;&#xA;&#xD;&#xA;I'm not looking for an answer to this dilemma, but rather for feedback on my different considerations:&#xD;&#xA;&#xD;&#xA;My current position is as follows:&#xD;&#xA;&#xD;&#xA;**In favour of Python:**&#xD;&#xA;&#xD;&#xA;+ Extensively used libraries&#xD;&#xA;+ Widely used (may be decisive in case of collaboration with others)&#xD;&#xA;+ A lot of online material to start learning it&#xD;&#xA;+ Conferences that are specifically dedicated to Scientific Computing with Python&#xD;&#xA;+ Learning Python won't be a waste of time anyway&#xD;&#xA;&#xD;&#xA;**In favour of a language I already know:**&#xD;&#xA;&#xD;&#xA;+ It's a way to deepen my knowledge of one language rather than getting superficial knowledge of one more language (under the motto: you should at least know one language really well)&#xD;&#xA;+ It is feasible. Both Racket and Scala have good mathematics and statistics libraries&#xD;&#xA;+ I can start right away with learning what I need to know rather than first having to learn the basics&#xD;&#xA;&#xD;&#xA;**Two concrete questions:**&#xD;&#xA;&#xD;&#xA;1. What am I forgetting?&#xD;&#xA;2. How big of a nuisance could the Python 2 vs 3 issue be?&#xD;&#xA;" />
  <row Id="955" PostHistoryTypeId="1" PostId="398" RevisionGUID="8cde3abb-ecd1-4ead-8b55-624be9da6eaa" CreationDate="2014-06-16T07:32:29.137" UserId="872" Text="What to consider before learning a new language for data analysis" />
  <row Id="956" PostHistoryTypeId="3" PostId="398" RevisionGUID="8cde3abb-ecd1-4ead-8b55-624be9da6eaa" CreationDate="2014-06-16T07:32:29.137" UserId="872" Text="&lt;python&gt;&lt;visualization&gt;" />
  <row Id="957" PostHistoryTypeId="2" PostId="399" RevisionGUID="b838af25-8fab-465c-887a-2693b94b777c" CreationDate="2014-06-16T09:52:24.913" UserId="735" Text="According to me, all the factors, you have mentioned are superficial in nature. You have not considered the core of tool selection. In this case, there are 2 aspects, you mentioned:&#xD;&#xA;&#xD;&#xA;1. Data analysis - What kind of analysis are you working on? There might be some analysis which are easier in some languages and more difficult in other.&#xD;&#xA;&#xD;&#xA;2. Visualization - R provides similar community and learning material (as Python) and has the best visualizations compared to other languages here.&#xD;&#xA;&#xD;&#xA;At this stage, you can be flexible with what language to learn, since you are starting from scratch.&#xD;&#xA;&#xD;&#xA;Hope this helps." />
  <row Id="958" PostHistoryTypeId="2" PostId="400" RevisionGUID="016ab287-599f-454b-817c-3cff220ce9c3" CreationDate="2014-06-16T11:45:05.800" UserId="846" Text="From my experience, the points to keep in mind when considering a data analysis platform are:&#xD;&#xA;&#xD;&#xA;  1. Can it handle the size of the data that I need? If your data sets fit in memory, there's usually no big trouble, although AFAIK Python is somewhat more memory-efficient than R. If you need to handle larger-than-memory data sets, the platform need to handle it conveniently. In this case, SQL would cover for basic statistics, Python + Apache Spark is another option.&#xD;&#xA;  1. Does the platform covers all of my analysis needs? The greatest annoyance I've encountered in data mining projects is having to juggle between several tools, because tool A handles web connections well, tool B does the statistics and tool C renders nice pictures. You want your weapon-of-choice to cover as many aspects of your projects as possible. When considering this issue, Python is very comprehensive, but R has a lot of build-in statistical tests ready-to-use, if that's what you need. " />
  <row Id="960" PostHistoryTypeId="2" PostId="401" RevisionGUID="3cb91df8-cf88-404f-a372-e59e40c54277" CreationDate="2014-06-16T13:21:58.777" UserId="791" Text="I like a multiple step strategy:&#xD;&#xA;&#xD;&#xA;1. Write clean easy to understand code, as opposed to short-tricky code. I know statisticians like tricky code, but spotting problems in tricky code is dangerous. &#xD;&#xA;( I am mentioning this because a supervisor of mine was fond of undocumented 500 lines python scrips - have fun debugging that mess and I have seen that pattern a lot, especially from people who are not from an IT background)&#xD;&#xA;&#xD;&#xA;2. Break down your code in smaller functions, which can be tested and evaluated in smaller stes.&#xD;&#xA;&#xD;&#xA;3. Look for connected elements, e.g. the number of cases with condition X is Y - so this query MUST return Y. Most often this is more complex, but doable.&#xD;&#xA;&#xD;&#xA;4. When you are running your script the first time, test it with a small subsample and carefully check if everything is in order. While I like unit tests in IT, bugs in statistics scripts are often so pronounced that they are easily visible doing a carefully check. Or they are methodical errors, which are probably never caught by unit tests.&#xD;&#xA;&#xD;&#xA;That should suffice to ensure a clean &quot;one - off &quot; job. But for a time series as you seem to have, I would add that you should check for values out of range, impossible combinations etc. For me, most scripts that have reached step 4 are probably bug free - and they will stay that way unless something changes. And most often, the data are changing - and that is something which should be checked for every run. Writing code for that can be time consuming and annoying, but it beats subtle errors due to data entry errors." />
  <row Id="961" PostHistoryTypeId="2" PostId="402" RevisionGUID="ef42f839-f3bf-4e9d-b717-dee913c9d7fc" CreationDate="2014-06-16T13:25:48.453" UserId="387" Text="The [brat annotation tool][1] might be useful for you as per my comment.  I have tried many of them and this is the best I have found.  It has a nice user interface and can support a number of different types of annotations.  The annotations are stored in a separate .annot file which contain each annotation as well as its location within the original document.  A word of warning though, if you ultimately want to feed the annotations into a classifier like the Stanford NER tool then you will have to do some manipulation to get the data into a format that it will accept.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://brat.nlplab.org/" />
  <row Id="962" PostHistoryTypeId="2" PostId="403" RevisionGUID="a01e7889-e1b8-4b5b-95ec-3cc4d96974d3" CreationDate="2014-06-16T13:30:01.320" UserId="880" Text="![enter image description here][1]I am trying to do Logistic Regression using SAS Enterprise Miner. &#xD;&#xA;My Independent variables are &#xD;&#xA;CPR/Inc (Categorical 1 to 7)&#xD;&#xA;OD/Inc (Categorical 1 to 4)&#xD;&#xA;Insurance (Binary 0 or 1)&#xD;&#xA;Income Loss (Binary 0 or 1)&#xD;&#xA;Living Arrangement (Categorical 1 to 7)&#xD;&#xA;Employment Status (categorical 1 to 8)&#xD;&#xA;&#xD;&#xA;My Dependent Variable is Default (Binary 0 or 1)&#xD;&#xA;&#xD;&#xA;The following is the output from running Regression Model.&#xD;&#xA;&#xD;&#xA; Analysis of Maximum Likelihood Estimates&#xD;&#xA; &#xD;&#xA;                                  Standard          Wald&#xD;&#xA;Parameter       DF    Estimate       Error    Chi-Square    Pr &gt; ChiSq    Exp(Est)&#xD;&#xA; &#xD;&#xA;Intercept        1     -0.4148      0.0645         41.30        &lt;.0001       0.660&#xD;&#xA;CPR___Inc  1     1     -0.8022      0.1051         58.26        &lt;.0001       0.448&#xD;&#xA;CPR___Inc  2     1     -0.4380      0.0966         20.57        &lt;.0001       0.645&#xD;&#xA;CPR___Inc  3     1      0.3100      0.0871         12.68        0.0004       1.363&#xD;&#xA;CPR___Inc  4     1    -0.00304      0.0898          0.00        0.9730       0.997&#xD;&#xA;CPR___Inc  5     1      0.1331      0.0885          2.26        0.1324       1.142&#xD;&#xA;CPR___Inc  6     1      0.1694      0.0881          3.70        0.0546       1.185&#xD;&#xA;Emp_Status 1     1     -0.2289      0.1006          5.18        0.0229       0.795&#xD;&#xA;Emp_Status 2     1      0.4061      0.0940         18.66        &lt;.0001       1.501&#xD;&#xA;Emp_Status 3     1     -0.2119      0.1004          4.46        0.0347       0.809&#xD;&#xA;Emp_Status 4     1      0.1100      0.0963          1.30        0.2534       1.116&#xD;&#xA;Emp_Status 5     1     -0.2280      0.1007          5.12        0.0236       0.796&#xD;&#xA;Emp_Status 6     1      0.3761      0.0943         15.91        &lt;.0001       1.457&#xD;&#xA;Emp_Status 7     1     -0.3337      0.1026         10.59        0.0011       0.716&#xD;&#xA;Inc_Loss   0     1     -0.1996      0.0449         19.76        &lt;.0001       0.819&#xD;&#xA;Insurance  0     1      0.1256      0.0559          5.05        0.0246       1.134&#xD;&#xA;Liv_Arran  1     1     -0.1128      0.0916          1.52        0.2178       0.893&#xD;&#xA;Liv_Arran  2     1      0.2576      0.0880          8.57        0.0034       1.294&#xD;&#xA;Liv_Arran  3     1      0.0235      0.0904          0.07        0.7950       1.024&#xD;&#xA;Liv_Arran  4     1      0.0953      0.0887          1.16        0.2825       1.100&#xD;&#xA;Liv_Arran  5     1     -0.0493      0.0907          0.29        0.5871       0.952&#xD;&#xA;Liv_Arran  6     1     -0.3732      0.0966         14.93        0.0001       0.689&#xD;&#xA;OD___Inc   1     1     -0.2136      0.0557         14.72        0.0001       0.808&#xD;&#xA;OD___Inc   2     1     -0.0279      0.0792          0.12        0.7248       0.973&#xD;&#xA;OD___Inc   3     1     -0.0249      0.0793          0.10        0.7534       0.975&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Now I used this Model to Score a new set of data. An example row of my new data is&#xD;&#xA;CPR - 7&#xD;&#xA;OD - 4&#xD;&#xA;Living Arrangement - 4&#xD;&#xA;Employment Status - 4&#xD;&#xA;Insurance - 0&#xD;&#xA;Income Loss - 1&#xD;&#xA;&#xD;&#xA;For this sample row, the model predicted output (Probability of default = 1) as 0.7335 &#xD;&#xA;To check this manually, I added the estimates&#xD;&#xA;Intercept + Emp Status 4 + Liv Arran 4 + Insurance 0&#xD;&#xA;-0.4148   + 0.1100  +   0.0953   +   0.1256    =   -0.0839&#xD;&#xA;&#xD;&#xA;Odds ratio = Exponential(-0.0839) = 0.9195&#xD;&#xA;&#xD;&#xA;Hence probability = 0.9195 / (1 + 0.9195)  =   0.4790&#xD;&#xA;&#xD;&#xA;I am unable to understand why there is such a mismatch between the Model's predicted probability and theoretical probability. &#xD;&#xA;&#xD;&#xA;Any help would be much appreciated .&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/4Ih6o.png" />
  <row Id="963" PostHistoryTypeId="1" PostId="403" RevisionGUID="a01e7889-e1b8-4b5b-95ec-3cc4d96974d3" CreationDate="2014-06-16T13:30:01.320" UserId="880" Text="Help with Logistic Regression" />
  <row Id="964" PostHistoryTypeId="3" PostId="403" RevisionGUID="a01e7889-e1b8-4b5b-95ec-3cc4d96974d3" CreationDate="2014-06-16T13:30:01.320" UserId="880" Text="&lt;categorical-data&gt;" />
  <row Id="965" PostHistoryTypeId="6" PostId="384" RevisionGUID="4c1ae7a3-e860-4ddd-b3a3-841f4038c3ba" CreationDate="2014-06-16T14:02:42.467" UserId="97" Comment="Add tag" Text="&lt;classification&gt;&lt;binary&gt;&lt;svm&gt;&lt;random-forest&gt;&lt;logistic-regression&gt;" />
  <row Id="966" PostHistoryTypeId="2" PostId="404" RevisionGUID="5eba5c03-3606-433b-93de-ffa39d8627dc" CreationDate="2014-06-16T14:35:20.980" UserId="548" Text="Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you've started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization)&#xD;&#xA;&#xD;&#xA;It might seem strange, but I would honestly suggest `JSON`. It's extremely well supported, supports a lot of structure, and is flexible enough that you shouldn't have to move from it for not being powerful enough. For your example, something like this:&#xD;&#xA;&#xD;&#xA;    {'text': 'I saw the company's manager last day.&quot;, {'Person': {'name': 'John'}, {'indices': [0:1]}, etc...}&#xD;&#xA;&#xD;&#xA;The one big advantage you've got over any NLP-specific formats here is that `JSON` can be parsed in any environment, and since you'll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats.&#xD;&#xA;&#xD;&#xA;You can also implicitly store tokenization information if you want:&#xD;&#xA;&#xD;&#xA;    {&quot;text&quot;: [&quot;I&quot;, &quot;saw&quot;, &quot;the&quot;, &quot;company's&quot;, &quot;manager&quot;, &quot;last&quot;, &quot;day.&quot;]}&#xD;&#xA;&#xD;&#xA;" />
  <row Id="967" PostHistoryTypeId="2" PostId="405" RevisionGUID="14ef9f5c-ebc6-4b24-a0fe-595bc33503d9" CreationDate="2014-06-16T15:00:04.577" UserId="548" Text="Personally going to make a strong argument in favor of Python here. There are a large number of reasons for this, but I'm going to build on some of the points that other people have mentioned here:&#xD;&#xA;&#xD;&#xA; 1. **Picking a single language:** It's definitely possible to mix and match languages, picking `d3` for your visualization needs, `FORTRAN` for your fast matrix multiplies, and `python` for all of your networking and scripting. You can do this down the line, but keeping your stack as simple as possible is a good move, especially early on.&#xD;&#xA; 2. **Picking something bigger than you:** You never want to be pushing up against the barriers of the language you want to use. This is a huge issue when it comes to languages like `Julia` and `FORTRAN`, which simply don't offer the full functionality of languages like `python` or `R`.&#xD;&#xA; 3. **Pick Community**: The one most difficult thing to find in any language is community. `Python` is the clear winner here. If you get stuck, you ask something on SO, and someone will answer in a matter of minutes, which is simply not the case for most other languages. If you're learning something in a vacuum you will simply learn much slower.&#xD;&#xA;&#xD;&#xA;In terms of the minus points, I might actually push back on them.&#xD;&#xA;&#xD;&#xA;Deepening your knowledge of one language is a decent idea, but knowing *only* one language, without having practice generalizing that knowledge to other languages is a good way to shoot yourself in the foot. I have changed my entire favored development stack three time over as many years, moving from `MATLAB` to `Java` to `haskell` to `python`. Learning to transfer your knowledge to another language is far more valuable than just knowing one.&#xD;&#xA;&#xD;&#xA;As far as feasibility, this is something you're going to see again and again in any programming career. Turing completeness means you could technically do everything with `HTML4` and `CSS3`, but you want to pick the right tool for the job. If you see the ideal tool and decide to leave it by the roadside you're going to find yourself slowed down wishing you had some of the tools you left behind.&#xD;&#xA;&#xD;&#xA;A great example of that last point is trying to deploy `R` code. 'R''s networking capabilities are hugely lacking compared to `python`, and if you want to deploy a service, or use slightly off-the-beaten path packages, the fact that `pip` has an order of magnitude more packages than `CRAN` is a huge help." />
  <row Id="968" PostHistoryTypeId="2" PostId="406" RevisionGUID="2da099ac-2471-4f38-b940-b257ee15937d" CreationDate="2014-06-16T15:49:55.673" UserId="886" Text="If I have a retail store and have a way to measure how many people enter my store every minute, and timestamp that data, how can I predict future foot traffic?  &#xD;&#xA;&#xD;&#xA;I have looked into machine learning algorithms, but I'm not sure which one to use.  In my test data, a year over year trend is more accurate compared to other things I've tried, like KNN(with what I think are sensible parameters and distance function).&#xD;&#xA;&#xD;&#xA;It almost seems like this could be similar to financial modeling, where you deal with time series data.  Any ideas?&#xD;&#xA;&#xD;&#xA;Thanks." />
  <row Id="969" PostHistoryTypeId="1" PostId="406" RevisionGUID="2da099ac-2471-4f38-b940-b257ee15937d" CreationDate="2014-06-16T15:49:55.673" UserId="886" Text="How can I predict traffic based on previous time series data?" />
  <row Id="970" PostHistoryTypeId="3" PostId="406" RevisionGUID="2da099ac-2471-4f38-b940-b257ee15937d" CreationDate="2014-06-16T15:49:55.673" UserId="886" Text="&lt;machine-learning&gt;&lt;time-series&gt;" />
  <row Id="971" PostHistoryTypeId="5" PostId="404" RevisionGUID="9d38a416-2b6b-4f73-8b90-0e7368a6d4cb" CreationDate="2014-06-16T16:04:45.320" UserId="548" Comment="added 2 characters in body" Text="Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you've started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization)&#xD;&#xA;&#xD;&#xA;It might seem strange, but I would honestly suggest `JSON`. It's extremely well supported, supports a lot of structure, and is flexible enough that you shouldn't have to move from it for not being powerful enough. For your example, something like this:&#xD;&#xA;&#xD;&#xA;    {'text': 'I saw the company's manager last day.&quot;, {'Person': [{'name': 'John'}, {'indices': [0:1]}, etc...]}&#xD;&#xA;&#xD;&#xA;The one big advantage you've got over any NLP-specific formats here is that `JSON` can be parsed in any environment, and since you'll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats.&#xD;&#xA;&#xD;&#xA;You can also implicitly store tokenization information if you want:&#xD;&#xA;&#xD;&#xA;    {&quot;text&quot;: [&quot;I&quot;, &quot;saw&quot;, &quot;the&quot;, &quot;company's&quot;, &quot;manager&quot;, &quot;last&quot;, &quot;day.&quot;]}&#xD;&#xA;&#xD;&#xA;" />
  <row Id="972" PostHistoryTypeId="2" PostId="407" RevisionGUID="1c7a3a0f-47b7-46bf-9f95-7e25624a46c5" CreationDate="2014-06-16T16:10:34.423" UserId="227" Text="Other answers recommended a good set of books about the mathematics behind data science. But as you mentioned, its not just mathematics and activities like data collection and inference from data has their own rules and theories, even if not being as rigorous as mathematical backgrounds (yet).&#xD;&#xA;&#xD;&#xA;For theses parts, I suggest the book [Beautiful Data: The Stories Behind Elegant Data Solutions][1] which contains twenty case-study like chapters written by people really engaged with real world data analysis problems. It does not contain any mathematics, but explores areas like collecting data, finding practical ways of using data in analyses, scaling and selecting the best solutions very well.&#xD;&#xA;&#xD;&#xA;Another really interesting book is [Thinking with Data: How to Turn Information into Insights][2], which is not technical (=programming tutorial) either, but covers important topics on how to really use the data science power in decision making and real world problems.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.amazon.com/Beautiful-Data-Stories-Elegant-Solutions/dp/0596157118&#xD;&#xA;  [2]: http://www.amazon.com/Thinking-Data-Turn-Information-Insights/dp/1449362931" />
  <row Id="974" PostHistoryTypeId="2" PostId="408" RevisionGUID="a3ca1e61-13f4-4d8d-b1ba-adefdd09b7d9" CreationDate="2014-06-16T16:34:50.317" UserId="178" Text="The problem with models like KNN is that they do not take into account seasonality (time-dependent variations in trend).  To take those into account, you should use Time Series analysis.&#xD;&#xA;&#xD;&#xA;For count data, such as yours, you can use generalized linear auto-regressive moving average models (GLARMA).  Fortunately, there is an R package that implements them ([glarma](http://cran.r-project.org/web/packages/glarma/index.html)).  &#xD;&#xA;&#xD;&#xA;The [vignette](http://cran.r-project.org/web/packages/glarma/vignettes/glarma.pdf) is a good resource for the theory behind the tool." />
  <row Id="975" PostHistoryTypeId="2" PostId="409" RevisionGUID="ce2064b8-de4b-4021-b273-0b6f84594c08" CreationDate="2014-06-16T17:26:17.697" UserId="889" Text="I think Christopher's answers above are entirely sensible.  As an alternate approach (or perhaps just in addition to the advise he's given), I might start by just visualizing the data a bit to try get a rough sense of what's going on.&#xD;&#xA;&#xD;&#xA;If you haven't already done this, you might try adding a date's month and day of week as features -- if you end up sticking with KNN, this will help the model pick up seasonality. &#xD;&#xA;&#xD;&#xA;As a different way of taking this on, you might consider starting with a really, really basic model (like OLS).. these often go a long way in generating reasonable predictions.  &#xD;&#xA;&#xD;&#xA;Finally, the more we know about your data, the easier it will be for us to help generate suggestions -- What time frame are you observing?  What are the features you're currently using?  etc.&#xD;&#xA;&#xD;&#xA;Hope this helps --&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; " />
  <row Id="976" PostHistoryTypeId="5" PostId="404" RevisionGUID="d13c3c12-c88a-499e-a25a-30e6825a3f15" CreationDate="2014-06-16T17:35:41.207" UserId="548" Comment="added 2 characters in body" Text="Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you've started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization)&#xD;&#xA;&#xD;&#xA;It might seem strange, but I would honestly suggest `JSON`. It's extremely well supported, supports a lot of structure, and is flexible enough that you shouldn't have to move from it for not being powerful enough. For your example, something like this:&#xD;&#xA;&#xD;&#xA;    {'text': 'I saw the company's manager last day.&quot;, {'Person': [{'name': 'John'}, {'indices': [0:1]}, etc...]}&#xD;&#xA;&#xD;&#xA;The one big advantage you've got over any NLP-specific formats here is that `JSON` can be parsed in any environment, and since you'll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats.&#xD;&#xA;&#xD;&#xA;You can also implicitly store tokenization information if you want:&#xD;&#xA;&#xD;&#xA;    {&quot;text&quot;: [&quot;I&quot;, &quot;saw&quot;, &quot;the&quot;, &quot;company's&quot;, &quot;manager&quot;, &quot;last&quot;, &quot;day.&quot;]}&#xD;&#xA;&#xD;&#xA;EDIT: To clarify the mapping of metadata is pretty open, but here's an example:&#xD;&#xA;&#xD;&#xA;    {'body': '&lt;some_text&gt;',&#xD;&#xA;     'metadata': &#xD;&#xA;      {'&lt;entity&gt;':&#xD;&#xA;        {'&lt;attribute&gt;': '&lt;value&gt;',&#xD;&#xA;         'location': [&lt;start_index&gt;, &lt;end_index&gt;]&#xD;&#xA;        }&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;Hope that helps, let me know if you've got any more questions." />
  <row Id="977" PostHistoryTypeId="2" PostId="410" RevisionGUID="5961e11a-d20e-4c7a-a1b0-f26ca4236a41" CreationDate="2014-06-16T18:08:38.623" UserId="890" Text="I'm currently working on implementing sgd for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate. Is the learning rate related to the shape of the error gradient, as it dictates the rate of descent? If so, how do you use this information to inform your decision about a value? If it's not what sort of values should I choose, and how should I choose them. It seems like you would want small values to avoid overshooting, but how do you choose one such that you don't get stuck in local minima or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?&#xD;&#xA;&#xD;&#xA;tl:dr How do I choose the learning rate for sgd?" />
  <row Id="978" PostHistoryTypeId="1" PostId="410" RevisionGUID="5961e11a-d20e-4c7a-a1b0-f26ca4236a41" CreationDate="2014-06-16T18:08:38.623" UserId="890" Text="Choosing a learning rate" />
  <row Id="979" PostHistoryTypeId="3" PostId="410" RevisionGUID="5961e11a-d20e-4c7a-a1b0-f26ca4236a41" CreationDate="2014-06-16T18:08:38.623" UserId="890" Text="&lt;machine-learning&gt;&lt;algorithms&gt;&lt;neuralnetwork&gt;" />
  <row Id="980" PostHistoryTypeId="5" PostId="410" RevisionGUID="5ad02684-fef8-4da9-b245-704b151ffc02" CreationDate="2014-06-16T18:47:55.867" UserId="322" Comment="improved formatting" Text="I'm currently working on implementing Stochastic Gradient Descent (SGD) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.&#xD;&#xA;&#xD;&#xA;- Is the learning rate related to the shape of the error gradient, as it dictates the rate of descent?&#xD;&#xA;- If so, how do you use this information to inform your decision about a value?&#xD;&#xA;- If it's not what sort of values should I choose, and how should I choose them?&#xD;&#xA;- It seems like you would want small values to avoid overshooting, but how do you choose one such that you don't get stuck in local minima or take to long to descend?&#xD;&#xA;- Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?&#xD;&#xA;&#xD;&#xA;In short: How do I choose the learning rate for SGD?" />
  <row Id="981" PostHistoryTypeId="24" PostId="410" RevisionGUID="5ad02684-fef8-4da9-b245-704b151ffc02" CreationDate="2014-06-16T18:47:55.867" Comment="Proposed by 322 approved by 890 edit id of 67" />
  <row Id="982" PostHistoryTypeId="2" PostId="411" RevisionGUID="269c88c1-25d4-4d93-a9e9-942f96921aa1" CreationDate="2014-06-16T19:14:38.553" UserId="890" Text="It seems as though most languages have some number of scientific computing libraries available. &#xD;&#xA;&#xD;&#xA; - Python has `Scipy`&#xD;&#xA; - `Rust` has `SciRust`&#xD;&#xA; - `C++` has several including `ViennaCL` and `Armadillo`&#xD;&#xA; - `Java` has `Java Numerics` and `Colt` as well as several other&#xD;&#xA;&#xD;&#xA;Not to mention languages like `R` and `Julia` designed explicitly for scientific computing.&#xD;&#xA;&#xD;&#xA; With so many options how do you choose the best language for a task? Additionally which languages will be the most performant? `Python` and `R` seem to have the most traction in the space, but logically a compiled language seems like it would be a better choice. Additionally compiled languages tend to have GPU acceleration, while interpreted languages like `R` and `Python` don't. What should I take into account when choosing a language, and which languages provide the best balance of utility and performance? Also are there any languages with significant scientific computing resources that I've missed?" />
  <row Id="983" PostHistoryTypeId="1" PostId="411" RevisionGUID="269c88c1-25d4-4d93-a9e9-942f96921aa1" CreationDate="2014-06-16T19:14:38.553" UserId="890" Text="Best languages for scientific computing" />
  <row Id="984" PostHistoryTypeId="3" PostId="411" RevisionGUID="269c88c1-25d4-4d93-a9e9-942f96921aa1" CreationDate="2014-06-16T19:14:38.553" UserId="890" Text="&lt;efficiency&gt;&lt;statistics&gt;&lt;tools&gt;&lt;knowledge-base&gt;" />
  <row Id="985" PostHistoryTypeId="5" PostId="411" RevisionGUID="7cdb10f4-9778-4dff-a543-a3653571562c" CreationDate="2014-06-16T19:22:00.133" UserId="890" Comment="added 45 characters in body" Text="It seems as though most languages have some number of scientific computing libraries available. &#xD;&#xA;&#xD;&#xA; - Python has `Scipy`&#xD;&#xA; - `Rust` has `SciRust`&#xD;&#xA; - `C++` has several including `ViennaCL` and `Armadillo`&#xD;&#xA; - `Java` has `Java Numerics` and `Colt` as well as several other&#xD;&#xA;&#xD;&#xA;Not to mention languages like `R` and `Julia` designed explicitly for scientific computing.&#xD;&#xA;&#xD;&#xA; With so many options how do you choose the best language for a task? Additionally which languages will be the most performant? `Python` and `R` seem to have the most traction in the space, but logically a compiled language seems like it would be a better choice. And will anything ever outperform `Fortran`? Additionally compiled languages tend to have GPU acceleration, while interpreted languages like `R` and `Python` don't. What should I take into account when choosing a language, and which languages provide the best balance of utility and performance? Also are there any languages with significant scientific computing resources that I've missed?" />
  <row Id="986" PostHistoryTypeId="5" PostId="369" RevisionGUID="c4381bf7-5f8c-4a95-b03e-1fb7384d33f0" CreationDate="2014-06-16T19:30:46.940" UserId="84" Comment="Improving formatting." Text="What kind of error measures do RMSE and nDCG give while evaluating a recommender system, and how do I know when to use one over the other? If you could give an example of when to use each, that would be great as well!" />
  <row Id="987" PostHistoryTypeId="4" PostId="369" RevisionGUID="c4381bf7-5f8c-4a95-b03e-1fb7384d33f0" CreationDate="2014-06-16T19:30:46.940" UserId="84" Comment="Improving formatting." Text="Difference between using RMSE and nDCG to evaluate Recommender Systems" />
  <row Id="988" PostHistoryTypeId="6" PostId="369" RevisionGUID="c4381bf7-5f8c-4a95-b03e-1fb7384d33f0" CreationDate="2014-06-16T19:30:46.940" UserId="84" Comment="Improving formatting." Text="&lt;machine-learning&gt;&lt;recommendation&gt;&lt;evaluation&gt;" />
  <row Id="989" PostHistoryTypeId="24" PostId="369" RevisionGUID="c4381bf7-5f8c-4a95-b03e-1fb7384d33f0" CreationDate="2014-06-16T19:30:46.940" Comment="Proposed by 84 approved by 838 edit id of 63" />
  <row Id="990" PostHistoryTypeId="2" PostId="412" RevisionGUID="e9508066-e052-4951-8166-705859cce859" CreationDate="2014-06-16T19:48:31.797" UserId="322" Text="I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability.&#xD;&#xA;&#xD;&#xA;For example, one task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as &quot;Dave&quot; and in another as &quot;David,&quot; commercial entities can have many different abbreviations, and there are always some typos.&#xD;&#xA;&#xD;&#xA;In the past, what we have done is to completely remove PII if we need to transfer any data to a third party. But this means that the third party has really no idea of the relationships between entities. We would prefer to be able to pass along some information about those *relationships*, without divulging identity.&#xD;&#xA;&#xD;&#xA;For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) any PII. But the only method I am familiar with for doing this is ROT13, which hardly even counts as encryption; it's like writing the names upside down and saying, &quot;promise you won't flip the paper over?&quot;&#xD;&#xA;&#xD;&#xA;Another **bad** solution would be to abbreviate everything. &quot;Ellen Roberts&quot; becomes &quot;ER&quot; and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; &quot;Benjamin Othello Ames&quot; and &quot;Bank of America&quot; will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.&#xD;&#xA;&#xD;&#xA;Are there accepted methods for obscuring data like this, or is it effectively impossible?" />
  <row Id="991" PostHistoryTypeId="1" PostId="412" RevisionGUID="e9508066-e052-4951-8166-705859cce859" CreationDate="2014-06-16T19:48:31.797" UserId="322" Text="How can I transform names in a confidential data set to make it anonymous, but preserve some of the characteristics of the names?" />
  <row Id="992" PostHistoryTypeId="3" PostId="412" RevisionGUID="e9508066-e052-4951-8166-705859cce859" CreationDate="2014-06-16T19:48:31.797" UserId="322" Text="&lt;processing&gt;&lt;data-cleaning&gt;" />
  <row Id="993" PostHistoryTypeId="2" PostId="413" RevisionGUID="8bc41458-802b-4d88-8cc7-8c2c99bcf8b3" CreationDate="2014-06-16T19:49:24.377" UserId="890" Text="Dimensionality reduction is choosing a basis within which you can represent most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to `PCA`, `ICA`, and `Matrix Feature Factorization`. These will take existing data and reduce it to the most discriminative components. &#xD;&#xA;&#xD;&#xA;Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you're making, and which aren't. Generally feature selection is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset." />
  <row Id="994" PostHistoryTypeId="2" PostId="414" RevisionGUID="dce0519c-4c4d-4d33-90ff-6784261a42ad" CreationDate="2014-06-16T19:53:09.957" UserId="548" Text=" - **Is the learning rate related to the shape of the error gradient, as&#xD;&#xA;   it dictates the rate of descent?**&#xD;&#xA;  - In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.&#xD;&#xA;&#xD;&#xA; - **If so, how do you use this information to inform your decision about a value?**&#xD;&#xA;&#xD;&#xA;  - Adagrad is the most widely known of these and scales a global learning rate *η* on each dimension based on l2 norm of the history of the error gradient *gt* on each dimension:&#xD;&#xA;&#xD;&#xA;     ![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  - [Adadelta][2] is another such training algorithm which uses both a error gradient history like adagrad and the weight update history.&#xD;&#xA;&#xD;&#xA; - **If it's not what sort of values should I choose, and how should I choose them?**&#xD;&#xA;&#xD;&#xA;  - Setting learning rates for plain SGD in neural nets is usually a&#xD;&#xA;   process of starting with a sane value such as 0.01 and then doing cross validation&#xD;&#xA;   to find an optimal value. Typical values range over a few orders of&#xD;&#xA;   magnitude from 0.001 up to 1.&#xD;&#xA;&#xD;&#xA; - **It seems like you would want small values to avoid overshooting, but&#xD;&#xA;   how do you choose one such that you don't get stuck in local minima&#xD;&#xA;   or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?**&#xD;&#xA;&#xD;&#xA;  - Usually the value that's best is near the highest stable learning&#xD;&#xA;   rate and learning rate decay/annealing (either linear or&#xD;&#xA;   exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/aP96K.png&#xD;&#xA;  [2]: http://arxiv.org/pdf/1212.5701v1.pdf&#xD;&#xA;&#xD;&#xA;" />
  <row Id="995" PostHistoryTypeId="5" PostId="412" RevisionGUID="7de82803-3bfd-44a7-8ee8-c5fdedac4185" CreationDate="2014-06-16T19:54:07.673" UserId="322" Comment="added 140 characters in body" Text="I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability.&#xD;&#xA;&#xD;&#xA;For example, one task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as &quot;Dave&quot; and in another as &quot;David,&quot; commercial entities can have many different abbreviations, and there are always some typos.&#xD;&#xA;&#xD;&#xA;In the past, what we have done is to completely remove PII if we need to transfer any data to a third party. But this means that the third party has really no idea of the relationships between entities. We would prefer to be able to pass along some information about those *relationships*, without divulging identity.&#xD;&#xA;&#xD;&#xA;For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) any PII. But the only method I am familiar with for doing this is ROT13, which hardly even counts as encryption; it's like writing the names upside down and saying, &quot;promise you won't flip the paper over?&quot;&#xD;&#xA;&#xD;&#xA;Another **bad** solution would be to abbreviate everything. &quot;Ellen Roberts&quot; becomes &quot;ER&quot; and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; &quot;Benjamin Othello Ames&quot; and &quot;Bank of America&quot; will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.&#xD;&#xA;&#xD;&#xA;Are there accepted methods for obscuring data like this, or is it effectively impossible?&#xD;&#xA;&#xD;&#xA;I have found [at least one paper](http://www.merl.com/publications/docs/TR2010-109.pdf) that looks relevant but it's a bit over my head." />
  <row Id="996" PostHistoryTypeId="5" PostId="413" RevisionGUID="fdc9df93-7f8e-4f04-b62e-d9262f60abf8" CreationDate="2014-06-16T19:54:30.377" UserId="890" Comment="added 93 characters in body" Text="Dimensionality reduction is choosing a basis within which you can represent most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to `PCA`, `ICA`, and `Matrix Feature Factorization`. These will take existing data and reduce it to the most discriminative components.These all allow you to represent most of the information in your dataset with fewer, more discriminative features.&#xD;&#xA;&#xD;&#xA;Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you're making, and which aren't. Generally feature selection is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset." />
  <row Id="997" PostHistoryTypeId="2" PostId="415" RevisionGUID="b4ab88bb-5667-4ea9-8d07-041aace35449" CreationDate="2014-06-16T20:10:12.167" UserId="548" Text="This is a pretty massive question, so this is not intended to be a full answer, but hopefully this can help to inform general practice around determining the best tool for the job when it comes to data science. Generally, I have a relatively short list of qualifications I look for when it comes to any tool in this space. In no particular order they are:&#xD;&#xA;&#xD;&#xA; - **Performance**: Basically boils down to how quickly the language does matrix multiplication, as that is more or less the most important task in data science.&#xD;&#xA; - **Scalability**: At least for me personally, this comes down to ease of building a distributed system. This is somewhere where languages like `Julia` really shine.&#xD;&#xA; - **Community**: With any language, you're really looking for an active community that can help you when you get stuck using whichever tool you're using. This is where `python` pulls very far ahead of most other languages. &#xD;&#xA; - **Flexibility**: Nothing is worse than being limited by the language that you use. It doesn't happen very often, but trying to represent graph structures in `haskell` is a notorious pain, and `Julia` is filled with a lot of code architectures pains as a result of being such a young language.&#xD;&#xA; - **Ease of Use**: If you want to use something in a larger environment, you want to make sure that setup is a straightforward and it can be automated. Nothing is worse than having to set up a finnicky build on half a dozen machines.&#xD;&#xA;&#xD;&#xA;There are a ton of articles out there about performance and scalability, but in general you're going to be looking at a performance differential of maybe 5-10x between languages, which may or may not matter depending on your specific application. As far as GPU acceleration goes, `cudamat` is a really seamless way of getting it working with `python`, and the `cuda` library in general has made GPU acceleration far more accessible than it used to be.&#xD;&#xA;&#xD;&#xA;The two primary metrics I use for both community and flexibility are to look at the language's package manager, and the language questions on a site like SO. If there are a large number of high-quality questions and answers, it's a good sign that the community is active. Number of packages and the general activity on those packages can also be a good proxy for this metric.&#xD;&#xA;&#xD;&#xA;As far as ease of use goes, I am a firm believer that the only way to actually know is to actually set it up yourself. There's a lot of superstition around a lot of Data Science tools, specifically things like databases and distributed computing architecture, but there's no way to really know if something is easy or hard to setup up and deploy without just building it yourself.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="998" PostHistoryTypeId="2" PostId="416" RevisionGUID="ef5af01c-fdf2-4ad9-bbf0-d265bb2e3779" CreationDate="2014-06-16T20:12:39.130" UserId="381" Text="If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a [hash of the string](http://www.cse.yorku.ca/~oz/hash.html) that is used to represent all related records (e.g., David instead of Dave).&#xD;&#xA;&#xD;&#xA;I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them." />
  <row Id="999" PostHistoryTypeId="2" PostId="417" RevisionGUID="f2830f26-15b1-46b9-96a4-49859414e0ab" CreationDate="2014-06-16T20:18:50.123" UserId="414" Text="Blacklists aren't  have value for a number of reasons:&#xD;&#xA;&#xD;&#xA;1. They're easy to set up and scale - it's just a key/value store, and you can probably just re-use some of your caching logic for the most basic implementation.&#xD;&#xA;2. Depending on the size and type of the spam attack, there will probably be some very specific terms or URLs being used. It's much faster to throw that term into a blacklist than wait for your model to adapt.&#xD;&#xA;3. You can remove items just as quickly as you added them.&#xD;&#xA;4. Everybody understands how they work and any admin can use them.&#xD;&#xA;&#xD;&#xA;The key to fighting spam is *monitoring*. Make sure you have some sort of interface showing which items are on your blacklist, how often they've been hit in the last 10 minutes / hour / day / month, and the ability to easily add and remove items.&#xD;&#xA;&#xD;&#xA;You'll want to combine a number of different spam detection models and tactics. Neural nets seem to be a good suggestion, and I'd recommend looking at *user behavior patterns* in addition to just content. Normal humans don't do things like send batches of 1,000 emails every 30 seconds for 12 consecutive hours." />
  <row Id="1000" PostHistoryTypeId="5" PostId="413" RevisionGUID="77ea72d3-d0fe-4130-91bb-dc780df97a16" CreationDate="2014-06-16T21:03:21.820" UserId="890" Comment="added 30 characters in body" Text="Dimensionality reduction is typically choosing a basis or mathematical representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to `PCA`, `ICA`, and `Matrix Feature Factorization`. These will take existing data and reduce it to the most discriminative components.These all allow you to represent most of the information in your dataset with fewer, more discriminative features.&#xD;&#xA;&#xD;&#xA;Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you're making, and which aren't. Generally feature selection is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset." />
  <row Id="1001" PostHistoryTypeId="5" PostId="413" RevisionGUID="e23dc06d-cc0e-4f00-960f-259a82b5b9b4" CreationDate="2014-06-16T21:44:32.587" UserId="890" Comment="added 220 characters in body" Text="Dimensionality reduction is typically choosing a basis or mathematical representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to `PCA`, `ICA`, and `Matrix Feature Factorization`. These will take existing data and reduce it to the most discriminative components.These all allow you to represent most of the information in your dataset with fewer, more discriminative features.&#xD;&#xA;&#xD;&#xA;Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you're making, and which aren't. Feature extraction usually involves generating new features which are composites of existing features. Both of these techniques fall into the category of feature engineering. Generally feature engineering is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset, and increasing your signal to noise ratio." />
  <row Id="1002" PostHistoryTypeId="5" PostId="414" RevisionGUID="cd0d2747-3242-45c1-85b2-f1e087960750" CreationDate="2014-06-16T22:22:03.023" UserId="548" Comment="added 61 characters in body" Text=" - **Is the learning rate related to the shape of the error gradient, as&#xD;&#xA;   it dictates the rate of descent?**&#xD;&#xA;  - In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.&#xD;&#xA;&#xD;&#xA; - **If so, how do you use this information to inform your decision about a value?**&#xD;&#xA;&#xD;&#xA;  - Adagrad is the most widely known of these and scales a global learning rate *η* on each dimension based on l2 norm of the history of the error gradient *gt* on each dimension:&#xD;&#xA;&#xD;&#xA;     ![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  - [Adadelta][2] is another such training algorithm which uses both a error gradient history like adagrad and the weight update history and has the advantage of *not having a learning rate at all*.&#xD;&#xA;&#xD;&#xA; - **If it's not what sort of values should I choose, and how should I choose them?**&#xD;&#xA;&#xD;&#xA;  - Setting learning rates for plain SGD in neural nets is usually a&#xD;&#xA;   process of starting with a sane value such as 0.01 and then doing cross validation&#xD;&#xA;   to find an optimal value. Typical values range over a few orders of&#xD;&#xA;   magnitude from 0.001 up to 1.&#xD;&#xA;&#xD;&#xA; - **It seems like you would want small values to avoid overshooting, but&#xD;&#xA;   how do you choose one such that you don't get stuck in local minima&#xD;&#xA;   or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?**&#xD;&#xA;&#xD;&#xA;  - Usually the value that's best is near the highest stable learning&#xD;&#xA;   rate and learning rate decay/annealing (either linear or&#xD;&#xA;   exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/aP96K.png&#xD;&#xA;  [2]: http://arxiv.org/pdf/1212.5701v1.pdf&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1003" PostHistoryTypeId="5" PostId="414" RevisionGUID="163c158a-2f30-4d42-9473-cd8bb41fa32e" CreationDate="2014-06-17T00:05:01.337" UserId="548" Comment="added 9 characters in body" Text=" - **Is the learning rate related to the shape of the error gradient, as&#xD;&#xA;   it dictates the rate of descent?**&#xD;&#xA;  - In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.&#xD;&#xA;&#xD;&#xA; - **If so, how do you use this information to inform your decision about a value?**&#xD;&#xA;&#xD;&#xA;  - Adagrad is the most widely known of these and scales a global learning rate *η* on each dimension based on l2 norm of the history of the error gradient *gt* on each dimension:&#xD;&#xA;&#xD;&#xA;     ![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  - [Adadelta][2] is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of *not having to set a learning rate at all*.&#xD;&#xA;&#xD;&#xA; - **If it's not what sort of values should I choose, and how should I choose them?**&#xD;&#xA;&#xD;&#xA;  - Setting learning rates for plain SGD in neural nets is usually a&#xD;&#xA;   process of starting with a sane value such as 0.01 and then doing cross validation&#xD;&#xA;   to find an optimal value. Typical values range over a few orders of&#xD;&#xA;   magnitude from 0.001 up to 1.&#xD;&#xA;&#xD;&#xA; - **It seems like you would want small values to avoid overshooting, but&#xD;&#xA;   how do you choose one such that you don't get stuck in local minima&#xD;&#xA;   or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?**&#xD;&#xA;&#xD;&#xA;  - Usually the value that's best is near the highest stable learning&#xD;&#xA;   rate and learning rate decay/annealing (either linear or&#xD;&#xA;   exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/aP96K.png&#xD;&#xA;  [2]: http://arxiv.org/pdf/1212.5701v1.pdf&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1004" PostHistoryTypeId="2" PostId="418" RevisionGUID="1b8fe372-c9f0-4af5-a3c4-71128ac009cb" CreationDate="2014-06-17T00:16:24.287" UserId="900" Text="I would like to know what is the best way to classify a data set composed of mixed types of attributes, for example, textual and numerical. I know I can convert textual to boolean, but the vocabulary is diverse and data become too sparse. I also tried to classify the types of attributes separately and combine the results through meta-learning techniques, but it did not work well." />
  <row Id="1005" PostHistoryTypeId="1" PostId="418" RevisionGUID="1b8fe372-c9f0-4af5-a3c4-71128ac009cb" CreationDate="2014-06-17T00:16:24.287" UserId="900" Text="Best way to classify datasets with mixed types of attributes" />
  <row Id="1006" PostHistoryTypeId="3" PostId="418" RevisionGUID="1b8fe372-c9f0-4af5-a3c4-71128ac009cb" CreationDate="2014-06-17T00:16:24.287" UserId="900" Text="&lt;machine-learning&gt;&lt;classification&gt;" />
  <row Id="1007" PostHistoryTypeId="2" PostId="419" RevisionGUID="fd667c16-8bd0-42e7-91c7-32ec18d63d50" CreationDate="2014-06-17T00:19:09.773" UserId="901" Text="First you need to decide what you want to do, then look for the right tool for that task.&#xD;&#xA;&#xD;&#xA;A very general approach is to use R for first versions and to see if your approach is correct. It lacks a little in speed, but has very powerful commands and addon libraries, that you can try almost anything with it:&#xD;&#xA;http://www.r-project.org/&#xD;&#xA;&#xD;&#xA;The second idea is if you want to understand the algorithms behind the libraries, you might wanna take a look at the Numerical Recipies. They are available for different languages and free to use for learning. If you want to use them in commercial products, you need to ourchase a licence:&#xD;&#xA;http://en.wikipedia.org/wiki/Numerical_Recipes&#xD;&#xA;&#xD;&#xA;Most of the time performance will not be the issue but finding the right algorithms and parameters for them, so it is important to have a fast scripting language instead of a monster program that first needs to compile 10 mins before calculating two numbers and putting out the result.&#xD;&#xA;&#xD;&#xA;And a big plus in using R is that it has built-in functions or libraries for almost any kind of diagram you might wanna need to visualize your data.&#xD;&#xA;&#xD;&#xA;If you then have a working version, it is almost easy to port it to any other language you think is more performant." />
  <row Id="1008" PostHistoryTypeId="2" PostId="420" RevisionGUID="43e49b6e-9a03-465c-a80f-0d9320e8fdae" CreationDate="2014-06-17T00:39:15.990" UserId="178" Text="It is hard to answer this question without knowing more about the data.  That said, I would offer the following advice:&#xD;&#xA;&#xD;&#xA;Most machine learning techniques can handle mixed-type data.  Tree based methods (such as AdaBoost and Random Forests) do well with this type of data.  The more important issue is actually the dimensionality, about which you are correct to be concerned.&#xD;&#xA;&#xD;&#xA;I would suggest that you do something to reduce that dimensionality.  For example, look for the words or phrases that separate the data the best and discard the other words (note: tree based methods do this automatically)." />
  <row Id="1010" PostHistoryTypeId="5" PostId="414" RevisionGUID="7760ca4e-56e6-4e1b-aa1b-5423af4321b9" CreationDate="2014-06-17T01:36:22.877" UserId="548" Comment="added 1 character in body" Text=" - **Is the learning rate related to the shape of the error gradient, as&#xD;&#xA;   it dictates the rate of descent?**&#xD;&#xA;  - In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.&#xD;&#xA;&#xD;&#xA; - **If so, how do you use this information to inform your decision about a value?**&#xD;&#xA;&#xD;&#xA;  - Adagrad is the most widely known of these and scales a global learning rate *η* on each dimension based on l2 norm of the history of the error gradient *gt* on each dimension:&#xD;&#xA;&#xD;&#xA;     ![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  - [Adadelta][2] is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of *not having to set a learning rate at all*.&#xD;&#xA;&#xD;&#xA; - **If it's not what sort of values should I choose, and how should I choose them?**&#xD;&#xA;&#xD;&#xA;  - Setting learning rates for plain SGD in neural nets is usually a&#xD;&#xA;   process of starting with a sane value such as 0.01 and then doing cross validation&#xD;&#xA;   to find an optimal value. Typical values range over a few orders of&#xD;&#xA;   magnitude from 0.0001 up to 1.&#xD;&#xA;&#xD;&#xA; - **It seems like you would want small values to avoid overshooting, but&#xD;&#xA;   how do you choose one such that you don't get stuck in local minima&#xD;&#xA;   or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?**&#xD;&#xA;&#xD;&#xA;  - Usually the value that's best is near the highest stable learning&#xD;&#xA;   rate and learning rate decay/annealing (either linear or&#xD;&#xA;   exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/aP96K.png&#xD;&#xA;  [2]: http://arxiv.org/pdf/1212.5701v1.pdf&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1011" PostHistoryTypeId="2" PostId="421" RevisionGUID="49470a49-6a18-4531-868c-93627b4195b8" CreationDate="2014-06-17T04:31:34.067" UserId="88" Text="Does anyone know some good tutorials on online machine learning technics?&#xD;&#xA;I.e. how it can be used in real-time environments, what are key differences compared to normal machine learning methods etc." />
  <row Id="1012" PostHistoryTypeId="1" PostId="421" RevisionGUID="49470a49-6a18-4531-868c-93627b4195b8" CreationDate="2014-06-17T04:31:34.067" UserId="88" Text="Online machine learning tutorial" />
  <row Id="1013" PostHistoryTypeId="3" PostId="421" RevisionGUID="49470a49-6a18-4531-868c-93627b4195b8" CreationDate="2014-06-17T04:31:34.067" UserId="88" Text="&lt;machine-learning&gt;" />
  <row Id="1014" PostHistoryTypeId="2" PostId="422" RevisionGUID="d56eb39a-b5fb-41a2-a494-4fc9f6d475af" CreationDate="2014-06-17T05:29:11.830" UserId="84" Text="As an extension to our great list of [publicly available datasets](http://datascience.stackexchange.com/questions/155/publicly-available-datasets), I'd like to know if there is any list of publicly available social network datasets/crawling APIs. It would be very nice if alongside with a link to the dataset/API, characteristics of the data available were added. Such information should be, and is not limited to:&#xD;&#xA;&#xD;&#xA;- the name of the social network;&#xD;&#xA;- whether it allows for crawling its contents via an API or simply provides a given dataset;&#xD;&#xA;- what kind of user information it provides (posts, profile, friendship network, ...).&#xD;&#xA;&#xD;&#xA;Any suggestions and further characteristics to be added are very welcome." />
  <row Id="1015" PostHistoryTypeId="1" PostId="422" RevisionGUID="d56eb39a-b5fb-41a2-a494-4fc9f6d475af" CreationDate="2014-06-17T05:29:11.830" UserId="84" Text="Publicly available social network datasets/APIs" />
  <row Id="1016" PostHistoryTypeId="3" PostId="422" RevisionGUID="d56eb39a-b5fb-41a2-a494-4fc9f6d475af" CreationDate="2014-06-17T05:29:11.830" UserId="84" Text="&lt;databases&gt;&lt;open-source&gt;&lt;crawling&gt;" />
  <row Id="1017" PostHistoryTypeId="5" PostId="422" RevisionGUID="237f71ee-c091-429b-b849-8579fd8a2691" CreationDate="2014-06-17T05:34:40.857" UserId="84" Comment="added 62 characters in body" Text="As an extension to our great list of [publicly available datasets](http://datascience.stackexchange.com/questions/155/publicly-available-datasets), I'd like to know if there is any list of publicly available social network datasets/crawling APIs. It would be very nice if alongside with a link to the dataset/API, characteristics of the data available were added. Such information should be, and is not limited to:&#xD;&#xA;&#xD;&#xA;- the name of the social network;&#xD;&#xA;- what kind of user information it provides (posts, profile, friendship network, ...);&#xD;&#xA;- whether it allows for crawling its contents via an API (and rate: 10/min, 1k/month, ...);&#xD;&#xA;- whether it simply provides a snapshot of the whole dataset.&#xD;&#xA;&#xD;&#xA;Any suggestions and further characteristics to be added are very welcome." />
  <row Id="1018" PostHistoryTypeId="2" PostId="423" RevisionGUID="275fee36-3971-4e60-bc06-5051918748ff" CreationDate="2014-06-17T05:55:04.710" UserId="84" Text="I'm planning to run some experiments with very large datasets, and I'd like to distribute the computation. In fact, the main result of this experiment is to evaluate efficiency gains in comparison to previous proposals.&#xD;&#xA;&#xD;&#xA;I have about ten machines available, each with 200GB of free space on hard disk. However, I would like to perform experiments on a greater number of nodes, to measure scalability more precisely.&#xD;&#xA;&#xD;&#xA;Since I don't have more computers, I thought about using a commodity cluster, but I'm not sure about the policies of usage, and I need to reliably measure execution times. Are there commodity services which would grant me that only my application would be running at a given time? Has anyone used such services yet?" />
  <row Id="1019" PostHistoryTypeId="1" PostId="423" RevisionGUID="275fee36-3971-4e60-bc06-5051918748ff" CreationDate="2014-06-17T05:55:04.710" UserId="84" Text="Measuring execution time on commodity computing" />
  <row Id="1020" PostHistoryTypeId="3" PostId="423" RevisionGUID="275fee36-3971-4e60-bc06-5051918748ff" CreationDate="2014-06-17T05:55:04.710" UserId="84" Text="&lt;bigdata&gt;&lt;scalability&gt;&lt;distributed&gt;&lt;experiments&gt;&lt;commodity&gt;" />
  <row Id="1021" PostHistoryTypeId="2" PostId="424" RevisionGUID="cd373c2d-4904-40c4-9212-09daa495a7c6" CreationDate="2014-06-17T06:05:39.653" UserId="906" Text="I have recently seen this really cool feature (that was once) available in google sheets: you start by writing a few related keywords in consecutive cells, say: &quot;blue&quot;, &quot;green&quot;, &quot;yellow&quot;, and it automatically generates the next keywords on the same theme (in this case, other colors).  &#xD;&#xA;See more examples in this video: http://youtu.be/dlslNhfrQmw.&#xD;&#xA;&#xD;&#xA;I would like to reproduce this in my own program: I'm thinking of using Freebase, and it would work like this intuitively: &#xD;&#xA;&#xD;&#xA;1. retrieve the list of given words in Freebase&#xD;&#xA;2. find their &quot;common denominator(s)&quot; and construct a distance metric based on this&#xD;&#xA;3. rank other concepts based on their &quot;distance&quot; to the original keywords&#xD;&#xA;4. display the next closest concepts&#xD;&#xA;&#xD;&#xA;As I'm not familiar with this area, my questions are:&#xD;&#xA;&#xD;&#xA;* Is this correct, or is there a better way to do this ?&#xD;&#xA;* What's the best tool to execute each step of the process ?" />
  <row Id="1022" PostHistoryTypeId="1" PostId="424" RevisionGUID="cd373c2d-4904-40c4-9212-09daa495a7c6" CreationDate="2014-06-17T06:05:39.653" UserId="906" Text="How it works: growing a list of related objects based on initial keywords?" />
  <row Id="1023" PostHistoryTypeId="3" PostId="424" RevisionGUID="cd373c2d-4904-40c4-9212-09daa495a7c6" CreationDate="2014-06-17T06:05:39.653" UserId="906" Text="&lt;nlp&gt;&lt;text-mining&gt;&lt;freebase&gt;" />
  <row Id="1024" PostHistoryTypeId="2" PostId="425" RevisionGUID="8669ef53-7beb-476f-ae63-e06bb1170c60" CreationDate="2014-06-17T06:32:22.883" UserId="791" Text="An example from germany: Xing a site similar to linkedin but limited to german speaking countries.&#xD;&#xA;&#xD;&#xA;Link to it's developer central: https://dev.xing.com/overview&#xD;&#xA;&#xD;&#xA;Provides access to: User profiles, Conversations between users (limited to the user itself), Job advertisings, Contacts and Contacts of Contacts, news from the network and some geolocation api.&#xD;&#xA;&#xD;&#xA;Yes it has an api, but I did not find information about the rate. But it seems to me, that some information is limited to the consent of the user.&#xD;&#xA;" />
  <row Id="1025" PostHistoryTypeId="2" PostId="426" RevisionGUID="87baac61-2a50-4807-8a24-b644d8052e29" CreationDate="2014-06-17T06:53:44.007" UserId="122" Text="There are plenty on youtube and here's a famous one by Andrew Ng from coursea: https://www.coursera.org/course/ml" />
  <row Id="1026" PostHistoryTypeId="2" PostId="427" RevisionGUID="0c99ef0e-7cda-4223-bbcb-6fbc84bef228" CreationDate="2014-06-17T07:46:39.783" UserId="908" Text="Are anyone using `Julia` ([http://julialang.org/][1]) for professional jobs?&#xD;&#xA;&#xD;&#xA;Or use it instead of R , Matlab or Mathematica? &#xD;&#xA;&#xD;&#xA;Is it a good language?&#xD;&#xA;&#xD;&#xA;If you have to predict next 5-10 years: Do you think it grow up enough to became such a standard in datascience like R or similar?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://julialang.org/" />
  <row Id="1027" PostHistoryTypeId="1" PostId="427" RevisionGUID="0c99ef0e-7cda-4223-bbcb-6fbc84bef228" CreationDate="2014-06-17T07:46:39.783" UserId="908" Text="does anyone use Julia programming language?" />
  <row Id="1028" PostHistoryTypeId="3" PostId="427" RevisionGUID="0c99ef0e-7cda-4223-bbcb-6fbc84bef228" CreationDate="2014-06-17T07:46:39.783" UserId="908" Text="&lt;tools&gt;" />
  <row Id="1029" PostHistoryTypeId="4" PostId="424" RevisionGUID="0570e571-34e0-4d60-bf15-aeea79df5f45" CreationDate="2014-06-17T08:46:10.590" UserId="906" Comment="edited title" Text="How to grow a list of related words based on initial keywords?" />
  <row Id="1030" PostHistoryTypeId="2" PostId="428" RevisionGUID="2bd84a8e-2b4e-4537-9cc7-67b0ecda0620" CreationDate="2014-06-17T09:33:37.230" UserId="906" Text="This critique is no longer justified:&#xD;&#xA;&#xD;&#xA;While it is true that most of the standard and most respected R libraries were restricted to in-memory computations, there is a growing number of specialized libraries to deal with data that doesn't fit into memory.  &#xD;&#xA;For instance, for random forests on large datasets, you have the library `bigrf`. More info here: http://cran.r-project.org/web/packages/bigrf/&#xD;&#xA;&#xD;&#xA;Another area of growth is R's connectedness to big data environments like hadoop, which opens another world of possibilities." />
  <row Id="1031" PostHistoryTypeId="2" PostId="429" RevisionGUID="32e9693c-9c0b-4c94-b3fc-c419d94a807f" CreationDate="2014-06-17T09:56:49.180" UserId="846" Text="It's not a social network per se, but Stackexchange publish their entire database dump periodically:&#xD;&#xA;&#xD;&#xA; - [Stackexchange data dump hosted on the archive.org][1]&#xD;&#xA; - [Post describing the database dump schema][2]&#xD;&#xA;&#xD;&#xA;You can extract some social information by analyzing which users ask and answer to each other. One nice thing is that since posts are tagged, you can analyze sub-communities easily.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://archive.org/details/stackexchange&#xD;&#xA;  [2]: http://meta.stackoverflow.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede" />
  <row Id="1032" PostHistoryTypeId="2" PostId="430" RevisionGUID="91b9254e-33c4-453b-98f5-9fd971c92e2a" CreationDate="2014-06-17T10:37:22.987" UserId="913" Text="I'm trying to understand how all the &quot;big data&quot; components play together in a real world use case, e.g. hadoop, monogodb/nosql, storm, kafka, ... I know that this is quite a wide range of tools used for different types, but I'd like to get to know more about their interaction in applications, e.g. thinking machine learning for an app, webapp, online shop: I have vistors/session, transaction data etc and store that; but if I want to make live on the fly recommendations, I can't run a slow map/reduce jobs for that on some big database of logs I have.&#xD;&#xA;&#xD;&#xA;Where can I learn more about the infrastructure aspects? I think I can use most of the tools on their own, but plugging them into each other seems to be an art of its own.&#xD;&#xA;Are there any public examples/use cases etc available?&#xD;&#xA;I understand that the individual pipelines strongly depend on the use case and the user, but just examples will probably be very useful to me.&#xD;&#xA;Thanks a lot!" />
  <row Id="1033" PostHistoryTypeId="1" PostId="430" RevisionGUID="91b9254e-33c4-453b-98f5-9fd971c92e2a" CreationDate="2014-06-17T10:37:22.987" UserId="913" Text="Looking for example infrastructure stacks/workflows/pipelines" />
  <row Id="1034" PostHistoryTypeId="3" PostId="430" RevisionGUID="91b9254e-33c4-453b-98f5-9fd971c92e2a" CreationDate="2014-06-17T10:37:22.987" UserId="913" Text="&lt;machine-learning&gt;&lt;bigdata&gt;" />
  <row Id="1035" PostHistoryTypeId="2" PostId="431" RevisionGUID="1bf752a2-cb7f-42dc-8fd0-231aa3e45af7" CreationDate="2014-06-17T12:37:17.150" UserId="743" Text="A small collection of such links can be found at [here][1]. Many of them are social graphs.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://lgylym.github.io/big-graph/dataset.html" />
  <row Id="1036" PostHistoryTypeId="2" PostId="432" RevisionGUID="56264721-5cf8-40fc-b78c-9f7d618c94a1" CreationDate="2014-06-17T12:41:58.900" UserId="241" Text="1. There is a very nice library of online machine learning algorithms from a group at NTU, called [`LIBOL`][1]. This would be a very good place to start experimenting with the algorithms.  &#xD;&#xA;The [accompanying user guide][3], and [associated JMLR publication][2] are very nice introductions to the basic algorithms in this field.  &#xD;&#xA;2. Avrim Blum has an older and more technical [survey paper][4] on online learning algorithms. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cais.ntu.edu.sg/~chhoi/libol/&#xD;&#xA;  [2]: http://jmlr.org/papers/v15/hoi14a.html&#xD;&#xA;  [3]: http://www.cais.ntu.edu.sg/~chhoi/libol/LIBOL_manual.pdf&#xD;&#xA;  [4]: http://www.cs.cmu.edu/~avrim/Papers/survey.pdf" />
  <row Id="1037" PostHistoryTypeId="2" PostId="433" RevisionGUID="7de04bcd-5c14-42fa-bfdc-af14e142cc5c" CreationDate="2014-06-17T13:31:30.503" UserId="241" Text="One of the most detailed and clear explanations of setting up a complex analytics pipeline is from the folks over at [Twitch][1].  &#xD;&#xA;They give detailed motivations of each of the architecture choices for collection, transportation, coordination, processing, storage, and querying their data.  &#xD;&#xA;Compelling reading! Find it [here][2] and [here][3]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.twitch.tv/&#xD;&#xA;  [2]: http://blog.twitch.tv/2014/04/twitch-data-analysis-part-1-the-twitch-statistics-pipeline/&#xD;&#xA;  [3]: http://blog.twitch.tv/2014/04/twitch-data-analysis-part-2-architectural-decisions/" />
  <row Id="1038" PostHistoryTypeId="5" PostId="430" RevisionGUID="c5f04580-b816-4bfc-8679-2b919342f4b0" CreationDate="2014-06-17T13:37:47.400" UserId="84" Comment="Improving formatting." Text="I'm trying to understand how all the &quot;big data&quot; components play together in a real world use case, e.g. hadoop, monogodb/nosql, storm, kafka, ... I know that this is quite a wide range of tools used for different types, but I'd like to get to know more about their interaction in applications, e.g. thinking machine learning for an app, webapp, online shop.&#xD;&#xA;&#xD;&#xA;I have vistors/session, transaction data etc and store that; but if I want to make recommendations on the fly, I can't run slow map/reduce jobs for that on some big database of logs I have. Where can I learn more about the infrastructure aspects? I think I can use most of the tools on their own, but plugging them into each other seems to be an art of its own.&#xD;&#xA;&#xD;&#xA;Are there any public examples/use cases etc available? I understand that the individual pipelines strongly depend on the use case and the user, but just examples will probably be very useful to me." />
  <row Id="1039" PostHistoryTypeId="6" PostId="430" RevisionGUID="c5f04580-b816-4bfc-8679-2b919342f4b0" CreationDate="2014-06-17T13:37:47.400" UserId="84" Comment="Improving formatting." Text="&lt;machine-learning&gt;&lt;bigdata&gt;&lt;efficiency&gt;&lt;scalability&gt;&lt;distributed&gt;" />
  <row Id="1040" PostHistoryTypeId="24" PostId="430" RevisionGUID="c5f04580-b816-4bfc-8679-2b919342f4b0" CreationDate="2014-06-17T13:37:47.400" Comment="Proposed by 84 approved by 913 edit id of 72" />
  <row Id="1041" PostHistoryTypeId="2" PostId="434" RevisionGUID="b5545b8d-e2f8-4ab2-87a8-1a85726c970b" CreationDate="2014-06-17T13:44:43.807" UserId="515" Text="There is really no question here as you ask for pure conjectures but consider at least that &#xD;&#xA;&#xD;&#xA;* this week has [Julia Con](http://juliacon.org/), the first Julia conference&#xD;&#xA;* you could search GitHub and/or the registered Julia modules&#xD;&#xA;" />
  <row Id="1042" PostHistoryTypeId="2" PostId="435" RevisionGUID="698ab671-65b3-4724-97e1-064bd097681e" CreationDate="2014-06-17T13:46:06.367" UserId="917" Text="I have an huge dataset from a relational database which I need to create a classification model for. Normally for this situation I would use ILP but due to special circumstances I can't do that.&#xD;&#xA;&#xD;&#xA;The other way to tackle this would be just to try to aggregate the values when I have a foreign relations however I have thousands of important and distinct rows for some nominal attributes (Ex: A patient with a relation to several distinct drug prescriptions) in which I just can't do that without creating a new attributes for each distinct row of that nominal attribute and furthermore most of the new columns would have NULL values if I do that.&#xD;&#xA;&#xD;&#xA;Is there any non-ILP algorithm that allows me to data mine relational databases without resort to technique like pivoting which would create thousands of new columns?" />
  <row Id="1043" PostHistoryTypeId="1" PostId="435" RevisionGUID="698ab671-65b3-4724-97e1-064bd097681e" CreationDate="2014-06-17T13:46:06.367" UserId="917" Text="Relational Data Mining without ILP" />
  <row Id="1044" PostHistoryTypeId="3" PostId="435" RevisionGUID="698ab671-65b3-4724-97e1-064bd097681e" CreationDate="2014-06-17T13:46:06.367" UserId="917" Text="&lt;data-mining&gt;&lt;classification&gt;&lt;relational-dbms&gt;" />
  <row Id="1045" PostHistoryTypeId="5" PostId="435" RevisionGUID="a52923f7-2b70-4ffe-85ac-f88a307ee9e2" CreationDate="2014-06-17T14:26:40.380" UserId="84" Comment="Improving formatting." Text="I have a huge dataset from a relational database which I need to create a classification model for. Normally for this situation I would use [Inductive Logic Programming](http://en.wikipedia.org/wiki/Inductive_logic_programming) (ILP), but due to special circumstances I can't do that.&#xD;&#xA;&#xD;&#xA;The other way to tackle this would be just to try to aggregate the values when I have a foreign relation. However, I have thousands of important and distinct rows for some nominal attributes (e.g.: A patient with a relation to several distinct drug prescriptions). So, I just can't do that without creating a new attribute for each distinct row of that nominal attribute, and furthermore most of the new columns would have NULL values if I do that.&#xD;&#xA;&#xD;&#xA;Is there any non-ILP algorithm that allows me to data mine relational databases without resorting to techniques like pivoting, which would create thousands of new columns?" />
  <row Id="1046" PostHistoryTypeId="24" PostId="435" RevisionGUID="a52923f7-2b70-4ffe-85ac-f88a307ee9e2" CreationDate="2014-06-17T14:26:40.380" Comment="Proposed by 84 approved by 917 edit id of 73" />
  <row Id="1047" PostHistoryTypeId="2" PostId="436" RevisionGUID="13eba611-8b71-451f-8dde-419f0d29dc9f" CreationDate="2014-06-17T14:55:47.103" UserId="548" Text="I personally have used `Julia` for a good number of professional projects, and while, as Dirk mentioned, this is purely conjecture, I can give some insights on where Julia really stands out. The question of whether or not these reasons will prove enough to have `Julia` succeed as a language is anyone's guess.&#xD;&#xA;&#xD;&#xA; - **Distributed Systems**: Julia is the easiest language I've ever dealt with in terms of building distributed systems. This is becoming more and more relevant in computing, and will potentially become a deciding factor, but the question of whether or not `Julia`'a relative ease decides this is up for debate&#xD;&#xA; - **JIT Performance**: Julia's JIT compiler is extremely fast, and while there is a lot of debate as to how accurate these benchmark numbers are, the [Julia Website][1] shows a series of relevant benchmarks&#xD;&#xA; - **Community**: This is an area where `Julia` just isn't quite there. The community that is there is generally supportive, but not quite as knowledgable as the `R` or `python` communities, which is a definite minus.&#xD;&#xA; - **Extensibility**: This is another place where `Julia` is currently lacking, there is a large disconnect between the implies code patterns that `Julia` steers you toward and what it can actually support. The type system is currently overly bulky and difficult to use effectively.&#xD;&#xA;&#xD;&#xA;Again, can't say what this means for the future, but these are just a couple of relevant points when it comes to evaluating `Julia` in my opinion.&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://julialang.org/" />
  <row Id="1048" PostHistoryTypeId="6" PostId="412" RevisionGUID="34827082-10b5-45a5-88b0-86fc1d521aed" CreationDate="2014-06-17T15:19:24.040" UserId="381" Comment="added some tags" Text="&lt;data-cleaning&gt;&lt;privacy&gt;&lt;anonymization&gt;" />
  <row Id="1049" PostHistoryTypeId="24" PostId="412" RevisionGUID="34827082-10b5-45a5-88b0-86fc1d521aed" CreationDate="2014-06-17T15:19:24.040" Comment="Proposed by 381 approved by 322 edit id of 68" />
  <row Id="1050" PostHistoryTypeId="6" PostId="421" RevisionGUID="058b159f-e186-4b5e-996c-d62ffaaccd7e" CreationDate="2014-06-17T16:17:09.043" UserId="97" Comment="Adding more relevant tags" Text="&lt;machine-learning&gt;&lt;education&gt;&lt;beginner&gt;" />
  <row Id="1051" PostHistoryTypeId="24" PostId="421" RevisionGUID="058b159f-e186-4b5e-996c-d62ffaaccd7e" CreationDate="2014-06-17T16:17:09.043" Comment="Proposed by 97 approved by 50 edit id of 71" />
  <row Id="1052" PostHistoryTypeId="6" PostId="14" RevisionGUID="01941218-7f3d-413e-b72e-288f960f38a7" CreationDate="2014-06-17T16:17:20.473" UserId="322" Comment="see http://meta.datascience.stackexchange.com/a/18/322" Text="&lt;data-mining&gt;&lt;definitions&gt;" />
  <row Id="1053" PostHistoryTypeId="24" PostId="14" RevisionGUID="01941218-7f3d-413e-b72e-288f960f38a7" CreationDate="2014-06-17T16:17:20.473" Comment="Proposed by 322 approved by 50 edit id of 69" />
  <row Id="1054" PostHistoryTypeId="6" PostId="403" RevisionGUID="17bf7c97-819f-4f8b-8d36-438c85a013e2" CreationDate="2014-06-17T16:17:36.433" UserId="97" Comment="Adding tag." Text="&lt;categorical-data&gt;&lt;logistic-regression&gt;" />
  <row Id="1055" PostHistoryTypeId="24" PostId="403" RevisionGUID="17bf7c97-819f-4f8b-8d36-438c85a013e2" CreationDate="2014-06-17T16:17:36.433" Comment="Proposed by 97 approved by 50 edit id of 66" />
  <row Id="1056" PostHistoryTypeId="5" PostId="406" RevisionGUID="ae6a1ede-42c8-4104-9869-b14445c26049" CreationDate="2014-06-17T16:17:46.027" UserId="84" Comment="Improving formatting." Text="If I have a retail store and have a way to measure how many people enter my store every minute, and timestamp that data, how can I predict future foot traffic?  &#xD;&#xA;&#xD;&#xA;I have looked into machine learning algorithms, but I'm not sure which one to use. In my test data, a year over year trend is more accurate compared to other things I've tried, like KNN (with what I think are sensible parameters and distance function).&#xD;&#xA;&#xD;&#xA;It almost seems like this could be similar to financial modeling, where you deal with time series data. Any ideas?" />
  <row Id="1057" PostHistoryTypeId="24" PostId="406" RevisionGUID="ae6a1ede-42c8-4104-9869-b14445c26049" CreationDate="2014-06-17T16:17:46.027" Comment="Proposed by 84 approved by 50 edit id of 75" />
  <row Id="1058" PostHistoryTypeId="5" PostId="189" RevisionGUID="12ca9902-e0f3-4e22-88d2-9040eb9f017b" CreationDate="2014-06-17T16:18:02.437" UserId="84" Comment="Improving formatting." Text="I'm currently using [General Algebraic Modeling System](http://en.wikipedia.org/wiki/General_Algebraic_Modeling_System) (GAMS), and more specifically CPLEX within GAMS, to solve a very large mixed integer programming problem. This allows me to parallelize the process over 4 cores (although I have more, CPLEX utilizes a maximum of 4 cores), and it finds an optimal solution in a relatively short amount of time.&#xD;&#xA;&#xD;&#xA;Is there an open source mixed integer programming tool that I could use as an alternative to GAMS and CPLEX? It must be comparable in speed or faster for me to consider it. I have a preference for R based solutions, but I'm open to suggestions of all kinds, and other users may be interested in different solutions." />
  <row Id="1059" PostHistoryTypeId="24" PostId="189" RevisionGUID="12ca9902-e0f3-4e22-88d2-9040eb9f017b" CreationDate="2014-06-17T16:18:02.437" Comment="Proposed by 84 approved by 50 edit id of 74" />
  <row Id="1060" PostHistoryTypeId="5" PostId="370" RevisionGUID="205c359b-e2a7-4b73-9ad6-d12622aed705" CreationDate="2014-06-17T16:18:05.737" UserId="84" Comment="Improving question." Text="I'd like to use my MSc thesis as an opportunity to explore 'data science'. Frankly, the term seems a little vague to me (or at least, I've heard so many people apply it to so many situations that it's become diluted), but I expect it to require:&#xD;&#xA;&#xD;&#xA;1. machine learning (rather than traditional statistics);&#xD;&#xA;2. a large enough dataset that you have to run analyses on clusters.&#xD;&#xA;&#xD;&#xA;Anyway, we don't have any relevantly qualified professors at my college, so I'd like a dataset and problem that is accessible to a statistician. But I also want it to allow me to foray into this data science thing.&#xD;&#xA;&#xD;&#xA;Any suggestions? To keep this as narrow as possible, I'd ideally like links to open, well used datasets and example problems.&#xD;&#xA;&#xD;&#xA;P.S.: I have an engineering background, so I'm fairly comfy with math/programming." />
  <row Id="1061" PostHistoryTypeId="24" PostId="370" RevisionGUID="205c359b-e2a7-4b73-9ad6-d12622aed705" CreationDate="2014-06-17T16:18:05.737" Comment="Proposed by 84 approved by 50 edit id of 64" />
  <row Id="1062" PostHistoryTypeId="10" PostId="427" RevisionGUID="2abe6b2f-eb97-4e73-8fe6-987f710010a7" CreationDate="2014-06-17T16:26:05.453" UserId="50" Comment="103" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:84,&quot;DisplayName&quot;:&quot;Rubens&quot;},{&quot;Id&quot;:50,&quot;DisplayName&quot;:&quot;Robert Cartaino&quot;}]}" />
  <row Id="1063" PostHistoryTypeId="5" PostId="412" RevisionGUID="167f2061-e2b5-4d09-bf1f-03a7aa3825b7" CreationDate="2014-06-17T16:53:59.450" UserId="322" Comment="added 2369 characters in body" Text="Motivation&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract. &#xD;&#xA;&#xD;&#xA;This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party's ability to perform QA/QC, adjust parameters or make refinements may be very limited.&#xD;&#xA;&#xD;&#xA;Anonymizing Confidential Data&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as &quot;Dave&quot; and in another as &quot;David,&quot; commercial entities can have many different abbreviations, and there are always some typos. I've developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID. &#xD;&#xA;&#xD;&#xA;At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.&#xD;&#xA;&#xD;&#xA;What Doesn't Work&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance &lt;= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance &lt;= 1.&#xD;&#xA;&#xD;&#xA;But the only method I am familiar with that does this is ROT13, which hardly even counts as encryption; it's like writing the names upside down and saying, &quot;promise you won't flip the paper over?&quot;&#xD;&#xA;&#xD;&#xA;Another **bad** solution would be to abbreviate everything. &quot;Ellen Roberts&quot; becomes &quot;ER&quot; and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; &quot;Benjamin Othello Ames&quot; and &quot;Bank of America&quot; will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.&#xD;&#xA;&#xD;&#xA;An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:&#xD;&#xA;&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | Row | ID | Name              | WordChars | Origin |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 1   | 17 | &quot;AMELIA BEDELIA&quot;  | (6, 7)    | Eng    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 2   | 18 | &quot;CHRISTOPH BAUER&quot; | (9, 5)    | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 3   | 18 | &quot;C J BAUER&quot;       | (1, 1, 5) | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 4   | 19 | &quot;FRANZ HELLER&quot;    | (5, 6)    | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;&#xD;&#xA;I call this &quot;inelegant&quot; because it requires anticipating which qualities might be interesting and it's relatively coarse. If the names are removed, there's not much you can reasonably conclude about the strength of the match between rows 2 &amp; 3, or about the distance between rows 2 &amp; 4 (i.e., how close they are to matching).&#xD;&#xA;&#xD;&#xA;Conclusion&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;The goal is to transform strings in such a way that as many useful qualities of the original string are preserves as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.&#xD;&#xA;&#xD;&#xA;I have found [at least one paper](http://www.merl.com/publications/docs/TR2010-109.pdf) that looks relevant but it's a bit over my head." />
  <row Id="1064" PostHistoryTypeId="2" PostId="437" RevisionGUID="d35fe905-6e75-4066-b672-6840fa14ad74" CreationDate="2014-06-17T18:13:46.230" UserId="199" Text="I think that Bootstrap can be useful in my work, where we have a lot a variables that we don't know the distribution of it. So, simulations could help.&#xD;&#xA;What are good sources to learn about Bootstrap/other useful simulation methods?" />
  <row Id="1065" PostHistoryTypeId="1" PostId="437" RevisionGUID="d35fe905-6e75-4066-b672-6840fa14ad74" CreationDate="2014-06-17T18:13:46.230" UserId="199" Text="What are good sources to learn about Bootstrap?" />
  <row Id="1066" PostHistoryTypeId="3" PostId="437" RevisionGUID="d35fe905-6e75-4066-b672-6840fa14ad74" CreationDate="2014-06-17T18:13:46.230" UserId="199" Text="&lt;data-mining&gt;&lt;statistics&gt;&lt;education&gt;" />
  <row Id="1067" PostHistoryTypeId="5" PostId="416" RevisionGUID="34437474-ce51-47df-89db-ab10fa3724d1" CreationDate="2014-06-17T18:16:29.693" UserId="381" Comment="added 537 characters in body" Text="If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a [hash of the string](http://www.cse.yorku.ca/~oz/hash.html) that is used to represent all related records (e.g., David instead of Dave).&#xD;&#xA;&#xD;&#xA;I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them.&#xD;&#xA;&#xD;&#xA;edit: You need to define and justify what kind of operations the third party needs to be able to do. For example, what is wrong with using initials followed by a number (e.g., BOA-1, BOA-2, etc.) to disambiguate Bank of America from Benjamin Othello Ames? If that's too revealing, you could bin some of the letters or names; e.g., [A-E] -&gt; 1, [F-J] -&gt; 2, etc. so BOA would 1OA, or [&quot;Bank&quot;, &quot;Barry&quot;, &quot;Bruce&quot;, etc.] -&gt; 1 so Bank of America is again 1OA. For more information see [k-anonymity](http://en.wikipedia.org/wiki/K-anonymity)." />
  <row Id="1071" PostHistoryTypeId="5" PostId="412" RevisionGUID="180de4c0-26c0-4022-bf7a-fb6f1ef6d856" CreationDate="2014-06-17T18:19:23.760" UserId="322" Comment="edited body" Text="Motivation&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract. &#xD;&#xA;&#xD;&#xA;This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party's ability to perform QA/QC, adjust parameters or make refinements may be very limited.&#xD;&#xA;&#xD;&#xA;Anonymizing Confidential Data&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as &quot;Dave&quot; and in another as &quot;David,&quot; commercial entities can have many different abbreviations, and there are always some typos. I've developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID. &#xD;&#xA;&#xD;&#xA;At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.&#xD;&#xA;&#xD;&#xA;What Doesn't Work&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance &lt;= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance &lt;= 1.&#xD;&#xA;&#xD;&#xA;But the only method I am familiar with that does this is ROT13, which hardly even counts as encryption; it's like writing the names upside down and saying, &quot;promise you won't flip the paper over?&quot;&#xD;&#xA;&#xD;&#xA;Another **bad** solution would be to abbreviate everything. &quot;Ellen Roberts&quot; becomes &quot;ER&quot; and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; &quot;Benjamin Othello Ames&quot; and &quot;Bank of America&quot; will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.&#xD;&#xA;&#xD;&#xA;An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:&#xD;&#xA;&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | Row | ID | Name              | WordChars | Origin |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 1   | 17 | &quot;AMELIA BEDELIA&quot;  | (6, 7)    | Eng    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 2   | 18 | &quot;CHRISTOPH BAUER&quot; | (9, 5)    | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 3   | 18 | &quot;C J BAUER&quot;       | (1, 1, 5) | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 4   | 19 | &quot;FRANZ HELLER&quot;    | (5, 6)    | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;&#xD;&#xA;I call this &quot;inelegant&quot; because it requires anticipating which qualities might be interesting and it's relatively coarse. If the names are removed, there's not much you can reasonably conclude about the strength of the match between rows 2 &amp; 3, or about the distance between rows 2 &amp; 4 (i.e., how close they are to matching).&#xD;&#xA;&#xD;&#xA;Conclusion&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;The goal is to transform strings in such a way that as many useful qualities of the original string are preserved as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.&#xD;&#xA;&#xD;&#xA;I have found [at least one paper](http://www.merl.com/publications/docs/TR2010-109.pdf) that looks relevant but it's a bit over my head." />
  <row Id="1072" PostHistoryTypeId="2" PostId="439" RevisionGUID="a4d101b1-f7c8-4b5a-820c-54ba76c0aa62" CreationDate="2014-06-17T18:42:55.423" UserId="780" Text="Halfway through reading your question, I realized Levenshtein Distance could be a nice solution to your problem.  Its good to see that you have a link to a paper on the topic, let me see if I can shed some light into what a Levenshtein solution would look like.&#xD;&#xA;&#xD;&#xA;Levenshtein distance is used across many industries for entity resolution, what makes it useful is that it is a measure of the difference between two sequences.  In the case of string comparison it is just sequences characters.&#xD;&#xA;&#xD;&#xA;This could help solve your problem by allowing you to provide one number that gives a measure of how similar the text of another field is.  &#xD;&#xA;&#xD;&#xA;Here is an example of a basic way of using Levenshtein with the data you gave:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;This provides an ok solution, the distance of 8 provides some indication of a relationship, and it is very PII compliant.  However, it is still not super useful, let see what happens if we do some text magic to take only the first initial of the first name and the full last name dropping anything in the middle:&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;As you can see the Levenshtein distance of 0 is pretty indicative of a relationship.  Commonly data providers will combine a bunch of Levenshtein permutations of the first and last names with 1, 2, or all of the characters just to give some dimensionality as to how entities are related while still maintaining anonymity within the data.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/vpaAH.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/JlPu9.png" />
  <row Id="1073" PostHistoryTypeId="2" PostId="440" RevisionGUID="b83b873e-2d85-43bc-9f0e-3d402270e61a" CreationDate="2014-06-17T18:59:14.947" UserId="922" Text="The [word2vec algorithm](https://code.google.com/p/word2vec/) may be a good way to retrieve more elements for a list of similar words. It is an unsupervised &quot;deep learning&quot; algorithm that has previously been demonstrated with Wikipedia-based training data (helper scripts are provided on the Google code page).&#xD;&#xA;&#xD;&#xA;There are currently [C](https://code.google.com/p/word2vec/) and [Python](http://radimrehurek.com/gensim/models/word2vec.html) implementations. This [tutorial](http://radimrehurek.com/2014/02/word2vec-tutorial) by [Radim Řehůřek](http://radimrehurek.com/), the author of the [Gensim topic modelling library](http://radimrehurek.com/gensim/), is an excellent place to start.&#xD;&#xA;&#xD;&#xA;The [&quot;single topic&quot;](http://radimrehurek.com/2014/02/word2vec-tutorial#single) demonstration on the tutorial is a good example of retreiving similar words to a single term (try searching on 'red' or 'yellow'). It should be possible to extend this technique to find the words that have the greatest overall similarity to a set of input words." />
  <row Id="1075" PostHistoryTypeId="5" PostId="416" RevisionGUID="585404fc-db48-44a2-81b2-d27c3d6799a4" CreationDate="2014-06-17T20:07:05.303" UserId="381" Comment="added 15 characters in body" Text="If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a [hash of the string](http://www.cse.yorku.ca/~oz/hash.html) that is used to represent all related records (e.g., David instead of Dave).&#xD;&#xA;&#xD;&#xA;I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them.&#xD;&#xA;&#xD;&#xA;**edit**: You need to define and justify what kind of operations the third party needs to be able to do. For example, what is wrong with using initials followed by a number (e.g., BOA-1, BOA-2, etc.) to disambiguate Bank of America from Benjamin Othello Ames? If that's too revealing, you could bin some of the letters or names; e.g., [A-E] -&gt; 1, [F-J] -&gt; 2, etc. so BOA would become 1OA, or [&quot;Bank&quot;, &quot;Barry&quot;, &quot;Bruce&quot;, etc.] -&gt; 1 so Bank of America is again 1OA. &#xD;&#xA;&#xD;&#xA;For more information see [k-anonymity](http://en.wikipedia.org/wiki/K-anonymity)." />
  <row Id="1076" PostHistoryTypeId="2" PostId="441" RevisionGUID="22daf4cf-40da-4248-85ac-d76d546f503c" CreationDate="2014-06-17T20:48:35.267" UserId="426" Text="With Hadoop 2.0 and YARN Hadoop is supposedly no longer tied only map-reduce solutions. With that advancement, what are the use cases for Apache Spark vs Hadoop considering both sit atop of HDFS? I've read through the introduction documentation for Spark, but I'm curious if anyone has encountered a problem that was more efficient and easier to solve with Spark compared to Hadoop." />
  <row Id="1077" PostHistoryTypeId="1" PostId="441" RevisionGUID="22daf4cf-40da-4248-85ac-d76d546f503c" CreationDate="2014-06-17T20:48:35.267" UserId="426" Text="What are the use cases for Apache Spark vs Hadoop" />
  <row Id="1078" PostHistoryTypeId="3" PostId="441" RevisionGUID="22daf4cf-40da-4248-85ac-d76d546f503c" CreationDate="2014-06-17T20:48:35.267" UserId="426" Text="&lt;hadoop&gt;&lt;distributed&gt;&lt;knowledge-base&gt;" />
  <row Id="1079" PostHistoryTypeId="2" PostId="442" RevisionGUID="35f5aaba-6d3b-44e4-8b70-67fe080e807e" CreationDate="2014-06-17T21:17:31.210" UserId="548" Text="Christopher's answers seem very reasonable. In particular tree based methods do well with this sort of data because they branch on discriminative features. It's a little hard to say without knowing your specific application, but in general if you think that some of your features might be significantly more discriminative than others, you could try some dimensionality reduction techniques to clean this up a bit.&#xD;&#xA;&#xD;&#xA;Also if you use a dimensionality reduction technique you end up getting a slightly more robust format for your feature vector (they generally end up being straight numerical vectors instead of mixed data types), which might let you leverage different methods. You could also look into hand engineering features. With properly hand engineered features `Random Forest` will get you very close to state of the art on most tasks." />
  <row Id="1081" PostHistoryTypeId="5" PostId="412" RevisionGUID="22aa434e-da71-497a-895c-fddf0200f083" CreationDate="2014-06-17T22:23:03.030" UserId="322" Comment="added 338 characters in body" Text="Motivation&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract. &#xD;&#xA;&#xD;&#xA;This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party's ability to perform QA/QC, adjust parameters or make refinements may be very limited.&#xD;&#xA;&#xD;&#xA;Anonymizing Confidential Data&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as &quot;Dave&quot; and in another as &quot;David,&quot; commercial entities can have many different abbreviations, and there are always some typos. I've developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID. &#xD;&#xA;&#xD;&#xA;At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.&#xD;&#xA;&#xD;&#xA;What Doesn't Work&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance &lt;= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance &lt;= 1.&#xD;&#xA;&#xD;&#xA;But the only method I am familiar with that does this is ROT13, which hardly even counts as encryption; it's like writing the names upside down and saying, &quot;promise you won't flip the paper over?&quot;&#xD;&#xA;&#xD;&#xA;Another **bad** solution would be to abbreviate everything. &quot;Ellen Roberts&quot; becomes &quot;ER&quot; and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; &quot;Benjamin Othello Ames&quot; and &quot;Bank of America&quot; will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.&#xD;&#xA;&#xD;&#xA;An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:&#xD;&#xA;&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | Row | ID | Name              | WordChars | Origin |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 1   | 17 | &quot;AMELIA BEDELIA&quot;  | (6, 7)    | Eng    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 2   | 18 | &quot;CHRISTOPH BAUER&quot; | (9, 5)    | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 3   | 18 | &quot;C J BAUER&quot;       | (1, 1, 5) | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;    | 4   | 19 | &quot;FRANZ HELLER&quot;    | (5, 6)    | Ger    |&#xD;&#xA;    +-----+----+-------------------+-----------+--------+&#xD;&#xA;&#xD;&#xA;I call this &quot;inelegant&quot; because it requires anticipating which qualities might be interesting and it's relatively coarse. If the names are removed, there's not much you can reasonably conclude about the strength of the match between rows 2 &amp; 3, or about the distance between rows 2 &amp; 4 (i.e., how close they are to matching).&#xD;&#xA;&#xD;&#xA;Conclusion&#xD;&#xA;=&#xD;&#xA;&#xD;&#xA;The goal is to transform strings in such a way that as many useful qualities of the original string are preserved as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.&#xD;&#xA;&#xD;&#xA;I've found a couple papers that might be relevant, but they're a bit over my head:&#xD;&#xA;&#xD;&#xA;- [Privacy Preserving String Comparisons Based on Levenshtein Distance](http://www.merl.com/publications/docs/TR2010-109.pdf)&#xD;&#xA;- [An Empirical Comparison of Approaches to Approximate String &#xD;&#xA;Matching in Private Record Linkage](https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf)" />
  <row Id="1082" PostHistoryTypeId="2" PostId="443" RevisionGUID="a78ab732-7ca7-4783-b04f-ecbdfd7f99d4" CreationDate="2014-06-17T22:28:36.070" UserId="474" Text="One option (depending on your dataset size) is to just provide edit distances (or other measures of similarity you're using) as an additional dataset.&#xD;&#xA;&#xD;&#xA;E.g.:&#xD;&#xA;&#xD;&#xA;1. Generate a set of unique names in the dataset&#xD;&#xA;2. For each name, calculate edit distance to each other name&#xD;&#xA;3. Generate an ID or irreversable hash for each name&#xD;&#xA;4. Replace names in the original dataset with this ID&#xD;&#xA;5. Provide matrix of edit distances between ID numbers as new dataset&#xD;&#xA;&#xD;&#xA;Though there's still a lot that could be done to deanonimise the data from these even. &#xD;&#xA;&#xD;&#xA;E.g. if &quot;Tim&quot; is known to be the most popular name for a boy, frequency counting of IDs that closely match the known percentage of Tims across the population might give that away.  From there you could then look for names with an edit distance of 1, and conclude that those IDs might refer to &quot;Tom&quot; or &quot;Jim&quot; (when combined with other info)." />
  <row Id="1083" PostHistoryTypeId="2" PostId="444" RevisionGUID="d12a863a-e403-4ffa-9574-f6a7a704ba70" CreationDate="2014-06-17T22:29:36.720" UserId="418" Text="A classic book is by B. Efron who created the technique:&#xD;&#xA;&#xD;&#xA; - Bradley Efron; Robert Tibshirani (1994). An Introduction to the Bootstrap. Chapman &amp; Hall/CRC. ISBN 978-0-412-04231-7." />
  <row Id="1084" PostHistoryTypeId="2" PostId="445" RevisionGUID="7abd7b77-0570-4b13-868b-3a8de7c9a86e" CreationDate="2014-06-17T23:30:45.897" UserId="609" Text="Is your Masters in Computer Science?  Statistics?&#xD;&#xA;&#xD;&#xA;Is 'data science' going to be at the center of your thesis?  Or a side topic?&#xD;&#xA;&#xD;&#xA;I'll assume your in Statistics and that you want to focus your thesis on a 'data science' problem.  If so, then I'm going to go against the grain and suggest that you *should not* start with a data set or an ML method.  Instead, you should seek an interesting research problem that's poorly understood or where ML methods have not yet been proven successful, or where there are many competing ML methods but none seem better than others.&#xD;&#xA;&#xD;&#xA;Consider this data source: [Stanford Large Network Dataset Collection](http://snap.stanford.edu/data/).  While you *could* pick one of these data sets, make up a problem statement, and then run some list of ML methods, that approach really doesn't tell you very much about what *data science* is all about, and in my opinion doesn't lead to a very good Masters thesis.&#xD;&#xA;&#xD;&#xA;Instead, you might do this: look for all the research papers that use ML on some specific category -- e.g. Collaboration networks (a.k.a. co-authorship). As you read each paper, try to find out what they *were* able to accomplish with each ML method and what they weren't able to address.  Especially look for their suggestions for &quot;future research&quot;.&#xD;&#xA;&#xD;&#xA;Maybe they all use the same method, but never tried competing ML methods.  Or maybe they don't adequately validate their results, or maybe there data sets are small, or maybe their research questions and hypothesis were simplistic or limited.&#xD;&#xA;&#xD;&#xA;Most important: try to find out where this line of research is going.  Why are they even bothering to do this?  What is significant about it?  Where and why are they encountering difficulties?" />
  <row Id="1085" PostHistoryTypeId="2" PostId="446" RevisionGUID="b6610aea-81ab-43df-bace-58bc3d35eeba" CreationDate="2014-06-17T23:38:20.133" UserId="322" Text="One of the references I mentioned in the OP led me to a potential solution that seems quite powerful, described in &quot;Privacy-preserving record linkage using Bloom filters&quot; ([doi:10.1186/1472-6947-9-41](http://www.biomedcentral.com/1472-6947/9/41)):&#xD;&#xA;&#xD;&#xA;&gt;A new protocol for privacy-preserving record linkage with encrypted identifiers allowing for errors in identifiers has been developed. The protocol is based on Bloom filters on q-grams of identifiers.&#xD;&#xA;&#xD;&#xA;The article goes into detail about the method, which I will summarize here to the best of my ability.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;A Bloom filter is a fixed-length series of bits storing the results of a fixed set of independent hash functions, each computed on the same input value. The output of each hash function should be an index value from among the possible indexes in the filter; i.e., if you have a 0-indexed series of 10 bits, hash functions should return values from 0 to 9. &#xD;&#xA;&#xD;&#xA;The filter starts with each bit set to 0. After hashing the input value with each function from the set of hash functions, each bit corresponding to an index value returned by any hash function is set to 1. If the same index is returned by more than one hash function, the bit at that index is only set once. You could consider the Bloom filter to be a superposition of the set of hashes onto the fixed range of bits.&#xD;&#xA;&#xD;&#xA;The protocol described in the above-linked article divides strings into n-grams, which are in this case sets of characters. As an example, `&quot;hello&quot;` might yield the following set of 2-grams:&#xD;&#xA;&#xD;&#xA;    [&quot;_h&quot;, &quot;he&quot;, &quot;el&quot;, &quot;ll&quot;, &quot;lo&quot;, &quot;o_&quot;]&#xD;&#xA;&#xD;&#xA;Padding the front and back with spaces seems to be generally optional when constructing n-grams; the examples given in the paper that proposes this method use such padding.&#xD;&#xA;&#xD;&#xA;Each n-gram can be hashed to produce a Bloom filter, and this set of Bloom filters can be superimposed on itself (bitwise OR operation) to produce the Bloom filter for the string.&#xD;&#xA;&#xD;&#xA;If the filter contains many more bits than there are hash functions or n-grams, arbitrary strings are relatively unlikely to produce exactly the same filter. However, the more n-grams two strings have in common, the more bits their filters will ultimately share. You can then compare any two filters `A, B` by means of their Dice coefficient:&#xD;&#xA;&#xD;&#xA;&gt; D&lt;sub&gt;A, B&lt;/sub&gt; = 2h / (a + b)&#xD;&#xA;&#xD;&#xA;Where `h` is the number of bits that are set to 1 in both filters, `a` is the number of bits set to 1 in *only* filter A, and `b` is the number of bits set to 1 in *only* filter B. If the strings are exactly the same, the Dice coefficient will be 1; the more they differ, the closer the coefficient will be to `0`.&#xD;&#xA;&#xD;&#xA;Because the hash functions are mapping an indeterminate number of unique inputs to a small number of possible bit indexes, different inputs may produce the same filter, so the coefficient indicates only a *probability* that the strings are the same or similar. The number of different hash functions and the number of bits in the filter are important parameters for determining the likelihood of false positives - pairs of inputs that are much less similar than the Dice coefficient produced by this method predicts.&#xD;&#xA;&#xD;&#xA;I found [this tutorial](http://billmill.org/bloomfilter-tutorial/) to be very helpful for understanding the Bloom filter.&#xD;&#xA;&#xD;&#xA;There is some flexibility in the implementation of this method; see also [this 2010 paper](https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf) (also linked at the end of the question) for some indications of how performant it is in relation to other methods, and with various parameters." />
  <row Id="1088" PostHistoryTypeId="2" PostId="447" RevisionGUID="14802b84-e425-45d8-a4f0-4bd8fbd43403" CreationDate="2014-06-18T03:19:07.557" UserId="890" Text="I have a large number of samples which represent Manchester encoded bit streams encoded as audio signals. The frequency at which they are encoded is the primary frequency component when it is high, and there is a consistent amount of white noise in the background. I have manually decoded these streams, but I was wondering I could use some sort of machine learning technique to learn the encoding schemes. This would save a great deal of time manually recognizing these schemes. The difficulty is that different signals are encoded differently. Is it possible to build a model which can learn to decode more than one encoding scheme? How robust would such a model be, and what sort of techniques would I want to employ? `ICA` seems like it could be useful for isolating the frequency I care about, but how would I learn the encoding scheme?" />
  <row Id="1089" PostHistoryTypeId="1" PostId="447" RevisionGUID="14802b84-e425-45d8-a4f0-4bd8fbd43403" CreationDate="2014-06-18T03:19:07.557" UserId="890" Text="Learning signal encoding" />
  <row Id="1090" PostHistoryTypeId="3" PostId="447" RevisionGUID="14802b84-e425-45d8-a4f0-4bd8fbd43403" CreationDate="2014-06-18T03:19:07.557" UserId="890" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;scalability&gt;&lt;algorithms&gt;&lt;feature-selection&gt;" />
  <row Id="1091" PostHistoryTypeId="2" PostId="448" RevisionGUID="50c0628e-c2cf-4455-9e8f-60b81d764f3a" CreationDate="2014-06-18T05:35:35.700" UserId="418" Text="Not sure about the YARN, but I think that Spark makes a real difference compared to Hadoop (advertised as 100 times faster) if data can fit nicely in the memory of the computational nodes. Simply because it avoids hard disk access. If data doesn't fit memory there's still some gain because of buffering.&#xD;&#xA;" />
  <row Id="1093" PostHistoryTypeId="5" PostId="421" RevisionGUID="6a3fa3cf-46bc-4a4b-b3fa-21634474e71e" CreationDate="2014-06-18T07:17:45.110" UserId="88" Comment="detalization of question" Text="Does anyone know some good tutorials on online machine learning technics?&#xD;&#xA;I.e. how it can be used in real-time environments, what are key differences compared to normal machine learning methods etc.&#xD;&#xA;&#xD;&#xA;UPD: Thank you everyone for answers, by &quot;online&quot; I mean methods which can be trained in a real-time mode, based on a new inputs one by one." />
  <row Id="1094" PostHistoryTypeId="2" PostId="449" RevisionGUID="91e4f92d-1e7b-49ca-8e6a-1e89a585ce64" CreationDate="2014-06-18T07:27:22.677" UserId="21" Text="Hadoop means HDFS, YARN, MapReduce, and a lot of other things. Do you mean Spark vs *MapReduce*? Because Spark runs on/with Hadoop, which is rather the point.&#xD;&#xA;&#xD;&#xA;The primary reason to use Spark is for speed, and this comes from the fact that its execution can keep data in memory between stages rather than always persist back to HDFS after a Map or Reduce. This advantage is very pronounced for iterative computations, which have tens of stages each of which is touching the same data. This is where things might be &quot;100x&quot; faster. For simple, one-pass ETL-like jobs for which MapReduce was designed, it's not in general faster.&#xD;&#xA;&#xD;&#xA;Another reason to use Spark is its nicer high-level language compared to MapReduce. It provides a functional programming-like view that mimics Scala, which is far nicer than writing MapReduce code. (Although you have to either use Scala, or adopt the slightly-less-developed Java or Python APIs for Spark). [Crunch][1] and [Cascading][2] already provide a similar abstraction on top of MapReduce, but this is still an area where Spark is nice.&#xD;&#xA;&#xD;&#xA;Finally Spark has as-yet-young but promising subprojects for ML, graph analysis, and streaming, which expose a similar, coherent API. With MapReduce, you would have to turn to several different other projects for this (Mahout, Giraph, Storm). It's nice to have it in one package, albeit not yet 'baked'.&#xD;&#xA;&#xD;&#xA;Why would you not use Spark? [paraphrasing][3] myself:&#xD;&#xA;&#xD;&#xA;- Spark is primarily Scala, with ported Java APIs; MapReduce might be friendlier and more native for Java-based developers&#xD;&#xA;- There is more MapReduce expertise out there now than Spark&#xD;&#xA;- For the data-parallel, one-pass, ETL-like jobs MapReduce was designed for, MapReduce is lighter-weight compared to the Spark equivalent&#xD;&#xA;- Spark is fairly mature, and so is YARN now, but Spark-on-YARN is still pretty new. The two may not be optimally integrated yet. For example until recently I don't think Spark could ask YARN for allocations based on number of cores? That is: MapReduce might be easier to understand, manage and tune&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://crunch.apache.org&#xD;&#xA;  [2]: http://cascading.org&#xD;&#xA;  [3]: https://www.quora.com/Apache-Spark/Assuming-you-have-a-system-with-both-Hadoop-and-Spark-installed-say-under-Yarn-is-there-any-reason-to-use-Hadoop-map-reduce-instead-of-the-equivalent-Spark-commands" />
  <row Id="1095" PostHistoryTypeId="5" PostId="443" RevisionGUID="6001d4f4-f404-4916-a40d-49c4ef0259b2" CreationDate="2014-06-18T08:08:02.163" UserId="474" Comment="Fixed typo" Text="One option (depending on your dataset size) is to just provide edit distances (or other measures of similarity you're using) as an additional dataset.&#xD;&#xA;&#xD;&#xA;E.g.:&#xD;&#xA;&#xD;&#xA;1. Generate a set of unique names in the dataset&#xD;&#xA;2. For each name, calculate edit distance to each other name&#xD;&#xA;3. Generate an ID or irreversable hash for each name&#xD;&#xA;4. Replace names in the original dataset with this ID&#xD;&#xA;5. Provide matrix of edit distances between ID numbers as new dataset&#xD;&#xA;&#xD;&#xA;Though there's still a lot that could be done to deanonymise the data from these even. &#xD;&#xA;&#xD;&#xA;E.g. if &quot;Tim&quot; is known to be the most popular name for a boy, frequency counting of IDs that closely match the known percentage of Tims across the population might give that away.  From there you could then look for names with an edit distance of 1, and conclude that those IDs might refer to &quot;Tom&quot; or &quot;Jim&quot; (when combined with other info)." />
  <row Id="1098" PostHistoryTypeId="5" PostId="370" RevisionGUID="0fa46d35-a777-4e9b-b216-1361b63a19da" CreationDate="2014-06-18T13:53:25.307" UserId="322" Comment="reformulated the question to be more concise and less specific to the OP's thesis" Text="I'd like to explore 'data science'. The term seems a little vague to me, but I expect it to require:&#xD;&#xA;&#xD;&#xA;1. machine learning (rather than traditional statistics);&#xD;&#xA;2. a large enough dataset that you have to run analyses on clusters.&#xD;&#xA;&#xD;&#xA;What are some good datasets and problems, accessible to a statistician with some programming background, that I can use to explore the field of data science?&#xD;&#xA;&#xD;&#xA;To keep this as narrow as possible, I'd ideally like links to open, well used datasets and example problems." />
  <row Id="1099" PostHistoryTypeId="24" PostId="370" RevisionGUID="0fa46d35-a777-4e9b-b216-1361b63a19da" CreationDate="2014-06-18T13:53:25.307" Comment="Proposed by 322 approved by 50 edit id of 76" />
  <row Id="1100" PostHistoryTypeId="5" PostId="403" RevisionGUID="bb95da8a-d2a9-498f-9aa4-c8714daab615" CreationDate="2014-06-18T13:53:29.630" UserId="368" Comment="cleaned up question" Text="![enter image description here][1]&#xD;&#xA;&#xD;&#xA;I am trying to do Logistic Regression using SAS Enterprise Miner. &#xD;&#xA;My Independent variables are &#xD;&#xA;&#xD;&#xA;    CPR/Inc (Categorical 1 to 7)&#xD;&#xA;    OD/Inc (Categorical 1 to 4)&#xD;&#xA;    Insurance (Binary 0 or 1)&#xD;&#xA;    Income Loss (Binary 0 or 1)&#xD;&#xA;    Living Arrangement (Categorical 1 to 7)&#xD;&#xA;    Employment Status (categorical 1 to 8)&#xD;&#xA;&#xD;&#xA;My Dependent Variable is Default (Binary 0 or 1)&#xD;&#xA;&#xD;&#xA;The following is the output from running Regression Model.&#xD;&#xA;&#xD;&#xA; Analysis of Maximum Likelihood Estimates&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;                                      Standard          Wald&#xD;&#xA;    Parameter       DF    Estimate       Error    Chi-Square    Pr &gt; ChiSq    Exp(Est)&#xD;&#xA;     &#xD;&#xA;    Intercept        1     -0.4148      0.0645         41.30        &lt;.0001       0.660&#xD;&#xA;    CPR___Inc  1     1     -0.8022      0.1051         58.26        &lt;.0001       0.448&#xD;&#xA;    CPR___Inc  2     1     -0.4380      0.0966         20.57        &lt;.0001       0.645&#xD;&#xA;    CPR___Inc  3     1      0.3100      0.0871         12.68        0.0004       1.363&#xD;&#xA;    CPR___Inc  4     1    -0.00304      0.0898          0.00        0.9730       0.997&#xD;&#xA;    CPR___Inc  5     1      0.1331      0.0885          2.26        0.1324       1.142&#xD;&#xA;    CPR___Inc  6     1      0.1694      0.0881          3.70        0.0546       1.185&#xD;&#xA;    Emp_Status 1     1     -0.2289      0.1006          5.18        0.0229       0.795&#xD;&#xA;    Emp_Status 2     1      0.4061      0.0940         18.66        &lt;.0001       1.501&#xD;&#xA;    Emp_Status 3     1     -0.2119      0.1004          4.46        0.0347       0.809&#xD;&#xA;    Emp_Status 4     1      0.1100      0.0963          1.30        0.2534       1.116&#xD;&#xA;    Emp_Status 5     1     -0.2280      0.1007          5.12        0.0236       0.796&#xD;&#xA;    Emp_Status 6     1      0.3761      0.0943         15.91        &lt;.0001       1.457&#xD;&#xA;    Emp_Status 7     1     -0.3337      0.1026         10.59        0.0011       0.716&#xD;&#xA;    Inc_Loss   0     1     -0.1996      0.0449         19.76        &lt;.0001       0.819&#xD;&#xA;    Insurance  0     1      0.1256      0.0559          5.05        0.0246       1.134&#xD;&#xA;    Liv_Arran  1     1     -0.1128      0.0916          1.52        0.2178       0.893&#xD;&#xA;    Liv_Arran  2     1      0.2576      0.0880          8.57        0.0034       1.294&#xD;&#xA;    Liv_Arran  3     1      0.0235      0.0904          0.07        0.7950       1.024&#xD;&#xA;    Liv_Arran  4     1      0.0953      0.0887          1.16        0.2825       1.100&#xD;&#xA;    Liv_Arran  5     1     -0.0493      0.0907          0.29        0.5871       0.952&#xD;&#xA;    Liv_Arran  6     1     -0.3732      0.0966         14.93        0.0001       0.689&#xD;&#xA;    OD___Inc   1     1     -0.2136      0.0557         14.72        0.0001       0.808&#xD;&#xA;    OD___Inc   2     1     -0.0279      0.0792          0.12        0.7248       0.973&#xD;&#xA;    OD___Inc   3     1     -0.0249      0.0793          0.10        0.7534       0.975&#xD;&#xA;&#xD;&#xA;Now I used this Model to Score a new set of data. An example row of my new data is&#xD;&#xA;&#xD;&#xA;    CPR - 7&#xD;&#xA;    OD - 4&#xD;&#xA;    Living Arrangement - 4&#xD;&#xA;    Employment Status - 4&#xD;&#xA;    Insurance - 0&#xD;&#xA;    Income Loss - 1&#xD;&#xA;&#xD;&#xA;For this sample row, the model predicted output (Probability of default = 1) as 0.7335 &#xD;&#xA;To check this manually, I added the estimates&#xD;&#xA;&#xD;&#xA;    Intercept + Emp Status 4 + Liv Arran 4 + Insurance 0&#xD;&#xA;    -0.4148   + 0.1100  +   0.0953   +   0.1256    =   -0.0839&#xD;&#xA;&#xD;&#xA;Odds ratio = Exponential(-0.0839) = 0.9195&#xD;&#xA;&#xD;&#xA;Hence probability = 0.9195 / (1 + 0.9195)  =   0.4790&#xD;&#xA;&#xD;&#xA;I am unable to understand why there is such a mismatch between the Model's predicted probability and theoretical probability. &#xD;&#xA;&#xD;&#xA;Any help would be much appreciated .&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/4Ih6o.png&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1101" PostHistoryTypeId="4" PostId="403" RevisionGUID="bb95da8a-d2a9-498f-9aa4-c8714daab615" CreationDate="2014-06-18T13:53:29.630" UserId="368" Comment="cleaned up question" Text="Why is there such a mismatch between the Model's predicted probability and theoretical probability in logistic regression?" />
  <row Id="1102" PostHistoryTypeId="24" PostId="403" RevisionGUID="bb95da8a-d2a9-498f-9aa4-c8714daab615" CreationDate="2014-06-18T13:53:29.630" Comment="Proposed by 368 approved by 50 edit id of 77" />
  <row Id="1103" PostHistoryTypeId="2" PostId="450" RevisionGUID="6ad3e111-deae-402c-82c0-1279ec5fccd8" CreationDate="2014-06-18T14:15:37.957" UserId="941" Text="I'm not quite sure, but maybe  locality-sensitive hashing is a good solution. It does hashing of input data (in your case - names), so original strings would be preserved. On the other side, the main idea of LSH is to maximize hashes likelihood for similar items. There are a lot of different LSH-implementations. I tried [Nilsimsa-hash][1] for comparing tweet texts, and it worked quite well. But I'm not sure, how well it will work in case of short strings (names) - this issue require testing. I tried your examples, and here is the result (name A, name B, &quot;distance&quot; - maximum is 120):&#xD;&#xA;&#xD;&#xA;    1. AMELIA BEDELIA  - CHRISTOPH BAUER - 107&#xD;&#xA;    2. AMELIA BEDELIA  - C J BAUER       - 82&#xD;&#xA;    3. AMELIA BEDELIA  - FRANZ HELLER    - 91&#xD;&#xA;    4. CHRISTOPH BAUER - C J BAUER       - 81&#xD;&#xA;    5. CHRISTOPH BAUER - FRANZ HELLER    - 98&#xD;&#xA;    6. C J BAUER       - FRANZ HELLER    - 83&#xD;&#xA;&#xD;&#xA;As you see, CHRISTOPH BAUER and C J BAUER turned up to be the closest pair. But difference is not significant.&#xD;&#xA;And just for example - hash representation of these names:&#xD;&#xA;&#xD;&#xA;    AMELIA BEDELIA  6b208299602b5000c3005a048122a43a828020889042240005011c1880864502&#xD;&#xA;    CHRISTOPH BAUER 22226448000ab10102e2860b52062487ff0000928e0822ee106028016cc01237&#xD;&#xA;    C J BAUER       2282204100961060048050004400240006032400148000802000a80130402002&#xD;&#xA;    FRANZ HELLER    58002002400880080b49172044020008030002442631e004009195020ad01158&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://spdp.di.unimi.it/papers/pdcs04.pdf" />
  <row Id="1104" PostHistoryTypeId="2" PostId="451" RevisionGUID="1bb27382-ad07-40db-be12-b98d6d1b4012" CreationDate="2014-06-18T14:36:07.980" UserId="84" Text="I'm developing a distributed application, and as it's been designed, there'll be a great load of communication during the processing. Since the communication is already as much *spread* along the entire process as possible, I'm wondering if there any standard solutions to improve the performance of the message passing layer of my application.&#xD;&#xA;&#xD;&#xA;What changes/improvements could I apply to my code to reduce the time spent sending messages? For what it's worth, I'm communicating up to 10GB between 9 computing nodes, and the framework I'm using is implemented with OpenMPI." />
  <row Id="1105" PostHistoryTypeId="1" PostId="451" RevisionGUID="1bb27382-ad07-40db-be12-b98d6d1b4012" CreationDate="2014-06-18T14:36:07.980" UserId="84" Text="How to speedup message passing between computing nodes" />
  <row Id="1106" PostHistoryTypeId="3" PostId="451" RevisionGUID="1bb27382-ad07-40db-be12-b98d6d1b4012" CreationDate="2014-06-18T14:36:07.980" UserId="84" Text="&lt;efficiency&gt;&lt;distributed&gt;&lt;performance&gt;&lt;mpi&gt;" />
  <row Id="1107" PostHistoryTypeId="5" PostId="446" RevisionGUID="2c9ab747-c896-441b-b4f4-57d9fd0212d7" CreationDate="2014-06-18T15:08:44.507" UserId="322" Comment="added 18 characters in body" Text="One of the references I mentioned in the OP led me to a potential solution that seems quite powerful, described in &quot;Privacy-preserving record linkage using Bloom filters&quot; ([doi:10.1186/1472-6947-9-41](http://www.biomedcentral.com/1472-6947/9/41)):&#xD;&#xA;&#xD;&#xA;&gt;A new protocol for privacy-preserving record linkage with encrypted identifiers allowing for errors in identifiers has been developed. The protocol is based on Bloom filters on q-grams of identifiers.&#xD;&#xA;&#xD;&#xA;The article goes into detail about the method, which I will summarize here to the best of my ability.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;A Bloom filter is a fixed-length series of bits storing the results of a fixed set of independent hash functions, each computed on the same input value. The output of each hash function should be an index value from among the possible indexes in the filter; i.e., if you have a 0-indexed series of 10 bits, hash functions should return (or be mapped to) values from 0 to 9. &#xD;&#xA;&#xD;&#xA;The filter starts with each bit set to 0. After hashing the input value with each function from the set of hash functions, each bit corresponding to an index value returned by any hash function is set to 1. If the same index is returned by more than one hash function, the bit at that index is only set once. You could consider the Bloom filter to be a superposition of the set of hashes onto the fixed range of bits.&#xD;&#xA;&#xD;&#xA;The protocol described in the above-linked article divides strings into n-grams, which are in this case sets of characters. As an example, `&quot;hello&quot;` might yield the following set of 2-grams:&#xD;&#xA;&#xD;&#xA;    [&quot;_h&quot;, &quot;he&quot;, &quot;el&quot;, &quot;ll&quot;, &quot;lo&quot;, &quot;o_&quot;]&#xD;&#xA;&#xD;&#xA;Padding the front and back with spaces seems to be generally optional when constructing n-grams; the examples given in the paper that proposes this method use such padding.&#xD;&#xA;&#xD;&#xA;Each n-gram can be hashed to produce a Bloom filter, and this set of Bloom filters can be superimposed on itself (bitwise OR operation) to produce the Bloom filter for the string.&#xD;&#xA;&#xD;&#xA;If the filter contains many more bits than there are hash functions or n-grams, arbitrary strings are relatively unlikely to produce exactly the same filter. However, the more n-grams two strings have in common, the more bits their filters will ultimately share. You can then compare any two filters `A, B` by means of their Dice coefficient:&#xD;&#xA;&#xD;&#xA;&gt; D&lt;sub&gt;A, B&lt;/sub&gt; = 2h / (a + b)&#xD;&#xA;&#xD;&#xA;Where `h` is the number of bits that are set to 1 in both filters, `a` is the number of bits set to 1 in *only* filter A, and `b` is the number of bits set to 1 in *only* filter B. If the strings are exactly the same, the Dice coefficient will be 1; the more they differ, the closer the coefficient will be to `0`.&#xD;&#xA;&#xD;&#xA;Because the hash functions are mapping an indeterminate number of unique inputs to a small number of possible bit indexes, different inputs may produce the same filter, so the coefficient indicates only a *probability* that the strings are the same or similar. The number of different hash functions and the number of bits in the filter are important parameters for determining the likelihood of false positives - pairs of inputs that are much less similar than the Dice coefficient produced by this method predicts.&#xD;&#xA;&#xD;&#xA;I found [this tutorial](http://billmill.org/bloomfilter-tutorial/) to be very helpful for understanding the Bloom filter.&#xD;&#xA;&#xD;&#xA;There is some flexibility in the implementation of this method; see also [this 2010 paper](https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf) (also linked at the end of the question) for some indications of how performant it is in relation to other methods, and with various parameters." />
  <row Id="1108" PostHistoryTypeId="2" PostId="452" RevisionGUID="59845e75-e644-4e41-a1e2-e9837d060038" CreationDate="2014-06-18T15:10:22.637" UserId="403" Text="As @Christopher Lauden mentioned above, time-series analysis is most appropriate for this sort of thing. If, however, you wished to do a more traditional &quot;machine learning approach&quot;, something that I have done in the past is to block up your data into overlapping windows of time as features, then use it to predict the next days (or weeks) traffic.&#xD;&#xA;&#xD;&#xA;Your feature matrix would be something like:&#xD;&#xA;&#xD;&#xA;    t1 | t2 | ... | tN&#xD;&#xA;    t2 | t3 | ... | tN+1&#xD;&#xA;    t3 | t4 | ... | tN+2&#xD;&#xA;    ...&#xD;&#xA;    tW | tW+1 | ... |tN+W&#xD;&#xA;&#xD;&#xA;where `tI` is the traffic on day `I`. The feature you'll be predicting is the traffic on the day after the last column. In essence, use a window of traffic to predict the next day's traffic. &#xD;&#xA;&#xD;&#xA;Any sort of ML model would work for this. &#xD;&#xA;&#xD;&#xA;" />
  <row Id="1109" PostHistoryTypeId="2" PostId="453" RevisionGUID="dc2a4cd0-a52a-4655-8c11-2eaa066e5b8c" CreationDate="2014-06-18T15:27:23.313" UserId="84" Text="I understand that compression methods may be split into two main sets: global and local. The first set works regardless of the data being processed, i.e., they do not rely on any characteristic of the data, and thus need not to perform any preprocessing over any part of the dataset (before the compression itself). On the other hand, local methods analyze the data, extracting informations that usually improve the compression rate.&#xD;&#xA;&#xD;&#xA;While reading about some of these methods, I noticed that the unary method is not universal, which surprised me since I thought &quot;globality&quot; and &quot;universality&quot; referred to the same thing. The unary method does not rely on characteristics of the data to yield its encoding (i.e., it is a global method), and therefore it should be global/universal, shouldn't it?&#xD;&#xA;&#xD;&#xA;What is the difference between universal and global methods? Aren't these classifications synonyms?" />
  <row Id="1110" PostHistoryTypeId="1" PostId="453" RevisionGUID="dc2a4cd0-a52a-4655-8c11-2eaa066e5b8c" CreationDate="2014-06-18T15:27:23.313" UserId="84" Text="What is the difference between global and universal compression methods?" />
  <row Id="1111" PostHistoryTypeId="3" PostId="453" RevisionGUID="dc2a4cd0-a52a-4655-8c11-2eaa066e5b8c" CreationDate="2014-06-18T15:27:23.313" UserId="84" Text="&lt;algorithms&gt;&lt;compression&gt;" />
  <row Id="1112" PostHistoryTypeId="2" PostId="454" RevisionGUID="101e8039-3357-4a9c-a419-4abaf71bc559" CreationDate="2014-06-18T15:48:19.497" UserId="403" Text="I have a highly biased binary dataset - I have 1000x more examples of the negative class than the positive class. I would like to train a Tree Ensemble (like Extra Random Trees or a Random Forest) on this data but it's difficult to create training datasets that contain enough examples of the positive class.&#xD;&#xA;&#xD;&#xA;What would be the implications of doing a stratified sampling approach to normalize the number of positive and negative examples? In other words, is it a bad idea to, for instance, artificially inflate (by resampling) the number of positive class examples in the training set?" />
  <row Id="1113" PostHistoryTypeId="1" PostId="454" RevisionGUID="101e8039-3357-4a9c-a419-4abaf71bc559" CreationDate="2014-06-18T15:48:19.497" UserId="403" Text="What are the implications for training a Tree Ensemble with highly biased datasets?" />
  <row Id="1114" PostHistoryTypeId="3" PostId="454" RevisionGUID="101e8039-3357-4a9c-a419-4abaf71bc559" CreationDate="2014-06-18T15:48:19.497" UserId="403" Text="&lt;machine-learning&gt;&lt;feature-selection&gt;" />
  <row Id="1115" PostHistoryTypeId="5" PostId="453" RevisionGUID="8ae7bdce-4cf0-444d-934f-0249674d96fa" CreationDate="2014-06-18T16:07:53.890" UserId="84" Comment="Improving question." Text="I understand that compression methods may be split into two main sets: global and local. The first set works regardless of the data being processed, i.e., they do not rely on any characteristic of the data, and thus need not to perform any preprocessing over any part of the dataset (before the compression itself). On the other hand, local methods analyze the data, extracting informations that usually improve the compression rate.&#xD;&#xA;&#xD;&#xA;While reading about some of these methods, I noticed that [the unary method is not universal](http://en.wikipedia.org/wiki/Universal_code_%28data_compression%29#Universal_and_non-universal_codes), which surprised me since I thought &quot;globality&quot; and &quot;universality&quot; referred to the same thing. The unary method does not rely on characteristics of the data to yield its encoding (i.e., it is a global method), and therefore it should be global/universal, shouldn't it?&#xD;&#xA;&#xD;&#xA;What is the difference between universal and global methods? Aren't these classifications synonyms?" />
  <row Id="1116" PostHistoryTypeId="2" PostId="455" RevisionGUID="a5314e2e-a19e-481a-9630-457405b1a9e4" CreationDate="2014-06-18T16:21:12.203" UserId="960" Text="I need a recommendation regarding datasets for text classification problem" />
  <row Id="1117" PostHistoryTypeId="1" PostId="455" RevisionGUID="a5314e2e-a19e-481a-9630-457405b1a9e4" CreationDate="2014-06-18T16:21:12.203" UserId="960" Text="Question related to text classification" />
  <row Id="1118" PostHistoryTypeId="3" PostId="455" RevisionGUID="a5314e2e-a19e-481a-9630-457405b1a9e4" CreationDate="2014-06-18T16:21:12.203" UserId="960" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;text-mining&gt;" />
  <row Id="1119" PostHistoryTypeId="2" PostId="456" RevisionGUID="5f6c7107-445a-467e-8900-5f4c8182ecbf" CreationDate="2014-06-18T17:22:46.207" UserId="322" Text="If you expect (or find) that nodes are requesting the same data more than once, perhaps you could benefit from a caching strategy? Especially where some data is used much more often than others, so you can target only the most frequently-used information.&#xD;&#xA;&#xD;&#xA;If the data is mutable, you also need a way to confirm that it hasn't changed since the last request that's less expensive than repeating the request.&#xD;&#xA;&#xD;&#xA;This is further complicated if each node has its own separate cache. Depending on the nature of your system and task(s), you could consider adding a node dedicated to serving information between the processing nodes, and building a single cache on that node.&#xD;&#xA;&#xD;&#xA;For an example of when that *might* be a good idea, let's suppose I retrieve some data from a remote data store over a low-bandwidth connection, and I have some task(s) requiring that data, which are distributed exclusively among local nodes. I definitely wouldn't want each node requesting information separately over that low-bandwidth connection, which another node might have previously requested. Since my local I/O is much less expensive than my I/O over the low-bandwidth connection, I might add a node between the processing nodes and the remote source that acts as an intermediate server. This node would take requests from the processing nodes, communicate with the remote data store, and cache frequently-requested data to minimize the use of that low-bandwidth connection.&#xD;&#xA;&#xD;&#xA;The core concepts here that *may* be applicable to your specific case are:&#xD;&#xA;&#xD;&#xA;- Eliminate or reduce redundant I/O;&#xD;&#xA;- Take advantage of trade-offs between memory use and computation time;&#xD;&#xA;- Not all I/O is created equal." />
  <row Id="1120" PostHistoryTypeId="2" PostId="457" RevisionGUID="40ac3eb9-194b-4013-baad-0e2b5f957fc2" CreationDate="2014-06-18T18:22:11.680" UserId="548" Text="Firstly, I would generally agree with everything that AirThomas suggested. Caching things is generally good if you can, but I find it slightly brittle since that's very dependent on exactly what your application is. Data compression is another very solid suggestion, but my impression on both of these is that the speedups you're looking at are going to be relatively marginal. Maybe as high as 2-5x, but I would be very surprised if they were any faster than that.&#xD;&#xA;&#xD;&#xA;Under the assumption that pure I/O (writing to/reading from memory) is *not* your limiting factor (if it is, you're probably not going to get a lot faster), I would make a strong plug for [zeromq][1]. In the words of the creators:&#xD;&#xA;&#xD;&#xA;&gt; We took a normal TCP socket, injected it with a mix of radioactive&#xD;&#xA;&gt; isotopes stolen from a secret Soviet atomic research project,&#xD;&#xA;&gt; bombarded it with 1950-era cosmic rays, and put it into the hands of a&#xD;&#xA;&gt; drug-addled comic book author with a badly-disguised fetish for&#xD;&#xA;&gt; bulging muscles clad in spandex. Yes, ØMQ sockets are the world-saving&#xD;&#xA;&gt; superheroes of the networking world.&#xD;&#xA;&#xD;&#xA;While that may be a little dramatic, `zeromq` sockets in my opinion are one of the most amazing pieces of software that the world of computer networks has put together in several years. I'm not sure what you're using for your message-passing layer right now, but if you're using something traditional like `rabbitmq`, you're liable to see speedups of multiple orders of magnitude (personally noticed about 500x, but depends a lot of architecture)&#xD;&#xA;&#xD;&#xA;Check out some basic benchmarks [here.][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://zeromq.org/&#xD;&#xA;  [2]: http://blog.x-aeon.com/2013/04/10/a-quick-message-queue-benchmark-activemq-rabbitmq-hornetq-qpid-apollo/" />
  <row Id="1121" PostHistoryTypeId="2" PostId="458" RevisionGUID="e10886bc-04b3-4fff-9cd9-1c26f3277669" CreationDate="2014-06-18T19:48:54.883" UserId="84" Text="[K-means](http://en.wikipedia.org/wiki/K-means_clustering) is a well known algorithm for clustering, but there is also an online variation of such algorithm (online K-means). What are the pros and cons of these approaches, and when should each be preferred?" />
  <row Id="1122" PostHistoryTypeId="1" PostId="458" RevisionGUID="e10886bc-04b3-4fff-9cd9-1c26f3277669" CreationDate="2014-06-18T19:48:54.883" UserId="84" Text="K-means vs. online K-means" />
  <row Id="1123" PostHistoryTypeId="3" PostId="458" RevisionGUID="e10886bc-04b3-4fff-9cd9-1c26f3277669" CreationDate="2014-06-18T19:48:54.883" UserId="84" Text="&lt;algorithms&gt;&lt;clustering&gt;" />
  <row Id="1124" PostHistoryTypeId="2" PostId="459" RevisionGUID="b08deea8-a528-4963-9c5d-995bcc82dbfc" CreationDate="2014-06-18T20:07:05.017" UserId="178" Text="Online k-means (more commonly known as [sequential k-means](http://stackoverflow.com/questions/3698532/online-k-means-clustering)) and traditional k-means are very similar.  The difference is that online k-means allows you to update the model as new data is received.&#xD;&#xA;&#xD;&#xA;Online k-means should be used when you expect the data to be received one by one (or maybe in chunks).  This allows you to update your model as you get more information about it.  The drawback of this method is that it is dependent on the order in which the data is received ([ref](http://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm)).  " />
  <row Id="1125" PostHistoryTypeId="2" PostId="460" RevisionGUID="2f99e130-afab-4a37-b33e-c0b2fd3c024d" CreationDate="2014-06-18T21:56:29.147" UserId="984" Text="A fast, easy an often effective way to approach this imbalance would be to randomly subsample the bigger class (which in your case is the negative class), run the classification N number of times with members from the two classes (one full and the other subsampled) and report the average metric values, the average being computed over N (say 1000) iterations.&#xD;&#xA;&#xD;&#xA;A more methodical approach would be to execute the Mapping Convergence (MC) algorithm, which involves identifying a subset of strong negative samples with the help of a one-class classifier, such as OSVM or SVDD, and then iteratively execute binary classification on the set of strong negative and positive samples. More details of the MC algorithm can be found in this [paper][3]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [3]: http://link.springer.com/article/10.1007/s10994-005-1122-7#page-1" />
  <row Id="1126" PostHistoryTypeId="2" PostId="461" RevisionGUID="c3117eab-4e00-40a7-a297-8f3992e5aa54" CreationDate="2014-06-18T22:10:58.497" UserId="986" Text="There’s this side project I’m working on where I need to structure a solution to the following problem.&#xD;&#xA;&#xD;&#xA; I have two groups of people (clients). Group “A” intends to buy and group “B” intends to sell a determined product “X”. &#xD;&#xA;The product has a series of attributes x_i and my objective is to facilitate the transaction between “A” e “B” by matching their preferences. The main idea is to point out to each member of “A” a corresponding in “B” who’s product better suits his needs, and vice versa. &#xD;&#xA;&#xD;&#xA;Some complicating aspects of the problem:&#xD;&#xA;&#xD;&#xA;1)The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design which is rare among the population and I can’t predict. Can’t previously list all the attributes;&#xD;&#xA;	 &#xD;&#xA;2)Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design);&#xD;&#xA;&#xD;&#xA;Any suggestion on how to approach this problem and solve it in an automated way?	 &#xD;&#xA;The idea is to really think out of the box here so feel free to “go wild” on your suggestions. &#xD;&#xA;&#xD;&#xA;I would also appreciate some references to other similar problems if possible. &#xD;&#xA;&#xD;&#xA;Thanks! &#xD;&#xA;" />
  <row Id="1127" PostHistoryTypeId="1" PostId="461" RevisionGUID="c3117eab-4e00-40a7-a297-8f3992e5aa54" CreationDate="2014-06-18T22:10:58.497" UserId="986" Text="Preference Matching Algorithm" />
  <row Id="1128" PostHistoryTypeId="3" PostId="461" RevisionGUID="c3117eab-4e00-40a7-a297-8f3992e5aa54" CreationDate="2014-06-18T22:10:58.497" UserId="986" Text="&lt;bigdata&gt;&lt;text-mining&gt;" />
  <row Id="1129" PostHistoryTypeId="2" PostId="462" RevisionGUID="3f77fc36-6c73-474d-a731-9acb4318d596" CreationDate="2014-06-18T22:27:06.503" UserId="548" Text="I would recommend training on more balanced subsets of your data. Training random forest on sets of randomly selected positive example with a similar number of negative samples. In particular if the discriminative features exhibit a lot of variance this will be fairly effective and avoid over-fitting. However in stratification it is important to find balance as over-fitting can become a problem regardless. I would suggest seeing how the model does with the whole data set then progressively increasing the ratio of positive to negative samples approaching an even ratio, and selecting for the one that maximizes your performance metric on some representative hold out data. &#xD;&#xA;&#xD;&#xA;This paper seems fairly relevant http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf it talks about a `weighted Random Forest` which more heavily penalizes misclassification of the minority class.  " />
  <row Id="1130" PostHistoryTypeId="2" PostId="463" RevisionGUID="201ec198-7bea-4ee8-9aa3-73ff30f65711" CreationDate="2014-06-18T22:45:25.677" UserId="984" Text="My first suggestion would be to somehow map the non-quantifiable attributes to quantities with the help of suitable mapping functions. Otherwise, simply leave them out.&#xD;&#xA;&#xD;&#xA;Secondly, I don't think that you need to assume that the list of attributes is not finite. A standard and intuitive approach is to represent each attribute as an individual dimension in a vector space. Each product is then simply a point in this space. In that case, if you want to dynamically add more attributes you simply have to remap the product vectors into the new feature space (with additional dimensions).    &#xD;&#xA;&#xD;&#xA;With this representation, a seller is a point in the feature space with product attributes and a buyer is a point in the same feature space with the preference attributes. The task is then to find out the most similar buyer point for a given seller point.&#xD;&#xA;&#xD;&#xA;If your dataset (i.e. the number of buyers/sellers) is not very large, you can solve this with a nearest neighbour approach implemented with the help of k-d trees.&#xD;&#xA;&#xD;&#xA;For very large sized data, you can take an IR approach. Index the set of sellers (i.e. the product attributes) by treating each attribute as a separate term with the term-weight being set to the attribute value. A query in this case is a buyer which is also encoded in the term space as a query vector with appropriate term weights. The retrieval step would return you a list of top K most similar matches." />
  <row Id="1131" PostHistoryTypeId="2" PostId="464" RevisionGUID="7159e266-22a2-4d67-bf1b-78a4681c6d6d" CreationDate="2014-06-18T22:48:53.350" UserId="984" Text="Some standard datasets for text classification are the 20-News group, Reuters (with 8 and 52 classes) and WebKb. You can find all of them [here][1]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://web.ist.utl.pt/~acardoso/datasets/" />
  <row Id="1132" PostHistoryTypeId="2" PostId="465" RevisionGUID="ca1ef62b-3916-4660-b6b2-06da21f37c8e" CreationDate="2014-06-18T22:58:35.260" UserId="984" Text="nDCG is used to evaluate a golden ranked list (typically human judged) against your output ranked list. The more is the correlation between the two ranked lists, i.e. the more similar are the ranks of the relevant items in the two lists, the closer is the value of nDCG to 1.&#xD;&#xA;&#xD;&#xA;RMSE (Root Mean Squared Error) is typically used to evaluate regression problems where the output (a predicted scalar value) is compared with the true scalar value output for a given data point.&#xD;&#xA;&#xD;&#xA;So, if you are simply recommending a score (such as recommending a movie rating), then use RMSE. Whereas, if you are recommending a list of items (such as a list of related movies), then use nDCG.  " />
  <row Id="1133" PostHistoryTypeId="2" PostId="466" RevisionGUID="cf096d39-c830-4141-ad06-324410236298" CreationDate="2014-06-18T23:02:54.200" UserId="989" Text="I tried to detect outliers in the energy gas consumption of some dutch buildings, building a neural network model. I have very bad results, but I don't find the reason. &#xD;&#xA;&#xD;&#xA;I am not an expert so I would like to ask you what I can improve and what I'm doing wrong. This is the complete description: https://github.com/denadai2/Gas-consumption-outliers" />
  <row Id="1134" PostHistoryTypeId="1" PostId="466" RevisionGUID="cf096d39-c830-4141-ad06-324410236298" CreationDate="2014-06-18T23:02:54.200" UserId="989" Text="Gas consumption outliers detection - Neural network project. Bad results" />
  <row Id="1135" PostHistoryTypeId="3" PostId="466" RevisionGUID="cf096d39-c830-4141-ad06-324410236298" CreationDate="2014-06-18T23:02:54.200" UserId="989" Text="&lt;neuralnetwork&gt;" />
  <row Id="1137" PostHistoryTypeId="2" PostId="468" RevisionGUID="3469df97-039b-41b5-a460-3ea50569d7fc" CreationDate="2014-06-19T03:43:23.853" UserId="994" Text="I'm working on a project and need resources to get me up to speed.&#xD;&#xA;&#xD;&#xA;The dataset is around 35000 observations on 30 or so variables.  About half the variables are categorical with some having many different possible values, i.e. if you split the categorical variables into dummy variables you would have a lot more than 30 variables.  But still probably on the order of a couple of hundred max.  (n&gt;p).  &#xD;&#xA;&#xD;&#xA;The response we want to predict is ordinal with 5 levels (1,2,3,4,5).  Predictors are a mix of continuous and categorical, about half of each.  These are my thoughts/plans so far:&#xD;&#xA;1.  Treat the response as continuous and run vanilla linear regression.&#xD;&#xA;2.  Run nominal and ordinal logistic and probit regression&#xD;&#xA;3.  Use MARS and/or another flavor of non-linear regression&#xD;&#xA;&#xD;&#xA;I'm familiar with linear regression.  MARS is well enough described by Hastie and Tibshirani.  But I'm at a loss when it comes to ordinal logit/probit, especially with so many variables and a big data set.  &#xD;&#xA;&#xD;&#xA;The r package [glmnetcr][1] seems to be my best bet so far, but the documentation hardly suffices to get me where I need to be.&#xD;&#xA;&#xD;&#xA;Where can I go to learn more?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cran.r-project.org/web/packages/glmnetcr/index.html" />
  <row Id="1138" PostHistoryTypeId="1" PostId="468" RevisionGUID="3469df97-039b-41b5-a460-3ea50569d7fc" CreationDate="2014-06-19T03:43:23.853" UserId="994" Text="Learning ordinal regression in R?" />
  <row Id="1139" PostHistoryTypeId="3" PostId="468" RevisionGUID="3469df97-039b-41b5-a460-3ea50569d7fc" CreationDate="2014-06-19T03:43:23.853" UserId="994" Text="&lt;logistic-regression&gt;" />
  <row Id="1140" PostHistoryTypeId="5" PostId="424" RevisionGUID="574d4027-47be-4068-8c5f-fb0852027fee" CreationDate="2014-06-19T05:48:43.540" UserId="322" Comment="language, add reference" Text="I recently saw a cool feature that [was once available](https://support.google.com/docs/answer/3543688?hl=en) in Google Sheets: you start by writing a few related keywords in consecutive cells, say: &quot;blue&quot;, &quot;green&quot;, &quot;yellow&quot;, and it automatically generates similar keywords (in this case, other colors). See more examples in [this YouTube video](http://youtu.be/dlslNhfrQmw).&#xD;&#xA;&#xD;&#xA;I would like to reproduce this in my own program. I'm thinking of using Freebase, and it would work like this intuitively: &#xD;&#xA;&#xD;&#xA;1. Retrieve the list of given words in Freebase;&#xD;&#xA;2. Find their &quot;common denominator(s)&quot; and construct a distance metric based on this;&#xD;&#xA;3. Rank other concepts based on their &quot;distance&quot; to the original keywords;&#xD;&#xA;4. Display the next closest concepts.&#xD;&#xA;&#xD;&#xA;As I'm not familiar with this area, my questions are:&#xD;&#xA;&#xD;&#xA;* Is there a better way to do this?&#xD;&#xA;* What tools are available for each step?" />
  <row Id="1141" PostHistoryTypeId="24" PostId="424" RevisionGUID="574d4027-47be-4068-8c5f-fb0852027fee" CreationDate="2014-06-19T05:48:43.540" Comment="Proposed by 322 approved by 906 edit id of 78" />
  <row Id="1142" PostHistoryTypeId="2" PostId="469" RevisionGUID="a280cf42-c4b6-4c22-b636-2b3d43d75b2d" CreationDate="2014-06-19T05:52:57.527" UserId="906" Text="I suggest this tutorial on ordered logit: http://www.ats.ucla.edu/stat/r/dae/ologit.htm&#xD;&#xA;&#xD;&#xA;It showcases the use of `polr` in the `MASS` package, and also explains the assumptions and how to interpret the results." />
  <row Id="1143" PostHistoryTypeId="2" PostId="470" RevisionGUID="73a3b3f6-e95b-414e-8d8d-42d89d001f53" CreationDate="2014-06-19T06:09:43.963" UserId="941" Text="Just an idea - your data is highly seasonal: daily and weekly cycles are quite perceptible. So first of all, try to decompose your variables (gas and electricity consumption, temperature, and solar radiation). [Here is][1] a nice tutorial on time series decomposition for R. &#xD;&#xA;&#xD;&#xA;After obtaining trend and seasonal components, the most interesting part begins. It's just an assumption, but I think, gas and electricity consumption variables would be quite predictable by means of time series analysis (e.g., [ARIMA model][2]). From my point of view, the most exiting part here is to try to predict residuals after decomposition, using available data (temperature anomalies, solar radiation, wind speed). I suppose, these residuals would be outliers, you are looking for. Hope, you will find this useful.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.r-bloggers.com/time-series-decomposition/&#xD;&#xA;  [2]: http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/tsa_arma_0.html" />
  <row Id="1144" PostHistoryTypeId="2" PostId="471" RevisionGUID="1bf91e3a-ac39-4898-b5c7-246de1b68f0f" CreationDate="2014-06-19T07:22:38.987" UserId="944" Text="One of the most widely used test collection for text categorization research (link below). I've used many times. Enjoy your exploration :)&#xD;&#xA;&#xD;&#xA;http://www.daviddlewis.com/resources/testcollections/reuters21578/&#xD;&#xA;or&#xD;&#xA;http://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection" />
  <row Id="1150" PostHistoryTypeId="5" PostId="466" RevisionGUID="5a509716-0342-4848-b9fb-10bf1c9b5008" CreationDate="2014-06-19T08:55:10.640" UserId="989" Comment="added 696 characters in body" Text="I tried to detect outliers in the energy gas consumption of some dutch buildings, building a neural network model. I have very bad results, but I don't find the reason. &#xD;&#xA;&#xD;&#xA;I am not an expert so I would like to ask you what I can improve and what I'm doing wrong. This is the complete description: https://github.com/denadai2/Gas-consumption-outliers.&#xD;&#xA;&#xD;&#xA;The neural network is a FeedFoward Network with Back Propagation. As described [here](http://nbviewer.ipython.org/github/denadai2/Gas-consumption-outliers/blob/master/3-%20Regression_NN.ipynb) I splitted the dataset in a &quot;small&quot; dataset of 41'000 rows, 9 features and I tried to add more features. &#xD;&#xA;&#xD;&#xA;I trained the networks but the results have 14.14 RMSE, so it can't predict so well the gas consumptions, consecutevely I can't run a good outlier detection mechanism. I see that in some papers that even if they predict daily or hourly consumption in the electric power, they have errors like MSE = 0.01.&#xD;&#xA;&#xD;&#xA;What can I improve? What am I doing wrong? Can you have a look of my description?" />
  <row Id="1151" PostHistoryTypeId="2" PostId="473" RevisionGUID="b87f711a-5d37-40b8-a9a0-c0238b58e1bd" CreationDate="2014-06-19T08:56:46.847" UserId="922" Text="The usual definition of regression (as far as I am aware) is _predicting a continuous output variable from a given set of input variables_. &#xD;&#xA;&#xD;&#xA;Logistic regression is a binary classification algorithm, so it produces a categorical output.&#xD;&#xA;&#xD;&#xA;Is it really a regression algorithm? If so, why?" />
  <row Id="1152" PostHistoryTypeId="1" PostId="473" RevisionGUID="b87f711a-5d37-40b8-a9a0-c0238b58e1bd" CreationDate="2014-06-19T08:56:46.847" UserId="922" Text="Is logistic regression a regression algorithm?" />
  <row Id="1153" PostHistoryTypeId="3" PostId="473" RevisionGUID="b87f711a-5d37-40b8-a9a0-c0238b58e1bd" CreationDate="2014-06-19T08:56:46.847" UserId="922" Text="&lt;algorithms&gt;&lt;logistic-regression&gt;" />
  <row Id="1154" PostHistoryTypeId="2" PostId="474" RevisionGUID="0ceee5d4-d4be-4a7a-bbdf-3b37355fba63" CreationDate="2014-06-19T09:42:28.160" UserId="957" Text="In network structure, what is the difference between **k-cliques** and **p-cliques**, can anyone give a brief explaination with examples? Thanks in advanced!" />
  <row Id="1155" PostHistoryTypeId="1" PostId="474" RevisionGUID="0ceee5d4-d4be-4a7a-bbdf-3b37355fba63" CreationDate="2014-06-19T09:42:28.160" UserId="957" Text="network structure — k-cliques vs p-cliques" />
  <row Id="1156" PostHistoryTypeId="3" PostId="474" RevisionGUID="0ceee5d4-d4be-4a7a-bbdf-3b37355fba63" CreationDate="2014-06-19T09:42:28.160" UserId="957" Text="&lt;social-network-analysis&gt;" />
  <row Id="1157" PostHistoryTypeId="2" PostId="475" RevisionGUID="ba76a6e2-b69e-4723-b3e5-c14b2993a39a" CreationDate="2014-06-19T09:50:53.657" UserId="418" Text="As you discuss the definition of regression is predicting a continuous variable. [Logistic regression][1] is a binary classifier.  Logistic regression is the application of a logit function, that turns (-inf,+inf) to [0,1], on the output of a usual regression approach. I think it is just for historical reasons that keeps that name. &#xD;&#xA;&#xD;&#xA;Saying something like &quot;I did some regression to classify images. In particular I used logistic regression.&quot; is wrong.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Logistic_regression" />
  <row Id="1158" PostHistoryTypeId="5" PostId="475" RevisionGUID="b562a709-afde-4f41-9f7a-8e8645aa119a" CreationDate="2014-06-19T09:59:24.653" UserId="418" Comment="added 9 characters in body" Text="As you discuss the definition of regression is predicting a continuous variable. [Logistic regression][1] is a binary classifier.  Logistic regression is the application of a logit function on the output of a usual regression approach. Logit function turns (-inf,+inf) to [0,1]. I think it is just for historical reasons that keeps that name. &#xD;&#xA;&#xD;&#xA;Saying something like &quot;I did some regression to classify images. In particular I used logistic regression.&quot; is wrong.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Logistic_regression" />
  <row Id="1160" PostHistoryTypeId="36" PostId="477" RevisionGUID="8e6903aa-1a4d-4fe9-9ea0-0864f03e149a" CreationDate="2014-06-19T10:30:43.993" UserId="-1" Comment="from http://programmers.stackexchange.com/questions/245430/preference-matching-algorithm" />
  <row Id="1161" PostHistoryTypeId="36" PostId="478" RevisionGUID="6d157aa2-efb6-4ee7-b314-ffbf3fbb9fec" CreationDate="2014-06-19T10:30:43.993" UserId="-1" Comment="from http://programmers.stackexchange.com/questions/245430/preference-matching-algorithm/245442#245442" />
  <row Id="1162" PostHistoryTypeId="2" PostId="478" RevisionGUID="671c143b-cc30-4630-aa60-210f1e7c36a1" CreationDate="2014-06-19T03:20:00.847" UserDisplayName="pipja" Text="This can be a cross between machine learning and simple matching exercise.&#xD;&#xA;&#xD;&#xA;I think X_i tend to be rather defined and finite, while A_i can be vague and not finite. From a pure algorithm perspective I would search for instances where X_i = A_i and store the results into a container of sort. The more hits for certain X'es where X_i_n = A_i_k the more points X scores. X'es are then presented to A in the order of points from best match to lowest match.&#xD;&#xA;&#xD;&#xA;Onto the machine learning mechanism, as the algorithm serves a lot of As (by that mean thousands and thousands, even millions) patterns will start to develop and certain combination of A_i's will be more prevalent, or in other words, worth more to other A_i's for a certain category of A. Using these patterns, the weighting of points will be re-balanced for higher chance of hitting the correct offers.&#xD;&#xA;&#xD;&#xA;Kind of like how a search engine works." />
  <row Id="1163" PostHistoryTypeId="2" PostId="477" RevisionGUID="fa93f037-4c55-4680-aee3-c6e59827ec32" CreationDate="2014-06-18T22:15:43.820" UserId="986" Text="There's this side project I'm working on where I need to structure a solution to the following problem.&#xD;&#xA;&#xD;&#xA; I have two groups of people (clients). Group &quot;A&quot; intends to buy and group &quot;B&quot; intends to sell a determined product &quot;X&quot;. &#xD;&#xA;&#xD;&#xA;The product has a series of attributes x_i and my objective is to facilitate the transaction between &quot;A&quot; e &quot;B&quot; by matching their preferences. The main idea is to point out to each member of &quot;A&quot; a corresponding in &quot;B&quot; who’s product better suits his needs, and vice versa. &#xD;&#xA;&#xD;&#xA;Some complicating aspects of the problem:&#xD;&#xA;&#xD;&#xA;1. The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design which is rare among the population and I can’t predict. Can’t previously list all the attributes;&#xD;&#xA;&#xD;&#xA;2. Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design).&#xD;&#xA;&#xD;&#xA;Any suggestion on how to approach this problem and solve it in an automated way?	 &#xD;&#xA;The idea is to really think out of the box here so feel free to &quot;go wild&quot; on your suggestions. &#xD;&#xA;&#xD;&#xA;I would also appreciate some references to other similar problems if possible. " />
  <row Id="1164" PostHistoryTypeId="1" PostId="477" RevisionGUID="fa93f037-4c55-4680-aee3-c6e59827ec32" CreationDate="2014-06-18T22:15:43.820" UserId="986" Text="Preference Matching Algorithm" />
  <row Id="1165" PostHistoryTypeId="3" PostId="477" RevisionGUID="fa93f037-4c55-4680-aee3-c6e59827ec32" CreationDate="2014-06-18T22:15:43.820" UserId="986" Text="&lt;algorithms&gt;" />
  <row Id="1167" PostHistoryTypeId="2" PostId="480" RevisionGUID="9bc0cfda-1405-49ca-804d-be124ea869c9" CreationDate="2014-06-19T10:35:37.190" UserId="1004" Text="One fairly powerful R package for regression with an ordinal categorical response is VGAM, on the CRAN. The vignette contains some examples of ordinal regression, but admittedly I have never tried it on such a large dataset, so I cannot estimate how long it may take. You may find some additional material about VGAM on the author's [page][1]. Alternatively you could take a look at Laura Thompson's [companion][2] to Agresti's book &quot;Categorical Data Analysis&quot;. Chapter 7 of Thompson's book describes cumulative logit models, which are frequently used with ordinal responses.&#xD;&#xA;&#xD;&#xA;Hope this helps!&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.stat.auckland.ac.nz/~yee/VGAM/&#xD;&#xA;  [2]: https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0CDUQFjAC&amp;url=http://home.comcast.net/~lthompson221/Splusdiscrete2.pdf&amp;ei=8bmiU4HSFcjA7Abk4YHgDg&amp;usg=AFQjCNHuBp2_nRpaPOFgdkcQWJGuSO9V6A&amp;sig2=hAy4d3mu9WCJZulqxCzraw&#xD;&#xA;" />
  <row Id="1168" PostHistoryTypeId="4" PostId="473" RevisionGUID="65a2b833-45f7-465a-9322-0fc1c214fd14" CreationDate="2014-06-19T10:55:38.920" UserId="922" Comment="edited title" Text="Is logistic regression actually a regression algorithm?" />
  <row Id="1171" PostHistoryTypeId="5" PostId="474" RevisionGUID="8753693b-33c0-4937-aa7b-8fe1c80fd139" CreationDate="2014-06-19T11:40:12.107" UserId="84" Comment="Improving formatting." Text="In network structure, what is the difference between **k-cliques** and **p-cliques**? Can anyone give a brief explaination with examples?" />
  <row Id="1172" PostHistoryTypeId="4" PostId="474" RevisionGUID="8753693b-33c0-4937-aa7b-8fe1c80fd139" CreationDate="2014-06-19T11:40:12.107" UserId="84" Comment="Improving formatting." Text="Network structure: k-cliques vs. p-cliques" />
  <row Id="1173" PostHistoryTypeId="6" PostId="474" RevisionGUID="8753693b-33c0-4937-aa7b-8fe1c80fd139" CreationDate="2014-06-19T11:40:12.107" UserId="84" Comment="Improving formatting." Text="&lt;definitions&gt;&lt;social-network-analysis&gt;&lt;graphs&gt;" />
  <row Id="1176" PostHistoryTypeId="2" PostId="481" RevisionGUID="6a8aa682-6c2a-4766-98eb-8df7f230bbbb" CreationDate="2014-06-19T12:22:42.017" UserId="59" Text="In graph theory a clique indicates a fully connected set of nodes: [as noted here][1], a p-clique simply indicates a clique comoprised of p nodes. A k-clique is an undirected graph and a number k, and the output is a clique of size k if one exists.&#xD;&#xA;&#xD;&#xA;[Clique Problem][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://books.google.com/books?id=E3-OSVSPbU0C&amp;pg=PA40&amp;lpg=PA40&amp;dq=%22graph%20theory%22,%20%22p-clique%22&amp;source=bl&amp;ots=smbhcK-9AC&amp;sig=X0v_EbqSqB4WBbudgPbqo_j1pvk&amp;hl=en&amp;sa=X&amp;ei=VNWiU8KaA9WxsQSQhYGIAg&amp;ved=0CDEQ6AEwBA#v=onepage&amp;q=%22graph%20theory%22,%20%22p-clique%22&amp;f=false&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Clique_problem" />
  <row Id="1177" PostHistoryTypeId="5" PostId="455" RevisionGUID="ada4a3e9-319b-471c-a3b1-bef861fc674a" CreationDate="2014-06-19T12:41:13.937" UserId="922" Comment="Make the title an actual question; tidy up content" Text="Which freely available datasets can I use to train a text classifier?" />
  <row Id="1178" PostHistoryTypeId="4" PostId="455" RevisionGUID="ada4a3e9-319b-471c-a3b1-bef861fc674a" CreationDate="2014-06-19T12:41:13.937" UserId="922" Comment="Make the title an actual question; tidy up content" Text="Suggest text classifier training datasets" />
  <row Id="1179" PostHistoryTypeId="24" PostId="455" RevisionGUID="ada4a3e9-319b-471c-a3b1-bef861fc674a" CreationDate="2014-06-19T12:41:13.937" Comment="Proposed by 922 approved by 84, 960 edit id of 81" />
  <row Id="1181" PostHistoryTypeId="2" PostId="482" RevisionGUID="2ac0a2a5-1cbf-47b0-9652-4a724d680c02" CreationDate="2014-06-19T13:23:53.387" UserId="178" Text="**Short Answer**&#xD;&#xA;&#xD;&#xA;Yes, logistic regression is a regression algorithm and it does predict a continuous outcome: the probability of an event.  That we use it as a binary classifier is due to the interpretation of the outcome.&#xD;&#xA;&#xD;&#xA;**Detail**&#xD;&#xA;&#xD;&#xA;Logistic regression is a type of generalize linear regression model.&#xD;&#xA;&#xD;&#xA;In an ordinary linear regression model, a continuous outcome, `y`, is modeled as the sum of the product of predictors and their effect:&#xD;&#xA;&#xD;&#xA;    y = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + e&#xD;&#xA;&#xD;&#xA;where `e` is the error.&#xD;&#xA;&#xD;&#xA;Generalized linear models do not model `y` directly.  Instead, they use transformations to expand the domain of `y` to all real numbers.  This transformation is called the link function.  For logistic regression the link function is the logit function (usually, see note below).&#xD;&#xA;&#xD;&#xA;The logit function is defined as &#xD;&#xA;&#xD;&#xA;    ln(y/(1 + y))&#xD;&#xA;&#xD;&#xA;Thus the form of logistic regression is:&#xD;&#xA;&#xD;&#xA;    ln(y/(1 + y)) = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + e&#xD;&#xA;&#xD;&#xA;where `y` is the probability of an event.&#xD;&#xA;&#xD;&#xA;The fact that we use it as a binary classifier is due to the interpretation of the outcome." />
  <row Id="1182" PostHistoryTypeId="5" PostId="482" RevisionGUID="a5a20204-17e2-48aa-bcb2-0bbdd28dae41" CreationDate="2014-06-19T13:47:55.017" UserId="178" Comment="Add forgotten note." Text="**Short Answer**&#xD;&#xA;&#xD;&#xA;Yes, logistic regression is a regression algorithm and it does predict a continuous outcome: the probability of an event.  That we use it as a binary classifier is due to the interpretation of the outcome.&#xD;&#xA;&#xD;&#xA;**Detail**&#xD;&#xA;&#xD;&#xA;Logistic regression is a type of generalize linear regression model.&#xD;&#xA;&#xD;&#xA;In an ordinary linear regression model, a continuous outcome, `y`, is modeled as the sum of the product of predictors and their effect:&#xD;&#xA;&#xD;&#xA;    y = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + e&#xD;&#xA;&#xD;&#xA;where `e` is the error.&#xD;&#xA;&#xD;&#xA;Generalized linear models do not model `y` directly.  Instead, they use transformations to expand the domain of `y` to all real numbers.  This transformation is called the link function.  For logistic regression the link function is the logit function (usually, see note below).&#xD;&#xA;&#xD;&#xA;The logit function is defined as &#xD;&#xA;&#xD;&#xA;    ln(y/(1 + y))&#xD;&#xA;&#xD;&#xA;Thus the form of logistic regression is:&#xD;&#xA;&#xD;&#xA;    ln(y/(1 + y)) = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + e&#xD;&#xA;&#xD;&#xA;where `y` is the probability of an event.&#xD;&#xA;&#xD;&#xA;The fact that we use it as a binary classifier is due to the interpretation of the outcome.&#xD;&#xA;&#xD;&#xA;Note: probit is another link function used for logistic regression but logit is the most widely used." />
  <row Id="1183" PostHistoryTypeId="2" PostId="483" RevisionGUID="078d7be5-da61-4907-b824-224a8dc778f5" CreationDate="2014-06-19T14:15:07.433" UserId="968" Text="You could try Neural Network. You can find 2 great explanations on how to apply NN on time series [here][1] and [here][2].&#xD;&#xA;&#xD;&#xA;Note that it is best practice to :&#xD;&#xA;&#xD;&#xA; - Deseasonalize/detrend the input data (so that the NN will not learn the seasonality).&#xD;&#xA; - Rescale/Normalize the input data.&#xD;&#xA;&#xD;&#xA;Because what you are looking for is a regression problem, the activation functions should be `linear` and not `sigmoid` or `tanh` and you aim to minimize the `sum-of-squares error` (as opposition to the maximization of the `negative log-likelihood` in a classification problem).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stats.stackexchange.com/questions/10162/how-to-apply-neural-network-to-time-series-forecasting&#xD;&#xA;  [2]: http://stackoverflow.com/questions/18670558/prediction-using-recurrent-neural-network-on-time-series-dataset" />
  <row Id="1184" PostHistoryTypeId="5" PostId="452" RevisionGUID="586d3767-846b-45d4-a1db-e0b7a83d96b1" CreationDate="2014-06-19T14:32:43.503" UserId="403" Comment="clarified feature matrix." Text="As @Christopher Lauden mentioned above, time-series analysis is most appropriate for this sort of thing. If, however, you wished to do a more traditional &quot;machine learning approach&quot;, something that I have done in the past is to block up your data into overlapping windows of time as features, then use it to predict the next days (or weeks) traffic.&#xD;&#xA;&#xD;&#xA;Your feature matrix would be something like:&#xD;&#xA;&#xD;&#xA;    t1 | t2 | ... | tN&#xD;&#xA;    t2 | t3 | ... | tN+1&#xD;&#xA;    t3 | t4 | ... | tN+2&#xD;&#xA;    ...&#xD;&#xA;    tW | tW+1 | ... |tN+W&#xD;&#xA;&#xD;&#xA;where `tI` is the traffic on day `I`. The feature you'll be predicting is the traffic on the day after the last column. In essence, use a window of traffic to predict the next day's traffic. &#xD;&#xA;&#xD;&#xA;Any sort of ML model would work for this. &#xD;&#xA;&#xD;&#xA;**Edit**&#xD;&#xA;&#xD;&#xA;In response to the question, &quot;can you elaborate on how you use this feature matrix&quot;:&#xD;&#xA;&#xD;&#xA;The feature matrix has values indicating past traffic over a period of time (for instance, hourly traffic over 1 week), and we use this to predict traffic for some specified time period in the future. We take our historic data and build a feature matrix of historic traffic and label this with the traffic at some period in the future (e.g. 2 days after the window in the feature). Using some sort of regression machine learning model, we can take historic traffic data, and try and build a model that can predict how traffic moved in our historic data set. The presumption is that future traffic will resemble past traffic.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1185" PostHistoryTypeId="2" PostId="484" RevisionGUID="43fd78ad-1954-498e-b8b9-6b94984e0eeb" CreationDate="2014-06-19T14:52:59.877" UserId="1011" Text="Logistic regression is regression, first and foremost. It becomes a classifier by adding a decision rule. I will give an example that goes backwards. That is, instead of taking data and fitting a model, I'm going to start with the model in order to show how this is truly a regression problem.&#xD;&#xA;&#xD;&#xA;In logistic regression, we are modeling the log odds, or logit, that an event occurs, which is a continuous quantity. If the probability that event A occurs is P(A), the odds are:&#xD;&#xA;&#xD;&#xA;P(A) / (1 - P(A)) &#xD;&#xA;&#xD;&#xA;The log odds, then, are:&#xD;&#xA;&#xD;&#xA;log { (P(A) / (1 - P(A))) }&#xD;&#xA;&#xD;&#xA;As in linear regression, we model this with a linear combination of coefficients and predictors:&#xD;&#xA;&#xD;&#xA;logit = b0 + b1 * x1 + b2 * x2 ...&#xD;&#xA;&#xD;&#xA;Imagine we are given a model of whether a person has gray hair. Our model uses age as the only predictor. Here, our event A = a person has gray hair:&#xD;&#xA;&#xD;&#xA;log odds of gray hair = -10 + 0.25 * age&#xD;&#xA;&#xD;&#xA;...Regression! Here is some Python code and a plot:&#xD;&#xA;&#xD;&#xA;    %matplotlib inline&#xD;&#xA;    import matplotlib.pyplot as plt&#xD;&#xA;    import numpy as np&#xD;&#xA;    import seaborn as sns&#xD;&#xA;    &#xD;&#xA;    x = np.linspace(0, 100, 100)&#xD;&#xA;    &#xD;&#xA;    def log_odds(x):&#xD;&#xA;        return -10 + .25 * x&#xD;&#xA;    &#xD;&#xA;    plt.plot(x, log_odds(x))&#xD;&#xA;    plt.xlabel(&quot;age&quot;)&#xD;&#xA;    plt.ylabel(&quot;log odds of gray hair&quot;)&#xD;&#xA;&#xD;&#xA;![plot of the log odds for our toy example][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Now, let's make it a classifier. First, we need to transform the log odds to get out our probability P(A). We can use the sigmoid function:&#xD;&#xA;&#xD;&#xA;P(A) = 1 / (1 + exp(-log odds))&#xD;&#xA;&#xD;&#xA;Here's the code:&#xD;&#xA;&#xD;&#xA;    plt.plot(x, 1 / (1 + np.exp(-log_odds(x))))&#xD;&#xA;    plt.xlabel(&quot;age&quot;)&#xD;&#xA;    plt.ylabel(&quot;probability of gray hair&quot;)&#xD;&#xA;&#xD;&#xA;![plot of the probability of gray hair for our toy example][2]&#xD;&#xA;&#xD;&#xA;The last thing we need to make this a classifier is to add a decision rule. One very common rule is to classify a success whenever P(A) &gt; 0.5. We will adopt that rule, which implies that our classifier will predict gray hair whenever a person is older than 40 and will predict non-gray hair whenever a person is under 40.&#xD;&#xA;&#xD;&#xA;Logistic regression works great as a classifier in more realistic examples too, but before it can be a classifier, it must be a regression technique!&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/xR0OT.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/hSpCa.png" />
  <row Id="1186" PostHistoryTypeId="5" PostId="461" RevisionGUID="be7b2d6f-ce4d-4ef1-91b0-7c299d62af01" CreationDate="2014-06-19T14:55:34.833" UserId="84" Comment="Improving formatting." Text="There's this side project I'm working on where I need to structure a solution to the following problem.&#xD;&#xA;&#xD;&#xA;I have two groups of people (clients). Group `A` intends to buy and group `B` intends to sell a determined product `X`. The product has a series of attributes `x_i`, and my objective is to facilitate the transaction between `A` and `B` by matching their preferences. The main idea is to point out to each member of `A` a corresponding in `B` whose product better suits his needs, and vice versa. &#xD;&#xA;&#xD;&#xA;Some complicating aspects of the problem:&#xD;&#xA;&#xD;&#xA;1. The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can't predict. Can't previously list all the attributes;&#xD;&#xA;	 &#xD;&#xA;2. Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design);&#xD;&#xA;&#xD;&#xA;Any suggestion on how to approach this problem and solve it in an automated way? The idea is to really think out of the box here, so feel free to &quot;go wild&quot; on your suggestions.&#xD;&#xA;&#xD;&#xA;I would also appreciate some references to other similar problems if possible. " />
  <row Id="1187" PostHistoryTypeId="6" PostId="461" RevisionGUID="be7b2d6f-ce4d-4ef1-91b0-7c299d62af01" CreationDate="2014-06-19T14:55:34.833" UserId="84" Comment="Improving formatting." Text="&lt;bigdata&gt;&lt;text-mining&gt;&lt;recommendation&gt;" />
  <row Id="1188" PostHistoryTypeId="24" PostId="461" RevisionGUID="be7b2d6f-ce4d-4ef1-91b0-7c299d62af01" CreationDate="2014-06-19T14:55:34.833" Comment="Proposed by 84 approved by 50 edit id of 79" />
  <row Id="1189" PostHistoryTypeId="5" PostId="447" RevisionGUID="e4bb9b6a-b180-459f-8233-2f1321bca0c2" CreationDate="2014-06-19T14:55:59.483" UserId="84" Comment="Improving formatting." Text="I have a large number of samples which represent Manchester encoded bit streams as audio signals. The frequency at which they are encoded is the primary frequency component when it is high, and there is a consistent amount of white noise in the background.&#xD;&#xA;&#xD;&#xA;I have manually decoded these streams, but I was wondering if I could use some sort of machine learning technique to learn the encoding schemes. This would save a great deal of time manually recognizing these schemes. The difficulty is that different signals are encoded differently.&#xD;&#xA;&#xD;&#xA;Is it possible to build a model which can learn to decode more than one encoding scheme? How robust would such a model be, and what sort of techniques would I want to employ? [Independent Component Analysis](http://en.wikipedia.org/wiki/Independent_component_analysis) (ICA) seems like could be useful for isolating the frequency I care about, but how would I learn the encoding scheme?" />
  <row Id="1190" PostHistoryTypeId="24" PostId="447" RevisionGUID="e4bb9b6a-b180-459f-8233-2f1321bca0c2" CreationDate="2014-06-19T14:55:59.483" Comment="Proposed by 84 approved by 50 edit id of 80" />
  <row Id="1191" PostHistoryTypeId="2" PostId="485" RevisionGUID="bf59d5a0-b268-42e7-92d2-4f9cc8edeb92" CreationDate="2014-06-19T16:07:29.907" UserId="1015" Text="My answer would be no. I consider Data mining to be one of the miscellaneous fields in Data science.Data Mining is mostly considered on yielding questions rather than answering them.It is often termed as &quot;detecting something new&quot; when compared to Data science where the data scientist try to solve complex problems to be able to reach their end results.However both terms have many commonalities between them" />
  <row Id="1192" PostHistoryTypeId="5" PostId="485" RevisionGUID="261a7094-5b73-4603-be64-374dd16be225" CreationDate="2014-06-19T16:52:55.683" UserId="84" Comment="Improving formatting." Text="My answer would be no. I consider Data mining to be one of the miscellaneous fields in Data science. Data Mining is mostly considered on yielding questions rather than answering them. It is often termed as &quot;detecting something new&quot;, when compared to Data science, where the data scientist try to solve complex problems to be able to reach their end results. However both terms have many commonalities between them." />
  <row Id="1196" PostHistoryTypeId="2" PostId="487" RevisionGUID="04fcc946-42ae-4b4e-93b7-32e9be24da70" CreationDate="2014-06-19T18:01:36.120" UserId="1011" Text="A good list of publicly available social network datasets can be found on the Stanford Network Analysis Project website: &#xD;&#xA;&#xD;&#xA;[SNAP datasets][1]&#xD;&#xA;&#xD;&#xA;The site contains internet social network data (Facebook, Twitter, Google Plus), Citation networks for academic journals, co-purchasing networks from Amazon and several others kinds of networks. They have directed, undirected, and bipartite graphs and all datasets are snapshots that can be downloaded in compressed form.&#xD;&#xA;&#xD;&#xA;  [1]: https://snap.stanford.edu/data/" />
  <row Id="1197" PostHistoryTypeId="2" PostId="488" RevisionGUID="9a9d7e43-e618-4ce6-a410-2a9d79ae8db4" CreationDate="2014-06-19T18:02:24.650" UserId="1021" Text="I thought that generalized linear model (GLM) would be considered a statistical model, but a friend told me that some papers classify it as a machine learning technique. Which one is true (or more precise)? Any explanation would be appreciated.&#xD;&#xA;&#xD;&#xA;P.S. I am obviously a beginner in this field." />
  <row Id="1198" PostHistoryTypeId="1" PostId="488" RevisionGUID="9a9d7e43-e618-4ce6-a410-2a9d79ae8db4" CreationDate="2014-06-19T18:02:24.650" UserId="1021" Text="Is GLM a statistical or machine learning model?" />
  <row Id="1199" PostHistoryTypeId="3" PostId="488" RevisionGUID="9a9d7e43-e618-4ce6-a410-2a9d79ae8db4" CreationDate="2014-06-19T18:02:24.650" UserId="1021" Text="&lt;machine-learning&gt;&lt;statistics&gt;" />
  <row Id="1200" PostHistoryTypeId="2" PostId="489" RevisionGUID="3eb2224f-342c-4dc1-9726-f07c9a7ae182" CreationDate="2014-06-19T18:05:51.070" UserId="1011" Text="A GLM is absolutely a statistical model, but statistical models and machine learning techniques are not mutually exclusive. In general, statistics is more concerned with inferring parameters, whereas in machine learning, prediction is the ultimate goal." />
  <row Id="1202" PostHistoryTypeId="2" PostId="491" RevisionGUID="95b2404a-77b5-46e2-939c-25af72d40fc1" CreationDate="2014-06-19T18:24:32.023" UserId="941" Text="As suggested, «going wild». First of all, correct, if I’m wrong:&#xD;&#xA;&#xD;&#xA;* for exact product just a few features exist;&#xD;&#xA;* there is no ultimate features list, and clients are able to add new features to their products;&#xD;&#xA;&#xD;&#xA;If so, constructing full product-feature table could be computational expensive. And final data table would be extremely sparse.&#xD;&#xA;&#xD;&#xA;So, the first step is narrowing customers (products) list for matching. Let’s build bipartite graph, where sellers would be type-1 nodes, and buyers would be type-2 nodes. Reference to similar product feature creates edge between seller and buyer. Here is a sketch:&#xD;&#xA;&#xD;&#xA;![graph][1]&#xD;&#xA;&#xD;&#xA;Using graph, described above, for every exact seller’s product you can select only buyers, who is interested in matching features (it’s possible to filter «at least one» common feature, full matching, or threshold level). But certainly, that’s not enough. Next step is to compare feature values, described by seller and buyer. There are a lot of variants (e.g., kNN, or something else). But why not to try to solve this quest, using already existing graph? Let’s add weights to edges:&#xD;&#xA;&#xD;&#xA;* for continuous features (e.g., price): &#xD;&#xA;&#xD;&#xA;  ![price_weight][2]&#xD;&#xA;&#xD;&#xA;* for binary and non-quantifiable features - just logical biconditional:&#xD;&#xA;&#xD;&#xA;  ![feature_weight][3]&#xD;&#xA;&#xD;&#xA;The main idea here is to «scale» every feature to `[0; 1]` interval. Additionally, we can use feature coefficients to determine most important features. E.g., assuming price is twice more important than availability of some rare function: &#xD;&#xA;&#xD;&#xA;![adj_w_1][4]&#xD;&#xA;&#xD;&#xA;![adj_w_2][5]&#xD;&#xA;&#xD;&#xA;Almost final step: simplifying graph structure and reducing variety of edges to one edge with weight equal to sum of previously calculated weights of different features. With reduced structure every pair of customers/products could have only one edge (no parallel edges). So, to find the best deal for exact seller you just need to select connected buyers with max weighted edges.&#xD;&#xA;&#xD;&#xA;&gt;! Future challenge: introduce cheap method for weighting edges on first step :)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/q1JTN.png&#xD;&#xA;  [2]: https://chart.googleapis.com/chart?cht=tx&amp;chl=weight%20%3D%20%5Cfrac%7B%7Cprice_s_e_l_l%20-%20price_b_u_y%7C%7D%7B%5Cmax%28price%29%20-%20%5Cmin%28price%29%7D&#xD;&#xA;  [3]: https://chart.googleapis.com/chart?cht=tx&amp;chl=weight%20%3D%20feature_s_e_l_l%20%5Cleftrightarrow%20feature_b_u_y&#xD;&#xA;  [4]: https://chart.googleapis.com/chart?cht=tx&amp;chl=adj.price.weight%20%3D%202%20%5Ctimes%20price.weight&#xD;&#xA;  [5]: https://chart.googleapis.com/chart?cht=tx&amp;chl=adj.feature.weight%20%3D%201%20%5Ctimes%20feature.weight" />
  <row Id="1204" PostHistoryTypeId="2" PostId="492" RevisionGUID="2353374e-483d-4c9d-ab15-54daa423ef88" CreationDate="2014-06-19T19:29:57.797" UserId="684" Text="I'm looking to use google's word2vec implementation to build a named entity recognition system.  I've heard that recursive neural nets with back propagation through structure are well suited for named entity recognition tasks, but I've been unable to find a decent implementation or a decent tutorial for that type of model. Because I'm working with an atypical corpus, standard NER tools in NLTK and similar have performed very poorly, and it looks like I'll have to train my own system.   &#xD;&#xA;&#xD;&#xA;In short, what resources are available for this kind of problem?  Is there a standard recursive neural net implementation available? " />
  <row Id="1205" PostHistoryTypeId="1" PostId="492" RevisionGUID="2353374e-483d-4c9d-ab15-54daa423ef88" CreationDate="2014-06-19T19:29:57.797" UserId="684" Text="Word2Vec for Named Entity Recognition" />
  <row Id="1206" PostHistoryTypeId="3" PostId="492" RevisionGUID="2353374e-483d-4c9d-ab15-54daa423ef88" CreationDate="2014-06-19T19:29:57.797" UserId="684" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;nlp&gt;&lt;neuralnetwork&gt;" />
  <row Id="1207" PostHistoryTypeId="2" PostId="493" RevisionGUID="527a0458-8c01-4119-a171-8bd189c5ce77" CreationDate="2014-06-19T19:33:12.067" UserId="1029" Text="Well firstly I would not even use things like Machine learning without having in depth knowledge.&#xD;&#xA;&#xD;&#xA;Simplistic things I would do if I had this time series is:&#xD;&#xA;&#xD;&#xA;1) Write sql queries to understand which of the times you have the busiest , avg and low foot traffic.&#xD;&#xA;&#xD;&#xA;2) Then try to visualize the whole time series , and you could use basic pattern matching algorithms to pick up patterns.&#xD;&#xA;&#xD;&#xA;This two things will help you understand what your data set is telling you.&#xD;&#xA;&#xD;&#xA;Then with that in hand you will probably in a better state to use machine learning algorithms.&#xD;&#xA;&#xD;&#xA;Also , i am currently working in building something on time series and using time series analysis will help you much more than machine learning.&#xD;&#xA;&#xD;&#xA;For example there are pattern recognition that you can use that uses every day data to show patterns and ones which use up as much as 3 - 6 months of data to catch a pattern.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1208" PostHistoryTypeId="2" PostId="494" RevisionGUID="a8b5d6c6-de68-4f84-9ee3-d4b8cd919d3c" CreationDate="2014-06-19T19:33:40.620" UserId="1028" Text="Here is the current format of my data set&#xD;&#xA;&#xD;&#xA;    User-id | Threat_score&#xD;&#xA;    aaa       45&#xD;&#xA;    bbb       32&#xD;&#xA;    ccc       20&#xD;&#xA;&#xD;&#xA;The list contains the top 100 users with the highest threat scores. I generate such a list monthly. and have them stored in separate monthly files.&#xD;&#xA;&#xD;&#xA;There are two things I would like to get from this data&#xD;&#xA;&lt;ul&gt;1. Users who are consistently showing up in this list&lt;/ul&gt;&#xD;&#xA;&lt;ul&gt;2. Users who are consistently showing up in this list with high risk score &lt;/ul&gt;&#xD;&#xA;&lt;ul&gt;3. Users who have/reaching the high risk level very fast.&lt;/ul&gt;&#xD;&#xA;&#xD;&#xA;I am thinking a visual summary would be something nice. Like each month (somehow) decide the users whose past trend i want to plot and draw a graph with historic threat scores.&#xD;&#xA;&#xD;&#xA;My Question(s): Are there any known visualization techniques that have dealt with similar requirements.&#xD;&#xA;What ways should i be transforming my current data to achieve what i am looking for.&#xD;&#xA;&#xD;&#xA;Any suggestions in the right direction are welcome!&#xD;&#xA;Thanks" />
  <row Id="1209" PostHistoryTypeId="1" PostId="494" RevisionGUID="a8b5d6c6-de68-4f84-9ee3-d4b8cd919d3c" CreationDate="2014-06-19T19:33:40.620" UserId="1028" Text="Techniques for trend extraction from dataset" />
  <row Id="1210" PostHistoryTypeId="3" PostId="494" RevisionGUID="a8b5d6c6-de68-4f84-9ee3-d4b8cd919d3c" CreationDate="2014-06-19T19:33:40.620" UserId="1028" Text="&lt;visualization&gt;&lt;graphs&gt;&lt;dataset&gt;" />
  <row Id="1211" PostHistoryTypeId="5" PostId="492" RevisionGUID="2b97fd9b-3bb3-4f3b-8d55-348653d76fff" CreationDate="2014-06-19T19:46:13.427" UserId="684" Comment="deleted 1 character in body" Text="I'm looking to use google's word2vec implementation to build a named entity recognition system.  I've heard that recursive neural nets with back propagation through structure are well suited for named entity recognition tasks, but I've been unable to find a decent implementation or a decent tutorial for that type of model. Because I'm working with an atypical corpus, standard NER tools in NLTK and similar have performed very poorly, and it looks like I'll have to train my own system.   &#xD;&#xA;&#xD;&#xA;In short, what resources are available for this kind of problem?  Is there a standard recursive neural net implementation available?" />
  <row Id="1212" PostHistoryTypeId="6" PostId="492" RevisionGUID="50af793a-04b7-4379-820c-afb32af7ce9c" CreationDate="2014-06-19T20:15:58.600" UserId="322" Comment="add recommendation tag" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;nlp&gt;&lt;neuralnetwork&gt;&lt;recommendation&gt;" />
  <row Id="1213" PostHistoryTypeId="24" PostId="492" RevisionGUID="50af793a-04b7-4379-820c-afb32af7ce9c" CreationDate="2014-06-19T20:15:58.600" Comment="Proposed by 322 approved by 684 edit id of 84" />
  <row Id="1214" PostHistoryTypeId="2" PostId="495" RevisionGUID="4b3a788f-dedd-4e1b-9d3f-98a251a32224" CreationDate="2014-06-19T22:02:44.050" UserId="1011" Text="I would add a third column called `month` and then concatenate each list. So if you have a top 100 list for 5 months you will create one big table with 500 entries:&#xD;&#xA;&#xD;&#xA;    User-id | Threat_score | month&#xD;&#xA;    aaa       45             1&#xD;&#xA;    bbb       32             1&#xD;&#xA;    ccc       20             1&#xD;&#xA;    ...       ...            ...&#xD;&#xA;    bbb       64             2&#xD;&#xA;    ccc       29             2&#xD;&#xA;    ...       ...            ...&#xD;&#xA;&#xD;&#xA;Then, to answer your first question, you could simply count the occurrences of each user-id. For example, if user `bbb` is in your concatenated table five times, then you know that person made your list all five months.&#xD;&#xA;&#xD;&#xA;To answer you second question, you could do a `group by` operation to compute some aggregate function of the users. A `group by` operation with an average function is a little crude and sensitive to outliers, but it would probably get you close to what you are looking for.&#xD;&#xA;&#xD;&#xA;One possibility for the third question is to compute the difference in threat score between month `n-1` and month `n`. That is, for each month (not including the first month) you subtract the user's previous threat score from the current threat score. You can make this a new column so your table would now look like:&#xD;&#xA;&#xD;&#xA;    User-id | Threat_score | month | difference&#xD;&#xA;    aaa       45             1       null&#xD;&#xA;    bbb       32             1       null&#xD;&#xA;    ccc       20             1       null&#xD;&#xA;    ...       ...            ...     ...&#xD;&#xA;    bbb       64             2       32&#xD;&#xA;    ccc       29             2       9&#xD;&#xA;    ...       ...            ...&#xD;&#xA;&#xD;&#xA;With this table, you could again do a `group by` operation to find people who consistently have a higher threat score than the previous month or you could simply find people with a large difference between the current month and the previous month.&#xD;&#xA;&#xD;&#xA;As you suggest, visualizing this data is a really good idea. If you care about these threat scores over time (which I think you do), I strongly recommend a simple line chart, with month on the x-axis and threat score on the y-axis. It's not fancy, but it's extremely easy to interpret and should give you useful information about the trends.&#xD;&#xA;&#xD;&#xA;Most of this stuff (not the visualization) can be done in SQL and all of it can be done in R or Python (and many other languages). Good luck!" />
  <row Id="1217" PostHistoryTypeId="5" PostId="455" RevisionGUID="6d001ad2-75be-475b-9cc1-78d84ea4d05e" CreationDate="2014-06-19T23:37:28.490" UserId="960" Comment="added 662 characters in body" Text="Which freely available datasets can I use to train a text classifier?&#xD;&#xA;&#xD;&#xA;We are trying to enhance our users engagement by recommending the most related content for him, so we thought If we classified our content based on a predefined bag of words we can recommend to him engaging content by getting his feedback on random number of posts already classified before.&#xD;&#xA;&#xD;&#xA;We can use this info to recommend for him pulses labeled with those classes. But we found If we used a predefined bag of words not related to our content the feature vector will be full of zeros, also categories may be not relevant to our content. so for those reasons we tried another solution that will be clustering our content not classifying it.&#xD;&#xA;&#xD;&#xA;Thanks :)" />
  <row Id="1222" PostHistoryTypeId="5" PostId="493" RevisionGUID="043f2bbb-1c42-4c36-adfa-aad5b7b3d470" CreationDate="2014-06-20T00:34:48.087" UserId="84" Comment="Improving answer." Text="Well, firstly, I would not even use things like Machine learning without having in depth knowledge. Simplistic things I would do if I had this time series is:&#xD;&#xA;&#xD;&#xA;1. Write sql queries to understand which of the times you have the busiest, average and low foot traffic.&#xD;&#xA;2. Then try to visualize the whole time series, and you could use basic pattern matching algorithms to pick up patterns.&#xD;&#xA;&#xD;&#xA;This two things will help you understand what your data set is telling you. Then, with that in hand, you will probably be in a better state to use machine learning algorithms.&#xD;&#xA;&#xD;&#xA;Also, I'm currently working in building something on time series, and using time series analysis will help you much more than machine learning. For example, there are pattern recognition algorithms that you can use that uses every day data to show patterns, and ones which use up to as much as 3 to 6 months of data to catch a pattern." />
  <row Id="1224" PostHistoryTypeId="6" PostId="422" RevisionGUID="c542f43c-3d05-43e6-b2d0-2dd854de6e81" CreationDate="2014-06-20T01:04:04.270" UserId="84" Comment="Correcting tag." Text="&lt;open-source&gt;&lt;dataset&gt;&lt;crawling&gt;" />
  <row Id="1225" PostHistoryTypeId="5" PostId="494" RevisionGUID="d37b276d-8a19-4c87-9d33-fc755755d0b5" CreationDate="2014-06-20T01:22:44.263" UserId="1011" Comment="Changed &quot;two things&quot; to &quot;three things&quot; since there are three items in the list" Text="Here is the current format of my data set&#xD;&#xA;&#xD;&#xA;    User-id | Threat_score&#xD;&#xA;    aaa       45&#xD;&#xA;    bbb       32&#xD;&#xA;    ccc       20&#xD;&#xA;&#xD;&#xA;The list contains the top 100 users with the highest threat scores. I generate such a list monthly. and have them stored in separate monthly files.&#xD;&#xA;&#xD;&#xA;There are three things I would like to get from this data&#xD;&#xA;&lt;ul&gt;1. Users who are consistently showing up in this list&lt;/ul&gt;&#xD;&#xA;&lt;ul&gt;2. Users who are consistently showing up in this list with high risk score &lt;/ul&gt;&#xD;&#xA;&lt;ul&gt;3. Users who have/reaching the high risk level very fast.&lt;/ul&gt;&#xD;&#xA;&#xD;&#xA;I am thinking a visual summary would be something nice. Like each month (somehow) decide the users whose past trend i want to plot and draw a graph with historic threat scores.&#xD;&#xA;&#xD;&#xA;My Question(s): Are there any known visualization techniques that have dealt with similar requirements.&#xD;&#xA;What ways should i be transforming my current data to achieve what i am looking for.&#xD;&#xA;&#xD;&#xA;Any suggestions in the right direction are welcome!&#xD;&#xA;Thanks" />
  <row Id="1226" PostHistoryTypeId="24" PostId="494" RevisionGUID="d37b276d-8a19-4c87-9d33-fc755755d0b5" CreationDate="2014-06-20T01:22:44.263" Comment="Proposed by 1011 approved by 84, 1028 edit id of 85" />
  <row Id="1229" PostHistoryTypeId="5" PostId="474" RevisionGUID="4e44d84b-cbb4-4a8a-a9c5-025fc0fff690" CreationDate="2014-06-20T02:27:28.790" UserId="957" Comment="added 261 characters in body" Text="In network structure, what is the difference between **k-cliques** and **p-cliques**, can anyone give a brief explaination with examples? Thanks in advanced!&#xD;&#xA;&#xD;&#xA;============================&#xD;&#xA;&lt;br&gt;EDIT:&#xD;&#xA;I found an online [ppt][1] while I am googling, please take a look on **p.37** and **p.39**, can you comment on them?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://open.umich.edu/sites/default/files/SI508-F08-Week7-Lab6.ppt" />
  <row Id="1230" PostHistoryTypeId="2" PostId="496" RevisionGUID="0c2cb9c2-18c2-469c-9e32-25aa0f3363e3" CreationDate="2014-06-20T03:18:07.653" UserId="434" Text="You have three questions to answer and 100 records per month to analyze.&#xD;&#xA;&#xD;&#xA;Based on this size, I'd recommend doing analysis in a simple SQL database or a spreadsheet to start off with.  The first two questions are fairly easy to figure out.  The third is a little more difficult.&#xD;&#xA;&#xD;&#xA;I'd definitely add a column for month and group all of that data together into a spreadsheet or database table given the questions you want to answer.&#xD;&#xA;&#xD;&#xA;question 1. Users who are consistently showing up in this list&#xD;&#xA;&#xD;&#xA;In excel, this answer should help you out:  http://superuser.com/questions/442653/ms-excel-how-count-occurence-of-item-in-a-list&#xD;&#xA;&#xD;&#xA;For a SQL database:  http://stackoverflow.com/questions/2516546/select-count-duplicates&#xD;&#xA;&#xD;&#xA;question 2. Users who are consistently showing up in this list with high risk score &#xD;&#xA;&#xD;&#xA;This is just adding a little complexity to the above.  For SQL, you would further qualify your query based on a minimum risk score value.&#xD;&#xA;&#xD;&#xA;In excel, a straight pivot isn't going to work, you'll have to copy the unique values in one column to another, then drag a CountIf function adjacent to each unique value, qualifying the countif function with a minimum risk score.&#xD;&#xA;&#xD;&#xA;question 3. Users who have/reaching the high risk level very fast.&#xD;&#xA;&#xD;&#xA;A fast rise in risk level could be defined as the difference between two months being larger than a given value.&#xD;&#xA;&#xD;&#xA;For each user record you want to know the previous month's threat value, or assume zero as the previous threat value.&#xD;&#xA;&#xD;&#xA;If that difference is greater than your risk threshold, you want to include it in your report.  If not, they can be filtered from the list.  &#xD;&#xA;&#xD;&#xA;If I had to do this month after month, I would spend the two hours it might take to automate a report after the first couple of months.  I'd throw all the data in a SQL database and write a quick script in perl or java to iterate through the 100 records, do the calculation, and output the users who crossed the threshold.&#xD;&#xA;&#xD;&#xA;If I needed it to look pretty, I'd use a reporting tool.  I'm not particularly partial to any of them.&#xD;&#xA;&#xD;&#xA;If I needed to trend threshold values over time, I'd output the results for all people into a second table add records to that table each month.&#xD;&#xA;&#xD;&#xA;If I just needed to do it once or twice, figuring out how to do it in excel by adding a new column using VLookUp and some basic math and a filter would probably be the fastest and easiest way to get it done.  I tend to avoid using excel for things I'll need to use with consistency because there are limits that you run into when your data gets sizeable." />
  <row Id="1231" PostHistoryTypeId="2" PostId="497" RevisionGUID="c96d893a-17a4-4ed5-9a38-afe08279c11c" CreationDate="2014-06-20T03:18:59.477" UserId="1047" Text="I am trying to find a formula, method, or model to use to analyze the likelihood that a specific event influenced some longitudinal data. I am having difficultly figuring out what to search for on Google.&#xD;&#xA;&#xD;&#xA;Here is an example scenario:&#xD;&#xA;&#xD;&#xA;Image you own a business that has an average of 100 walk-in customers every day. One day, you decide you want to increase the number of walk-in customers arriving at your store each day, so you pull a crazy stunt outside your store to get attention. Over the next week, you see on average 125 customers a day.&#xD;&#xA;&#xD;&#xA;Over the next few months, you again decide that you want to get some more business, and perhaps sustain it a bit longer, so you try some other random things to get more customers in your store. Unfortunately, you are not the best marketer, and some of your tactics have little or no effect, and others even have a negative impact.&#xD;&#xA;&#xD;&#xA;What methodology could I use to determine the probability that any one individual event positively or negatively impacted the number of walk-in customers? I am fully aware that correlation does not necessarily equal causation, but what methods could I use to determine the likely increase or decrease in your business's daily walk in client's following a specific event?&#xD;&#xA;&#xD;&#xA;I am not interested in analyzing whether or not there is a correlation between your attempts to increase the number of walk-in customers, but rather whether or not any one single event, independent of all others, was impactful.&#xD;&#xA;&#xD;&#xA;I realize that this example is rather contrived and simplistic, so I will also give you a brief description of the actual data that I am using:&#xD;&#xA;&#xD;&#xA;I am attempting to determine the impact that a particular marketing agency has on their client's website when they publish new content, perform social media campaigns, etc. For any one specific agency, they may have anywhere from 1 to 500 clients. Each client has websites ranging in size from 5 pages to well over 1 million. Over the course of the past 5 year, each agency has annotated all of their work for each client, including the type of work that was done, the number of webpages on a website that were influenced, the number of hours spent, etc.&#xD;&#xA;&#xD;&#xA;Using the above data, which I have assembled into a data warehouse (placed into a bunch of star/snowflake schemas), I need to determine how likely it was that any one piece of work (any one event in time) had an impact on the traffic hitting any/all pages influenced by a specific piece of work. I have created models for 40 different types of content that are found on a website that describes the typical traffic pattern a page with said content type might experience from launch date until present. Normalized relative to the appropriate model, I need to determine the highest and lowest number of increased or decreased visitors a specific page received as the result of a specific piece of work.&#xD;&#xA;&#xD;&#xA;While I have experience with basic data analysis (linear and multiple regression, correlation, etc), I am at a loss for how to approach solving this problem. Whereas in the past I have typically analyzed data with multiple measurements for a given axis (for example temperature vs thirst vs animal and determined the impact on thirst that increased temperate has across animals), I feel that above, I am attempting to analyze the impact of a single event at some point in time for a non-linear, but predictable (or at least model-able), longitudinal dataset. I am stumped :(&#xD;&#xA;&#xD;&#xA;Any help, tips, pointers, recommendations, or directions would be extremely helpful and I would be eternally grateful!!!&#xD;&#xA;&#xD;&#xA;Thank you" />
  <row Id="1232" PostHistoryTypeId="1" PostId="497" RevisionGUID="c96d893a-17a4-4ed5-9a38-afe08279c11c" CreationDate="2014-06-20T03:18:59.477" UserId="1047" Text="What statistical model should I use to analyze the likelihood that a single event influenced longitudinal data" />
  <row Id="1233" PostHistoryTypeId="3" PostId="497" RevisionGUID="c96d893a-17a4-4ed5-9a38-afe08279c11c" CreationDate="2014-06-20T03:18:59.477" UserId="1047" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;statistics&gt;" />
  <row Id="1234" PostHistoryTypeId="5" PostId="491" RevisionGUID="fe4b4d6e-dc6d-44fc-9f55-076130b55a96" CreationDate="2014-06-20T03:28:55.463" UserId="322" Comment="use standard emphasis in place of double angle quotation characters; improve grammar; correct google chart subscripts" Text="As suggested, **going wild**. First of all, correct me if I’m wrong:&#xD;&#xA;&#xD;&#xA;* Just a few features exist for each unique product;&#xD;&#xA;* There is no ultimate features list, and clients are able to add new features to their products.&#xD;&#xA;&#xD;&#xA;If so, constructing full product-feature table could be computational expensive. And final data table would be extremely sparse.&#xD;&#xA;&#xD;&#xA;The first step is narrowing customers (products) list for matching. Let’s build a bipartite graph, where sellers would be type-1 nodes, and buyers would be type-2 nodes. Create an edge between any seller and buyer every time they reference a similar product feature, as in the following sketch:&#xD;&#xA;&#xD;&#xA;![graph][1]&#xD;&#xA;&#xD;&#xA;Using the above graph, for every unique seller’s product you can select only buyers who are interested in features that match the product (it’s possible to filter **at least one** common feature, match the full set of features, or set a threshold level). But certainly, that’s not enough. The next step is to compare feature values, as described by the seller and buyer. There are a lot of variants (e.g., k-Nearest-Neighbors). But why not try to solve this question using the existing graph? Let’s add weights to the edges:&#xD;&#xA;&#xD;&#xA;* for continuous features (e.g., price): &#xD;&#xA;&#xD;&#xA;  ![price_weight][2]&#xD;&#xA;&#xD;&#xA;* for binary and non-quantifiable features - just logical biconditional:&#xD;&#xA;&#xD;&#xA;  ![feature_weight][3]&#xD;&#xA;&#xD;&#xA;The main idea here is to **scale** every feature to the interval `[0, 1]`. Additionally, we can use feature coefficients to determine most important features. E.g., assuming price is twice as important as availability of some rare function: &#xD;&#xA;&#xD;&#xA;![adj_w_1][4]&#xD;&#xA;&#xD;&#xA;![adj_w_2][5]&#xD;&#xA;&#xD;&#xA;One of the final steps is simplifying the graph structure and reducing many edges to one edge with weight equal to the sum of the previously calculated weights of each feature. With such a reduced structure every pair of customers/products could have only one edge (no parallel edges). So, to find the best deal for exact seller you just need to select connected buyers with max weighted edges.&#xD;&#xA;&#xD;&#xA;Future challenge: introduce a cheap method for weighting edges on first step :)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/q1JTN.png&#xD;&#xA;  [2]: https://chart.googleapis.com/chart?cht=tx&amp;chl=weight%20%3D%20%5Cfrac%7B%7Cprice_%7Bsell%7D%20-%20price_%7Bbuy%7D%7C%7D%7B%5Cmax%28price%29%20-%20%5Cmin%28price%29%7D&#xD;&#xA;  [3]: https://chart.googleapis.com/chart?cht=tx&amp;chl=weight%20%3D%20feature_%7Bsell%7D%20%5Cleftrightarrow%20feature_%7Bbuy%7D&#xD;&#xA;  [4]: https://chart.googleapis.com/chart?cht=tx&amp;chl=adj.price.weight%20%3D%202%20%5Ctimes%20price.weight&#xD;&#xA;  [5]: https://chart.googleapis.com/chart?cht=tx&amp;chl=adj.feature.weight%20%3D%201%20%5Ctimes%20feature.weight" />
  <row Id="1235" PostHistoryTypeId="24" PostId="491" RevisionGUID="fe4b4d6e-dc6d-44fc-9f55-076130b55a96" CreationDate="2014-06-20T03:28:55.463" Comment="Proposed by 322 approved by 84, 941 edit id of 83" />
  <row Id="1236" PostHistoryTypeId="2" PostId="498" RevisionGUID="2fa6fc8a-092e-4cf2-b558-d6cb353ce0fc" CreationDate="2014-06-20T04:09:04.230" UserId="819" Text="Have you considered a frequency-based approach exploiting simple word co-occurence in corpora? At least, that's what I've seen most folks use for this. I think it might be covered briefly in Manning and Schütze's book, and I seem to remember something like this as a homework assignment back in grad school...&#xD;&#xA;&#xD;&#xA;More background here: http://nlp.stanford.edu/IR-book/html/htmledition/automatic-thesaurus-generation-1.html&#xD;&#xA;&#xD;&#xA;For this step:&#xD;&#xA;&gt; Rank other concepts based on their &quot;distance&quot; to the original keywords;&#xD;&#xA;&#xD;&#xA;There are several semantic similarity metrics you could look into. Here's a link to some slides I put together for a class project using a few of these similarity metrics in WordNet: http://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/greenbacker-WordNet-Similarity.pdf" />
  <row Id="1237" PostHistoryTypeId="2" PostId="499" RevisionGUID="7565825b-5e57-4554-a670-ffd4d894f8c7" CreationDate="2014-06-20T04:27:02.467" UserId="819" Text="Anecdotally, I've never been impressed with the output from hierarchical LDA. It just doesn't seem to find an optimal level of granularity for choosing the number of topics. I've gotten much better results by running a few iterations of regular LDA, manually inspecting the topics it produced, deciding whether to increase or decrease the number of topics, and continue iterating until I get the granularity I'm looking for.&#xD;&#xA;&#xD;&#xA;Remember: hierarchical LDA can't read your mind... it doesn't know what you actually intend to use the topic modeling for. Just like with k-means clustering, you should choose the k that makes the most sense for your use case." />
  <row Id="1240" PostHistoryTypeId="2" PostId="500" RevisionGUID="52f081df-669d-41ec-922a-b36a6aeddee9" CreationDate="2014-06-20T06:49:25.237" UserId="514" Text="In addition to Ben's answer, the subtle distinction between statistical models and machine learning models is that, in statistical models, you explicitly decide the output equation structure prior to building the model. The model is built to compute the parameters/coefficients.&#xD;&#xA;&#xD;&#xA;Take linear model or GLM for example,&#xD;&#xA;&#xD;&#xA;    y = a1x1 + a2x2 + a3x3&#xD;&#xA;&#xD;&#xA;Your independent variables are x1, x2, x3 and the coefficients to be determined are a1,a2,a3. You define your equation structure this way prior to building the model and compute a1,a2,a3. If you believe that y is somehow correlated to x2 in a non-linear way, you could try something like this.&#xD;&#xA;&#xD;&#xA;    y = a1x1 + a2(x2)^2 + a3x3.&#xD;&#xA;&#xD;&#xA;Thus, you put a restriction in terms of the output structure. Inherently statistical models are linear models unless you explicitly apply transformations like sigmoid or kernel to make them nonlinear (GLM and SVM).&#xD;&#xA;&#xD;&#xA;In case of machine learning models, you rarely specify output structure and algorithms like decision trees are inherently non-linear and work efficiently.&#xD;&#xA;&#xD;&#xA;Contrary to what Ben pointed out, machine learning models aren't just about prediction, they do classification, regression etc which can be used to make predictions which are also done by various statistical models." />
  <row Id="1241" PostHistoryTypeId="16" PostId="500" RevisionGUID="1c40ee77-73d2-4659-966c-70583c77bdd3" CreationDate="2014-06-20T06:49:25.237" UserId="514" />
  <row Id="1242" PostHistoryTypeId="2" PostId="501" RevisionGUID="1f804ba5-d361-448d-8b31-4b5906faaba5" CreationDate="2014-06-20T07:11:40.053" UserId="119" Text="The best language depends on what you want to do. First remark: don't limit yourself to one language. Learning a new language is always a good thing, but at some point you will need to choose. Facilities offered by the language itself are an obvious thing to keep into account *but* in my opinion the following are more important:&#xD;&#xA;&#xD;&#xA; - **available libraries**: do you have to implement everything from scratch or can you reuse existing stuff? Note that this these libraries need not be in whatever language you are considering, as long as you can interface easily. Working in a language without library access won't help you get things done.&#xD;&#xA; - **number of experts**: if you want external developers or start working in a team, you have to consider how many people actually know the language. As an extreme example: if you decide to work in Brainfuck because you happen to like it, know that you will likely work alone. Many surveys exists that can help assess the popularity of languages, including the number of questions per language on SO.&#xD;&#xA; - **toolchain**: do you have access to *good* debuggers, profilers, documentation tools and (if you're into that) IDEs?&#xD;&#xA;&#xD;&#xA;I am aware that most of my points favor established languages. This is from a 'get-things-done' perspective.&#xD;&#xA;&#xD;&#xA;That said, I personally believe it is far better to become proficient in a low level language and a high level language:&#xD;&#xA;&#xD;&#xA; - low level: C++, C, Fortran, ... using which you can implement certain profiling hot spots *only if you need to* because developing in these languages is typically slower (though this is subject to debate). These languages remain king of the hill in terms of critical performance and are likely to stay on top for a long time.&#xD;&#xA; - high level: Python, R, Clojure, ... to 'glue' stuff together and do non-performance critical stuff (preprocessing, data handling, ...). I find this to be important simply because it is much easier to do rapid development and prototyping in these languages." />
  <row Id="1243" PostHistoryTypeId="5" PostId="202" RevisionGUID="e2f4e775-379c-42da-9f7b-11957df27af4" CreationDate="2014-06-20T07:14:16.953" UserId="108" Comment="edited body" Text="The Dirichlet distribution is a multivariate distribution. We can denote the parameters of the Dirichlet as a vector of size K of the form ~ 1/B(a) * Product(x_i ^ (a_i-1)), where a is the vector of size K of the parameters, and sum of x_i = 1.&#xD;&#xA;&#xD;&#xA;Now the LDA uses some constructs like:&#xD;&#xA;- a document can have multiple topics (because of this multiplicity, we need the Dirichlet distribution); and there is a Dirichlet distribution which models this relation&#xD;&#xA;- words can also belong to multiple topics, when you consider them outside of a document; so here we need another Dirichlet to model this&#xD;&#xA;&#xD;&#xA;The previous two are distributions which you do not really see from data, this is why is called latent, or hidden.&#xD;&#xA;&#xD;&#xA;Now, in Bayesian inference you use the Bayes rule to infer the posterior probability. For simplicity, let's say you have data *x* and you have a model for this data governed by some parameters theta. In order to infer values for this parameters, in full Bayesian inference you will infer the posterior probability of these parameters using Bayes' rule with *p(theta|x) = p(x|theta)p(theta|alpha)/p(x|alpha)*. In plain words is *posterior probability = likelihood x prior probability / marginal likelihood*. Note that here comes an *alpha*. This is your initial belief about this distribution, and is the parameter of the prior distribution. Usually this is chosen in such a way that will have a conjugate prior (so the distribution of the posterior is the same with the distribution of the prior) and often to encode some knowledge if you have one or to have maximum entropy if you know nothing.&#xD;&#xA;&#xD;&#xA;The parameters of the prior are called *hyperparameters*. So, in LDA, both topic distributions, over documents and over words have also correspondent priors, which are denoted usually with alpha and beta, and because are the parameters of the prior distributions are called hyperparameters. &#xD;&#xA;&#xD;&#xA;Now about choosing priors. If you plot some Dirichlet distributions you will note that if the individual parameters *alpha_k* have the same value, the pdf is symmetric in the simplex defined by the *x* values, which is the minimum or maximum for pdf is at the center. &#xD;&#xA;&#xD;&#xA;If all the alpha_k have values lower than unit the maximum is found at corners&#xD;&#xA;&#xD;&#xA;&lt;img src=&quot;http://i.stack.imgur.com/5khZE.png&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;&#xD;&#xA;&#xD;&#xA;or can if all values alpha_k are the same and greater than 1 the maximum will be found in center like &#xD;&#xA;&#xD;&#xA;&lt;img src=&quot;http://research.microsoft.com/en-us/um/people/cmbishop/prml/prmlfigs-png/Figure2.5c.png&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;&#xD;&#xA;&#xD;&#xA;It is easy to see that if values for alpha_k are not equal the symmetry is broken and the maximum will be found near bigger values. &#xD;&#xA;&#xD;&#xA;Additional, please note that values for priors parameters produce smooth pdfs of the distribution as the values of the parameters are near 1. So if you have great confidence that something is clearly distributed in a way you know, with a high degree of confidence, than values far from 1 in absolute value are to be used, if you do not have such kind of knowledge than values near 1 would be encode this lack of knowledge. It is easy to see why 1 plays such a role in Dirichlet distribution from the formula of the distribution itself.&#xD;&#xA;&#xD;&#xA;Another way to understand this is to see that prior encode prior-knowledge. In the same time you might think that prior encode some prior seen data. This data was not saw by the algorithm itself, it was saw by you, you learned something, and you can model prior according to what you know (learned). So in the prior parameters (hyperparameters) you encode also how big this data set you apriori saw, because the sum of alpha_k can be that also as the size of this more or less imaginary data set. So the bigger the prior data set, the bigger is the confidence, the bigger the values of alpha_k you can choose, the sharper the surface near maximum value, which means also less doubts. &#xD;&#xA;&#xD;&#xA;Hope it helped.&#xD;&#xA;&#xD;&#xA;PS: It's a hell to write something without LaTeX notation. I hope moderators/administrators will do something." />
  <row Id="1244" PostHistoryTypeId="2" PostId="502" RevisionGUID="5acb87ff-6fd7-46b2-8f06-38e6d6fda114" CreationDate="2014-06-20T10:01:03.363" UserId="108" Text="Regarding prediction, statistics and machine learning sciences started to solve mostly the same problem from different perspectives. &#xD;&#xA;&#xD;&#xA;Basically statistics assumes that the data were produced by a given stochastic model. So, from a statistical perspective, a model is assumed and given various assumptions the errors are treated and the model parameters and other questions are inferred. &#xD;&#xA;&#xD;&#xA;Machine learning comes from a computer science perspective. The models are algorithmic and usually very few assumptions are required regarding the data. We work with hypothesis space and learning bias. The best exposition of machine learning I found is contained in Tom Mitchell's book called [Machine Learning][1].&#xD;&#xA;&#xD;&#xA;For a more exhaustive and complete idea regarding the two cultures you can read the Leo Broiman paper called [Statistical Modeling: The Two Cultures][2]&#xD;&#xA;&#xD;&#xA;However what must be added is that even if the two sciences started with different perspectives, both of them now now share a fair amount of common knowledge and techniques. Why, because the problems were the same, but the tools were different. So now machine learning is mostly treated from a statistical perspective (check the Hastie,Tibshirani, Friedman book [The Elements of Statistical Learning][3] from a machine learning point of view with a statistical treatement, and perhaps Kevin P. Murphy 's book [Machine Learning: A probabilistic perspective][4], to name just a few of the best books available today). &#xD;&#xA;&#xD;&#xA;Even the history of the development of this field show the benefits of this merge of perspectives. I will describe two events. &#xD;&#xA;&#xD;&#xA;The first is the creation of CART trees, which was created by Breiman with a solid statistical background. At approximately the same time, Quinlan developed ID3,C45,See5, and so on, decision tree suite with a more computer science background. Now both this families of trees and the ensemble methods like bagging and forests become quite similar. &#xD;&#xA;&#xD;&#xA;The second story is about boosting. Initially they were developed by Freund and Shapire when they discovered AdaBoost. The choices for designing AdaBoost were done mostly from a computational perspective. Even the authors did not understood well why it works. Only 5 years later Breiman (again!) described the adaboost model from a statistical perspective and gave an explanation for why that works. Since then, various eminent scientists, with both type of backgrounds, developed further those ideas leading to a Pleiads of boosting algorithms, like logistic boosting, gradient boosting, gentle boosting ans so on. It is hard now to think at boosting without a solid statistical background.&#xD;&#xA;&#xD;&#xA;GLM is a statistical development. However new Bayesian treatments puts this algorithm also in machine learning playground. So I believe both claims could be right, since the interpretation and treatment of how it works could be different. &#xD;&#xA;&#xD;&#xA;  [1]: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html&#xD;&#xA;  [2]: http://www.google.ie/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CDIQFjAA&amp;url=http://strimmerlab.org/courses/ss09/current-topics/download/breiman2001.pdf&amp;ei=RQGkU_DgH8Le7AaRmID4DA&amp;usg=AFQjCNGNnrlqadmBT2fZMT_NfoUQ1rEuow&amp;sig2=g5qovUTsuuJUGGE64g0nrQ&#xD;&#xA;  [3]: http://statweb.stanford.edu/~tibs/ElemStatLearn/&#xD;&#xA;  [4]: http://mitpress.mit.edu/books/machine-learning-2" />
  <row Id="1245" PostHistoryTypeId="6" PostId="455" RevisionGUID="29c7d025-3fe9-4f27-8e6c-ebb7eade8ef8" CreationDate="2014-06-20T13:18:04.417" UserId="960" Comment="edited tags" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;clustering&gt;&lt;text-mining&gt;" />
  <row Id="1246" PostHistoryTypeId="2" PostId="503" RevisionGUID="abb7299f-a7bb-4822-a5b1-2eb77c243892" CreationDate="2014-06-20T14:17:27.970" UserId="1057" Text="I'm not an expert but I guess the main problem is to answer this question:&#xD;&#xA;&#xD;&#xA;Has an/any event affected the number of hits on a certain day?&#xD;&#xA;&#xD;&#xA;But I don't know how to treat multiple events, so I would try to answer this question:&#xD;&#xA;&#xD;&#xA;* Does event X affected the number of hits on a certain day?&#xD;&#xA;&#xD;&#xA;Which can be answered using hypothesis testing with p-values (what scientist do to evaluate for instance if a medicine affects a disease or not).&#xD;&#xA;&#xD;&#xA;By using p-values, you could determinate if the number of hits in a certain day were mere random and acceptable under normal circumstances or that they must correspond to a change in your model.&#xD;&#xA;&#xD;&#xA;You can read more about p-values in [Open Intro to Statistics Book](http://www.openintro.org/stat/), I've actually learn about them from there.&#xD;&#xA;&#xD;&#xA;Then, the other parts of the problem are how to identify your events and calculate the necessary parameters to answer your question (average/median, variance, etc.) and also how to keep that up-to-date and working.&#xD;&#xA;  " />
  <row Id="1247" PostHistoryTypeId="2" PostId="504" RevisionGUID="6df5a394-a844-4c32-a55e-d2a9ef83cb59" CreationDate="2014-06-20T14:58:09.320" UserId="684" Text="I'm currently working with a dataset with a wide range of document lengths -- anywhere from a single word to a full page of text.  In addition, the grammatical structure and use of punctuation varies wildly from document to document.  The goal is to classify those documents into one of about 10-15 categories.  I'm currently using ridge regression and logistic regression for the task, and CV for the alpha values of ridge.  The feature vectors are tf-idf ngrams.&#xD;&#xA;&#xD;&#xA;Recently I've noticed that longer documents are much less likely to be categorized. Why might this be the case, and how can one &quot;normalize&quot; for this kind of variation?  As a more general question, how does one typically deal with diverse data sets?  Should documents be grouped based off of metrics like document length, use of punctuation, grammatical rigor, etc. and then fed through different classifiers?  " />
  <row Id="1248" PostHistoryTypeId="1" PostId="504" RevisionGUID="6df5a394-a844-4c32-a55e-d2a9ef83cb59" CreationDate="2014-06-20T14:58:09.320" UserId="684" Text="Dealing with diverse text data" />
  <row Id="1249" PostHistoryTypeId="3" PostId="504" RevisionGUID="6df5a394-a844-4c32-a55e-d2a9ef83cb59" CreationDate="2014-06-20T14:58:09.320" UserId="684" Text="&lt;classification&gt;&lt;nlp&gt;" />
  <row Id="1251" PostHistoryTypeId="2" PostId="506" RevisionGUID="116e7792-19a1-4391-8120-1a7bc9b52f6b" CreationDate="2014-06-20T16:09:41.637" UserId="1011" Text="For the record, I think this is the type of question that's perfect for the data science Stack Exchange. I hope we get a bunch of real world examples of data problems and several perspectives on how best to solve them.&#xD;&#xA;&#xD;&#xA;I would encourage you *not* to use p-values as they can be pretty misleading ([1][1], [2][2]). My approach hinges on you being able to summarize traffic on a given page before and after some intervention. What you care about is the difference in the *rate* before and after the intervention. That is, how does the number of hits per day change? Below, I explain a first stab approach with some simulated example data. I will then explain one potential pitfall (and what I would do about it).&#xD;&#xA;&#xD;&#xA;First, let's think about one page before and after an intervention. Pretend the intervention increases hits per day by roughly 15%:&#xD;&#xA;&#xD;&#xA;    import numpy as np&#xD;&#xA;    import matplotlib.pyplot as plt&#xD;&#xA;    import seaborn as sns&#xD;&#xA;    &#xD;&#xA;    def simulate_data(true_diff=0):&#xD;&#xA;        #First choose a number of days between [1, 1000] before the intervention&#xD;&#xA;        num_before = np.random.randint(1, 1001)&#xD;&#xA;    &#xD;&#xA;        #Next choose a number of days between [1, 1000] after the intervention&#xD;&#xA;        num_after = np.random.randint(1, 1001)&#xD;&#xA;        &#xD;&#xA;        #Next choose a rate for before the intervention. How many views per day on average?&#xD;&#xA;        rate_before = np.random.randint(50, 151)&#xD;&#xA;        &#xD;&#xA;        #The intervention causes a `true_diff` increase on average (but is also random)&#xD;&#xA;        rate_after = np.random.normal(1 + true_diff, .1) * rate_before&#xD;&#xA;        &#xD;&#xA;        #Simulate viewers per day:&#xD;&#xA;        vpd_before = np.random.poisson(rate_before, size=num_before)&#xD;&#xA;        vpd_after = np.random.poisson(rate_after, size=num_after)&#xD;&#xA;            &#xD;&#xA;        return vpd_before, vpd_after&#xD;&#xA;&#xD;&#xA;    vpd_before, vpd_after = simulate_data(.15)&#xD;&#xA;&#xD;&#xA;    plt.hist(vpd_before, histtype=&quot;step&quot;, bins=20, normed=True, lw=2)&#xD;&#xA;    plt.hist(vpd_after, histtype=&quot;step&quot;, bins=20, normed=True, lw=2)&#xD;&#xA;    plt.legend((&quot;before&quot;, &quot;after&quot;))&#xD;&#xA;    plt.title(&quot;Views per day before and after intervention&quot;)&#xD;&#xA;    plt.xlabel(&quot;Views per day&quot;)&#xD;&#xA;    plt.ylabel(&quot;Frequency&quot;)&#xD;&#xA;    plt.show()&#xD;&#xA;&#xD;&#xA;![Distribution of hits per day before and after the intervention][3]&#xD;&#xA;&#xD;&#xA;We can clearly see that the intervention increased the number of hits per day, on average. But in order to quantify the difference in rates, we should use one company's intervention for multiple pages. Since the underlying rate will be different for each page, we should compute the percent change in rate (again, the rate here is hits per day).&#xD;&#xA;&#xD;&#xA;Now, let's pretend we have data for `n = 100` pages, each of which received an intervention from the same company. To get the percent difference we take (mean(hits per day before) - mean(hits per day after)) / mean(hits per day before):&#xD;&#xA;&#xD;&#xA;    n = 100&#xD;&#xA;&#xD;&#xA;    pct_diff = np.zeros(n)&#xD;&#xA;&#xD;&#xA;    for i in xrange(n):&#xD;&#xA;        vpd_before, vpd_after = simulate_data(.15)&#xD;&#xA;        # % difference. Note: this is the thing we want to infer&#xD;&#xA;        pct_diff[i] = (vpd_after.mean() - vpd_before.mean()) / vpd_before.mean()&#xD;&#xA;&#xD;&#xA;    plt.hist(pct_diff)&#xD;&#xA;    plt.title(&quot;Distribution of percent change&quot;)&#xD;&#xA;    plt.xlabel(&quot;Percent change&quot;)&#xD;&#xA;    plt.ylabel(&quot;Frequency&quot;)&#xD;&#xA;    plt.show()&#xD;&#xA;&#xD;&#xA;![Distribution of percent change][4]&#xD;&#xA;&#xD;&#xA;Now we have the distribution of our parameter of interest! We can query this result in different ways. For example, we might want to know the mode, or (approximation of) the most likely value for this percent change:&#xD;&#xA;&#xD;&#xA;    def mode_continuous(x, num_bins=None):&#xD;&#xA;        if num_bins is None:&#xD;&#xA;            counts, bins = np.histogram(x)&#xD;&#xA;        else:&#xD;&#xA;            counts, bins = np.histogram(x, bins=num_bins)&#xD;&#xA;&#xD;&#xA;        ndx = np.argmax(counts)&#xD;&#xA;        return bins[ndx:(ndx+1)].mean()&#xD;&#xA;&#xD;&#xA;    mode_continuous(pct_diff, 20)&#xD;&#xA;&#xD;&#xA;When I ran this I got 0.126, which is not bad, considering our true percent change is 15. We can also see the number of positive changes, which approximates the probability that a given company's intervention improves hits per day:&#xD;&#xA;&#xD;&#xA;    (pct_diff &gt; 0).mean()&#xD;&#xA;&#xD;&#xA;Here, my result is 0.93, so we could say there's a pretty good chance that this company is effective.&#xD;&#xA;&#xD;&#xA;Finally, a potential pitfall: Each page probably has some underlying trend that you should probably account for. That is, even without the intervention, hits per day may increase. To account for this, I would estimate a simple linear regression where the outcome variable is hits per day and the independent variable is day (start at day=0 and simply increment for all the days in your sample). Then subtract the estimate, y_hat, from each number of hits per day to de-trend your data. Then you can do the above procedure and be confident that a positive percent difference is not due to the underlying trend. Of course, the trend may not be linear, so use discretion! Good luck!&#xD;&#xA;&#xD;&#xA;  [1]: http://andrewgelman.com/2013/03/12/misunderstanding-the-p-value/&#xD;&#xA;  [2]: http://occamstypewriter.org/boboh/2008/08/19/why_p_values_are_evil/&#xD;&#xA;  [3]: http://i.stack.imgur.com/FJJqD.png&#xD;&#xA;  [4]: http://i.stack.imgur.com/CAitf.png" />
  <row Id="1252" PostHistoryTypeId="2" PostId="507" RevisionGUID="842e553b-dcf9-40d4-91f9-24c4f8e87b0b" CreationDate="2014-06-20T16:29:27.673" UserId="780" Text="Back in my data analyst days this type of problem was pretty typical.  Basically, everyone in marketing would come up with a crazy idea that the sold to higher ups as the single event that would boost KPI's by 2000%.  The higher ups would approve them and then they would begin their &quot;test&quot;.  Results would come back, and management would dump it on the data analysts to determine what worked and who did it.&#xD;&#xA;&#xD;&#xA;The short answer is you cant really know if it wasn't run as a random A/B style test on like time periods.  But I am very aware of how deficient that answer is, especially if the fact that a pure answer doesn't exist is irrelevant to the urgency of future business decisions.  Here are some of the techniques I would use to salvage the analysis in this situation, bear in mind this is more of an art then a science.&#xD;&#xA;&#xD;&#xA;**Handles**&#xD;&#xA;&#xD;&#xA;A handle is something that exists in the data that you can hold onto.  From what you are telling me in your situation you have a lot of info on who the marketing agency is, when they tried a tactic, and to which site they applied it to.  These are your starting point and information like this going to be the corner stone of your analysis.&#xD;&#xA;&#xD;&#xA;**Methodology**&#xD;&#xA;&#xD;&#xA;The methodology is going to probably hold the strongest impact on which agencies are given credit for any and all gains so you are going to need to make sure that it is clearly outlines and all stake holders agree that it makes sense.  If you cant do that it is going to be difficult for people to trust your analysis.  &#xD;&#xA;&#xD;&#xA;An example of this are conversions. Say the marketing department purchases some leads and they arrive at our landing page, we would track them for 3 days, if they made a purchase within that time we would count them as having been converted.  Why 3 days, why not 5 or 1?  Thats not important as long as everyone agrees, you now have a definition you can build off of.  &#xD;&#xA;&#xD;&#xA;**Comparisons**&#xD;&#xA;&#xD;&#xA;In an ideal would you would have a nice A/B test to prove a definitive relationship, I am going to assume that you are running short on those, still, you can learn something from a simple comparison of like data.  When companies are trying to determine the efficacy of radio advertising they will often run ads on offset months in the same market, or for several months in one market and compare that with the results in a separate but similar market.  Its doesn't pass for science, but even with all that noise a strong results will almost always be noticeable.&#xD;&#xA;&#xD;&#xA;I would combine these in your case to determine how long an event is given to register an effect.  Once you have the data from that time period run it against your modeled out traffic prediction, week over week growth, month over month etc. Which, can then allow a meaningful comparison between agencies, and across time periods.&#xD;&#xA;&#xD;&#xA;**Pragmatism**&#xD;&#xA;&#xD;&#xA;The aspiration is to be able to provide a deep understanding of cause and effect, but it is probably not realistic.  Because of how messy outside factors make your analysis, you are constantly going to run up against the question over and over again: Did this event raise volume/sales/click throughs, or would doing anything at all have had the same effect?  The best advise I can give for this is set very realistic goals for what you are looking to measure.  A good starting point is, within the methodology you have, which event had the largest impact.  Once you have those open your aperture from there.&#xD;&#xA;&#xD;&#xA;**Summary**&#xD;&#xA;&#xD;&#xA;Once you have reasoned out all of these aspects you can go about building a general solution which can then be automated.  The advantage to designing your solution in this manner is that the business logic is already built in.  This will make your results much more approachable and intuitive to non-technical business leaders." />
  <row Id="1253" PostHistoryTypeId="2" PostId="508" RevisionGUID="eda9e8e3-9e85-4f42-a5f8-e52c2ff5ae3d" CreationDate="2014-06-20T16:56:41.160" UserId="984" Text="I'm not sure how you are applying a regression framework for document classification. The way I'd approach the problem is to apply a standard discriminative classification approach such as SVM.&#xD;&#xA;&#xD;&#xA;In a discriminative classification approach the notion of similarity or inverse distance between data points (documents in this case) is pivotal. Fortunately for documents, there is a standard way of defining pairwise similarity. This is the standard [cosine similarity][1] measure which makes use of document length normalization to take different document lengths into account.&#xD;&#xA;&#xD;&#xA;Thus, practically speaking, in cosine similarity you would work with relative term weights normalized by document lengths and hence document length diversity should not be a major issue in the similarity computation.&#xD;&#xA;&#xD;&#xA;One also has to be careful when applying idf in term weights. If the number of documents is not significantly large the idf measure may be statistically imprecise thus adding noise to the term weights. It's also a standard practice to ignore stopwords and punctuations.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Cosine_similarity" />
  <row Id="1254" PostHistoryTypeId="2" PostId="509" RevisionGUID="035af321-923b-4516-aabe-7a3dd464e264" CreationDate="2014-06-20T17:30:39.097" UserId="1015" Text="Both feature selection and extraction are a part of spatial data mining.For example...if u have an agricultural land then selecting one particular area of that land would be feature selection.If u aim to find the affected plants in that area den u need to observe each plant based on a particular feature that is common in each plant so as to find the abnormalities...for this u would be considering feature extraction.In this example the original agricultural land corresponds to Dimensionality reduction." />
  <row Id="1255" PostHistoryTypeId="2" PostId="510" RevisionGUID="6309b724-f955-4c6c-8bf8-91ff9c6b9a08" CreationDate="2014-06-20T17:34:51.633" UserId="941" Text="A couple of words about social networks APIs. About a year ago I wrote a review of popular social networks’ APIs for researchers. Unfortunately, it is in Russian. Here is a summary:&#xD;&#xA;&#xD;&#xA;**Twitter** (https://dev.twitter.com/docs/api/1.1)&#xD;&#xA;&#xD;&#xA;* almost all data about tweets/texts and users is available;&#xD;&#xA;* lack of sociodemographic data;&#xD;&#xA;* great streaming API: useful for real time text processing;&#xD;&#xA;* a lot of wrappers for programing languages;&#xD;&#xA;* getting network structure (connections) is possible, but time-expensive (1 request per 1 minute).&#xD;&#xA;&#xD;&#xA;**Facebook** (https://developers.facebook.com/docs/reference/api/)&#xD;&#xA;&#xD;&#xA;* rate limits: about 1 request per second;&#xD;&#xA;* well documented, sandbox present;&#xD;&#xA;* FQL (SQL-like) and «regular Rest» Graph API;&#xD;&#xA;* friendship data and sociodemographic features present;&#xD;&#xA;* a lot of data is beyond *event horizon*: only friends' and friends' of friends data is more or less complete, almost nothing could be investigated about random user;&#xD;&#xA;* some strange API bugs, and looks like nobody cares about it (e.g., some features available through FQL, but not through Graph API synonym).&#xD;&#xA;&#xD;&#xA;**Instagram** (http://instagram.com/developer/)&#xD;&#xA;&#xD;&#xA;* rate limits: 5000 requests per hour;&#xD;&#xA;* real-time API (like Streaming API for Twitter, but with photos) - connection to it is a little bit tricky: callbacks are used;&#xD;&#xA;* lack of sociodemographic data;&#xD;&#xA;* photos, filters data available;&#xD;&#xA;* unexpected imperfections (e.g., it’s possible to collect only 150 comments to post/photo).&#xD;&#xA;&#xD;&#xA;**Foursquare** (https://developer.foursquare.com/overview/)&#xD;&#xA;&#xD;&#xA;* rate limits: 5000 requests per hour;&#xD;&#xA;* kingdom of geosocial data :)&#xD;&#xA;* quite closed from researches because of privacy issues. To collect checkins data one need to build composite parser working with 4sq, bit.ly, and twitter APIs at once;&#xD;&#xA;* again: lack of sociodemographic data.&#xD;&#xA;&#xD;&#xA;**Google+** (https://developers.google.com/+/api/latest/)&#xD;&#xA;&#xD;&#xA;* about 5 requests per second (try to verify);&#xD;&#xA;* main methods: activities and people;&#xD;&#xA;* like on Facebook, a lot of personal data for random user is hidden;&#xD;&#xA;* lack of user connections data.&#xD;&#xA;&#xD;&#xA;And out-of-competition: I reviewed social networks for Russian readers, and #1 network here is [**vk.com**][1]. It’s translated to many languages, but popular only in Russia and other CIS countries. API docs link: http://vk.com/dev/. And from my point of view, it’s the best choice for homebrew social media research. At least, in Russia. That’s why:&#xD;&#xA;&#xD;&#xA;* rate limits: 3 requests per second;&#xD;&#xA;* public text and media data available;&#xD;&#xA;* sociodemographic data available: for random user availability level is about 60-70%;&#xD;&#xA;* connections between users are also available: almost all friendships data for random user is available;&#xD;&#xA;* some special methods: e.g., there is a method to get online/offline status for exact user in realtime, and one could build schedule for his audience.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/VK_%28social_network%29" />
  <row Id="1256" PostHistoryTypeId="5" PostId="485" RevisionGUID="3c0ce509-8f9b-4ab0-932a-ca76f2478b3f" CreationDate="2014-06-20T17:36:05.023" UserId="1015" Comment="added 359 characters in body" Text="My answer would be no. I consider Data mining to be one of the miscellaneous fields in Data science. Data Mining is mostly considered on yielding questions rather than answering them. It is often termed as &quot;detecting something new&quot;, when compared to Data science, where the data scientist try to solve complex problems to be able to reach their end results. However both terms have many commonalities between them.&#xD;&#xA;For example..if u have an agricultural land where u aim to find the affected plants..Here spatial data mining plays a key role in doing this job.There are good chances that you may end  up with not only finding out the affected plants in the land but also the extent to which they are affected.......this is something that is not possible with data science." />
  <row Id="1257" PostHistoryTypeId="2" PostId="511" RevisionGUID="d301b0cc-dbf6-403e-a745-1690dd130ee9" CreationDate="2014-06-20T17:57:46.363" UserId="97" Text="I wonder which type of model cross-validation to choose: K-fold or random sub-sampling?&#xD;&#xA;&#xD;&#xA;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&#xD;&#xA;&#xD;&#xA;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&#xD;&#xA;&#xD;&#xA;On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training, and some will be used more than once." />
  <row Id="1258" PostHistoryTypeId="1" PostId="511" RevisionGUID="d301b0cc-dbf6-403e-a745-1690dd130ee9" CreationDate="2014-06-20T17:57:46.363" UserId="97" Text="Cross-validation: K-fold vs Repeated random sub-sampling" />
  <row Id="1259" PostHistoryTypeId="3" PostId="511" RevisionGUID="d301b0cc-dbf6-403e-a745-1690dd130ee9" CreationDate="2014-06-20T17:57:46.363" UserId="97" Text="&lt;cross-validation&gt;&lt;k-fold&gt;&lt;sampling&gt;&lt;random-sub-sampling&gt;" />
  <row Id="1260" PostHistoryTypeId="5" PostId="511" RevisionGUID="af49ddbe-c13a-480a-8f99-60b1702ce7e5" CreationDate="2014-06-20T18:04:53.227" UserId="97" Comment="added 11 characters in body" Text="I wonder which type of model cross-validation to choose: K-fold or random sub-sampling?&#xD;&#xA;&#xD;&#xA;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&#xD;&#xA;&#xD;&#xA;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&#xD;&#xA;&#xD;&#xA;On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training/validation, and some will be used more than once." />
  <row Id="1261" PostHistoryTypeId="5" PostId="511" RevisionGUID="dc351b36-c664-40d1-b433-a0193ce86c9b" CreationDate="2014-06-20T18:33:42.860" UserId="97" Comment="added 72 characters in body" Text="I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling?&#xD;&#xA;&#xD;&#xA;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&#xD;&#xA;&#xD;&#xA;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&#xD;&#xA;&#xD;&#xA;On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training/validation, and some will be used more than once.&#xD;&#xA;&#xD;&#xA;Classification algorithms used: random forest &amp; logistic regression." />
  <row Id="1262" PostHistoryTypeId="2" PostId="512" RevisionGUID="637bde0c-9019-46bf-bb19-e141973288a6" CreationDate="2014-06-20T18:56:52.110" UserId="609" Text="Here's an approach I didn't see mentioned: separate the process into two steps: the first step focused on encoding names so that alternative versions of the same name are encoded the same (or nearly the same), and the second step focused on making them anonymous.&#xD;&#xA;&#xD;&#xA;For the first step, you could use one of the [Phonetic Algorithms (Soundex and variants)](https://en.wikipedia.org/wiki/Phonetic_encoding), applied to first name, last name, and initials in various orders.  (See [this article](http://www.stevemorse.org/phonetics/bmpm2.htm), also).  It's in this step where you resolve similarities vs. differences in names to balance false positives from false negatives.&#xD;&#xA;&#xD;&#xA;For the second step, you can pick any hashing or cryptographic method you like, without concern for how that method affects name matching.  This gives you freedom to use a method that has the best characteristics for both performance, robustness, and anonymity." />
  <row Id="1263" PostHistoryTypeId="2" PostId="513" RevisionGUID="95f44e16-69bd-463a-9c4d-28cbd1e37990" CreationDate="2014-06-20T19:15:52.093" UserId="964" Text="If you have an adequate number of samples and want to use all the data, then k-fold cross-validation is the way to go. Having ~1,500 seems like a lot but whether it is adequate for k-fold cross-validation also depends on the dimensionality of the data (number of attributes and number of attribute values). For example, if each observation has 100 attributes, then 1,500 observations is low.&#xD;&#xA;&#xD;&#xA;Another potential downside to k-fold cross-validation is the possibility of a single, extreme outlier skewing the results. For example, if you have one extreme outlier that can heavily bias your classifier, then in a 10-fold cross-validation, 9 of the 10 partitions will be affected (though for random forests, I don't think you would have that problem).&#xD;&#xA;&#xD;&#xA;Random subsampling (e.g., bootstrap sampling) is preferable when you are either undersampled or when you have the situation above, where you don't want each observation to appear in k-1 folds.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1264" PostHistoryTypeId="5" PostId="503" RevisionGUID="3e8ff600-6fbd-4238-af10-803f113430f8" CreationDate="2014-06-20T19:39:41.533" UserId="1057" Comment="added 104 characters in body" Text="**Edit: Warning, i leave my message but my answer seems wrong, please check out the comment below!**&#xD;&#xA;&#xD;&#xA;I'm not an expert but I guess the main problem is to answer this question:&#xD;&#xA;&#xD;&#xA;Has an/any event affected the number of hits on a certain day?&#xD;&#xA;&#xD;&#xA;But I don't know how to treat multiple events, so I would try to answer this question:&#xD;&#xA;&#xD;&#xA;* Does event X affected the number of hits on a certain day?&#xD;&#xA;&#xD;&#xA;Which can be answered using hypothesis testing with p-values (what scientist do to evaluate for instance if a medicine affects a disease or not).&#xD;&#xA;&#xD;&#xA;By using p-values, you could determinate if the number of hits in a certain day were mere random and acceptable under normal circumstances or that they must correspond to a change in your model.&#xD;&#xA;&#xD;&#xA;You can read more about p-values in [Open Intro to Statistics Book](http://www.openintro.org/stat/), I've actually learn about them from there.&#xD;&#xA;&#xD;&#xA;Then, the other parts of the problem are how to identify your events and calculate the necessary parameters to answer your question (average/median, variance, etc.) and also how to keep that up-to-date and working.&#xD;&#xA;  " />
  <row Id="1265" PostHistoryTypeId="2" PostId="514" RevisionGUID="fcc488f3-6be9-4adc-8306-1ed6b1764309" CreationDate="2014-06-20T20:03:51.247" UserId="609" Text="I like Amir Ali Akbari's suggestions, and I'll add a few of my own, focusing on topics and skills that are not adequately covered in most machine learning and data analysis books that focus on math and/or programming.&#xD;&#xA;&#xD;&#xA;Data Cleaning:&#xD;&#xA;&#xD;&#xA; - [Osborne 2012, *Best Practices in Data Cleaning*](http://www.amazon.com/Best-Practices-Data-Cleaning-Everything/dp/1412988012/)&#xD;&#xA; - [McCallom 2012, *Bad Data Handbook: Cleaning Up The Data So You Can Get Back To Work*](http://www.amazon.com/Bad-Data-Handbook-Cleaning-Back/dp/1449321887/)&#xD;&#xA;&#xD;&#xA;Bayesian Data Analysis (alternative to Fisher-style Null Hypothesis Significance Testing):&#xD;&#xA;&#xD;&#xA; - [Kruschke 2011, *Doing Bayesian Data Analysis*](http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855/)&#xD;&#xA;&#xD;&#xA;Inference in the face of uncertainty, incompleteness, contradictions, ambiguity, imprecision, ignorance, etc.:&#xD;&#xA;&#xD;&#xA; - [Schum &amp; Starace, 2001, *The Evidential Foundations of Probabilistic Reasoning*](http://www.amazon.com/Evidential-Foundations-Probabilistic-Reasoning/dp/0810118211/)&#xD;&#xA; - [Bammer &amp; Smithson 2008, *Uncertainty and Risk: Multidisciplinary Perspectives*](http://www.amazon.com/Uncertainty-Risk-Multidisciplinary-Perspectives-Earthscan/dp/1844074749/)&#xD;&#xA; - [Smithson 1989, *Ignorance and Uncertainty*](http://www.amazon.com/Ignorance-Uncertainty-Emerging-Paradigms-Cognitive/dp/0387969454/)&#xD;&#xA; - [CIA 2008, *A Tradecraft Primer: Structured Analytic Techniques for Improving Intelligence Analysis*](https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/Tradecraft%20Primer-apr09.pdf) (FREE! as PDF)&#xD;&#xA; - [Morgan &amp; Winship 2007, *Counterfactuals and Causal Inference: Methods and Principles for Social Research*](http://www.amazon.com/Counterfactuals-Causal-Inference-Principles-Analytical/dp/0521671930/)&#xD;&#xA;&#xD;&#xA;Experiments:&#xD;&#xA;&#xD;&#xA; - [Glennerster &amp; Takavarasha 2013, *Running Randomized Evaluations: A Practical Guide*](http://www.amazon.com/Running-Randomized-Evaluations-Practical-Guide/dp/0691159270/)&#xD;&#xA; - [Dunning 2012, *Natural Experiments in the Social Sciences*](http://www.amazon.com/Natural-Experiments-Social-Sciences-Design-Based/dp/1107698006/)&#xD;&#xA;&#xD;&#xA;Simulation:&#xD;&#xA;&#xD;&#xA; - [Epstein 2006, *Generative Social Science: Studies in Agent-Based Computational Modeling*](http://www.amazon.com/Generative-Social-Science-Agent-Based-Computational/dp/0691125473/)&#xD;&#xA; - [Nelson 2010, *Stochastic Modeling: Analysis and Simulation*](http://www.amazon.com/Stochastic-Modeling-Analysis-Simulation-Mathematics/dp/0486477703/)&#xD;&#xA;&#xD;&#xA;Expert elicitation, probabilistic estimation:&#xD;&#xA;&#xD;&#xA; - [Hubbard 2014, *How to Measure Anything: Finding the Value of Intangibles in Business*](http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/1118539273/)" />
  <row Id="1266" PostHistoryTypeId="2" PostId="515" RevisionGUID="b9e03ca9-8da1-4d86-be30-0e704cc76aaa" CreationDate="2014-06-20T20:44:39.280" UserId="426" Text="I can't add a comment yet, but just FYI ESL is [available for free online as a pdf][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://statweb.stanford.edu/~tibs/ElemStatLearn/" />
  <row Id="1268" PostHistoryTypeId="2" PostId="516" RevisionGUID="c03cd23d-966f-4934-b5da-752ab7b3e34f" CreationDate="2014-06-21T04:59:06.620" UserId="985" Text="I'm  working on multiclass logistic regression model with a large number of features (numFeatures&gt;100).  Using a Maximum Likelihood Estimation based cost function and gradient, the fmincg algorithm solves the problem quickly... however, I am also experimenting with a different cost function and do not have a gradient... is there a good way to speed up the calculation process? E.g. a different algorithm or fmincg setting?" />
  <row Id="1269" PostHistoryTypeId="1" PostId="516" RevisionGUID="c03cd23d-966f-4934-b5da-752ab7b3e34f" CreationDate="2014-06-21T04:59:06.620" UserId="985" Text="efficient solution of fmincg without providing gradient?" />
  <row Id="1270" PostHistoryTypeId="3" PostId="516" RevisionGUID="c03cd23d-966f-4934-b5da-752ab7b3e34f" CreationDate="2014-06-21T04:59:06.620" UserId="985" Text="&lt;bigdata&gt;&lt;data-mining&gt;" />
  <row Id="1271" PostHistoryTypeId="5" PostId="516" RevisionGUID="3bba4392-57dc-45ae-b609-29f955a9eae9" CreationDate="2014-06-21T06:38:58.260" UserId="84" Comment="Improving formatting." Text="I'm working on multiclass logistic regression model with a large number of features (numFeatures &gt; 100). Using a Maximum Likelihood Estimation based on the cost function and gradient, the fmincg algorithm solves the problem quickly. However, I'm also experimenting with a different cost function and do not have a gradient.&#xD;&#xA;&#xD;&#xA;Is there a good way to speed up the calculation process? E.g., is there a different algorithm or fmincg setting that I can use?" />
  <row Id="1272" PostHistoryTypeId="4" PostId="516" RevisionGUID="3bba4392-57dc-45ae-b609-29f955a9eae9" CreationDate="2014-06-21T06:38:58.260" UserId="84" Comment="Improving formatting." Text="Efficient solution of fmincg without providing gradient?" />
  <row Id="1273" PostHistoryTypeId="2" PostId="517" RevisionGUID="b29fd315-6285-44dc-8887-a781a3a791a8" CreationDate="2014-06-21T07:02:10.687" UserId="119" Text="If you do not have a gradient available, but the problem is convex, you can use the [Nelder-Mead simplex method][1]. It is available in most optimization packages, for example in [scipy.optimize][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method&#xD;&#xA;  [2]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" />
  <row Id="1274" PostHistoryTypeId="2" PostId="518" RevisionGUID="d9b42b23-4e5a-4363-8323-1e8aec1daa10" CreationDate="2014-06-21T10:55:41.700" UserId="960" Text="Please, could someone recommend a paper or blog post that describes the online k-means algorithm in a good way with code samples if there.&#xD;&#xA;&#xD;&#xA;Thanks :)" />
  <row Id="1275" PostHistoryTypeId="1" PostId="518" RevisionGUID="d9b42b23-4e5a-4363-8323-1e8aec1daa10" CreationDate="2014-06-21T10:55:41.700" UserId="960" Text="Online k-means explanation" />
  <row Id="1276" PostHistoryTypeId="3" PostId="518" RevisionGUID="d9b42b23-4e5a-4363-8323-1e8aec1daa10" CreationDate="2014-06-21T10:55:41.700" UserId="960" Text="&lt;machine-learning&gt;&lt;clustering&gt;" />
  <row Id="1277" PostHistoryTypeId="2" PostId="519" RevisionGUID="775af418-0262-47d1-9063-15e8bb45006d" CreationDate="2014-06-21T12:59:15.373" UserId="454" Text="The main problem here is that even before attempting to apply anomaly detection algorithms, you are not getting good enough predictions of gas consumption using neural networks. &#xD;&#xA;&#xD;&#xA;If the main goal here is to reach the stage when anomaly detection algorithms could be used and you state that you have access to examples of successful application of linear regression for this problem, this approach could be more productive. One of the principles of successful machine learning application is that several different algorithms can be tried out before final selection based on results.&#xD;&#xA;&#xD;&#xA;It you choose to tune your neural network performance, [learning curve][1] plotting the effect of change in different hyperparameters on the error rate can be used. The number of features, the order of the polynomial features, regularization parameter or number of layers in the network can be modified and selected by the best performance on cross validation set.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/questions/4617365/what-is-a-learning-curve-in-machine-learning" />
  <row Id="1278" PostHistoryTypeId="5" PostId="519" RevisionGUID="322a0244-d0d4-4825-bca9-1cd14b15f392" CreationDate="2014-06-21T13:36:29.167" UserId="454" Comment="Improved formatting of the list." Text="The main problem here is that even before attempting to apply anomaly detection algorithms, you are not getting good enough predictions of gas consumption using neural networks. &#xD;&#xA;&#xD;&#xA;If the main goal here is to reach the stage when anomaly detection algorithms could be used and you state that you have access to examples of successful application of linear regression for this problem, this approach could be more productive. One of the principles of successful machine learning application is that several different algorithms can be tried out before final selection based on results.&#xD;&#xA;&#xD;&#xA;It you choose to tune your neural network performance, [learning curve][1] plotting the effect of change in different hyperparameters on the error rate can be used. Hyperparameters that can be modified are: &#xD;&#xA;&#xD;&#xA; - number of features&#xD;&#xA; - order of the polynomial&#xD;&#xA; - regularization parameter &#xD;&#xA; - number of layers in the network&#xD;&#xA;&#xD;&#xA;Best settings can be selected by the performance on cross validation set.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/questions/4617365/what-is-a-learning-curve-in-machine-learning" />
  <row Id="1279" PostHistoryTypeId="2" PostId="520" RevisionGUID="e7099a30-e093-4b56-be81-de719d5a301a" CreationDate="2014-06-21T19:13:47.757" UserId="1085" Text="In your training notebook you present results for training with 20 epochs. Have you tried varying that parameter, to see if it affects your performance? This is an important parameter for back-propagation. &#xD;&#xA;&#xD;&#xA;For estimating your model parameters, as user tomaskazemekas pointed out, plotting Learning Curves is a very good approach. In addition to that, you could also create a plot using a model parameter (e.g. training epochs or hidden layer size) vs. Training and Validation error. This will allow you to understand the bias/variance tradeoff, and help you pick a good value for your parameters. [Some info can be found here][1]. Naturally, it is a good idea to keep a small percentage of your data for a (third) Test set.&#xD;&#xA;&#xD;&#xA;As a side note, it seems that increasing the number of neurons in your model show no significant improvement for your RMSE. This suggests that you could also try with a simpler model, i.e. with less neurons and see how your model behaves.&#xD;&#xA;&#xD;&#xA;In fact, I would suggest (if you haven't done so already) trying a simple model with few or no parameters first e.g. Linear Regression, and compare your results with the literature, just as a sanity check. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.astroml.org/sklearn_tutorial/practical.html#cross-validation-and-testing" />
  <row Id="1281" PostHistoryTypeId="5" PostId="511" RevisionGUID="bd9459b2-c9f1-4aad-b3aa-98a034412551" CreationDate="2014-06-22T10:03:50.983" UserId="97" Comment="Use more accurate term in header" Text="I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling (bootstrap sampling)?&#xD;&#xA;&#xD;&#xA;My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.&#xD;&#xA;&#xD;&#xA;In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.&#xD;&#xA;&#xD;&#xA;On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training/validation, and some will be used more than once.&#xD;&#xA;&#xD;&#xA;Classification algorithms used: random forest &amp; logistic regression." />
  <row Id="1282" PostHistoryTypeId="4" PostId="511" RevisionGUID="bd9459b2-c9f1-4aad-b3aa-98a034412551" CreationDate="2014-06-22T10:03:50.983" UserId="97" Comment="Use more accurate term in header" Text="Cross-validation: K-fold vs Bootstrap sampling" />
  <row Id="1283" PostHistoryTypeId="6" PostId="511" RevisionGUID="bd9459b2-c9f1-4aad-b3aa-98a034412551" CreationDate="2014-06-22T10:03:50.983" UserId="97" Comment="Use more accurate term in header" Text="&lt;cross-validation&gt;&lt;k-fold&gt;&lt;sampling&gt;" />
  <row Id="1285" PostHistoryTypeId="2" PostId="521" RevisionGUID="a907c87a-f7c4-4655-92a6-b0bdba7c0a9c" CreationDate="2014-06-22T12:02:52.773" UserId="1061" Text="Conjugate Gradient -- the cg in Function MINimization (nonlinear) Conjugate Gradiant -- requires you to have a gradient function (or approximation) since that is a critical part of the algorithm itself: it needs to find the steepest descent direction quickly.&#xD;&#xA;&#xD;&#xA;``fminsearch`` implements Nelder-Mead, a nonlinear gradient-free method. Its convergence properties are not anywhere near as good.&#xD;&#xA;&#xD;&#xA;What is your cost function? Are there approximations that are differentiable (pref. twice so you can use the very powerful quasi-Newton methods)?" />
  <row Id="1286" PostHistoryTypeId="2" PostId="522" RevisionGUID="0ce70e5c-1052-4c77-a43a-cdb6fc1a634f" CreationDate="2014-06-22T13:18:38.760" UserId="1011" Text="I have been able to optimize very strange functions with simulated annealing, and it does not require a gradient. Instead, it uses random numbers in a way very similar to Markov Chain Monte Carlo, which helps it avoid getting stuck in local optima. A decent explanation that gives the intuition behind it can be found in this lecture: [Simulated Annealing][1]. scipy 0.14 includes this algorithm in its optimization module: [scipy.optimize.anneal][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://iacs-courses.seas.harvard.edu/courses/am207/blog/lecture-13.html&#xD;&#xA;  [2]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.anneal.html" />
  <row Id="1287" PostHistoryTypeId="2" PostId="523" RevisionGUID="46fc4ea9-843c-43a6-bd04-7c3d6bf98bf0" CreationDate="2014-06-22T16:03:39.047" UserId="1096" Text="In your notebooks, I did not see your neural network model, can you point which library is using, how many layers you have and what type of neural network are you using?&#xD;&#xA;&#xD;&#xA;In your notebooks, it seems you are using the noisy and outlier dataset to train the neural network, I think you should train the neural network on the dataset that you do not have any outliers so that you could see the observation distance from the prediction of the neural network to label the observation either outlier or not. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I wrote [couple](http://bugra.github.io/work/notes/2014-03-31/outlier-detection-in-time-series-signals-fft-median-filtering/) of [things](http://bugra.github.io/work/notes/2014-05-11/robust-regression-and-outlier-detection-via-gaussian-processes/) on outlier detection in time-series signals, your data is highly seasonal as sobach mentioned and you could use FFT(first link above) to get the overall trend in the signal. After you get the frequency component in the gas consumption, you could look at the high frequency components to get the outliers. &#xD;&#xA;&#xD;&#xA;Also if you want to insist on using neural network for seasonal data, you may want to check recurrent neural networks out as they could incorporate the past observations better than a vanilla neural network, and supposedly may provide a better result for the data that you have. " />
  <row Id="1288" PostHistoryTypeId="5" PostId="518" RevisionGUID="c01cdb68-d303-4a07-8229-e830556a9d66" CreationDate="2014-06-22T16:44:13.780" UserId="960" Comment="deleted 41 characters in body" Text="Please, could someone recommend a paper or blog post that describes the online k-means algorithm.&#xD;&#xA;&#xD;&#xA;Thanks :)" />
  <row Id="1292" PostHistoryTypeId="2" PostId="525" RevisionGUID="29075854-a65f-46d1-8ee9-68405bc83a6e" CreationDate="2014-06-22T18:35:22.207" UserId="406" Text="In order to understand the variety of ways machine learning can be integrated into production applications, I think it is useful to look at open source projects and papers/blog posts from companies describing their infrastructure.&#xD;&#xA;&#xD;&#xA;The common theme that these systems have is the separation of model training from model application. In production systems, model application needs to be fast, on the order of 100s of ms, but there is more freedom in how frequently fitted model parameters (or equivalent) need to be updated.&#xD;&#xA;&#xD;&#xA;People use a wide range of solutions for model training and deployment:&#xD;&#xA;&#xD;&#xA;* Build a model in R/Python and export it as PMML&#xD;&#xA;&gt; * [AirBnB describes their model training][1] in R/Python and deployment of PMML models via OpenScoring.&#xD;&#xA;&#xD;&#xA;* Build a model in MapReduce and access values in a custom system&#xD;&#xA;&gt; * [Conjecture is an open source project from Etsy][2] that allows for model training with [Scalding][3], an easier to use scala wrapper around MapReduce, and  deployment via Php.&#xD;&#xA;&gt;&#xD;&#xA;&gt; * [Kiji is an open source project from WibiData][4] that allows for real-time model scoring (application) as well as functioanlity for persisting user data and training models on that data via [Scalding.][5]&#xD;&#xA;&#xD;&#xA;* Use an online system that allows for continuously updating model parameters.&#xD;&#xA;&gt; * [Google released a great paper about an online collaborative filtering][6] they implemented to deal with recommendations in Google News.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://nerds.airbnb.com/architecting-machine-learning-system-risk/&#xD;&#xA;  [2]: http://codeascraft.com/2014/06/18/conjecture-scalable-machine-learning-in-hadoop-with-scalding/&#xD;&#xA;  [3]: https://github.com/twitter/scalding&#xD;&#xA;  [4]: http://www.kiji.org/&#xD;&#xA;  [5]: https://github.com/twitter/scalding&#xD;&#xA;  [6]: http://dl.acm.org/citation.cfm?id=1242610" />
  <row Id="1293" PostHistoryTypeId="2" PostId="526" RevisionGUID="cc32ec69-a278-4c28-9a18-81edd5bf5a84" CreationDate="2014-06-22T18:50:07.193" UserId="1103" Text="Chapter 1 of Practical Data Science with R (http://www.manning.com/zumel/) has a great breakdown of the data science process, including team roles and how they relate to specific tasks. The book follows the models laid out in the chapter by referencing which stages/personnel this or that particular task would be performed by.  " />
  <row Id="1294" PostHistoryTypeId="5" PostId="526" RevisionGUID="ec3950fc-b66b-4646-bdef-7b1a553d5d2d" CreationDate="2014-06-22T19:31:48.937" UserId="84" Comment="Improving formatting." Text="Chapter 1 of [Practical Data Science with R](http://www.manning.com/zumel/) has a great breakdown of the data science process, including team roles and how they relate to specific tasks. The book follows the models laid out in the chapter by referencing which stages/personnel this or that particular task would be performed by." />
  <row Id="1296" PostHistoryTypeId="5" PostId="525" RevisionGUID="5bea5e9f-44d6-4da2-88dc-84cef77d6bd5" CreationDate="2014-06-22T19:54:28.380" UserId="406" Comment="Added Pattern to the list of examples." Text="In order to understand the variety of ways machine learning can be integrated into production applications, I think it is useful to look at open source projects and papers/blog posts from companies describing their infrastructure.&#xD;&#xA;&#xD;&#xA;The common theme that these systems have is the separation of model training from model application. In production systems, model application needs to be fast, on the order of 100s of ms, but there is more freedom in how frequently fitted model parameters (or equivalent) need to be updated.&#xD;&#xA;&#xD;&#xA;People use a wide range of solutions for model training and deployment:&#xD;&#xA;&#xD;&#xA;* Build a model, then export and deploy it with PMML&#xD;&#xA;&gt; * [AirBnB describes their model training][1] in R/Python and deployment of PMML models via OpenScoring.&#xD;&#xA;&gt;&#xD;&#xA;&gt; * [Pattern][2] is project related to [Cascading][3] that can consume PMML and deploy predictive models.&#xD;&#xA;&#xD;&#xA;* Build a model in MapReduce and access values in a custom system&#xD;&#xA;&gt; * [Conjecture is an open source project from Etsy][4] that allows for model training with [Scalding][5], an easier to use scala wrapper around MapReduce, and  deployment via Php.&#xD;&#xA;&gt;&#xD;&#xA;&gt; * [Kiji is an open source project from WibiData][6] that allows for real-time model scoring (application) as well as functioanlity for persisting user data and training models on that data via [Scalding.][7]&#xD;&#xA;&#xD;&#xA;* Use an online system that allows for continuously updating model parameters.&#xD;&#xA;&gt; * [Google released a great paper about an online collaborative filtering][8] they implemented to deal with recommendations in Google News.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://nerds.airbnb.com/architecting-machine-learning-system-risk/&#xD;&#xA;  [2]: http://www.cascading.org/projects/pattern/&#xD;&#xA;  [3]: http://www.cascading.org/&#xD;&#xA;  [4]: http://codeascraft.com/2014/06/18/conjecture-scalable-machine-learning-in-hadoop-with-scalding/&#xD;&#xA;  [5]: https://github.com/twitter/scalding&#xD;&#xA;  [6]: http://www.kiji.org/&#xD;&#xA;  [7]: https://github.com/twitter/scalding&#xD;&#xA;  [8]: http://dl.acm.org/citation.cfm?id=1242610" />
  <row Id="1297" PostHistoryTypeId="2" PostId="527" RevisionGUID="4b4b0e39-4c50-49db-a49d-ac2f2766fc92" CreationDate="2014-06-22T21:45:35.800" UserId="1107" Text="I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.&#xD;&#xA;&#xD;&#xA;I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the Smith-Waterman Algorithm to compare the similarity.&#xD;&#xA;&#xD;&#xA;So I've drawn a picture of how I'm thinking about representing the data - &#xD;&#xA;&#xD;&#xA;http://i.imgur.com/sNmGf98&#xD;&#xA;&#xD;&#xA;The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).&#xD;&#xA;&#xD;&#xA;Image that this matrix represents a key type of &quot;things&quot; I then need to add the &quot;things&quot; similarity score into a vector of 0 or 1. Thats ok.&#xD;&#xA;&#xD;&#xA;What I can't figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I'll just set a threshold to score it as either 0 or 1.&#xD;&#xA;&#xD;&#xA;Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable)&#xD;&#xA;&#xD;&#xA;Cheers&#xD;&#xA;&#xD;&#xA;Dave" />
  <row Id="1298" PostHistoryTypeId="1" PostId="527" RevisionGUID="4b4b0e39-4c50-49db-a49d-ac2f2766fc92" CreationDate="2014-06-22T21:45:35.800" UserId="1107" Text="Score matrix string similarity" />
  <row Id="1299" PostHistoryTypeId="3" PostId="527" RevisionGUID="4b4b0e39-4c50-49db-a49d-ac2f2766fc92" CreationDate="2014-06-22T21:45:35.800" UserId="1107" Text="&lt;algorithms&gt;&lt;similarity&gt;" />
  <row Id="1300" PostHistoryTypeId="5" PostId="527" RevisionGUID="dcecb510-2af3-4ca3-b6ff-757789ce8a4d" CreationDate="2014-06-22T22:03:48.810" UserId="84" Comment="Improving formatting." Text="I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.&#xD;&#xA;&#xD;&#xA;I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the Smith-Waterman Algorithm to compare the similarity.&#xD;&#xA;&#xD;&#xA;So I've drawn a picture of how I'm thinking about representing the data - &#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).&#xD;&#xA;&#xD;&#xA;Image that this matrix represents a key type of &quot;things&quot; I then need to add the &quot;things&quot; similarity score into a vector of 0 or 1. Thats ok.&#xD;&#xA;&#xD;&#xA;What I can't figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I'll just set a threshold to score it as either 0 or 1.&#xD;&#xA;&#xD;&#xA;Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/VC4em.png" />
  <row Id="1301" PostHistoryTypeId="5" PostId="527" RevisionGUID="a7b1eb05-82c2-4eb4-9650-a59cf2cb511f" CreationDate="2014-06-22T22:44:19.880" UserId="84" Comment="added 65 characters in body" Text="I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.&#xD;&#xA;&#xD;&#xA;I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the [Smith-Waterman Algorithm](http://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm) to compare the similarity.&#xD;&#xA;&#xD;&#xA;So I've drawn a picture of how I'm thinking about representing the data - &#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).&#xD;&#xA;&#xD;&#xA;Image that this matrix represents a key type of &quot;things&quot; I then need to add the &quot;things&quot; similarity score into a vector of 0 or 1. Thats ok.&#xD;&#xA;&#xD;&#xA;What I can't figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I'll just set a threshold to score it as either 0 or 1.&#xD;&#xA;&#xD;&#xA;Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/VC4em.png" />
  <row Id="1305" PostHistoryTypeId="2" PostId="529" RevisionGUID="6106b197-7ea8-47f5-baec-631dfc09aac7" CreationDate="2014-06-23T03:44:57.643" UserId="159" Text="[Airbnb][1] and [Etsy][2] both recently posted detailed information about their workflows.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://nerds.airbnb.com/architecting-machine-learning-system-risk/&#xD;&#xA;  [2]: http://codeascraft.com/2014/06/18/conjecture-scalable-machine-learning-in-hadoop-with-scalding/" />
  <row Id="1306" PostHistoryTypeId="2" PostId="530" RevisionGUID="b6c2fd67-73c5-41a1-b318-0e4c3148d435" CreationDate="2014-06-23T04:39:26.623" UserId="454" Text="There is a general recommendation that algorithms in ensemble learning combinations should be different in nature. Is there a classification table, a scale or some rules that allow to evaluate how far away are the algorithms from each other? What are the best combinations? " />
  <row Id="1307" PostHistoryTypeId="1" PostId="530" RevisionGUID="b6c2fd67-73c5-41a1-b318-0e4c3148d435" CreationDate="2014-06-23T04:39:26.623" UserId="454" Text="How to select algorithms for ensemble methods?" />
  <row Id="1308" PostHistoryTypeId="3" PostId="530" RevisionGUID="b6c2fd67-73c5-41a1-b318-0e4c3148d435" CreationDate="2014-06-23T04:39:26.623" UserId="454" Text="&lt;machine-learning&gt;" />
  <row Id="1309" PostHistoryTypeId="2" PostId="531" RevisionGUID="62feffeb-5ca9-4187-ba80-c79dd44e682c" CreationDate="2014-06-23T07:03:15.643" UserId="793" Text="I have a dataset with following specifications:&#xD;&#xA;&#xD;&#xA; - Training dataset with 193176 samples with 2821 positives&#xD;&#xA; - Test Dataset with 82887 samples with 673 positives&#xD;&#xA; - There are 10 features.&#xD;&#xA;&#xD;&#xA;I want to perform a binary classification ( say, 0/1 ). The issue I am facing is that the data is very biased or rather sparse. After normalization and scaling the data along with some feature engineering and using a couple of different algorithms, these are the best results I could achieve:&#xD;&#xA;&#xD;&#xA;    mean square error : 0.00804710026904&#xD;&#xA;    Confusion matrix : [[82214   667]&#xD;&#xA;                       [    0     6]]&#xD;&#xA;&#xD;&#xA;i.e only 6 correct positive hits. This is using logistic regression. Here are the various things I tried with this:&#xD;&#xA;&#xD;&#xA; - Different algorithms like RandomForest, DecisionTree, SVM&#xD;&#xA; - Changing parameters value to call the function&#xD;&#xA; - Some intuition based feature engineering to include compounded features&#xD;&#xA;&#xD;&#xA;Now, my questions are:&#xD;&#xA;&#xD;&#xA; 1. What can I do to improve the number of positive hits ? &#xD;&#xA; 2. How can one determine if there is an overfit in such a case ? ( I have tried plotting etc. )&#xD;&#xA; 3. At what point could one conclude if maybe this is the best possible fit I could have? ( which seems sad considering only 6 hits out of 673 )&#xD;&#xA; 4. Is there a way I could make the positive sample instances weigh more so the pattern recognition improves leading to more hits ?&#xD;&#xA; 5. Which graphical plots could help detect outliers or some pattern intuition?&#xD;&#xA;&#xD;&#xA;I am using the scikit-learn library with Python and all implementations are library functions." />
  <row Id="1310" PostHistoryTypeId="1" PostId="531" RevisionGUID="62feffeb-5ca9-4187-ba80-c79dd44e682c" CreationDate="2014-06-23T07:03:15.643" UserId="793" Text="Binary classification model for sparse / biased data" />
  <row Id="1311" PostHistoryTypeId="3" PostId="531" RevisionGUID="62feffeb-5ca9-4187-ba80-c79dd44e682c" CreationDate="2014-06-23T07:03:15.643" UserId="793" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;classification&gt;&lt;logistic-regression&gt;&lt;topic-model&gt;" />
  <row Id="1312" PostHistoryTypeId="2" PostId="532" RevisionGUID="7e470ab0-5e5a-4c23-a0a0-15fa19869307" CreationDate="2014-06-23T08:03:50.327" UserId="418" Text="In general in an ensemble you try to combine the opinions of multiple classifiers. The idea is like asking a bunch of experts on the same thing. You get multiple opinions and you later have to combine their answers (e.g. by a voting scheme). For this trick to work you want the classifier to be different from each other, that is you don't want to ask the same &quot;expert&quot; twice for the same thing.&#xD;&#xA;&#xD;&#xA;In practice, the classifiers do not have to be different in the sense of a different algorithm. What you can do is train the same algorithm with different subset of the data or a different subset of futures (or both). If you use different training sets you end up with different models and different &quot;independent&quot; classifiers. &#xD;&#xA;&#xD;&#xA;There is no gold rule on what's work best in general. You have to try to see if there is any improvement for your specific problem. " />
  <row Id="1313" PostHistoryTypeId="2" PostId="533" RevisionGUID="11f26ba5-157b-4fd3-91b3-770dcc1ad8cf" CreationDate="2014-06-23T08:13:22.837" UserId="1085" Text="1. Since you are doing binary classification, have you tried adjusting the classification threshold? Since your algorithm seems rather insensitive, I would try lowering it and check if there is an improvement.&#xD;&#xA;&#xD;&#xA;2. You can always use [Learning Curves][1], or a plot of one model parameter vs. Training and Validation error to determine whether your model is overfitting. It seems it is under fitting in your case, but that's just intuition.&#xD;&#xA;&#xD;&#xA;3. Well, ultimately it depends on your dataset, and the different models you have tried. At this point, and without further testing, there can not be a definite answer.&#xD;&#xA;&#xD;&#xA;4. Without claiming to be an expert on the topic, there are a number of different techniques you may follow (hint: [first link on google][2]), but in my opinion you should first make sure you choose your cost function carefully, so that it represents what you are actually looking for. &#xD;&#xA;&#xD;&#xA;5. Not sure what you mean by pattern intuition, can you elaborate?&#xD;&#xA;&#xD;&#xA;By the way, what were your results with the different algorithms you tried? Were they any different?&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/questions/4617365/what-is-a-learning-curve-in-machine-learning&#xD;&#xA;  [2]: http://florianhartl.com/thoughts-on-machine-learning-dealing-with-skewed-classes.html" />
  <row Id="1314" PostHistoryTypeId="5" PostId="532" RevisionGUID="ddb8ad0f-c11c-4b26-a4f3-333c8e39f7a3" CreationDate="2014-06-23T08:14:15.807" UserId="418" Comment="deleted 1 character in body" Text="In general in an ensemble you try to combine the opinions of multiple classifiers. The idea is like asking a bunch of experts on the same thing. You get multiple opinions and you later have to combine their answers (e.g. by a voting scheme). For this trick to work you want the classifier to be different from each other, that is you don't want to ask the same &quot;expert&quot; twice for the same thing.&#xD;&#xA;&#xD;&#xA;In practice, the classifiers do not have to be different in the sense of a different algorithm. What you can do is train the same algorithm with different subset of the data or a different subset of futures (or both). If you use different training sets you end up with different models and different &quot;independent&quot; classifiers. &#xD;&#xA;&#xD;&#xA;There is no gold rule on what's work best in general. You have to try to see if there is an improvement for your specific problem. " />
  <row Id="1315" PostHistoryTypeId="2" PostId="534" RevisionGUID="a47cb18f-58a5-41d6-b0e1-a1f123898233" CreationDate="2014-06-23T09:23:27.617" UserId="883" Text="If your goal is to transform your matrix into a number (your similarity measure), you may want to use a [matrix norm][1].&#xD;&#xA;&#xD;&#xA;For instance, using the [Frobenius norm][2] on your example would return 1.488086.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Matrix_norm&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm" />
  <row Id="1316" PostHistoryTypeId="2" PostId="535" RevisionGUID="0e099b84-2007-48dc-a0a3-c6b51f8a6828" CreationDate="2014-06-23T09:42:46.450" UserId="941" Text="As I understood, Document 1 and Document 2 may have different number of keys. And you wand to get final similarity evaluation between 0 and 1. If so, I would propose following algorithm:&#xD;&#xA;&#xD;&#xA;1. Sum of max. vals is equal to 0.&#xD;&#xA;2. Select maximum value from doc-doc matrix and add it to Sum of max. vals.&#xD;&#xA;3. Remove row and column with maximum value from the matrix.&#xD;&#xA;4. Repeat steps 2-3 until rows or columns are ended.&#xD;&#xA;5. Denominate Sum of max. vals by average number of key words in two texts.&#xD;&#xA;&#xD;&#xA;Final estimation would be equal to 1, if both documents have identical length, and every word from Doc 1 has equivalent in Doc 2.&#xD;&#xA;&#xD;&#xA;You haven't mentioned software, you are using, but here is **R** example of function, computing such similarity (it takes object of class matrix as input):&#xD;&#xA;&#xD;&#xA;    eval.sim &lt;- function(sim.matrix){&#xD;&#xA;      similarity &lt;- 0&#xD;&#xA;      denominator &lt;- sum(dim(sim.matrix)) / 2&#xD;&#xA;      for(i in 1:(min(c(nrow(sim.matrix), ncol(sim.matrix))) - 1)){&#xD;&#xA;        extract &lt;- which(sim.matrix == max(sim.matrix), arr.ind=T)[1, ]&#xD;&#xA;        similarity &lt;- similarity + sim.matrix[extract[1], extract[2]]&#xD;&#xA;        sim.matrix &lt;- sim.matrix[-extract[1], -extract[2]]&#xD;&#xA;      }&#xD;&#xA;      similarity &lt;- similarity + max(sm.copy)&#xD;&#xA;      similarity &lt;- similarity / denominator&#xD;&#xA;    }" />
  <row Id="1317" PostHistoryTypeId="5" PostId="531" RevisionGUID="6cd8d47a-68cf-409f-88fb-b8881d6edf56" CreationDate="2014-06-23T10:38:31.273" UserId="793" Comment="added 66 characters in body" Text="I have a dataset with following specifications:&#xD;&#xA;&#xD;&#xA; - Training dataset with 193176 samples with 2821 positives&#xD;&#xA; - Test Dataset with 82887 samples with 673 positives&#xD;&#xA; - There are 10 features.&#xD;&#xA;&#xD;&#xA;I want to perform a binary classification ( say, 0/1 ). The issue I am facing is that the data is very biased or rather sparse. After normalization and scaling the data along with some feature engineering and using a couple of different algorithms, these are the best results I could achieve:&#xD;&#xA;&#xD;&#xA;    mean square error : 0.00804710026904&#xD;&#xA;    Confusion matrix : [[82214   667]&#xD;&#xA;                       [    0     6]]&#xD;&#xA;&#xD;&#xA;i.e only 6 correct positive hits. This is using logistic regression. Here are the various things I tried with this:&#xD;&#xA;&#xD;&#xA; - Different algorithms like RandomForest, DecisionTree, SVM&#xD;&#xA; - Changing parameters value to call the function&#xD;&#xA; - Some intuition based feature engineering to include compounded features&#xD;&#xA;&#xD;&#xA;Now, my questions are:&#xD;&#xA;&#xD;&#xA; 1. What can I do to improve the number of positive hits ? &#xD;&#xA; 2. How can one determine if there is an overfit in such a case ? ( I have tried plotting etc. )&#xD;&#xA; 3. At what point could one conclude if maybe this is the best possible fit I could have? ( which seems sad considering only 6 hits out of 673 )&#xD;&#xA; 4. Is there a way I could make the positive sample instances weigh more so the pattern recognition improves leading to more hits ?&#xD;&#xA; 5. Which graphical plots could help detect outliers or some intuition about which pattern would fit the best?&#xD;&#xA;&#xD;&#xA;I am using the scikit-learn library with Python and all implementations are library functions.&#xD;&#xA;&#xD;&#xA;**edit:**&#xD;&#xA;&#xD;&#xA;Here are the results with a few other algorithms:&#xD;&#xA;&#xD;&#xA;Random Forest Classifier(n_estimators=100)&#xD;&#xA;&#xD;&#xA;    [[82211   667]&#xD;&#xA;    [    3     6]]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Decision Trees:&#xD;&#xA;&#xD;&#xA;    [[78611   635]&#xD;&#xA;    [ 3603    38]]&#xD;&#xA;" />
  <row Id="1318" PostHistoryTypeId="2" PostId="536" RevisionGUID="1ed3b8bb-d889-43f2-b954-c73bcb9ff032" CreationDate="2014-06-23T11:03:09.767" UserId="289" Text="More than often data I am working with is not 100% clean. Even if it is reasonably clean, still there are there are pieces that need to be fixed. &#xD;&#xA;&#xD;&#xA;When a fraction of data needs it, I write a script and incorporate it in data processing.&#xD;&#xA;&#xD;&#xA;But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let's focus on &quot;small data&quot;, in CSV files or as SQL database.&#xD;&#xA;&#xD;&#xA;The practical problems I encountered:&#xD;&#xA;&#xD;&#xA;* Writing a general script trying solve all similar errors may give unintended consequences.&#xD;&#xA;* Copying and modifying data may make a mess, as:&#xD;&#xA;  * Generating it again will destroy all fixes.&#xD;&#xA;  * When there are more errors of different kinds, it is easy to get to many copies of the same file and loose track of them.&#xD;&#xA;* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best).&#xD;&#xA;&#xD;&#xA;If there any good practice?" />
  <row Id="1319" PostHistoryTypeId="1" PostId="536" RevisionGUID="1ed3b8bb-d889-43f2-b954-c73bcb9ff032" CreationDate="2014-06-23T11:03:09.767" UserId="289" Text="Good practices for manual modifications of data" />
  <row Id="1320" PostHistoryTypeId="3" PostId="536" RevisionGUID="1ed3b8bb-d889-43f2-b954-c73bcb9ff032" CreationDate="2014-06-23T11:03:09.767" UserId="289" Text="&lt;data-cleaning&gt;" />
  <row Id="1321" PostHistoryTypeId="5" PostId="536" RevisionGUID="dd8d1e6e-6dcb-42b3-b05f-c19bfe581783" CreationDate="2014-06-23T11:05:16.030" UserId="1085" Comment="fixed typo" Text="More than often data I am working with is not 100% clean. Even if it is reasonably clean, still there are pieces that need to be fixed. &#xD;&#xA;&#xD;&#xA;When a fraction of data needs it, I write a script and incorporate it in data processing.&#xD;&#xA;&#xD;&#xA;But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let's focus on &quot;small data&quot;, in CSV files or as SQL database.&#xD;&#xA;&#xD;&#xA;The practical problems I encountered:&#xD;&#xA;&#xD;&#xA;* Writing a general script trying solve all similar errors may give unintended consequences.&#xD;&#xA;* Copying and modifying data may make a mess, as:&#xD;&#xA;  * Generating it again will destroy all fixes.&#xD;&#xA;  * When there are more errors of different kinds, it is easy to get to many copies of the same file and loose track of them.&#xD;&#xA;* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best).&#xD;&#xA;&#xD;&#xA;If there any good practice?" />
  <row Id="1322" PostHistoryTypeId="24" PostId="536" RevisionGUID="dd8d1e6e-6dcb-42b3-b05f-c19bfe581783" CreationDate="2014-06-23T11:05:16.030" Comment="Proposed by 1085 approved by 289 edit id of 89" />
  <row Id="1323" PostHistoryTypeId="5" PostId="536" RevisionGUID="8202ac86-2b21-45fb-b97e-2f294105440b" CreationDate="2014-06-23T11:20:05.343" UserId="289" Comment="added 194 characters in body" Text="More than often data I am working with is not 100% clean. Even if it is reasonably clean, still there are pieces that need to be fixed. &#xD;&#xA;&#xD;&#xA;When a fraction of data needs it, I write a script and incorporate it in data processing.&#xD;&#xA;&#xD;&#xA;But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let's focus on &quot;small data&quot;, in CSV files or as SQL database.&#xD;&#xA;&#xD;&#xA;The practical problems I encountered:&#xD;&#xA;&#xD;&#xA;* Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).&#xD;&#xA;* Copying and modifying data may make a mess, as:&#xD;&#xA;  * Generating it again will destroy all fixes.&#xD;&#xA;  * When there are more errors of different kinds, it is easy to get to many copies of the same file and loose track of them.&#xD;&#xA;* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.&#xD;&#xA;&#xD;&#xA;If there any good practice?" />
  <row Id="1324" PostHistoryTypeId="5" PostId="532" RevisionGUID="b1ab6990-7925-42a9-9f37-c5c3e772c949" CreationDate="2014-06-23T11:21:51.677" UserId="1085" Comment="grammar and typos" Text="In general in an ensemble you try to combine the opinions of multiple classifiers. The idea is like asking a bunch of experts on the same thing. You get multiple opinions and you later have to combine their answers (e.g. by a voting scheme). For this trick to work you want the classifiers to be different from each other, that is you don't want to ask the same &quot;expert&quot; twice for the same thing.&#xD;&#xA;&#xD;&#xA;In practice, the classifiers do not have to be different in the sense of a different algorithm. What you can do is train the same algorithm with different subset of the data or a different subset of features (or both). If you use different training sets you end up with different models and different &quot;independent&quot; classifiers. &#xD;&#xA;&#xD;&#xA;There is no golden rule on what works best in general. You have to try to see if there is an improvement for your specific problem. " />
  <row Id="1325" PostHistoryTypeId="24" PostId="532" RevisionGUID="b1ab6990-7925-42a9-9f37-c5c3e772c949" CreationDate="2014-06-23T11:21:51.677" Comment="Proposed by 1085 approved by 418 edit id of 90" />
  <row Id="1329" PostHistoryTypeId="2" PostId="538" RevisionGUID="5793bd79-ac07-4663-b98d-d6be47c959b7" CreationDate="2014-06-23T13:31:41.443" UserId="1125" Text="Data Science and Machine Learning include a lot of different topics and it´s hard to stay up-to-date about all the news about papers, researches or new tutorials and tools.&#xD;&#xA;&#xD;&#xA;What sources do you use to get all the information?&#xD;&#xA;&#xD;&#xA;I use mostly Reddit as my first source and the subreddits Machine Learning and R&#xD;&#xA;[http://www.reddit.com/r/MachineLearning][1]&#xD;&#xA;&#xD;&#xA;[http://www.reddit.com/r/rstats][2]&#xD;&#xA;&#xD;&#xA;But also datatau.com and of course the great KDNuggets page &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.reddit.com/r/MachineLearning&#xD;&#xA;  [2]: http://www.reddit.com/r/rstats&#xD;&#xA;  &#xD;&#xA;  " />
  <row Id="1330" PostHistoryTypeId="1" PostId="538" RevisionGUID="5793bd79-ac07-4663-b98d-d6be47c959b7" CreationDate="2014-06-23T13:31:41.443" UserId="1125" Text="What are your favorite sources for news about Machine Learning and Data Science?" />
  <row Id="1331" PostHistoryTypeId="3" PostId="538" RevisionGUID="5793bd79-ac07-4663-b98d-d6be47c959b7" CreationDate="2014-06-23T13:31:41.443" UserId="1125" Text="&lt;machine-learning&gt;" />
  <row Id="1333" PostHistoryTypeId="2" PostId="539" RevisionGUID="9c160522-39ce-4fb3-95bb-c446e0f4466f" CreationDate="2014-06-23T13:36:18.273" UserId="1127" Text="I like following certain tags on stats.stackexchange.com (a.k.a. CrossValidated), as well as the ```r``` tag on StackOverflow.&#xD;&#xA;&#xD;&#xA;Arxiv.org is also pretty cool, it has several subsections devoted to stats and ML. " />
  <row Id="1334" PostHistoryTypeId="8" PostId="526" RevisionGUID="4295ceea-6189-4d93-a2c8-655acd933980" CreationDate="2014-06-23T13:36:51.493" UserId="1103" Comment="Rollback to [cc32ec69-a278-4c28-9a18-81edd5bf5a84]" Text="Chapter 1 of Practical Data Science with R (http://www.manning.com/zumel/) has a great breakdown of the data science process, including team roles and how they relate to specific tasks. The book follows the models laid out in the chapter by referencing which stages/personnel this or that particular task would be performed by.  " />
  <row Id="1336" PostHistoryTypeId="2" PostId="540" RevisionGUID="ba2d429a-4dea-4fe3-a53a-280aaf727686" CreationDate="2014-06-23T15:38:10.920" UserId="1131" Text="What is the very first thing you do when you get your hands on a new data set (assuming it is cleaned and well structured)? Please share sample code snippets as I am sure this would be extremely helpful for both beginners and experienced. " />
  <row Id="1337" PostHistoryTypeId="1" PostId="540" RevisionGUID="ba2d429a-4dea-4fe3-a53a-280aaf727686" CreationDate="2014-06-23T15:38:10.920" UserId="1131" Text="First steps on a new cleaned dataset" />
  <row Id="1338" PostHistoryTypeId="3" PostId="540" RevisionGUID="ba2d429a-4dea-4fe3-a53a-280aaf727686" CreationDate="2014-06-23T15:38:10.920" UserId="1131" Text="&lt;knowledge-base&gt;&lt;dataset&gt;" />
  <row Id="1339" PostHistoryTypeId="2" PostId="541" RevisionGUID="f09894c2-6b46-4d01-b90c-5eb5e9d78cef" CreationDate="2014-06-23T15:48:10.033" UserId="322" Text="The #1 most important thing is to *explicitly document your process*.&#xD;&#xA;&#xD;&#xA;Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one of the huge strengths of peer review, and of science as a community effort.&#xD;&#xA;&#xD;&#xA;You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won't reasonably accept it. Applying an algorithm to the entire data set is not the only tool at your disposal.&#xD;&#xA;&#xD;&#xA;But, if you use your individual judgment to make exceptions and make &quot;manual&quot; corrections in your data, be prepared to defend your judgment and argue your case. You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field.&#xD;&#xA;&#xD;&#xA;On the other hand, why do all this extra work before you know it's necessary? Plenty of &quot;dirty&quot; data sets will still produce useful results. Perhaps 0.5% of your data is &quot;dirty&quot; in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise." />
  <row Id="1340" PostHistoryTypeId="5" PostId="541" RevisionGUID="84894177-7b1f-4646-9bd1-309b6ca64dac" CreationDate="2014-06-23T15:54:19.253" UserId="322" Comment="added 396 characters in body" Text="The #1 most important thing is to *explicitly document your process*.&#xD;&#xA;&#xD;&#xA;Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one reason that science is the pursuit of a society, not of individuals, and underscores the importance of peer review.&#xD;&#xA;&#xD;&#xA;You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won't reasonably accept it. Applying an algorithm to the entire data set can be powerful when it works, but it's not the only tool at your disposal. Sometimes you need to use your judgment as the scientist.&#xD;&#xA;&#xD;&#xA;But, if you use your individual judgment to make exceptions and to manually correct your data, be prepared to defend your judgment and argue your case. Be prepared to show that you considered all the data. If you're going to manually correct 8 observations out of a set of 100,000, you need to do more than justify those 8 manual corrections - you also need to justify **not** correcting the other 99,992 observations.&#xD;&#xA;&#xD;&#xA;You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field.&#xD;&#xA;&#xD;&#xA;On the other hand, why do all this extra work before you know it's necessary? Plenty of &quot;dirty&quot; data sets will still produce useful results. Perhaps 0.5% of your data is &quot;dirty&quot; in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise." />
  <row Id="1341" PostHistoryTypeId="5" PostId="541" RevisionGUID="295fb4d7-bbfd-460f-bd86-7fbd2f3d9bef" CreationDate="2014-06-23T16:14:33.900" UserId="322" Comment="added 396 characters in body" Text="The #1 most important thing is to *explicitly document your process*.&#xD;&#xA;&#xD;&#xA;Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one reason that science is the pursuit of a society, not of individuals, and underscores the importance of peer review.&#xD;&#xA;&#xD;&#xA;You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won't reasonably accept it. Applying an algorithm to the entire data set can be powerful when it works, but it's not the only tool at your disposal. Sometimes you need to use your judgment as the scientist.&#xD;&#xA;&#xD;&#xA;But, if you use your individual judgment to make exceptions and to manually correct your data, be prepared to defend your judgment and argue your case. Be prepared to show that you considered all the data. If you're going to manually correct 8 observations out of a set of 100,000, you need to do more than justify those 8 manual corrections - you also need to justify **not** correcting the other 99,992 observations.&#xD;&#xA;&#xD;&#xA;You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field.&#xD;&#xA;&#xD;&#xA;On the other hand, why do all this extra work before you know it's necessary? Plenty of &quot;dirty&quot; data sets will still produce useful results. Perhaps 0.5% of your data is &quot;dirty&quot; in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise.&#xD;&#xA;&#xD;&#xA;For example...&#xD;&#xA;-&#xD;&#xA;&#xD;&#xA;Say you have a set of test results from a survey of wells, showing the concentrations of certain dissolved substances, hourly over the course of a year. And you observe in this set certain spikes, of short duration, and orders of magnitude higher than the surrounding data. If they are few, and obvious, and you know that the sensors used to produce the data set occasionally malfunction, then there's no reason to apply an algorithmic solution to the entire data set. You have a choice between excluding some data or modifying it. &#xD;&#xA;&#xD;&#xA;I recommend the exclusion route whenever possible, since you are making fewer assumptions when you don't have to additionally choose a &quot;correct&quot; value. But if your analysis will absolutely fail with a discontinuity, there are many options. You could choose to replace &quot;bad&quot; values via linear interpolation. You could hold constant the previous value. If there is some great previous literature to apply, and you really have a strong case that the &quot;bad&quot; data is purely due to equipment malfunction, perhaps there's an established model that you can apply to fill in those regions.&#xD;&#xA;&#xD;&#xA;There are a great many approaches, which is why the most important thing is to document your process, explicitly and exhaustively. Arguing your case is important too, but not strictly necessary; the community is just more likely to ignore you if you don't make a full effort to engage the review process." />
  <row Id="1345" PostHistoryTypeId="5" PostId="536" RevisionGUID="4d127859-f20b-4c12-b270-a007b0338834" CreationDate="2014-06-23T17:19:54.087" UserId="289" Comment="added 316 characters in body" Text="More than often data I am working with is not 100% clean. Even if it is reasonably clean, still there are pieces that need to be fixed. &#xD;&#xA;&#xD;&#xA;When a fraction of data needs it, I write a script and incorporate it in data processing.&#xD;&#xA;&#xD;&#xA;But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let's focus on &quot;small data&quot;, in CSV files or as SQL database.&#xD;&#xA;&#xD;&#xA;The practical problems I encountered:&#xD;&#xA;&#xD;&#xA;* Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).&#xD;&#xA;* Copying and modifying data may make a mess, as:&#xD;&#xA;  * Generating it again will destroy all fixes.&#xD;&#xA;  * When there are more errors of different kinds, it is easy to get to many copies of the same file and loose track of them.&#xD;&#xA;* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.&#xD;&#xA;&#xD;&#xA;If there any good practice?&#xD;&#xA;&#xD;&#xA;EDIT:The question is on the workflow, not whether to use it or not.&#xD;&#xA;&#xD;&#xA;(In my particular case I don't want the end-user to see misspelled cities and, even worse, see two points of data, for the same city but with different spelling; the data is small, ~500 different cities, so manual corrections do make sense.)" />
  <row Id="1346" PostHistoryTypeId="2" PostId="543" RevisionGUID="21129bfb-9858-42c0-a3d4-38bb23a43a7b" CreationDate="2014-06-23T17:50:51.320" UserId="793" Text="Well, you mention in your question of the data being 'clean and well structured'. In practice, close to 70% of the time is spent in doing these two steps. Of course the first thing you do is to separate the training and test data. Considering the plethora of libraries and tools available irrespective of which technology/language you prefer to use, the next step would be to understand the data via graph plots and drawing useful intuitions specific to your target goal. This would then be followed by various other problem specific methods. As pointed out, the question is very broad and citing code snippets is simply not feasible." />
  <row Id="1347" PostHistoryTypeId="2" PostId="544" RevisionGUID="9aec4919-be72-4062-ae54-ee5172bbff15" CreationDate="2014-06-23T17:56:32.917" UserId="793" Text="You could also try:&#xD;&#xA;&#xD;&#xA; - [Quora][1] &#xD;&#xA; - [Machine Learning Daily][2]&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;  [1]: http://www.quora.com&#xD;&#xA;  [2]: http://paper.li/MachineCoding/1370791453" />
  <row Id="1348" PostHistoryTypeId="2" PostId="545" RevisionGUID="1e0ba174-c077-4a10-bb57-b10b011d55c3" CreationDate="2014-06-23T18:38:34.260" UserId="1011" Text="I think this is a reasonable question. Here is what I do:&#xD;&#xA;&#xD;&#xA; 1. Peak at the first few rows&#xD;&#xA; 2. Visualize the distribution of the features I care about (histograms)&#xD;&#xA; 3. Visualize the relationship between pairs of features (scatterplots)&#xD;&#xA;&#xD;&#xA;I downloaded the abalone dataset from the UCI Machine Learning repository [here][1]. Let's say I care about how height and diameter can be used to predict whole weight. For completeness, I've included the step of reading the data from file.&#xD;&#xA;&#xD;&#xA;    import pandas as pd&#xD;&#xA;    import matplotlib.pyplot as plt&#xD;&#xA;    import seaborn as sns&#xD;&#xA;&#xD;&#xA;    data = pd.read_csv(&quot;abalone.data&quot;, header=False)&#xD;&#xA;    data.columns = [&quot;sex&quot;, &quot;length&quot;, &quot;diameter&quot;, &quot;height&quot;, &#xD;&#xA;                    &quot;whole_weight&quot;, &quot;shucked_weight&quot;,&#xD;&#xA;                    &quot;viscera_weight&quot;, &quot;shell_weight&quot;, &quot;rings&quot;]&#xD;&#xA;&#xD;&#xA;Now we can take a peak at the first few rows:&#xD;&#xA;&#xD;&#xA;    data.head()&#xD;&#xA;&#xD;&#xA;![Head of dataset][2]&#xD;&#xA;&#xD;&#xA;Now, I know that the variables I care about are floating point values and they can be treated as continuous. I want to take a look to see how these three variables are distributed:&#xD;&#xA;&#xD;&#xA;    fig = plt.figure(figsize=(20,5))&#xD;&#xA;    plt.subplot(1, 3, 1)&#xD;&#xA;    plt.hist(data['diameter'], normed=True)&#xD;&#xA;    plt.title(&quot;Diameter&quot;)&#xD;&#xA;    plt.subplot(1, 3, 2)&#xD;&#xA;    plt.hist(data['height'], normed=True)&#xD;&#xA;    plt.title(&quot;Height&quot;)&#xD;&#xA;    plt.subplot(1, 3, 3)&#xD;&#xA;    plt.hist(data['whole_weight'], normed=True)&#xD;&#xA;    plt.title(&quot;Whole Weight&quot;)&#xD;&#xA;    plt.show()&#xD;&#xA;&#xD;&#xA;![histograms][3]&#xD;&#xA;&#xD;&#xA;Great! Now, I know that diameter and whole weight are skewed left and right (respectively). I also know that there are some outliers in terms of height (which is why matplotlib gives me extra room to the right of the distribution). Finally, I'd like to see if I can find any visual patterns between my predictors and outcome variable. I use a scatter plot for this:&#xD;&#xA;&#xD;&#xA;    plt.figure(figsize=(15,5))&#xD;&#xA;    plt.subplot(1, 2, 1)&#xD;&#xA;    plt.plot(data['diameter'], data['whole_weight'], 'o')&#xD;&#xA;    plt.title(&quot;Diameter vs. Whole Weight&quot;)&#xD;&#xA;    plt.ylabel(&quot;Whole Weight&quot;)&#xD;&#xA;    plt.xlabel(&quot;Diameter&quot;)&#xD;&#xA;    plt.subplot(1, 2, 2)&#xD;&#xA;    plt.plot(data['height'], data['whole_weight'], 'o')&#xD;&#xA;    plt.title(&quot;Height vs. Whole Weight&quot;)&#xD;&#xA;    plt.ylabel(&quot;Whole Weight&quot;)&#xD;&#xA;    plt.xlabel(&quot;Height&quot;)&#xD;&#xA;    plt.show()&#xD;&#xA;&#xD;&#xA;![scatterplots][4]&#xD;&#xA;&#xD;&#xA;Here, I see there is a non-linear relationship between diameter and whole weight and I'm going to have to deal with my height outliers. Now, I'm ready to do some analysis!&#xD;&#xA;&#xD;&#xA;  [1]: https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data&#xD;&#xA;  [2]: http://i.stack.imgur.com/ZEFH6.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/Rmq4n.png&#xD;&#xA;  [4]: http://i.stack.imgur.com/FMmN1.png" />
  <row Id="1350" PostHistoryTypeId="2" PostId="546" RevisionGUID="0c7a707e-5877-476e-9953-d1da4ba1f6b9" CreationDate="2014-06-23T18:56:11.577" UserId="1011" Text="Even if you are effectively modifying certain records by hand, as in the city name example you give, I would recommend doing it in code. The reason to strongly prefer code over hand-tweaking records is that the code makes a result reproducible. You want to make sure that you can always go from raw data to final result without any human intervention.&#xD;&#xA;&#xD;&#xA;Here's a quick example. Let's say I have a list of city names in a pandas dataframe and I am certain they should all be &quot;omaha&quot; (you need to be absolutely certain, because changing values by hand is fraught with danger). But instead I have the following strings:&#xD;&#xA;&#xD;&#xA;    pd.unique(data.city)&#xD;&#xA;    array(['omaha', 'omahd', 'imaha', 'omaka'], dtype=object)&#xD;&#xA;&#xD;&#xA;You could make the change like this:&#xD;&#xA;&#xD;&#xA;    data.city.values[data.city.values == 'omahd'] = 'omaha'&#xD;&#xA;    data.city.values[data.city.values == 'imaha'] = 'omaha'&#xD;&#xA;    data.city.values[data.city.values == 'omaka'] = 'omaha'&#xD;&#xA;&#xD;&#xA;That code is ugly, but if you run it on the same raw dataset, you will *always* get the same result." />
  <row Id="1352" PostHistoryTypeId="2" PostId="548" RevisionGUID="5fd58f20-ce2d-4e2b-8275-cd58af9cbbdc" CreationDate="2014-06-23T23:20:36.180" UserId="1138" Text="I often am building a model (classification or regression) where I have some predictor variables that are sequences and I have been trying to find technique recommendations for summarizing them in the best way possible for inclusion as predictors in the model.&#xD;&#xA;&#xD;&#xA;As a concrete example, say a model is being built to predict if a customer will leave the company in the next 90 days (anytime between t and t+90; thus a binary outcome). One of the predictors available is the level of the customers financial balance for periods t_0 to t-1. Maybe this represents monthly observations for the prior 12 months (i.e. 12 measurements). &#xD;&#xA;&#xD;&#xA;I am looking for ways to construct features from this series. I use descriptives of each customers series such as the mean, high, low, std dev., fit a OLS regression to get the trend. Are their other methods of calculating features? Other measures of change or volatility? " />
  <row Id="1353" PostHistoryTypeId="1" PostId="548" RevisionGUID="5fd58f20-ce2d-4e2b-8275-cd58af9cbbdc" CreationDate="2014-06-23T23:20:36.180" UserId="1138" Text="Feature Extraction Technique - Summarizing a Sequence of Data" />
  <row Id="1354" PostHistoryTypeId="3" PostId="548" RevisionGUID="5fd58f20-ce2d-4e2b-8275-cd58af9cbbdc" CreationDate="2014-06-23T23:20:36.180" UserId="1138" Text="&lt;machine-learning&gt;&lt;feature-selection&gt;&lt;time-series&gt;" />
  <row Id="1355" PostHistoryTypeId="2" PostId="549" RevisionGUID="482ba8d2-292e-4ea0-9987-052d216e7bcb" CreationDate="2014-06-24T00:54:13.110" UserId="1011" Text="What you're trying to do here is reduce the dimensionality of your features. You can search for dimensionality reduction to get several options, but one very popular technique is principal components analysis (PCA). Principal components are not interpretable like the options you've mentioned, but they do a good job of summarizing all of the information." />
  <row Id="1356" PostHistoryTypeId="2" PostId="550" RevisionGUID="82b3e8df-f62f-46c5-ae13-9fc41dc59845" CreationDate="2014-06-24T03:50:41.410" UserId="418" Text="Feature extraction is always a challenge and the less addressed topic in literature, since it's widely application dependant.&#xD;&#xA;&#xD;&#xA;Some ideas you can try:&#xD;&#xA;&#xD;&#xA;- Raw data, measured day-by-day. That's kind of obvious with some implications and extra preprocessing (normalisation) in order to make timelines of different length comparable.&#xD;&#xA;- Higher moments: skewness, kurtosis, etc&#xD;&#xA;- Derivative(s): speed of evolution&#xD;&#xA;- Time span is not that large but maybe it is worth trying some time series analysis features like for example autocorrelation. &#xD;&#xA;- Some customised features like breaking timeline in weeks and measure the quantities you already measure in each week separately. Then a non-linear classifier would be able to combine e.g first-week features with last-week features in order to get insight of evolution in time.&#xD;&#xA; " />
  <row Id="1358" PostHistoryTypeId="5" PostId="416" RevisionGUID="f81ea09e-e737-4be1-b578-93dd85771299" CreationDate="2014-06-24T05:05:21.900" UserId="381" Comment="added 63 characters in body" Text="If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a [salted](http://en.wikipedia.org/wiki/Salt_%28cryptography%29) [hash of the string](http://www.cse.yorku.ca/~oz/hash.html) that is used to represent all related records (e.g., David instead of Dave).&#xD;&#xA;&#xD;&#xA;I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them.&#xD;&#xA;&#xD;&#xA;**edit**: You need to define and justify what kind of operations the third party needs to be able to do. For example, what is wrong with using initials followed by a number (e.g., BOA-1, BOA-2, etc.) to disambiguate Bank of America from Benjamin Othello Ames? If that's too revealing, you could bin some of the letters or names; e.g., [A-E] -&gt; 1, [F-J] -&gt; 2, etc. so BOA would become 1OA, or [&quot;Bank&quot;, &quot;Barry&quot;, &quot;Bruce&quot;, etc.] -&gt; 1 so Bank of America is again 1OA. &#xD;&#xA;&#xD;&#xA;For more information see [k-anonymity](http://en.wikipedia.org/wiki/K-anonymity)." />
  <row Id="1360" PostHistoryTypeId="2" PostId="552" RevisionGUID="c668f8f3-d684-4daf-a846-b803ab7fdba2" CreationDate="2014-06-24T07:03:35.567" UserId="941" Text="At first glance, you need to extract features from your time series (x - 12) - x. One possible approach is to compute summary metrics: average, dispersion, etc. But doing so, you will loose all time-series related information. But data, extracted from curve shape may be quite useful. I recommend you to look through [this][1] article, where authors propose algorithm for time series clustering. Hope, it will be useful. Additionally to such clustering you can add summary statistics to your feature list. &#xA;&#xA;&#xA;  [1]: http://cs.stanford.edu/people/jure/pubs/memeshapes-wsdm11.pdf" />
  <row Id="1364" PostHistoryTypeId="2" PostId="554" RevisionGUID="96fa7305-513d-4281-89f6-1ff641fc908a" CreationDate="2014-06-24T07:29:57.787" UserId="728" Text="I am a CS master student in data mining. My supervisor once told me that before I run any classifier or do anything with a dataset I must fully understand the data and make sure that the data is clean and correct.&#xD;&#xA;&#xD;&#xA;My questions:&#xD;&#xA;&#xD;&#xA;- What are the best practices to understand a dataset (high dimensional with numerical and nominal attributes)?&#xD;&#xA;&#xD;&#xA;- Practices to make sure the dataset is clean?&#xD;&#xA;&#xD;&#xA;- Practices to make sure the dataset doesn't have wrong values or so?" />
  <row Id="1365" PostHistoryTypeId="1" PostId="554" RevisionGUID="96fa7305-513d-4281-89f6-1ff641fc908a" CreationDate="2014-06-24T07:29:57.787" UserId="728" Text="Datasets understanding best practices" />
  <row Id="1366" PostHistoryTypeId="3" PostId="554" RevisionGUID="96fa7305-513d-4281-89f6-1ff641fc908a" CreationDate="2014-06-24T07:29:57.787" UserId="728" Text="&lt;statistics&gt;&lt;dataset&gt;" />
  <row Id="1367" PostHistoryTypeId="2" PostId="555" RevisionGUID="a4547fec-0b62-4727-b885-f2e5751192d3" CreationDate="2014-06-24T07:35:16.930" UserId="434" Text="**1.  Do not modify the original data**&#xD;&#xA;&#xD;&#xA;Having the original data source intact is important.  You may find that updates you make to the data are not valid.  You may also find a more efficient way to make updates and you will want to regression test those updates.&#xD;&#xA;&#xD;&#xA;Always work with a copy of the data, and add columns/properties/metadata that includes any processed corrections.&#xD;&#xA;&#xD;&#xA;Example, if your data is a .csv file that includes a city name that contains several misspellings:&#xD;&#xA;&#xD;&#xA;    1. Copy the .csv to a new file&#xD;&#xA;    2. Add a column for containing &quot;adjusted_city_name&quot;&#xD;&#xA;    3. Copy data from the city_name column to the &#xD;&#xA;       adjusted_city_name column and make corrections in that column.&#xD;&#xA;&#xD;&#xA;**2. Document proposed changes**&#xD;&#xA;&#xD;&#xA;Any changes you want to make to data should be documented so that they can be replicated moving forward.&#xD;&#xA;&#xD;&#xA;Version control and timestamp the document every time you change it.  That will help in troubleshooting at a later date.&#xD;&#xA;&#xD;&#xA;Be explicit.  Do not simply state &quot;correct capitalization problems&quot;, state &quot;ensure that the first letter of each city name begins with a capital letter and the remaining letters are lower-case.&quot;&#xD;&#xA;&#xD;&#xA;Update the document with references to any automation routines that have been built to manage data cleansing.&#xD;&#xA;&#xD;&#xA;**3. Decide on a standard data cleansing technology**&#xD;&#xA;&#xD;&#xA;Whether you use perl, python, java, a particular utility, a manual process or something else is not the issue.  The issue is that in the future you want to hand the data cleansing process to somebody else.  If they have to know 12 different data cleansing technologies, delegating the cleansing procedure will be very difficult.&#xD;&#xA;&#xD;&#xA;**4. Standardize the workflow**&#xD;&#xA;&#xD;&#xA;There should be a standard way to handle new data.  Ideally, it will be as simple as dropping a file in a specific location and a predictable automated process cleanses it and hands off a cleansed set of data to the next processing step.&#xD;&#xA;&#xD;&#xA;**5. Make as few changes as is absolutely necessary**&#xD;&#xA;&#xD;&#xA;It's always better to have a fault tolerant analysis than one that makes assumptions about the data.  &#xD;&#xA;&#xD;&#xA;**6. Avoid manual updates**&#xD;&#xA;&#xD;&#xA;It's always tempting, but people are error-prone and again it makes delegation difficult.&#xD;&#xA;&#xD;&#xA;**Notes on manual processing**&#xD;&#xA;&#xD;&#xA;To more completely address the original question as to whether there's a &quot;good&quot; way to do manual processing, I would say no, there is not.  My answer is based on experience and is not one that I make arbitrarily.&#xD;&#xA;&#xD;&#xA;I have had more than one project lose days of time due to a client insisting that a manual data cleansing process was just fine and could be handled internally.  You do not want your projects to be dependent on a single individual accomplishing a judgement based task of varying scale.&#xD;&#xA;&#xD;&#xA;It's much better to have that individual build and document a rule set based on what they would do than to have them manually cleanse data.  (And then automating that rule set)&#xD;&#xA;&#xD;&#xA;If automation fails you in the end or is simply not possible, the ability to delegate that rule set to others without domain specific knowledge is vital.&#xD;&#xA;&#xD;&#xA;In the end, routines to do something like correct city names can be applied to other data sets." />
  <row Id="1368" PostHistoryTypeId="2" PostId="556" RevisionGUID="17e7aa77-d4a4-4d42-8b63-87d97c1c99c5" CreationDate="2014-06-24T08:09:49.300" UserId="434" Text="There are basic things you can do with any set of data:&#xD;&#xA;&#xD;&#xA;1. Validate values (String length tolerance, data type, formatting masks, required field presence, etc.)&#xD;&#xA;2. Range correctness (Does this seemingly correct data fall within expected ranges of values)&#xD;&#xA;3. Preliminary processing (If I attempt to analyze this data, can I perform the basics without running into errors)&#xD;&#xA;4. Preliminary reporting (run a report against a data set and ensure that it passes a sanity test)&#xD;&#xA;5. Defining null vs. empty vs. zero vs. False for any given column of data&#xD;&#xA;6. Identifying data that is out of place (numeric values dramatically different than other values in a data set, string values that look like they might be misspelled, etc.)&#xD;&#xA;7. Eliminating or correcting obviously errant data&#xD;&#xA;&#xD;&#xA;Understanding data to identify errors is a whole different ball game, and it is very important.&#xD;&#xA;&#xD;&#xA;For instance, you can have a rule that says a serial number must be present in a given data set and that serial number must be alphanumeric with a maximum string length of 255 and a minimum string length of 5.&#xD;&#xA;&#xD;&#xA;Looking at the data, you may find one particular serial number value reads `&quot;PLEASE ENTER SERIAL&quot;`  It's perfectly valid, but wrong.&#xD;&#xA;&#xD;&#xA;That's kind of an obvious one, but say you're processing stock data and you had a price range for 1000 stocks that was under a dollar.  A lot of people would not know that a stock price so low is invalid on certain exchanges and perfectly valid on others.  You need knowledge about your data to understand if what you are seeing is problematic or not.  &#xD;&#xA;&#xD;&#xA;In the real world, you don't always have the luxury of understanding your data intimately.&#xD;&#xA;&#xD;&#xA;The way I avoid problems is by leveraging the people around me.  For small data sets, I can ask someone to review the data in it's entirety.  For large ones, pulling a set of random samples and asking someone to do a sanity check on the data is more appropriate.&#xD;&#xA;&#xD;&#xA;Further, questioning the source of the data and how well that data source can be trusted is imperative.  I often have multiple conflicting sources of data and we create rules to determine the &quot;source of truth&quot;.  Sometimes one data set has great data in a given aspect, but other data sets are stronger in other areas.&#xD;&#xA;&#xD;&#xA;Manually entered data is usually what I'm most skeptical about, but in some cases it is stronger than anything that can be acquired through automation." />
  <row Id="1373" PostHistoryTypeId="2" PostId="559" RevisionGUID="4aec521f-d54c-482c-9433-026800cb81be" CreationDate="2014-06-24T12:28:10.990" UserId="1147" Text="I have a hobby project which I am contemplating committing to as a way of increasing my so far limited experience of machine learning. I have taken and completed the Coursera MOOC on the topic. My question is with regards to the feasibility of the project.&#xD;&#xA;&#xD;&#xA;The task is the following:&#xD;&#xA;&#xD;&#xA;Neighboring cats are from time to time visiting my garden, which I dislike since they tend to defecate on my lawn. I would like to have a warning system that alerts me when there's a cat present so that I may go chase it off using my super soaker. For simplicity's sake, say that I only care about a cat with black and white coloring.&#xD;&#xA;&#xD;&#xA;I have setup a raspberry pi with camera module that can capture video and/or pictures of a part of the garden. &#xD;&#xA;&#xD;&#xA;Sample image:&#xD;&#xA;&#xD;&#xA;![Sample garden image][1]&#xD;&#xA;&#xD;&#xA;My first idea was to train a classifier to identify cat or cat-like objects, but after realizing that I will be unable to obtain a large enough number of positive samples, I have abandoned that in favor of anomaly detection.&#xD;&#xA;&#xD;&#xA;I estimate that if I captured a photo every second of the day, I would end up with maybe five photos containing cats (out of about 60,000 with sunlight) per day. &#xD;&#xA;&#xD;&#xA;Is this feasible using anomaly detection? If so, what features would you suggest? My ideas so far would be to simply count the number of pixels with that has certain colors; do some kind of blob detection/image segmenting (which I do not know how do to, and would thus like to avoid) and perform the same color analysis on them.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/cNqus.jpg" />
  <row Id="1374" PostHistoryTypeId="1" PostId="559" RevisionGUID="4aec521f-d54c-482c-9433-026800cb81be" CreationDate="2014-06-24T12:28:10.990" UserId="1147" Text="Detecting cats visually by means of anomaly detection" />
  <row Id="1375" PostHistoryTypeId="3" PostId="559" RevisionGUID="4aec521f-d54c-482c-9433-026800cb81be" CreationDate="2014-06-24T12:28:10.990" UserId="1147" Text="&lt;machine-learning&gt;" />
  <row Id="1376" PostHistoryTypeId="2" PostId="560" RevisionGUID="61ca7eb4-4629-4687-8e67-7b7dc4593d74" CreationDate="2014-06-24T12:55:26.457" UserId="1085" Text="This is an interesting and also quite ambitious project :)&#xD;&#xA;&#xD;&#xA;I am not sure anomaly detection (at least in the sense described in the course you followed) would be a very fitting algorithm in this case. &#xD;&#xA;&#xD;&#xA;I would consider a more viable approach to be what has been discussed at the end of that course where a Photo OCR workflow was demonstrated.&#xD;&#xA;&#xD;&#xA;The approach would consist of segmenting your image in smaller &quot;blocks&quot;, and going through them one-by-one using a supervised learning algorithm and try to classify each block according to whether it contains a cat or not. If one block contains a cat, the alarm goes off. As a bonus, you get the position of the cat as well, so that you may think of incorporating some &quot;automatic&quot; response as a future step to your project. &#xD;&#xA;&#xD;&#xA;The benefit here is that you will not have to train your algorithm using a dataset specific to your garden (which, as you mention is difficult to create), but you can use images of cats taken off the net (e.g. perhaps you can search for &quot;cat on grass&quot; or something), and perhaps patches of photos from your (or other) gardens. Therefore you don;t have to spend your time collecting photos from your camera, and you avoid the risk of having a very small (comparable) sample of positives (i.e. cats).&#xD;&#xA;&#xD;&#xA;Now, of course how easy it is to build an accurate cat detector is another topic.." />
  <row Id="1377" PostHistoryTypeId="2" PostId="561" RevisionGUID="8fc893c9-b02b-4272-9f54-0bc5c00ca29f" CreationDate="2014-06-24T13:34:12.107" UserId="964" Text="You could simplify your problem significantly by using a motion/change detection approach. For example, you could compare each image/frame with one from an early time (e.g., a minute earlier), then only consider pixels that have changed since the earlier time. You could then extract the rectangular region of change and use that as the basis for your classification or anomaly detection.&#xD;&#xA;&#xD;&#xA;Taking this type of approach can significantly simplify your classifier and reduce your false target rate because you can ignore anything that is not roughly the size of a cat (e.g., a person or bird). You would then use the extracted change regions that were not filtered out to form the training set for your classifier (or anomaly detector).&#xD;&#xA;&#xD;&#xA;Just be sure to get your false target rate sufficiently low before mounting a laser turret to your feline intrusion detection system." />
  <row Id="1378" PostHistoryTypeId="5" PostId="548" RevisionGUID="e3429e93-8490-4083-aaa4-e729af9f0709" CreationDate="2014-06-24T14:03:22.697" UserId="1138" Comment="added 454 characters in body" Text="I often am building a model (classification or regression) where I have some predictor variables that are sequences and I have been trying to find technique recommendations for summarizing them in the best way possible for inclusion as predictors in the model.&#xD;&#xA;&#xD;&#xA;As a concrete example, say a model is being built to predict if a customer will leave the company in the next 90 days (anytime between t and t+90; thus a binary outcome). One of the predictors available is the level of the customers financial balance for periods t_0 to t-1. Maybe this represents monthly observations for the prior 12 months (i.e. 12 measurements). &#xD;&#xA;&#xD;&#xA;I am looking for ways to construct features from this series. I use descriptives of each customers series such as the mean, high, low, std dev., fit a OLS regression to get the trend. Are their other methods of calculating features? Other measures of change or volatility? &#xD;&#xA;&#xD;&#xA;ADD:&#xD;&#xA;&#xD;&#xA;As mentioned in a response below, I also considered (but forgot to add here) using Dynamic Time Warping (DTW) and then hierarchical clustering on the resulting distance matrix - creating some number of clusters and then using the cluster membership as a feature. Scoring test data would likely have to follow a process where the DTW was done on new cases and the cluster centroids - matching the new data series to their closest centroids... " />
  <row Id="1379" PostHistoryTypeId="2" PostId="562" RevisionGUID="683ec7fe-4fb3-4d0b-a968-3370ae6a1875" CreationDate="2014-06-24T15:44:52.540" UserId="1155" Text="As a rule of thumb I always propose three different options:&#xD;&#xA;&#xD;&#xA; - Use a bagging learning technique, similar to that one followed by Random Forest. This technique allows the training of 'small' classifiers which see a small portion of the whole data. Afterwards, a simple voting scheme (as in Random Forest) will lead you to a very interesting and robust classification.&#xD;&#xA; - Use any technique related to fusioning information or probabilistic fusion. This is a very suitable solution in order to combine different likelihoods from different classifiers.&#xD;&#xA; - My last suggestion is the use of fuzzy logic, a very adequate tool in order to combine information properly from a probabilistic (belonging) perspective.&#xD;&#xA;&#xD;&#xA;The selection of specific methods or strategies will depend enormously on the data." />
  <row Id="1380" PostHistoryTypeId="2" PostId="563" RevisionGUID="34c5f9ec-1abe-473d-8599-47b7b6d0290b" CreationDate="2014-06-24T15:55:37.110" UserId="1155" Text="The strategy of motion/change detection is certainly adequate, but I would add an extra operation. I would detect those regions that are more likely to be changed, for instance, the ladder seems a place where humans can be (also cats) and grass where dogs, cats or humans can be.&#xD;&#xA;&#xD;&#xA;I would capture a map with size of the object and trajectory and with this I would create a cluster with the aim of detecting an object (with specific size within the image in terms of pixels) that moves with a certain speed and trajectory.&#xD;&#xA;&#xD;&#xA;You can achieve this by using R or I would suggest OpenCV in order to detect movement and follow different objects." />
  <row Id="1381" PostHistoryTypeId="2" PostId="564" RevisionGUID="7571f267-5c5c-4975-b961-84378482a2dc" CreationDate="2014-06-24T18:59:30.560" UserId="237" Text="I've written a simple recommender which generates recommendations for users based on what they  have clicked. The recommender generates a data file of this format : &#xD;&#xA;&#xD;&#xA;    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)&#xD;&#xA;    a,b,.2&#xD;&#xA;    a,c,.3&#xD;&#xA;    a,d,.4&#xD;&#xA;    a,e,.1&#xD;&#xA;    e,b,.3&#xD;&#xA;    e,c,.5&#xD;&#xA;    e,d,.8&#xD;&#xA;&#xD;&#xA;I've looked at these graphs : &#xD;&#xA;https://github.com/mbostock/d3/wiki/Gallery&#xD;&#xA;&#xD;&#xA;But im not sure which to use or are there other's that will better visualize user similarities based on this dataset?&#xD;&#xA;" />
  <row Id="1382" PostHistoryTypeId="1" PostId="564" RevisionGUID="7571f267-5c5c-4975-b961-84378482a2dc" CreationDate="2014-06-24T18:59:30.560" UserId="237" Text="What visualization technique to best describe this dataset ?" />
  <row Id="1383" PostHistoryTypeId="3" PostId="564" RevisionGUID="7571f267-5c5c-4975-b961-84378482a2dc" CreationDate="2014-06-24T18:59:30.560" UserId="237" Text="&lt;visualization&gt;&lt;javascript&gt;" />
  <row Id="1384" PostHistoryTypeId="2" PostId="565" RevisionGUID="d1467c00-43c4-457f-b105-1c0a4dc4e27c" CreationDate="2014-06-24T19:43:24.643" UserId="1162" Text="As I increase the number of trees in scikit learn's GradientBoostingRegressor I get more negative predictions even though there are no negative values in my training or testing set. " />
  <row Id="1385" PostHistoryTypeId="1" PostId="565" RevisionGUID="d1467c00-43c4-457f-b105-1c0a4dc4e27c" CreationDate="2014-06-24T19:43:24.643" UserId="1162" Text="Gradient Boosting Regression- negative predictions despite no negative y values in training set" />
  <row Id="1386" PostHistoryTypeId="3" PostId="565" RevisionGUID="d1467c00-43c4-457f-b105-1c0a4dc4e27c" CreationDate="2014-06-24T19:43:24.643" UserId="1162" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;algorithms&gt;" />
  <row Id="1387" PostHistoryTypeId="36" PostId="566" RevisionGUID="e06b4913-3667-4f45-b2e4-dbd6d42eb34e" CreationDate="2014-06-24T19:59:24.757" UserId="-1" Comment="from http://stats.stackexchange.com/questions/104589/named-entity-recognition-nltk-using-regular-expression" />
  <row Id="1388" PostHistoryTypeId="2" PostId="566" RevisionGUID="a61cfd73-c314-4450-b6e5-0bdedf475892" CreationDate="2014-06-24T17:06:10.310" UserId="1165" UserDisplayName="mousecoder" Text="I have been playing with NLTK toolkit. I come across this problem a lot and searched for solution online but nowhere I got a satisfying answer. So I am putting my query here. &#xD;&#xA;&#xD;&#xA;Many times NER doesn't tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.&#xD;&#xA;&#xD;&#xA;Example:&#xD;&#xA;&#xD;&#xA;Input: &quot;Barack Obama is a great person.&quot;&#xD;&#xA;&#xD;&#xA;Output:  Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('.', '.')])&#xD;&#xA;&#xD;&#xA;where as &#xD;&#xA;&#xD;&#xA;input: 'Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he &quot;was honored&quot; to be compared to Darth Vader while in office.'&#xD;&#xA;&#xD;&#xA;Output: Tree('S', [('Former', 'JJ'), ('Vice', 'NNP'), ('President', 'NNP'), Tree('NE', [('Dick', 'NNP'), ('Cheney', 'NNP')]), ('told', 'VBD'), ('conservative', 'JJ'), ('radio', 'NN'), ('host', 'NN'), Tree('NE', [('Laura', 'NNP'), ('Ingraham', 'NNP')]), ('that', 'IN'), ('he', 'PRP'), ('``', '``'), ('was', 'VBD'), ('honored', 'VBN'), (&quot;''&quot;, &quot;''&quot;), ('to', 'TO'), ('be', 'VB'), ('compared', 'VBN'), ('to', 'TO'), Tree('NE', [('Darth', 'NNP'), ('Vader', 'NNP')]), ('while', 'IN'), ('in', 'IN'), ('office', 'NN'), ('.', '.')])&#xD;&#xA;&#xD;&#xA;Here Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP) , is correctly extracted. &#xD;&#xA;&#xD;&#xA;So I think if nltk.ne_chunk is used first and then if two consecutive trees are NNP there are high chances that both refers to one entity. &#xD;&#xA;&#xD;&#xA;Any suggestion will be really appreciated. I am looking for flaws in my approach.&#xD;&#xA;&#xD;&#xA;Thanks." />
  <row Id="1389" PostHistoryTypeId="1" PostId="566" RevisionGUID="a61cfd73-c314-4450-b6e5-0bdedf475892" CreationDate="2014-06-24T17:06:10.310" UserId="1165" UserDisplayName="mousecoder" Text="Named Entity Recognition: NLTK using Regular Expression" />
  <row Id="1390" PostHistoryTypeId="3" PostId="566" RevisionGUID="a61cfd73-c314-4450-b6e5-0bdedf475892" CreationDate="2014-06-24T17:06:10.310" UserId="1165" UserDisplayName="mousecoder" Text="&lt;nlp&gt;" />
  <row Id="1391" PostHistoryTypeId="5" PostId="564" RevisionGUID="d130bfb8-0b1e-44e6-a344-7f87849d2933" CreationDate="2014-06-24T20:25:06.887" UserId="84" Comment="Improving formatting." Text="I've written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:&#xD;&#xA;&#xD;&#xA;    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)&#xD;&#xA;    a,b,.2&#xD;&#xA;    a,c,.3&#xD;&#xA;    a,d,.4&#xD;&#xA;    a,e,.1&#xD;&#xA;    e,b,.3&#xD;&#xA;    e,c,.5&#xD;&#xA;    e,d,.8&#xD;&#xA;&#xD;&#xA;I've looked at [some graphs](https://github.com/mbostock/d3/wiki/Gallery), but I'm not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?&#xD;&#xA;" />
  <row Id="1392" PostHistoryTypeId="4" PostId="564" RevisionGUID="d130bfb8-0b1e-44e6-a344-7f87849d2933" CreationDate="2014-06-24T20:25:06.887" UserId="84" Comment="Improving formatting." Text="What visualization technique to best describe a recommendation dataset?" />
  <row Id="1393" PostHistoryTypeId="2" PostId="567" RevisionGUID="2064a06f-3962-4f6c-9b57-05bc07de03f9" CreationDate="2014-06-24T20:28:33.693" UserId="1163" Text="I think you're looking for a [similarity matrix][1] (see bottom of the page). If you don't have data on similarity between certain pairs, you can always leave them as grey or white. Also, this will only work for data sets small enough to actually make out what's going on. I'd say 25 rows / columns maximum.&#xD;&#xA;&#xD;&#xA;In a similarity matrix, x, and y coordinates correspond to the two things you're comparing, while a [colormap][2] magnitude represents similarity &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cgi.mtc.sri.com/Cluster-Lab/&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/False_color" />
  <row Id="1394" PostHistoryTypeId="5" PostId="564" RevisionGUID="6c495d45-4592-4119-880c-a82cf7f4b722" CreationDate="2014-06-24T20:33:20.620" UserId="237" Comment="added 191 characters in body" Text="I've written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:&#xD;&#xA;&#xD;&#xA;    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)&#xD;&#xA;    a,b,.2&#xD;&#xA;    a,c,.3&#xD;&#xA;    a,d,.4&#xD;&#xA;    a,e,.1&#xD;&#xA;    e,b,.3&#xD;&#xA;    e,c,.5&#xD;&#xA;    e,d,.8&#xD;&#xA;&#xD;&#xA;I've looked at [some graphs](https://github.com/mbostock/d3/wiki/Gallery), but I'm not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?&#xD;&#xA;&#xD;&#xA;I'm aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users and so convince these users that the recommendation system is useful." />
  <row Id="1395" PostHistoryTypeId="2" PostId="568" RevisionGUID="4ad2500c-d4d9-4568-aa0f-643404e315fd" CreationDate="2014-06-24T20:34:18.473" UserId="1155" Text="I suggest the use of Hidden Markov Models, with two possible states: (1) high levels and (0) low levels. &#xD;&#xA;&#xD;&#xA;This technique might be helpful to decode your signal. Probably you would need a specific HMM for each codification.&#xD;&#xA;&#xD;&#xA;Concerning noise, you may use a Butterworth high pass filter to smooth your signal." />
  <row Id="1396" PostHistoryTypeId="2" PostId="569" RevisionGUID="abe520e8-189d-402a-b733-2335be433637" CreationDate="2014-06-24T20:35:44.713" UserId="1164" Text="Is it a bird? Is it a cat? We have black-and-white magpies here. so that would fail.&#xD;&#xA;&#xD;&#xA;First thing would be to exclude all areas that are green, cats are seldom green.&#xD;&#xA;&#xD;&#xA;Then compare the rest to a reference image to remove static things like stones and stairs.&#xD;&#xA;&#xD;&#xA;Detecting objects of a minimum size should be possible, but for a classification the resolution is too low. Could be also your neighbor testing his new remote controlled drone.&#xD;&#xA;&#xD;&#xA;With two cameras you could do a 3d mapping of the objects and eliminate flying objects." />
  <row Id="1397" PostHistoryTypeId="5" PostId="567" RevisionGUID="53215b3a-caf0-4917-8a79-c9274630e26f" CreationDate="2014-06-24T20:37:02.377" UserId="1163" Comment="added 318 characters in body" Text="I think you're looking for a [similarity matrix][1] (see bottom of the page). If you don't have data on similarity between certain pairs, you can always leave them as grey or white. Also, this will only work for data sets small enough to actually make out what's going on. I'd say 25 rows / columns maximum.&#xD;&#xA;&#xD;&#xA;In a similarity matrix, x, and y coordinates correspond to the two things you're comparing, while a [colormap][2] magnitude represents similarity &#xD;&#xA;&#xD;&#xA;**EDIT**:&#xD;&#xA;One thing you could do to replace the colormap is the insert, say, circles of different sizes according to the similarity metric. Or you could insert the numbers themselves, again, varying the size of the number as the magnitude of that number varies. Size usually works best is business visualizations.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cgi.mtc.sri.com/Cluster-Lab/&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/False_color" />
  <row Id="1398" PostHistoryTypeId="5" PostId="566" RevisionGUID="938889b5-5cd5-429a-8aec-8d08adc33696" CreationDate="2014-06-24T20:44:25.993" UserId="84" Comment="Improving formatting." Text="Many times [Named Entity Recognition](http://en.wikipedia.org/wiki/Named-entity_recognition) (NER) doesn't tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.&#xD;&#xA;&#xD;&#xA;For example, consider the following input:&#xD;&#xA;&#xD;&#xA;&gt; &quot;Barack Obama is a great person.&quot;&#xD;&#xA;&#xD;&#xA;And the output:&#xD;&#xA;&#xD;&#xA;    Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('ORGANIZATION', [('Obama', 'NNP')]),&#xD;&#xA;        ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('person', 'NN'), ('.', '.')])&#xD;&#xA;&#xD;&#xA;where as for the input: &#xD;&#xA;&#xD;&#xA;&gt; 'Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he &quot;was honored&quot; to be compared to Darth Vader while in office.'&#xD;&#xA;&#xD;&#xA;the output is:&#xD;&#xA;&#xD;&#xA;    Tree('S', [('Former', 'JJ'), ('Vice', 'NNP'), ('President', 'NNP'),&#xD;&#xA;        Tree('NE', [('Dick', 'NNP'), ('Cheney', 'NNP')]), ('told', 'VBD'), ('conservative', 'JJ'),&#xD;&#xA;        ('radio', 'NN'), ('host', 'NN'), Tree('NE', [('Laura', 'NNP'), ('Ingraham', 'NNP')]),&#xD;&#xA;        ('that', 'IN'), ('he', 'PRP'), ('``', '``'), ('was', 'VBD'), ('honored', 'VBN'),&#xD;&#xA;        (&quot;''&quot;, &quot;''&quot;), ('to', 'TO'), ('be', 'VB'), ('compared', 'VBN'), ('to', 'TO'),&#xD;&#xA;        Tree('NE', [('Darth', 'NNP'), ('Vader', 'NNP')]), ('while', 'IN'), ('in', 'IN'),&#xD;&#xA;        ('office', 'NN'), ('.', '.')])&#xD;&#xA;&#xD;&#xA;Here `Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP)` is correctly extracted. So, I think if `nltk.ne_chunk` is used first, and then if two consecutive trees are NNP, there are higher chances that both refer to one entity. &#xD;&#xA;&#xD;&#xA;I have been playing with NLTK toolkit, and I came across this problem a lot, but couldn't find a satisfying answer. Any suggestion will be really appreciated. I'm looking for flaws in my approach." />
  <row Id="1399" PostHistoryTypeId="5" PostId="569" RevisionGUID="aca3e32f-2026-486f-b283-58b6d1d329b0" CreationDate="2014-06-24T20:48:37.837" UserId="1164" Comment="added 11 characters in body" Text="Is it a bird? Is it a cat? We have black-and-white cat-sized! magpies here. so that would fail.&#xD;&#xA;&#xD;&#xA;First thing would be to exclude all areas that are green, cats are seldom green.&#xD;&#xA;&#xD;&#xA;Then compare the rest to a reference image to remove static things like stones and stairs.&#xD;&#xA;&#xD;&#xA;Detecting objects of a minimum size should be possible, but for a classification the resolution is too low. Could be also your neighbor testing his new remote controlled drone.&#xD;&#xA;&#xD;&#xA;With two cameras you could do a 3d mapping of the objects and eliminate flying objects." />
  <row Id="1400" PostHistoryTypeId="5" PostId="564" RevisionGUID="7a5b0990-980a-45db-a764-55a0238e1a89" CreationDate="2014-06-24T21:25:13.257" UserId="237" Comment="added 17 characters in body" Text="I've written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:&#xD;&#xA;&#xD;&#xA;    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)&#xD;&#xA;    a,b,.2&#xD;&#xA;    a,c,.3&#xD;&#xA;    a,d,.4&#xD;&#xA;    a,e,.1&#xD;&#xA;    e,b,.3&#xD;&#xA;    e,c,.5&#xD;&#xA;    e,d,.8&#xD;&#xA;&#xD;&#xA;I've looked at [some graphs](https://github.com/mbostock/d3/wiki/Gallery), but I'm not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?&#xD;&#xA;&#xD;&#xA;I'm aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users are and so convince the business these users that the recommendation system is useful." />
  <row Id="1401" PostHistoryTypeId="5" PostId="565" RevisionGUID="2a32c7bd-a175-4251-b7e7-0c2a5a0cc89d" CreationDate="2014-06-25T00:09:07.697" UserId="1162" Comment="added 451 characters in body" Text="As I increase the number of trees in scikit learn's GradientBoostingRegressor I get more negative predictions even though there are no negative values in my training or testing set. I have about 10 features most of them are binary. So some of the parameters that I was tuning were the number of trees/iterations, learning depth, and learning rate. The percentage of negative values seemed to max at ~ 2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions." />
  <row Id="1402" PostHistoryTypeId="2" PostId="570" RevisionGUID="dbfde5f2-5dbe-4b39-b7c4-73d02c7df1ac" CreationDate="2014-06-25T00:13:50.803" UserId="434" Text="Personally, I think Netflix got it right.  Break it down into a confidence rating from 1-5 and show your recommendations based on the number of yellow stars.  &#xD;&#xA;&#xD;&#xA;It doesn't have to be stars, but those icon based graphs are very easy to interpret and get the point across clearly.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1403" PostHistoryTypeId="2" PostId="571" RevisionGUID="be8d13b8-b338-4ca0-aeb2-08f10902eaa9" CreationDate="2014-06-25T02:33:22.673" UserId="1174" Text="I'm trying to rank some percentages. I have numerators and denominators for each ratio.&#xD;&#xA;To give a concrete example:&#xD;&#xA;ratio is total graduates/total students in the school.&#xD;&#xA;But the issue is total number of students vary over a long range. (1000-20000)&#xD;&#xA;Smaller schools seem to have higher percentage of students graduating but I want to standardize it and not let the size of the school affect the ranking.&#xD;&#xA;Is there a way to do it?" />
  <row Id="1404" PostHistoryTypeId="1" PostId="571" RevisionGUID="be8d13b8-b338-4ca0-aeb2-08f10902eaa9" CreationDate="2014-06-25T02:33:22.673" UserId="1174" Text="Standardize numbers for ranking ratios" />
  <row Id="1405" PostHistoryTypeId="3" PostId="571" RevisionGUID="be8d13b8-b338-4ca0-aeb2-08f10902eaa9" CreationDate="2014-06-25T02:33:22.673" UserId="1174" Text="&lt;statistics&gt;" />
  <row Id="1406" PostHistoryTypeId="5" PostId="565" RevisionGUID="94b160f8-2406-4fc8-823c-4c7e79a8dcdf" CreationDate="2014-06-25T02:51:25.207" UserId="84" Comment="Improving formatting." Text="As I increase the number of trees in [scikit learn](http://scikit-learn.org/stable/)'s [`GradientBoostingRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features most of them are binary.&#xD;&#xA;&#xD;&#xA;So, some of the parameters that I was tuning were:&#xD;&#xA;&#xD;&#xA;- the number of trees/iterations;&#xD;&#xA;- learning depth;&#xD;&#xA;- and learning rate.&#xD;&#xA;&#xD;&#xA;The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions." />
  <row Id="1407" PostHistoryTypeId="4" PostId="565" RevisionGUID="94b160f8-2406-4fc8-823c-4c7e79a8dcdf" CreationDate="2014-06-25T02:51:25.207" UserId="84" Comment="Improving formatting." Text="Gradient Boosting Regression: negative predictions despite no negative y values in training set" />
  <row Id="1408" PostHistoryTypeId="6" PostId="565" RevisionGUID="94b160f8-2406-4fc8-823c-4c7e79a8dcdf" CreationDate="2014-06-25T02:51:25.207" UserId="84" Comment="Improving formatting." Text="&lt;machine-learning&gt;&lt;python&gt;&lt;algorithms&gt;&lt;scikit&gt;" />
  <row Id="1409" PostHistoryTypeId="2" PostId="572" RevisionGUID="c68d64c4-0214-4209-957e-e812b2e0e73f" CreationDate="2014-06-25T03:11:02.457" UserId="609" Text="This is relatively simple to do mathematically.  First, fit a regression line to the scatter plot of &quot;total graduates&quot; (y) vs. &quot;total students&quot; (x).  You will probably see a downward sloping line if your assertion is correct (smaller schools graduate a higher %).&#xD;&#xA;&#xD;&#xA;You can identify the slope and y-intercept for this line to convert it into an equation y = mx + b, and then do a little algebra to convert the equation  into normalized form: &quot;y / x = m + b / x&quot;&#xD;&#xA;&#xD;&#xA;Then, with all the ratios in your data , you should *subtract* this RHS:  &#xD;&#xA;&#xD;&#xA;normalized ratio = (total grads / total students) - (m + b / total students)&#xD;&#xA;&#xD;&#xA;If the result is postive, then the ratio is above normal for that size (i.e. above the regression line) and if it is negative it is below the regression line.  If you want all positive numbers, you can add a positive constant to move all results above zero.&#xD;&#xA;&#xD;&#xA;----&#xD;&#xA;This is how to do it mathematically, but I suggest that you consider whether it is wise, from a data analysis point of view, to normalize by school size.  This depends on the purpose of your analysis and specifically how this ratio is being analyzed in relation to other data." />
  <row Id="1411" PostHistoryTypeId="2" PostId="573" RevisionGUID="553f3bbc-09b6-4740-aca0-5e9c1d330eaa" CreationDate="2014-06-25T03:30:19.787" UserId="1176" Text="OpenCV's [background subtraction][1] will find objects moving about your harden. After that you could use a classifier or shape analysis to differentiate between cats, people, trees and etc.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://docs.opencv.org/trunk/doc/tutorials/video/background_subtraction/background_subtraction.html &quot;background subtraction&quot;" />
  <row Id="1412" PostHistoryTypeId="5" PostId="571" RevisionGUID="66676955-1260-4c3c-be0e-f28bd51d6c83" CreationDate="2014-06-25T03:59:46.827" UserId="84" Comment="Improving formatting." Text="I'm trying to rank some percentages. I have numerators and denominators for each ratio. To give a concrete example, consider ratio as `total graduates / total students` in a school.&#xD;&#xA;&#xD;&#xA;But the issue is that `total students` vary over a long range (1000-20000). Smaller schools seem to have higher percentage of students graduating, but I want to standardize it, and not let the size of the school affect the ranking. Is there a way to do it?" />
  <row Id="1413" PostHistoryTypeId="5" PostId="568" RevisionGUID="853361ff-a7e0-4374-b9cd-7d0a735c1494" CreationDate="2014-06-25T06:58:43.670" UserId="890" Comment="Wouldn't want to high pass, high frequency noise could also be a concern, and a Butterworth is non-ideal. " Text="I suggest the use of Hidden Markov Models, with two possible states: (1) high levels and (0) low levels. &#xD;&#xA;&#xD;&#xA;This technique might be helpful to decode your signal. Probably you would need a specific HMM for each codification.&#xD;&#xA;&#xD;&#xA;If noise is an issue an FIR filter with a Blackman-Harris window function would allow you to isolate the frequency you're concerned with. " />
  <row Id="1414" PostHistoryTypeId="24" PostId="568" RevisionGUID="853361ff-a7e0-4374-b9cd-7d0a735c1494" CreationDate="2014-06-25T06:58:43.670" Comment="Proposed by 890 approved by 1155 edit id of 91" />
  <row Id="1415" PostHistoryTypeId="2" PostId="574" RevisionGUID="56c14e02-9a2d-4493-a7ec-6fc29ea5da25" CreationDate="2014-06-25T07:22:33.170" UserId="1155" Text="This is a very interesting problem.&#xD;&#xA;&#xD;&#xA;I faced a similar one by analyzing the pictures users upload to the social network. I did the following approach:&#xD;&#xA;&#xD;&#xA; - Rather than associating data to ages (15 y.o., 27 y.o., ...) what I did is to establish different groups of ages: Less than 18, from 18 to 30 and greater than 30 (this is due to the specific problem we were facing, but you can choose whatever intervals you want). This division helps a lot to solve the problem.&#xD;&#xA; - Afterwards, I created a hierarchical clustering (divisive or aggregative). Then I choose those branches where I had users with known ages (or group ages) and then for that branch I extended the same age to that group.&#xD;&#xA;&#xD;&#xA;This approach is **semi-supervised learning** and I recommended it in case you only have some data labeled.&#xD;&#xA;&#xD;&#xA;Please, notice that on a social network, people usually lie about the age (just for fun, or sometimes because they want to camuflate themselves on the social net)." />
  <row Id="1416" PostHistoryTypeId="5" PostId="564" RevisionGUID="f6db9830-e60d-4462-96a6-3f0fd83e46ed" CreationDate="2014-06-25T07:56:29.733" UserId="237" Comment="added 4 characters in body" Text="I've written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:&#xD;&#xA;&#xD;&#xA;    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)&#xD;&#xA;    a,b,.2&#xD;&#xA;    a,c,.3&#xD;&#xA;    a,d,.4&#xD;&#xA;    a,e,.1&#xD;&#xA;    e,b,.3&#xD;&#xA;    e,c,.5&#xD;&#xA;    e,d,.8&#xD;&#xA;&#xD;&#xA;I've looked at [some graphs](https://github.com/mbostock/d3/wiki/Gallery), but I'm not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?&#xD;&#xA;&#xD;&#xA;I'm aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users are and so convince the business that for these users the recommendation system is useful.&#xD;&#xA;&#xD;&#xA;@Steve Kallestad do you mean something like this : &#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/4zyQR.png" />
  <row Id="1417" PostHistoryTypeId="2" PostId="575" RevisionGUID="2b4f6344-353a-4266-a88d-5eab7749857a" CreationDate="2014-06-25T09:19:25.560" UserId="1155" Text="This is a very good question and a common situation.&#xD;&#xA;&#xD;&#xA;In my opinion there are three different factors that must be controlled:&#xD;&#xA;&#xD;&#xA; - Data: There exist already different benchmarks in order to evaluate algorithms and architectures. These data must be publicly available so that everybody can contrast their approaches.&#xD;&#xA; - Architecture: My suggestion is to test everything on the cloud, so that everyone can contrast their results and also there is no doubt the same machines and software is used.&#xD;&#xA; - Algorithms: If you have developed a distributed algorithm, it would be great to compare your algorithm on a specific data. In this case, algorithms must not be public.&#xD;&#xA;&#xD;&#xA;So, answering your question, if you want to compare different experiments and state to what extent your distributed algorithm outperforms others, you should try to replicate as accurate as possible the same environment (data and architecture) where the experiments were carried out.&#xD;&#xA;&#xD;&#xA;If this is not possible, my suggestion is that you test your algorithm with public data and cloud architecture so that **you become a referent** as you are facilitating the comparison of future algorithms." />
  <row Id="1418" PostHistoryTypeId="2" PostId="576" RevisionGUID="d22374a9-f0bf-4c32-9a2d-ee1c7cf2217a" CreationDate="2014-06-25T10:15:31.477" UserId="1155" Text="You should try _arules_ package in R. It allows you to create not only the association rules but also to specify the length of each rule, the importance of each rule and also you can filter them, which is what you are looking for (try the rhs() command of this package).&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1419" PostHistoryTypeId="2" PostId="577" RevisionGUID="674adb8e-db34-4f30-8653-843a3fcbe0d5" CreationDate="2014-06-25T10:59:56.737" UserId="743" Text="Though it's easy to say, it's better to treat the environment that changes as variables, describe/estimate your algorithm's performance base on these variables. And hopefully others will do the same. Of interest, [Experiments as Research Validation -- Have We Gone too Far?][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://infolab.stanford.edu/~ullman/pub/experiments.pdf" />
  <row Id="1426" PostHistoryTypeId="2" PostId="580" RevisionGUID="cc1a9275-37e2-4e5c-be4a-27df51c4ac3d" CreationDate="2014-06-25T14:30:10.597" UserId="1155" Text="I have read some about it and I had the following blog in mind:&#xD;&#xA;&#xD;&#xA;http://fellgernon.tumblr.com/post/46117939292/predicting-who-will-win-a-nfl-match-at-half-time#.UtehM7TWtQg&#xD;&#xA;&#xD;&#xA;This blog deals with the prediction of a NFL match after the half time is already over. The prediction is 80% accurate with simple GLM model.&#xD;&#xA;&#xD;&#xA;I do not know if that is suitable for soccer." />
  <row Id="1427" PostHistoryTypeId="2" PostId="581" RevisionGUID="1307f496-c73f-4a19-a19a-e0745ccbcb1d" CreationDate="2014-06-25T14:36:29.163" UserId="1155" Text="In my opinion, there are solutions to deal with categorical data in clustering. R comes with a specific distance for categorical data. This distance is called Gower (http://www.inside-r.org/packages/cran/StatMatch/docs/gower.dist) and it works pretty well.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1428" PostHistoryTypeId="2" PostId="582" RevisionGUID="861798d6-d2ea-4baf-9ebd-3d42a87683da" CreationDate="2014-06-25T14:43:53.680" UserId="743" Text="Some research from [D. Nguyen et al.][1] try to predict twitter user's age based on their tweets. Maybe you find them useful. They use logistic and linear regression.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.dongnguyen.nl/publications.html" />
  <row Id="1429" PostHistoryTypeId="2" PostId="583" RevisionGUID="4b0106ec-e1c5-4a30-96a1-99ea52f35c5c" CreationDate="2014-06-25T15:42:19.287" UserId="924" Text="The original MacQueen k-means publication (the first to use the name &quot;kmeans&quot;) is an online algorithm.&#xD;&#xA;&#xD;&#xA;&gt; MacQueen, J. B. (1967). &quot;Some Methods for classification and Analysis of Multivariate Observations&quot;. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press. pp. 281–297&#xD;&#xA;&#xD;&#xA;After assigning each point, the mean is incrementally updated.&#xD;&#xA;&#xD;&#xA;As far as I can tell, it was also meant to be a single pass over the data only, although it can be trivially repeated multiple times to reassign points until convergence.&#xD;&#xA;&#xD;&#xA;MacQueen usually takes fewer iterations than Lloyds to converge if your data is shuffled. On ordered data, it can have problems. On the downside, it requires more computation for each object, so each iteration takes slightly longer.&#xD;&#xA;&#xD;&#xA;When you implement a parallel version of k-means, make sure to study the update formulas in MacQueens publication. They're useful." />
  <row Id="1430" PostHistoryTypeId="2" PostId="584" RevisionGUID="08da9b63-4cba-4ec1-97d2-ceb78f5384b9" CreationDate="2014-06-25T15:46:21.267" UserId="924" Text="The original MacQueen k-means publication (the first to use the name &quot;kmeans&quot;) is an online algorithm.&#xD;&#xA;&#xD;&#xA;&gt; MacQueen, J. B. (1967). &quot;Some Methods for classification and Analysis of Multivariate Observations&quot;. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press. pp. 281–297&#xD;&#xA;&#xD;&#xA;After assigning each point, the mean is incrementally updated using a simple weighted-average formula (old mean is weighted with n, the new observation is weighted with 1, if the mean had n observations before).&#xD;&#xA;&#xD;&#xA;As far as I can tell, it was also meant to be a single pass over the data only, although it can be trivially repeated multiple times to reassign points until convergence.&#xD;&#xA;&#xD;&#xA;MacQueen usually takes fewer iterations than Lloyds to converge if your data is shuffled (because it updates the mean faster!). On ordered data, it can have problems. On the downside, it requires more computation for each object, so each iteration takes slightly longer (additional math operations, obviously)." />
  <row Id="1431" PostHistoryTypeId="2" PostId="585" RevisionGUID="3fcab999-77da-45a5-9c8e-d9aa20b6a78a" CreationDate="2014-06-25T15:53:30.723" UserId="924" Text="DBSCAN (see also: Generalized DBSCAN) does not require a distance.&#xD;&#xA;All it needs is a *binary decision*. Commonly, one would use &quot;distance &lt; epsilon&quot; but nothing says you cannot use &quot;similarity &gt; epsilon&quot; instead. Triangle inequality etc. are not required.&#xD;&#xA;&#xD;&#xA;Affinity propagation, as the name says, uses similarities.&#xD;&#xA;&#xD;&#xA;Hierarchical clustering, except for maybe Ward linkage, does not make any assumption. In many implementations you can just use negative distances when you have similarities, and it will work just fine. Because all that is needed is min, max, and &lt;.&#xD;&#xA;&#xD;&#xA;Kernel k-means could work IF your similarity is a good kernel function. Think of it as computing k-means in a different vector space, where Euclidean distance corresponds to your similarity function. But then you need to know k.&#xD;&#xA;&#xD;&#xA;PAM (K-medoids) should work. Assign each object to the most similary medoid, then choose the object with the highest average similarity as new medoid... no triangle inequality needed.&#xD;&#xA;&#xD;&#xA;... and probably many many more. There are literally hundreds of clustering algorithms. **Most should work** IMHO. Very few seem to actually require metric properties. K-means has probably the strongest requirements: it minimizes *variance* (not distance, or similarity), and you must be able to compute means." />
  <row Id="1432" PostHistoryTypeId="5" PostId="565" RevisionGUID="d95d49fc-473c-4629-822a-2109cf89fb34" CreationDate="2014-06-25T15:56:42.597" UserId="1162" Comment="added 299 characters in body" Text="As I increase the number of trees in [scikit learn](http://scikit-learn.org/stable/)'s [`GradientBoostingRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features most of them are binary.&#xD;&#xA;&#xD;&#xA;So, some of the parameters that I was tuning were:&#xD;&#xA;&#xD;&#xA;- the number of trees/iterations;&#xD;&#xA;- learning depth;&#xD;&#xA;- and learning rate.&#xD;&#xA;&#xD;&#xA;The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.&#xD;&#xA;&#xD;&#xA;code is something like:&#xD;&#xA;&#xD;&#xA;from sklearn.ensemble import GradientBoostingRegressor&#xD;&#xA;&#xD;&#xA;X_train, X_test, y_train, y_test = train_test_split(X, y)&#xD;&#xA;&#xD;&#xA;reg = GradientBoostingRegressor(n_estimators=8000, max_depth=1, loss = 'ls', learning_rate = .01)&#xD;&#xA;&#xD;&#xA;reg.fit(X_train, y_train)&#xD;&#xA;&#xD;&#xA;ypred = reg.predict(X_test)&#xD;&#xA;" />
  <row Id="1433" PostHistoryTypeId="5" PostId="269" RevisionGUID="dab43e19-5685-4c06-80dc-1a29c84ff3c7" CreationDate="2014-06-25T16:03:21.447" UserId="322" Comment="added explicit citation in case the link rots" Text="Definitely they can.&#xD;&#xA;I can target you to a **[nice paper][1]**. Once I used it for soccer league results prediction algorithm implementation, primarily aiming at having some value against bookmakers.&#xD;&#xA;&#xD;&#xA;From paper's abstract:&#xD;&#xA;&gt; a Bayesian dynamic generalized model to estimate the time dependent skills of all teams in a league, and to predict next weekend's soccer matches.&#xD;&#xA;&#xD;&#xA;Keywords:&#xD;&#xA;&#xD;&#xA;&gt; Dynamic Models, Generalized Linear Models, Graphical Models, Markov&#xD;&#xA;&gt; Chain Monte Carlo Methods, Prediction of Soccer Matches&#xD;&#xA;&#xD;&#xA;Citation:&#xD;&#xA;&#xD;&#xA;&gt;Rue, Havard, and Oyvind Salvesen. &quot;Prediction and retrospective analysis of soccer matches in a league.&quot; Journal of the Royal Statistical Society: Series D (The Statistician) 49.3 (2000): 399-418.&#xD;&#xA;&#xD;&#xA;  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7448&amp;rep=rep1&amp;type=pdf" />
  <row Id="1434" PostHistoryTypeId="24" PostId="269" RevisionGUID="dab43e19-5685-4c06-80dc-1a29c84ff3c7" CreationDate="2014-06-25T16:03:21.447" Comment="Proposed by 322 approved by 97 edit id of 92" />
  <row Id="1435" PostHistoryTypeId="5" PostId="565" RevisionGUID="a8508cbf-86e3-4ffc-b717-834f654c625e" CreationDate="2014-06-25T16:46:30.323" UserId="322" Comment="title, code format, add kaggle tag" Text="As I increase the number of trees in [scikit learn](http://scikit-learn.org/stable/)'s [`GradientBoostingRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features, most of which are binary.&#xD;&#xA;&#xD;&#xA;Some of the parameters that I was tuning were:&#xD;&#xA;&#xD;&#xA;- the number of trees/iterations;&#xD;&#xA;- learning depth;&#xD;&#xA;- and learning rate.&#xD;&#xA;&#xD;&#xA;The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.&#xD;&#xA;&#xD;&#xA;My code is something like:&#xD;&#xA;&#xD;&#xA;    from sklearn.ensemble import GradientBoostingRegressor&#xD;&#xA;    &#xD;&#xA;    X_train, X_test, y_train, y_test = train_test_split(X, y)&#xD;&#xA;    &#xD;&#xA;    reg = GradientBoostingRegressor(n_estimators=8000, max_depth=1, loss = 'ls', learning_rate = .01)&#xD;&#xA;    &#xD;&#xA;    reg.fit(X_train, y_train)&#xD;&#xA;    &#xD;&#xA;    ypred = reg.predict(X_test)&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1436" PostHistoryTypeId="4" PostId="565" RevisionGUID="a8508cbf-86e3-4ffc-b717-834f654c625e" CreationDate="2014-06-25T16:46:30.323" UserId="322" Comment="title, code format, add kaggle tag" Text="Why does Gradient Boosting regression predict negative values when there are no negative y-values in my training set?" />
  <row Id="1437" PostHistoryTypeId="6" PostId="565" RevisionGUID="a8508cbf-86e3-4ffc-b717-834f654c625e" CreationDate="2014-06-25T16:46:30.323" UserId="322" Comment="title, code format, add kaggle tag" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;algorithms&gt;&lt;scikit&gt;&lt;kaggle&gt;" />
  <row Id="1438" PostHistoryTypeId="24" PostId="565" RevisionGUID="a8508cbf-86e3-4ffc-b717-834f654c625e" CreationDate="2014-06-25T16:46:30.323" Comment="Proposed by 322 approved by 1162 edit id of 93" />
  <row Id="1439" PostHistoryTypeId="2" PostId="586" RevisionGUID="113771fc-da23-4ef4-b8cc-fa03a308714b" CreationDate="2014-06-25T17:37:23.380" UserId="1192" Text="I'm new to the world of text mining and have been reading up on annotators at places like the &lt;a href=&quot;http://uima.apache.org/&quot;&gt;UIMA website&lt;/a&gt;. I'm encountering many new terms like named entity recognition, tokenizer, lemmatizer, gazetteer, etc. Coming from a layman background, this is all very confusing so can anyone tell me or link to resources that can explain what the main categories of annotators are and what they do?" />
  <row Id="1440" PostHistoryTypeId="1" PostId="586" RevisionGUID="113771fc-da23-4ef4-b8cc-fa03a308714b" CreationDate="2014-06-25T17:37:23.380" UserId="1192" Text="What are the main types of NLP annotators?" />
  <row Id="1441" PostHistoryTypeId="3" PostId="586" RevisionGUID="113771fc-da23-4ef4-b8cc-fa03a308714b" CreationDate="2014-06-25T17:37:23.380" UserId="1192" Text="&lt;nlp&gt;&lt;text-mining&gt;" />
  <row Id="1442" PostHistoryTypeId="2" PostId="587" RevisionGUID="b4551880-520d-432d-b9d6-0ead54005d46" CreationDate="2014-06-25T18:06:14.293" UserId="793" Text="There are plenty of sources which provide the historical stock data but they only provide the OHLC fields along with volume and adjusted close. Also a couple of sources I found provide market cap data sets but they're restricted to US stocks. Yahoo Finance provides this data online but there's no option to download it ( or none I am aware of ). &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; - Where can I download this data for stocks belonging to various top stock exchanges across countries by using their ticker name ?&#xD;&#xA; - Is there some way to download it via Yahoo Finance or Google Finance ?&#xD;&#xA;&#xD;&#xA;I need data for the last decade or so and hence need some script or API which would do this.&#xD;&#xA;" />
  <row Id="1443" PostHistoryTypeId="1" PostId="587" RevisionGUID="b4551880-520d-432d-b9d6-0ead54005d46" CreationDate="2014-06-25T18:06:14.293" UserId="793" Text="Where can i download historical market capitalization and daily turnover data for stocks?" />
  <row Id="1444" PostHistoryTypeId="3" PostId="587" RevisionGUID="b4551880-520d-432d-b9d6-0ead54005d46" CreationDate="2014-06-25T18:06:14.293" UserId="793" Text="&lt;machine-learning&gt;&lt;data-mining&gt;" />
  <row Id="1445" PostHistoryTypeId="2" PostId="588" RevisionGUID="160314c6-628a-4f16-a029-535a491eaa41" CreationDate="2014-06-25T18:12:04.010" UserId="97" Text="Quant SE is better place for questions related to getting financial data:&#xD;&#xA;&#xD;&#xA; - [What data sources are available online][1]&#xD;&#xA; - http://quant.stackexchange.com/search?q=market+capitalization+data&#xD;&#xA;&#xD;&#xA;  [1]: http://quant.stackexchange.com/questions/141/what-data-sources-are-available-online" />
  <row Id="1446" PostHistoryTypeId="4" PostId="587" RevisionGUID="3abf116f-8697-4822-8f08-3ffc4e725b6e" CreationDate="2014-06-25T19:04:31.130" UserId="97" Comment="Adding more relevant tag. This question is probably off-topic." Text="Where can I download historical market capitalization and daily turnover data for stocks?" />
  <row Id="1447" PostHistoryTypeId="6" PostId="587" RevisionGUID="3abf116f-8697-4822-8f08-3ffc4e725b6e" CreationDate="2014-06-25T19:04:31.130" UserId="97" Comment="Adding more relevant tag. This question is probably off-topic." Text="&lt;data-sources&gt;" />
  <row Id="1448" PostHistoryTypeId="24" PostId="587" RevisionGUID="3abf116f-8697-4822-8f08-3ffc4e725b6e" CreationDate="2014-06-25T19:04:31.130" Comment="Proposed by 97 approved by 793 edit id of 94" />
  <row Id="1450" PostHistoryTypeId="10" PostId="538" RevisionGUID="1ac4ac00-851a-4e16-92d1-d7d33c0caa8d" CreationDate="2014-06-25T20:26:56.000" UserId="84" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:434,&quot;DisplayName&quot;:&quot;Steve Kallestad&quot;},{&quot;Id&quot;:84,&quot;DisplayName&quot;:&quot;Rubens&quot;},{&quot;Id&quot;:84,&quot;DisplayName&quot;:&quot;Rubens&quot;}]}" />
  <row Id="1452" PostHistoryTypeId="10" PostId="518" RevisionGUID="93834e72-ebd1-424e-bfac-bab55448abba" CreationDate="2014-06-26T01:41:16.660" UserId="62" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="1453" PostHistoryTypeId="10" PostId="324" RevisionGUID="3cf478d1-34f0-4fcb-ba9f-27031905e25b" CreationDate="2014-06-26T01:43:32.487" UserId="84" Comment="103" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:84,&quot;DisplayName&quot;:&quot;Rubens&quot;},{&quot;Id&quot;:108,&quot;DisplayName&quot;:&quot;rapaio&quot;},{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="1458" PostHistoryTypeId="6" PostId="468" RevisionGUID="b533514d-b9ea-4ab3-b3cb-fb0f715b82f7" CreationDate="2014-06-26T01:50:53.147" UserId="906" Comment="specific question about r -&gt; r tag" Text="&lt;r&gt;&lt;logistic-regression&gt;" />
  <row Id="1459" PostHistoryTypeId="24" PostId="468" RevisionGUID="b533514d-b9ea-4ab3-b3cb-fb0f715b82f7" CreationDate="2014-06-26T01:50:53.147" Comment="Proposed by 906 approved by 84, 62 edit id of 87" />
  <row Id="1462" PostHistoryTypeId="5" PostId="494" RevisionGUID="3a21de8a-71b1-46eb-975c-164351f759cd" CreationDate="2014-06-26T01:51:28.873" UserId="322" Comment="clarify the type of dataset and add appropriate tag; grammar/language" Text="My data set is formatted like this:&#xD;&#xA;&#xD;&#xA;    User-id | Threat_score&#xD;&#xA;    aaa       45&#xD;&#xA;    bbb       32&#xD;&#xA;    ccc       20&#xD;&#xA;&#xD;&#xA;The list contains the top 100 users with the highest threat scores. I generate such a list monthly and store each month's list in its own file.&#xD;&#xA;&#xD;&#xA;There are three things I would like to get from this data:&#xD;&#xA;&lt;ul&gt;1. Users who are consistently showing up in this list&lt;/ul&gt;&#xD;&#xA;&lt;ul&gt;2. Users who are consistently showing up in this list with high threat scores&lt;/ul&gt;&#xD;&#xA;&lt;ul&gt;3. Users whose threat scores are increasing very quickly&lt;/ul&gt;&#xD;&#xA;&#xD;&#xA;I am thinking a visual summary would be nice; each month (somehow) decide which users I want to plot on a graph of historic threat scores.&#xD;&#xA;&#xD;&#xA;Are there any known visualization techniques that deal with similar requirements?&#xD;&#xA;&#xD;&#xA;How should I be transforming my current data to achieve what I am looking for?" />
  <row Id="1463" PostHistoryTypeId="4" PostId="494" RevisionGUID="3a21de8a-71b1-46eb-975c-164351f759cd" CreationDate="2014-06-26T01:51:28.873" UserId="322" Comment="clarify the type of dataset and add appropriate tag; grammar/language" Text="Techniques for trend extraction from unbalanced panel data" />
  <row Id="1464" PostHistoryTypeId="6" PostId="494" RevisionGUID="3a21de8a-71b1-46eb-975c-164351f759cd" CreationDate="2014-06-26T01:51:28.873" UserId="322" Comment="clarify the type of dataset and add appropriate tag; grammar/language" Text="&lt;visualization&gt;&lt;dataset&gt;&lt;graphs&gt;&lt;panel-data&gt;" />
  <row Id="1465" PostHistoryTypeId="24" PostId="494" RevisionGUID="3a21de8a-71b1-46eb-975c-164351f759cd" CreationDate="2014-06-26T01:51:28.873" Comment="Proposed by 322 approved by 84, 62 edit id of 88" />
  <row Id="1466" PostHistoryTypeId="10" PostId="411" RevisionGUID="028ad651-6d67-41d6-86f7-86ba51a6e24e" CreationDate="2014-06-26T01:58:15.697" UserId="62" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="1467" PostHistoryTypeId="10" PostId="313" RevisionGUID="1167023b-b46d-4ea1-be02-4bb0a8f88654" CreationDate="2014-06-26T02:01:00.157" UserId="62" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="1489" PostHistoryTypeId="10" PostId="477" RevisionGUID="4f867f7e-af98-4d29-be50-f0a8ec1ef20d" CreationDate="2014-06-26T05:31:41.193" UserId="84" Comment="101" Text="{&quot;OriginalQuestionIds&quot;:[461],&quot;Voters&quot;:[{&quot;Id&quot;:84,&quot;DisplayName&quot;:&quot;Rubens&quot;},{&quot;Id&quot;:108,&quot;DisplayName&quot;:&quot;rapaio&quot;},{&quot;Id&quot;:434,&quot;DisplayName&quot;:&quot;Steve Kallestad&quot;},{&quot;Id&quot;:84,&quot;DisplayName&quot;:&quot;Rubens&quot;}]}" />
  <row Id="1491" PostHistoryTypeId="2" PostId="591" RevisionGUID="9f60fd7d-a1ec-41ce-9977-50ea05fb44c3" CreationDate="2014-06-26T06:51:12.243" UserId="791" Text="If you are totally unfamiliar with ordinal regression, I would try to read the Tabachnick / Fidell (http://www.pearsonhighered.com/educator/product/Using-Multivariate-Statistics-6E/0205849571.page)  chapter on the topic first - while not written for R, the book is very good at conveying the general logic and the &quot;do's&quot; and &quot;do nots&quot;. &#xD;&#xA;&#xD;&#xA;As a question: What are your response catgeories exactly? If they are some sort of scale, like &quot;good - bad&quot; it would be ok to use a linear regression (market research does it all the time...), but if the items are more disjunct, an ordinal regression might be better.&#xD;&#xA;I dimly remember that some books about structural equatiotion modelling mentioned that linear regression was superior for good scales than probit - bit I cannot recall the book at the moment, sorry!&#xD;&#xA;&#xD;&#xA;The most serious problem might be the number of dummy variables - a couple of hundred dummy variables will make the analysis slow, hard to interpret and probably unstable - are there enough cases for each dummy / dummy-combination?" />
  <row Id="1492" PostHistoryTypeId="2" PostId="592" RevisionGUID="adff2f85-48f3-4234-b813-e641bf35804a" CreationDate="2014-06-26T07:41:52.403" UserId="172" Text="I usually take a two-step approach &#xD;&#xA;&#xD;&#xA;1. compute univariate (variable by variable) summary statistics such as mean, range, variance, number of missing, cardinality, etc. for each variable and look for oddities (e.g. range not plausible given the meaning of the variable). Plot histograms for those odd variables.&#xD;&#xA;&#xD;&#xA;2. split the data into manageable subsets (choose a meaningful variable and split the data according to it e.g. all positive examples, and all negative) and explore them visually (e.g. with [ggobi][1] ). Especially use tools like brushing and scatter plots to understand how variables are linked together.&#xD;&#xA;&#xD;&#xA;And when you start building models, make sure to plot the residuals, looking for extreme errors that might be due to an outlier, or look at the confusion matrix and make sure it is balanced. Use k-fold cross validation to optimize your models and look at the variance of the training error for each fold, if one fold performs much worse than the others, it may contain outliers.&#xD;&#xA;&#xD;&#xA;  [1]: http://www.ggobi.org/" />
  <row Id="1493" PostHistoryTypeId="2" PostId="593" RevisionGUID="667f5feb-d0c4-4dd5-9cda-ec06ea4cb5f6" CreationDate="2014-06-26T08:01:40.763" UserId="172" Text="As many have answered, it is always best to avoid anything done manually as it is less reproducible/documentable. Your point about the overhead of writing a script vs. opening and fixing the file is valid, though.&#xD;&#xA;&#xD;&#xA;Best practice is often to&#xD;&#xA; &#xD;&#xA;- keep an untouched version of the data&#xD;&#xA;- build a working copy of the data with errors fixed&#xD;&#xA;- have a way to recreated a working copy from the original data&#xD;&#xA;&#xD;&#xA;The last point can be done with a script. Then make sure to be as specific as needed to modify only the data you want to modify, and to write the script in such a way that adding a fix by modifying the script is as easy as modifying the data directly.&#xD;&#xA;&#xD;&#xA;If your data lie in files, you can also use diffs/patches to store the original data along with the patches needed to produce the working data. To generate them, duplicate your working copy, perform the change, extract the diff/patch, save it, and delete the previous working copy." />
  <row Id="1507" PostHistoryTypeId="2" PostId="594" RevisionGUID="7d12badc-79fc-46fb-9f2b-b6b46d4dfc0b" CreationDate="2014-06-26T12:11:51.253" UserId="1206" Text="When doing a Google image search, across the top it has figured out categories for the images of the topic being search for. I'm interested to know how this works and how it chooses and creates categories, unfortunately I can't find much about it at all. Is anyone able to shed some light on algorithms they may be using to do this and what basis these categories are created from?&#xD;&#xA;&#xD;&#xA;For example, if I search for &quot;animals&quot; I get the categories &quot;cute&quot;, &quot;baby&quot;, &quot;wild&quot;, &quot;farm&quot;, &quot;zoo&quot;, &quot;clipart&quot;. If I go into &quot;wild&quot; I then have subcategories &quot;forest&quot;, &quot;baby&quot;, &quot;africa&quot;, &quot;clipart&quot;, &quot;rainforest&quot;, &quot;domestic&quot;." />
  <row Id="1508" PostHistoryTypeId="1" PostId="594" RevisionGUID="7d12badc-79fc-46fb-9f2b-b6b46d4dfc0b" CreationDate="2014-06-26T12:11:51.253" UserId="1206" Text="How does Google categorize results from it's image search?" />
  <row Id="1509" PostHistoryTypeId="3" PostId="594" RevisionGUID="7d12badc-79fc-46fb-9f2b-b6b46d4dfc0b" CreationDate="2014-06-26T12:11:51.253" UserId="1206" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;google&gt;&lt;search&gt;" />
  <row Id="1510" PostHistoryTypeId="2" PostId="595" RevisionGUID="0f5f8ff0-7a57-45e4-a833-eafed0160da3" CreationDate="2014-06-26T12:25:55.663" UserId="1207" Text="&lt;br&gt;&#xD;&#xA;I am newbie with machine learning but I had an interesting problem.&#xD;&#xA;I have a large sample of people and visited sites. Some people have indicated gender, age and other parameters. I want to restore these parameters to all people. Which way I look?&lt;br&gt;&#xD;&#xA;I am familiar with neural networks, but it seems they don't fit." />
  <row Id="1511" PostHistoryTypeId="1" PostId="595" RevisionGUID="0f5f8ff0-7a57-45e4-a833-eafed0160da3" CreationDate="2014-06-26T12:25:55.663" UserId="1207" Text="How use neural networks with variable count of inputs... and really many inputs?" />
  <row Id="1512" PostHistoryTypeId="3" PostId="595" RevisionGUID="0f5f8ff0-7a57-45e4-a833-eafed0160da3" CreationDate="2014-06-26T12:25:55.663" UserId="1207" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;algorithms&gt;&lt;neuralnetwork&gt;" />
  <row Id="1513" PostHistoryTypeId="5" PostId="593" RevisionGUID="e3e7f831-ea04-4c48-96aa-5a8b2103b729" CreationDate="2014-06-26T12:39:40.443" UserId="172" Comment="edited body" Text="As many have answered, it is always best to avoid anything done manually as it is less reproducible/documentable. Your point about the overhead of writing a script vs. opening and fixing the file is valid, though.&#xD;&#xA;&#xD;&#xA;Best practice is often to&#xD;&#xA; &#xD;&#xA;- keep an untouched version of the data&#xD;&#xA;- build a working copy of the data with errors fixed&#xD;&#xA;- have a way to re-create a working copy from the original data&#xD;&#xA;&#xD;&#xA;The last point can be done with a script. Then make sure to be as specific as needed to modify only the data you want to modify, and to write the script in such a way that adding a fix by modifying the script is as easy as modifying the data directly.&#xD;&#xA;&#xD;&#xA;If your data lie in files, you can also use diffs/patches to store the original data along with the patches needed to produce the working data. To generate them, duplicate your working copy, perform the change, extract the diff/patch, save it, and delete the previous working copy." />
  <row Id="1514" PostHistoryTypeId="5" PostId="595" RevisionGUID="348222f1-6d83-423f-94a3-29cdf0429695" CreationDate="2014-06-26T13:15:16.667" UserId="1155" Comment="Name of the methods must be in capital letters" Text="&lt;br&gt;&#xD;&#xA;I am newbie with machine learning but I had an interesting problem.&#xD;&#xA;I have a large sample of people and visited sites. Some people have indicated gender, age and other parameters. I want to restore these parameters to all people. Which way I look?&lt;br&gt;&#xD;&#xA;I am familiar with Neural Networks (supervised learning), but it seems they don't fit." />
  <row Id="1515" PostHistoryTypeId="24" PostId="595" RevisionGUID="348222f1-6d83-423f-94a3-29cdf0429695" CreationDate="2014-06-26T13:15:16.667" Comment="Proposed by 1155 approved by 1207 edit id of 96" />
  <row Id="1516" PostHistoryTypeId="5" PostId="595" RevisionGUID="6d52f718-1fc9-4ce3-950d-a7ab516fe198" CreationDate="2014-06-26T13:15:57.177" UserId="1207" Comment="added 29 characters in body" Text="&lt;br&gt;&#xD;&#xA;I am newbie with machine learning but I had an interesting problem.&#xD;&#xA;I have a large sample of people and visited sites. Some people have indicated gender, age and other parameters. I want to restore these parameters to all people. Which way I look? Which algorithm is suitable?&lt;br&gt;&#xD;&#xA;I am familiar with Neural Networks (supervised learning), but it seems they don't fit." />
  <row Id="1517" PostHistoryTypeId="2" PostId="596" RevisionGUID="ec561019-1864-480f-aa36-aa7c8693ed27" CreationDate="2014-06-26T13:32:18.050" UserId="941" Text="There are several classic datasets for machine learning classification/regression tasks. The most popular are:&#xD;&#xA;&#xD;&#xA;* [Iris Flower Data Set][1];&#xD;&#xA;* [Titanic Data Set][2];&#xD;&#xA;* [Motor Trend Cars][3];&#xD;&#xA;* etc.&#xD;&#xA;&#xD;&#xA;But does anyone know similar datasets for networks analysis / graph theory? More concrete - I'm looking for **Gold standard** datasets for comparing/evaluating/learning:&#xD;&#xA;&#xD;&#xA;1. centrality measures;&#xD;&#xA;2. network clustering algorithms.&#xD;&#xA;&#xD;&#xA;I don't need a huge list of publicly available networks/graphs, but a couple of actually must-know datasets.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html&#xD;&#xA;  [2]: http://www.kaggle.com/c/titanic-gettingStarted&#xD;&#xA;  [3]: http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html" />
  <row Id="1518" PostHistoryTypeId="1" PostId="596" RevisionGUID="ec561019-1864-480f-aa36-aa7c8693ed27" CreationDate="2014-06-26T13:32:18.050" UserId="941" Text="Network analysis classic datasets" />
  <row Id="1519" PostHistoryTypeId="3" PostId="596" RevisionGUID="ec561019-1864-480f-aa36-aa7c8693ed27" CreationDate="2014-06-26T13:32:18.050" UserId="941" Text="&lt;dataset&gt;&lt;graphs&gt;" />
  <row Id="1520" PostHistoryTypeId="2" PostId="597" RevisionGUID="e203da03-fd7a-4f79-bd2d-0191a229679b" CreationDate="2014-06-26T13:45:16.573" UserId="478" Text="I am not working at Google, but I think it is some sort of recommendation system based on the words which millions of users searched before. So those people who search for &quot;animals&quot; often search for &quot;wild animals&quot; for example. Like in many online stores they recommend you to buy something in addition to the product you are looking for based on the previous purchases of other users.&#xD;&#xA;&#xD;&#xA;There are many approaches how to build such recommendation system using machine learning, no one knows for sure what google uses." />
  <row Id="1521" PostHistoryTypeId="2" PostId="598" RevisionGUID="9626dce1-0b12-45cd-b8f9-245f4e6e547f" CreationDate="2014-06-26T14:17:44.077" UserId="1155" Text="There exist many possibilities for populating empty gaps on data.&#xD;&#xA;&#xD;&#xA; - **Most repeated value**: Fill the gaps with the most common value.&#xD;&#xA; - **Create a distribution**: Make the histogram and drop values according to that distribution.&#xD;&#xA; - **Create a new label**: Since you do not have information, do not assume any value and create another label/category to indicate that value is empty.&#xD;&#xA; - **Create a classifier**: Make a relation among the variable with empty gaps and the rest of the data and create a simple classifier. With this, populate the rest of the data.&#xD;&#xA;&#xD;&#xA;There exist many others, but these are the most common strategies. My suggestion is not to populate and to keep unknown what is unknown." />
  <row Id="1522" PostHistoryTypeId="2" PostId="599" RevisionGUID="62a399b1-f4ce-4fd1-8527-df5eaefc7a67" CreationDate="2014-06-26T14:21:10.513" UserId="1155" Text="The only thing I know about is benchmark data for Graph Databases, such as Neo4j.&#xD;&#xA;&#xD;&#xA;You may find links similar to this one:&#xD;&#xA;http://istc-bigdata.org/index.php/benchmarking-graph-databases/&#xD;&#xA;&#xD;&#xA;where you can find data to test network analysis and graph theory.&#xD;&#xA;&#xD;&#xA;Furthermore, you could play with the API of Twitter/Facebook to collect your own data. This is also a suggestion in case you do not find the data you are looking for." />
  <row Id="1523" PostHistoryTypeId="2" PostId="600" RevisionGUID="eb3901eb-55e4-4d4a-8039-7b020bd55bde" CreationDate="2014-06-26T14:39:06.410" UserId="1208" Text="I am working on a political campaign where dozens of volunteers will be conducting door-knocking promotions over the next few weeks. Given a list with names, addresses and long/lat coordinates, what algorithms can be used to create an optimized walk list." />
  <row Id="1524" PostHistoryTypeId="1" PostId="600" RevisionGUID="eb3901eb-55e4-4d4a-8039-7b020bd55bde" CreationDate="2014-06-26T14:39:06.410" UserId="1208" Text="How do you create an optimized walk list given longitude and latitude coordinates?" />
  <row Id="1525" PostHistoryTypeId="3" PostId="600" RevisionGUID="eb3901eb-55e4-4d4a-8039-7b020bd55bde" CreationDate="2014-06-26T14:39:06.410" UserId="1208" Text="&lt;sql&gt;" />
  <row Id="1529" PostHistoryTypeId="6" PostId="587" RevisionGUID="35fe2905-e9e6-4bf5-841c-8d40a9fea929" CreationDate="2014-06-26T16:12:02.640" UserId="97" Comment="Adding more relevant tags." Text="&lt;dataset&gt;&lt;data-sources&gt;&lt;market-data&gt;" />
  <row Id="1530" PostHistoryTypeId="24" PostId="587" RevisionGUID="35fe2905-e9e6-4bf5-841c-8d40a9fea929" CreationDate="2014-06-26T16:12:02.640" Comment="Proposed by 97 approved by -1 edit id of 95" />
  <row Id="1531" PostHistoryTypeId="6" PostId="587" RevisionGUID="0577251f-8109-4a6c-82ad-1dc10204477b" CreationDate="2014-06-26T16:12:02.640" UserId="84" Comment="Adding more relevant tags." Text="&lt;dataset&gt;&lt;market-data&gt;" />
  <row Id="1532" PostHistoryTypeId="2" PostId="602" RevisionGUID="eee0e8c9-5c8b-4024-bfb7-980f3ef1f497" CreationDate="2014-06-26T16:19:46.480" UserId="984" Text="I thought of expanding a bit on the answer by Stanpol. While recommendation system is one approach of suggesting related queries, one more standard information retrieval based approach is the query expansion technique.&#xD;&#xA;&#xD;&#xA;Generally speaking, query expansion involves selecting additional terms from the top ranked documents retrieved in response to an initial query. Terms are typically selected by a combination of a term scoring function such as tf-idf and a co-occurrence based measure.&#xD;&#xA;&#xD;&#xA;For example, in response to a query term &quot;animal&quot;, a term selection function may choose the term &quot;zoo&quot;, because&#xD;&#xA;&#xD;&#xA; - &quot;zoo&quot; may be a dominating term (high tf-idf) in the top (say 10) documents&#xD;&#xA; - &quot;zoo&quot; may occur frequently with the original query term &quot;animal&quot;." />
  <row Id="1533" PostHistoryTypeId="5" PostId="594" RevisionGUID="87f6cb96-ca06-45bc-bd3b-660893ee316b" CreationDate="2014-06-26T16:19:49.043" UserId="84" Comment="Fixed grammar, and improving formatting." Text="While doing a Google image search, the page displays some figured out categories for the images of the topic being searched for. I'm interested in learning how this works, and how it chooses and creates categories.&#xD;&#xA;&#xD;&#xA;Unfortunately, I couldn't find much about it at all. Is anyone able to shed some light on algorithms they may be using to do this, and what basis these categories are created from?&#xD;&#xA;&#xD;&#xA;For example, if I search for &quot;animals&quot; I get the categories:&#xD;&#xA;&#xD;&#xA;&gt; &quot;cute&quot;, &quot;baby&quot;, &quot;wild&quot;, &quot;farm&quot;, &quot;zoo&quot;, &quot;clipart&quot;.&#xD;&#xA;&#xD;&#xA;If I go into &quot;wild&quot;, I then have subcategories:&#xD;&#xA;&#xD;&#xA;&gt; &quot;forest&quot;, &quot;baby&quot;, &quot;africa&quot;, &quot;clipart&quot;, &quot;rainforest&quot;, &quot;domestic&quot;." />
  <row Id="1534" PostHistoryTypeId="4" PostId="594" RevisionGUID="87f6cb96-ca06-45bc-bd3b-660893ee316b" CreationDate="2014-06-26T16:19:49.043" UserId="84" Comment="Fixed grammar, and improving formatting." Text="How does Google categorize results from its image search?" />
  <row Id="1535" PostHistoryTypeId="5" PostId="595" RevisionGUID="46d5c60e-c102-4d0e-a1cb-d8dca4fea812" CreationDate="2014-06-26T16:25:31.680" UserId="84" Comment="Fixed grammar, and improving formatting." Text="I'm new to machine learning, but I have an interesting problem. I have a large sample of people and visited sites. Some people have indicated gender, age, and other parameters. Now I want to restore these parameters to each user.&#xD;&#xA;&#xD;&#xA;Which way do I look for? Which algorithm is suitable to solve this problem? I'm familiar with Neural Networks (supervised learning), but it seems they don't fit." />
  <row Id="1536" PostHistoryTypeId="4" PostId="595" RevisionGUID="46d5c60e-c102-4d0e-a1cb-d8dca4fea812" CreationDate="2014-06-26T16:25:31.680" UserId="84" Comment="Fixed grammar, and improving formatting." Text="How to use neural networks with large and variable number of inputs?" />
  <row Id="1542" PostHistoryTypeId="2" PostId="603" RevisionGUID="ab1c02b7-7670-4aa3-b6fe-d58b0bf34854" CreationDate="2014-06-26T18:01:06.443" UserId="434" Text="People generally here something closely related to the [Travelling Salesman Problem][1] and think that it can't be solved. &#xD;&#xA;&#xD;&#xA;A good deal of work has been done on this topic and not all of it indicates that a solution is not available.  Depending on the parameters and the desired solution, you may be able to find something that will work.&#xD;&#xA;&#xD;&#xA;You may want to give a look at the [OpenOpt][2] python library. &#xD;&#xA;&#xD;&#xA;Another resource to look at would  be the [TSP Solver and Generator][3].&#xD;&#xA;&#xD;&#xA;If you are using R, there is a [TSP package available][4].&#xD;&#xA;&#xD;&#xA;Actually implementing a solution to your problem is a little too much to cover here, but this should provide a good starting point.  Within these packages and at the documentation within the links that I provided for you, you will find that there are a fairly wide variety of algorithmic strategies available.  You have a small geographic region and a small set of &quot;salespeople&quot;, so the computational power needed to calculate a strategy within a reasonable time frame should be available on your desktop.  &#xD;&#xA;&#xD;&#xA;In practical terms, you don't need to find the absolutely most optimal strategy.  You just need a very good one.  Pick a TSP package that looks the least overwhelming and give it a go.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Travelling_salesman_problem&#xD;&#xA;  [2]: http://openopt.org/TSP&#xD;&#xA;  [3]: http://tspsg.info/&#xD;&#xA;  [4]: http://tsp.r-forge.r-project.org/" />
  <row Id="1543" PostHistoryTypeId="5" PostId="603" RevisionGUID="6e571a15-6c64-4443-9125-0d439e2e28f5" CreationDate="2014-06-26T18:10:42.907" UserId="434" Comment="deleted 11 characters in body" Text="People see something closely related to the [Travelling Salesman Problem][1] and think that it can't be solved. &#xD;&#xA;&#xD;&#xA;A good deal of work has been done on this topic and not all of it indicates that a solution is not available.  Depending on the parameters and the desired solution, you may be able to find something that will work.&#xD;&#xA;&#xD;&#xA;You may want to give a look at the [OpenOpt][2] python library. &#xD;&#xA;&#xD;&#xA;Another resource to look at would  be the [TSP Solver and Generator][3].&#xD;&#xA;&#xD;&#xA;If you are using R, there is a [TSP package available][4].&#xD;&#xA;&#xD;&#xA;Actually implementing a solution to your problem is a little too much to cover here, but this should provide a good starting point.  Within these packages and at the documentation within the links that I provided for you, you will find that there are a fairly wide variety of algorithmic strategies available.  You have a small geographic region and a small set of &quot;salespeople&quot;, so the computational power needed to calculate a strategy within a reasonable time frame should be available on your desktop.  &#xD;&#xA;&#xD;&#xA;In practical terms, you don't need to find the absolutely most optimal strategy.  You just need a very good one.  Pick a TSP package that looks the least overwhelming and give it a go.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Travelling_salesman_problem&#xD;&#xA;  [2]: http://openopt.org/TSP&#xD;&#xA;  [3]: http://tspsg.info/&#xD;&#xA;  [4]: http://tsp.r-forge.r-project.org/" />
  <row Id="1544" PostHistoryTypeId="6" PostId="600" RevisionGUID="c4faca95-0b42-49d7-b3ca-1218549c89c3" CreationDate="2014-06-26T18:15:57.907" UserId="434" Comment="edited tags" Text="&lt;algorithms&gt;&lt;tsp&gt;&lt;starting-point&gt;" />
  <row Id="1545" PostHistoryTypeId="24" PostId="600" RevisionGUID="c4faca95-0b42-49d7-b3ca-1218549c89c3" CreationDate="2014-06-26T18:15:57.907" Comment="Proposed by 434 approved by 84 edit id of 97" />
  <row Id="1546" PostHistoryTypeId="2" PostId="604" RevisionGUID="abd43b02-d844-494f-8446-2516f7dbb600" CreationDate="2014-06-26T19:16:31.470" UserId="434" Text="Google isn't going to give away their proprietary work, but we can speculate.&#xD;&#xA;&#xD;&#xA;Here's what I can gather from my limited usage:&#xD;&#xA;&#xD;&#xA;1. The recommendations do not seem to be user, geography, or history specific.&#xD;&#xA;2. There is never an empty recommendation (one that returns no results)&#xD;&#xA;3. There is not always a recommendation (some searches just return images)&#xD;&#xA;4. The recommendations are not always the same (consecutive searches sometimes return different recommendations)&#xD;&#xA;5. Result ordering shifts regularly (search for a specific image and it won't always be in the same place)&#xD;&#xA;6. Very popular searches seem to be pre-calculated and more static than unpopular searches.&#xD;&#xA;7. Recommendations are not always one additional word, and recommendations do not always include the base query.&#xD;&#xA;&#xD;&#xA;It seems to me that they do this based on the history of the general end user population, they rotate recommendations when there are many popular ones, and they do some additional processing to determine that the result set is of a reasonable size.&#xD;&#xA;&#xD;&#xA;I would postulate that it works as follows:&#xD;&#xA;&#xD;&#xA;1. Use consecutive search strings from users (short-to-long-tail searches) as training data for a machine-learning algorithm.&#xD;&#xA;2. Run searches that occur &gt; N amount of times a week against that recommendation algorithm.&#xD;&#xA;3. Validate and clean results.&#xD;&#xA;4. Push them out to the general population in rotation/A-B testing.&#xD;&#xA;5. Track click-throughs.&#xD;&#xA;6. Refine results over time." />
  <row Id="1547" PostHistoryTypeId="2" PostId="605" RevisionGUID="e83d2288-d1ae-4d21-bd74-2764b37ab706" CreationDate="2014-06-26T19:23:46.043" UserId="838" Text="I'm building a recommender system and using SVD as one of the preprocessing techniques.&#xD;&#xA;However, I want to normalize all my preprocessed data between 0 and 1 because all of my similarity measures (cosine, pearson, euclidean) depend on that assumption.&#xD;&#xA;&#xD;&#xA;After I take the SVD, is there a standard way to normalize the matrix between 0 and 1? Thanks!" />
  <row Id="1548" PostHistoryTypeId="1" PostId="605" RevisionGUID="e83d2288-d1ae-4d21-bd74-2764b37ab706" CreationDate="2014-06-26T19:23:46.043" UserId="838" Text="How to normalize results of Singular Value Decomposition (SVD) between 0 and 1?" />
  <row Id="1549" PostHistoryTypeId="3" PostId="605" RevisionGUID="e83d2288-d1ae-4d21-bd74-2764b37ab706" CreationDate="2014-06-26T19:23:46.043" UserId="838" Text="&lt;machine-learning&gt;&lt;statistics&gt;&lt;recommendation&gt;&lt;feature-selection&gt;" />
  <row Id="1555" PostHistoryTypeId="2" PostId="606" RevisionGUID="88912743-d7e7-40b2-b37e-54f61db6f1b5" CreationDate="2014-06-26T20:49:38.470" UserId="1220" Text="Folks here stated great steps, but I think there are great information at the following link [what I do when I get a new data set as told through tweets][1],&#xD;&#xA;It sums up the steps the folks tweeted answering the great @hmason question &quot;Data people: What is the very first thing you do when you get your hands on a new data set?&quot;&#xD;&#xA;&#xD;&#xA;Hope it will be useful. &#xD;&#xA;&#xD;&#xA;  [1]: http://simplystatistics.org/2014/06/13/what-i-do-when-i-get-a-new-data-set-as-told-through-tweets/" />
  <row Id="1556" PostHistoryTypeId="2" PostId="607" RevisionGUID="14df6955-68c1-4b02-8faa-7e8a922b7c9c" CreationDate="2014-06-26T22:17:52.570" UserId="1223" Text="What I'd like to start with this question is a compilation of the differences between these two &quot;box/kits&quot;... I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These two are full on VMs with alot of relevant software on them, but I haven't been able to find any side-by-side comparison.  &#xD;&#xA;&#xD;&#xA;Here's a start from my research, but if someone could tell me that one is objectively more rich-featured and useful to get started then that would help greatly:&#xD;&#xA;&#xD;&#xA;Kit -&gt; vm is on vagrant cloud (4 GB) and seems to be more &quot;hip&quot; with R, iPython notebook, and other useful command-line tools (html-&gt;txt, json-&gt;xml, etc). There is a book being released in August with detail.&#xD;&#xA;&#xD;&#xA;Tool -&gt; vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1557" PostHistoryTypeId="1" PostId="607" RevisionGUID="14df6955-68c1-4b02-8faa-7e8a922b7c9c" CreationDate="2014-06-26T22:17:52.570" UserId="1223" Text="www.datasciencetoolBOX.org vs www.datasciencetoolKIT.org" />
  <row Id="1558" PostHistoryTypeId="3" PostId="607" RevisionGUID="14df6955-68c1-4b02-8faa-7e8a922b7c9c" CreationDate="2014-06-26T22:17:52.570" UserId="1223" Text="&lt;tools&gt;" />
  <row Id="1567" PostHistoryTypeId="5" PostId="607" RevisionGUID="f52bcb3e-e3f7-461d-bfab-155b794236dd" CreationDate="2014-06-26T22:37:12.007" UserId="1223" Comment="deleted 25 characters in body" Text="What I'd like to start with this question is a compilation of the differences between these two &quot;box/kits&quot;... I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven't been able to find any side-by-side comparison.  &#xD;&#xA;&#xD;&#xA;Here's a start from my research, but if someone could tell me that one is objectively more rich-featured and useful to get started then that would help greatly:&#xD;&#xA;&#xD;&#xA;Kit -&gt; vm is on vagrant cloud (4 GB) and seems to be more &quot;hip&quot; with R, iPython notebook, and other useful command-line tools (html-&gt;txt, json-&gt;xml, etc). There is a book being released in August with detail.&#xD;&#xA;&#xD;&#xA;Tool -&gt; vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1569" PostHistoryTypeId="2" PostId="608" RevisionGUID="6d735bba-cc76-4c0b-81d1-2b13aad421df" CreationDate="2014-06-26T22:58:32.380" UserId="836" Text="I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and MLP that I put together.&#xD;&#xA;&#xD;&#xA;The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of `lambda * current_weight` is added as well before the combined delta is subtracted at the end of the batch, where `lambda` is the regularisation parameter.&#xD;&#xA;&#xD;&#xA;My implementation of backpropagation uses per-item weight updates. *Does a smaller regularisation term per item work just as well?* &#xD;&#xA;&#xD;&#xA;For instance `lambda * current_weight / N` where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.&#xD;&#xA;" />
  <row Id="1570" PostHistoryTypeId="1" PostId="608" RevisionGUID="6d735bba-cc76-4c0b-81d1-2b13aad421df" CreationDate="2014-06-26T22:58:32.380" UserId="836" Text="Any differences in regularisation in MLP between batch and individual updates?" />
  <row Id="1571" PostHistoryTypeId="3" PostId="608" RevisionGUID="6d735bba-cc76-4c0b-81d1-2b13aad421df" CreationDate="2014-06-26T22:58:32.380" UserId="836" Text="&lt;neuralnetwork&gt;" />
  <row Id="1572" PostHistoryTypeId="5" PostId="602" RevisionGUID="1c298bf2-8f71-4242-9bf9-6e6516c5e7ed" CreationDate="2014-06-26T23:01:36.187" UserId="984" Comment="added 86 characters in body" Text="I thought of expanding a bit on the answer by Stanpol. While recommendation system is one approach of suggesting related queries, one more standard information retrieval based approach is the query expansion technique.&#xD;&#xA;&#xD;&#xA;Generally speaking, query expansion involves selecting additional terms from the top ranked documents retrieved in response to an initial query. Terms are typically selected by a combination of a term scoring function such as tf-idf and a co-occurrence based measure.&#xD;&#xA;&#xD;&#xA;For example, in response to a query term &quot;animal&quot;, a term selection function may choose the term &quot;zoo&quot;, because&#xD;&#xA;&#xD;&#xA; - &quot;zoo&quot; may be a dominating term (high tf-idf) in the top (say 10) documents retrieved in response to the query &quot;animal&quot;&#xD;&#xA; - &quot;zoo&quot; may co-occur frequently (in close proximity) with the original query term &quot;animal&quot; in these documents" />
  <row Id="1573" PostHistoryTypeId="5" PostId="607" RevisionGUID="a43da90f-38a2-4a24-b102-af3878ad07f0" CreationDate="2014-06-26T23:16:13.523" UserId="1223" Comment="deleted 73 characters in body" Text="I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven't been able to find any side-by-side comparison.  &#xD;&#xA;&#xD;&#xA;Here's a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:&#xD;&#xA;&#xD;&#xA;Kit -&gt; vm is on vagrant cloud (4 GB) and seems to be more &quot;hip&quot; with R, iPython notebook, and other useful command-line tools (html-&gt;txt, json-&gt;xml, etc). There is a book being released in August with detail.&#xD;&#xA;&#xD;&#xA;Tool -&gt; vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1574" PostHistoryTypeId="2" PostId="609" RevisionGUID="c6bcd4c5-2f4b-43ab-bcbc-c6c14400f965" CreationDate="2014-06-26T23:46:22.487" UserId="1224" Text="Below is a very good note (page 12) on learning rate in Neural Nets (Back Prob) by Andrew Ng. You will find details relating to learning rate. &#xD;&#xA;&#xD;&#xA;http://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf&#xD;&#xA;&#xD;&#xA;For your 4th-point, you're right that normally one has to choose a &quot;balanced&quot; learning rate, that should neither overshoot nor converge too slowly. One can plot the learning rate w.r.t. the descent of the cost function to diagnose/ fine tune. In practice, Andrew normally uses the L-BFGS algorithm (mentioned in page 12) to get the &quot;good enough&quot; learning rate.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1575" PostHistoryTypeId="5" PostId="607" RevisionGUID="5b1d654c-acdc-4c86-ad38-5c271a50a345" CreationDate="2014-06-26T23:51:05.580" UserId="1223" Comment="deleted 1 character in body" Text="I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven't been able to find any side-by-side comparison.  &#xD;&#xA;&#xD;&#xA;Here's a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:&#xD;&#xA;&#xD;&#xA;KIT -&gt; vm is on vagrant cloud (4 GB) and seems to be more &quot;hip&quot; with R, iPython notebook, and other useful command-line tools (html-&gt;txt, json-&gt;xml, etc). There is a book being released in August with detail.&#xD;&#xA;&#xD;&#xA;BOX -&gt; vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1576" PostHistoryTypeId="5" PostId="605" RevisionGUID="de0b32ff-5179-40f5-aed9-11dc2372acc5" CreationDate="2014-06-27T00:22:33.810" UserId="838" Comment="added 213 characters in body" Text="I'm building a recommender system and using SVD as one of the preprocessing techniques.&#xD;&#xA;However, I want to normalize all my preprocessed data between 0 and 1 because all of my similarity measures (cosine, pearson, euclidean) depend on that assumption.&#xD;&#xA;&#xD;&#xA;After I take the SVD (A = USV^T), is there a standard way to normalize the matrix 'A' between 0 and 1? Thanks!&#xD;&#xA;&#xD;&#xA;Edit: I want all of my similarity measurements to give results between 0 and 1 and my normalized euclidean distance in particular fails if the input matrix does not have values between 0 and 1." />
  <row Id="1577" PostHistoryTypeId="5" PostId="608" RevisionGUID="ac63404f-3f9a-4fee-a307-3e2438906746" CreationDate="2014-06-27T02:30:51.597" UserId="84" Comment="Improving formatting." Text="I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and [Multilayer perceptron](http://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) that I put together.&#xD;&#xA;&#xD;&#xA;The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of `lambda * current_weight` is added as well before the combined delta is subtracted at the end of the batch, where `lambda` is the regularisation parameter.&#xD;&#xA;&#xD;&#xA;My implementation of backpropagation uses per-item weight updates. *Does a smaller regularisation term per item work just as well?* &#xD;&#xA;&#xD;&#xA;For instance `lambda * current_weight / N` where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.&#xD;&#xA;" />
  <row Id="1578" PostHistoryTypeId="5" PostId="596" RevisionGUID="36329d10-6030-4bfe-84af-9045da715162" CreationDate="2014-06-27T05:22:25.863" UserId="941" Comment="added features for &quot;gold standard dataset&quot;" Text="There are several classic datasets for machine learning classification/regression tasks. The most popular are:&#xD;&#xA;&#xD;&#xA;* [Iris Flower Data Set][1];&#xD;&#xA;* [Titanic Data Set][2];&#xD;&#xA;* [Motor Trend Cars][3];&#xD;&#xA;* etc.&#xD;&#xA;&#xD;&#xA;But does anyone know similar datasets for networks analysis / graph theory? More concrete - I'm looking for **Gold standard** datasets for comparing/evaluating/learning:&#xD;&#xA;&#xD;&#xA;1. centrality measures;&#xD;&#xA;2. network clustering algorithms.&#xD;&#xA;&#xD;&#xA;I don't need a huge list of publicly available networks/graphs, but a couple of actually must-know datasets.&#xD;&#xA;&#xD;&#xA;EDIT:&#xD;&#xA;&#xD;&#xA;It's quite difficult to provide exact features for &quot;gold standard dataset&quot;, but here are some thoughts. I think, real classic dataset should satisfy these criteria:&#xD;&#xA;&#xD;&#xA;* Multiple references in articles and textbooks;&#xD;&#xA;* Inclusion in well-known network analysis software packages;&#xD;&#xA;* Sufficient time of existence;&#xD;&#xA;* Usage in a number of courses on graph analysis.&#xD;&#xA;&#xD;&#xA;Concerning my field of interest, I also need labeled classes for vertices and/or precomputed (or predefined) &quot;authority scores&quot; (i.e. centrality estimates). After asking this question I continued searching, and here are some suitable examples:&#xD;&#xA;&#xD;&#xA;* [Zachary's Karate Club][4]: introduced in 1977, cited more than 1.5k times (according to Google Scholar), vertexes have attribute Faction (which can be used for clustering).&#xD;&#xA;* [Erdos Collaboration Network][5]: unfortunately, I haven't find this network in form of data-file, but it's rather famous, and if someone will enrich network with  mathematicians' specialisations data, it also could be used for testing clustering algorithms.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html&#xD;&#xA;  [2]: http://www.kaggle.com/c/titanic-gettingStarted&#xD;&#xA;  [3]: http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&#xD;&#xA;  [4]: http://networkdata.ics.uci.edu/data.php?id=105&#xD;&#xA;  [5]: http://www.orgnet.com/Erdos.html" />
  <row Id="1582" PostHistoryTypeId="2" PostId="611" RevisionGUID="ab0fc4a0-1719-4a09-9eeb-159914351593" CreationDate="2014-06-27T07:06:14.297" UserId="941" Text="I had almost the same problem: 'restoring' age, gender, location for social network users. But I used users' ego-networks, not visited sites statistics. And I faced with two almost independent tasks:&#xD;&#xA;&#xD;&#xA;1. 'Restoring' or 'predicting' data. You can use a bunch of different technics to complete this task, but my vote is for simplest ones (KISS, yes). E.g., in my case, for age prediction, mean of ego-network users' ages gave satisfactory results (for about 70% of users error was less than +/-3 years, in my case it was enough). It's just an idea, but you can try to use for age prediction weighted average, defining weight as similarity measure between visited sites sets of current user and others.&#xD;&#xA;2. Evaluating prediction quality. Algorithm from task-1 will produce prediction almost in all cases. And second task is to determine, if prediction is reliable. E.g., in case of ego network and age prediction: can we trust in prediction, if a user has only one 'friend' in his ego network? This task is more about machine-learning: it's a binary classification problem. You need to compose features set, form training and test samples from your data with both right and wrong predictions. Creating appropriate classifier will help you to filter out unpredictable users. But you need to determine, what are your features set. I used a number of network metrics, and summary statistics on feature of interest distribution among ego-network.&#xD;&#xA;&#xD;&#xA;This approach wouldn't populate all the gaps, but only predictable ones." />
  <row Id="1583" PostHistoryTypeId="2" PostId="612" RevisionGUID="18bea3d8-9a3e-417d-b467-f8e1674b0d1d" CreationDate="2014-06-27T07:19:11.420" UserId="1085" Text="Why not just determine the correct value of lambda for your case using validation? Since you are not doing online learning (and thus have a dataset), it should be easy. I'm not sure if lambda in per-item learning is inversely proportional to the size of the training set. &#xD;&#xA;&#xD;&#xA;Rather, the value of lambda should be considered per update of the parameter vector; dividing by the number of samples would affect lambda for each update. In other words: for batch training you have one parameter update *per dataset pass*, while for online one update *per sample*.&#xD;&#xA;&#xD;&#xA;I recently stumbled upon this [Crossvalidated Question][1], which seems quite similar to yours. There is a link to a paper about [a new SGD algorithm][2], with some relevant content. It might be useful to take a look (especially pages 1742-1743).&#xD;&#xA;&#xD;&#xA;  [1]: http://stats.stackexchange.com/questions/64224/regularization-and-feature-scaling-in-online-learning&#xD;&#xA;  [2]: http://leon.bottou.org/publications/pdf/jmlr-2009.pdf" />
  <row Id="1584" PostHistoryTypeId="5" PostId="608" RevisionGUID="30175be9-f2c1-4a92-acfb-1abeb2d1445f" CreationDate="2014-06-27T09:45:14.983" UserId="836" Comment="explain that I already cross-validate" Text="I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and [Multilayer perceptron](http://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) that I put together.&#xD;&#xA;&#xD;&#xA;Currently I tend to cross-validate and keep the network with best score on the validation set to avoid over-fitting. This works OK, but adding regularisation would benefit me in that correct choice of the regularisation algorithm and parameter would make my network converge on a non-overfit model more systematically.&#xD;&#xA;&#xD;&#xA;The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of `lambda * current_weight` is added as well before the combined delta is subtracted at the end of the batch, where `lambda` is the regularisation parameter.&#xD;&#xA;&#xD;&#xA;My implementation of backpropagation uses per-item weight updates. I am concerned that I cannot just copy the batch approach, although it looks OK intuitively to me. *Does a smaller regularisation term per item work just as well?* &#xD;&#xA;&#xD;&#xA;For instance `lambda * current_weight / N` where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.&#xD;&#xA;" />
  <row Id="1585" PostHistoryTypeId="5" PostId="608" RevisionGUID="a667bfd5-c115-4d5e-ad0d-261be6354b56" CreationDate="2014-06-27T09:52:53.277" UserId="836" Comment="edited body" Text="I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and [Multilayer perceptron](http://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) that I put together.&#xD;&#xA;&#xD;&#xA;Currently to avoid over-fitting, I cross-validate and keep the network with best score so far on the validation set. This works OK, but adding regularisation would benefit me in that correct choice of the regularisation algorithm and parameter would make my network converge on a non-overfit model more systematically.&#xD;&#xA;&#xD;&#xA;The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of `lambda * current_weight` is added as well before the combined delta is subtracted at the end of the batch, where `lambda` is the regularisation parameter.&#xD;&#xA;&#xD;&#xA;My implementation of backpropagation uses per-item weight updates. I am concerned that I cannot just copy the batch approach, although it looks OK intuitively to me. *Does a smaller regularisation term per item work just as well?* &#xD;&#xA;&#xD;&#xA;For instance `lambda * current_weight / N` where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.&#xD;&#xA;" />
  <row Id="1586" PostHistoryTypeId="5" PostId="612" RevisionGUID="92c30f9a-1354-4e96-b081-51210dd8e374" CreationDate="2014-06-27T09:54:54.977" UserId="1085" Comment="Updated according to comments and edited second paragraph." Text="Regularization is relevant in per-item learning as well. I would suggest to start with a basic validation approach for finding out lambda, whether you are doing batch or per-item learning. This is the easiest and safest approach. Try manually with a number of different values. e.g. 0.001. 0.003, 0.01, 0.03, 0.1 etc. and see how your validation set behaves. Later on you may automate this process by introducing a linear or local search method. &#xD;&#xA;&#xD;&#xA;As a side note, I believe the value of lambda should be considered in relation to the updates of the parameter vector, rather than the training set size. For batch training you have one parameter update *per dataset pass*, while for online one update *per sample* (regardless of the training set size).&#xD;&#xA;&#xD;&#xA;I recently stumbled upon this [Crossvalidated Question][1], which seems quite similar to yours. There is a link to a paper about [a new SGD algorithm][2], with some relevant content. It might be useful to take a look (especially pages 1742-1743).&#xD;&#xA;&#xD;&#xA;  [1]: http://stats.stackexchange.com/questions/64224/regularization-and-feature-scaling-in-online-learning&#xD;&#xA;  [2]: http://leon.bottou.org/publications/pdf/jmlr-2009.pdf" />
  <row Id="1588" PostHistoryTypeId="2" PostId="613" RevisionGUID="de9443ce-65c6-431e-90b3-01d0e64ceef3" CreationDate="2014-06-27T12:03:53.357" UserId="1235" Text="I have A  Doubt regarding MapReduce. Actually I understood the Hadoop Mapreduce and its features.&#xD;&#xA;But What i am confused is R Mapreduce..&#xD;&#xA;one difference i know from book is R utilize maximum of RAM. so do perform parallel processing integrated  R with Hadoop.&#xD;&#xA;&#xD;&#xA;My Doubt is&#xD;&#xA;------------------&#xD;&#xA;&#xD;&#xA;     1.  R can do All Stats, Math and Data Science related stuff. but why R mapreduce.&#xD;&#xA;     2.  By  Using R Mapreduce Any New Task can achieve. If Yes Please specify.&#xD;&#xA;     3.  We can Achieve the task with using R+Hadoop (directly) but what is the importance of Mapreduce in R  and How it is different from Normal Mapreduce." />
  <row Id="1589" PostHistoryTypeId="1" PostId="613" RevisionGUID="de9443ce-65c6-431e-90b3-01d0e64ceef3" CreationDate="2014-06-27T12:03:53.357" UserId="1235" Text="Difference Between Hadoop Mapreduce(Java) and RHadoop mapreduce" />
  <row Id="1590" PostHistoryTypeId="3" PostId="613" RevisionGUID="de9443ce-65c6-431e-90b3-01d0e64ceef3" CreationDate="2014-06-27T12:03:53.357" UserId="1235" Text="&lt;machine-learning&gt;&lt;r&gt;&lt;hadoop&gt;&lt;map-reduce&gt;" />
  <row Id="1591" PostHistoryTypeId="2" PostId="614" RevisionGUID="c72634fa-0f7a-4278-b315-33933bd00ba6" CreationDate="2014-06-27T12:28:25.467" UserId="968" Text="To complement what **insys** said :&#xD;&#xA;&#xD;&#xA;Regularization is used when computing the backpropagation for all the weights in your MLP.&#xD;&#xA;Therefore, instead of computing the gradient in regard to all the input of the training set (`batch`) you only use some/one item(s) (`stochastic or semi-stochastic`).&#xD;&#xA;You will end up limiting a result of the update in regard to one item instead of all which is also correct.&#xD;&#xA;&#xD;&#xA;Also, if i remember correctly, Andrew NG used `L2-regularization`.&#xD;&#xA;The `/N` in `lambda * current_weight / N` is not mandatory, it just helps rescaling the input. However if you choose not to use it, you will have (in most of the case) to select another value for `lambda`.&#xD;&#xA;&#xD;&#xA;You can also use the [Grid-search algorithm][1] to choose the best value for `lambda` (the *hyperparameter* =&gt; the one you have to choose).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Hyperparameter_optimization" />
  <row Id="1592" PostHistoryTypeId="2" PostId="615" RevisionGUID="a0401bf9-b052-44f5-81c8-ddfdcf093326" CreationDate="2014-06-27T13:22:22.577" UserId="1237" Text="One standard reference written from social science perspective is [J Scott Long's Limited Dependent Variables](http://www.amazon.com/Regression-Categorical-Dependent-Quantitative-Techniques/dp/0803973748) book. It goes much deeper than say Tabachnik suggested in [another answer](http://datascience.stackexchange.com/a/591/1237): Tabachnik is a cookbook at best, with little to no explanations of the &quot;why&quot;, and it seems like you would benefit from figuring this out in more detail that can be found in Long's book. Ordinal regression should be covered in most introductory econometrics courses (Wooldridge's [Cross-Section and Panel Data](http://www.amazon.com/Econometric-Analysis-Cross-Section-Panel/dp/0262232588/) is a great graduate-level book), as well as quantitative social science courses (sociology, psychology), although I would imagine that the latter will loop back to Long's book.&#xD;&#xA;&#xD;&#xA;Given that your number of variables is wa-a-ay lower than the sample size, the R package you should be looking is probably [`ordinal`](http://cran.r-project.org/web/packages/ordinal/) rather than `glmnetcr`. [Another answer](http://datascience.stackexchange.com/a/469/1237) mentioned that you can find this functionality in a more mainstream `MASS` package." />
  <row Id="1593" PostHistoryTypeId="2" PostId="616" RevisionGUID="2ab2703e-d5b7-4942-8daf-b9830d53467a" CreationDate="2014-06-27T15:58:19.340" UserId="1240" Text="I have a matrix that is populated with discrete elements, and I need to cluster them (using R) into intact groups. So, for example, take this matrix:&#xD;&#xA;&#xD;&#xA;&gt;[A B B C A]  &#xD;&#xA;&gt;[A A B A A]  &#xD;&#xA;&gt;[A B B C C]  &#xD;&#xA;&gt;[A A A A A]  &#xD;&#xA;&#xD;&#xA;There would be two separate clusters for A, two separate clusters for C, and one cluster for B.&#xD;&#xA;&#xD;&#xA;The output I'm looking for would ideally assign a unique ID to each cluster, something like this:&#xD;&#xA;&#xD;&#xA;&gt;[1 2 2 3 4]  &#xD;&#xA;&gt;[1 1 2 4 4]  &#xD;&#xA;&gt;[1 2 2 5 5]  &#xD;&#xA;&gt;[1 1 1 1 1]&#xD;&#xA;&#xD;&#xA;Right now I wrote a code that does this recursively by just iteratively checking nearest neighbor, but it quickly overflows when the matrix gets large (i.e., 100x100).&#xD;&#xA;&#xD;&#xA;Is there a built in function in R that can do this? I looked into raster and image processing, but no luck. I'm convinced it must be out there.&#xD;&#xA;&#xD;&#xA;Thanks!" />
  <row Id="1594" PostHistoryTypeId="1" PostId="616" RevisionGUID="2ab2703e-d5b7-4942-8daf-b9830d53467a" CreationDate="2014-06-27T15:58:19.340" UserId="1240" Text="Identifying “clusters” or “groups” in a matrix" />
  <row Id="1595" PostHistoryTypeId="3" PostId="616" RevisionGUID="2ab2703e-d5b7-4942-8daf-b9830d53467a" CreationDate="2014-06-27T15:58:19.340" UserId="1240" Text="&lt;r&gt;&lt;clusters&gt;" />
  <row Id="1596" PostHistoryTypeId="2" PostId="617" RevisionGUID="11b9385f-3a52-4681-be73-c0a34713e585" CreationDate="2014-06-27T16:40:17.393" UserId="97" Text="What do you think is distance measure in your case?&#xD;&#xA;&#xD;&#xA;I assume there are three dimensions here:&#xD;&#xA;&#xD;&#xA; - `RowN` (row number)&#xD;&#xA; - `ColN` (column number)&#xD;&#xA; - `Value` (value: A, B or C)&#xD;&#xA;&#xD;&#xA;Is `value` scaled? In other words, is `A &lt; B &lt; C`? &#xD;&#xA;&#xD;&#xA;If yes, then&#xD;&#xA;&#xD;&#xA; - you can replace `{A, B, C}` with `{0, 1, 2}` (or may be `{10, 11, 12}`, if you want this difference be less important than RowN and ColN attributes)&#xD;&#xA; - normalize your data&#xD;&#xA; - use, for example, K-Means clustering algorithm (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html) from `stats` R package&#xD;&#xA;&#xD;&#xA;In that case the distance between two will be:&#xD;&#xA;&#xD;&#xA;    Sqrt( (RowN1-RowN2)^2 + (ColN1-ColN2)^2 + (Value1-Value2)^2 )&#xD;&#xA;&#xD;&#xA;If `value` is not scaled (regular categorical variable), use some [modifications of K-Means that work with categorical data][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data&#xD;&#xA;&#xD;&#xA;So in case of 100x100 matrix you have 10000 observations and three variables, which is pretty trivial sample size." />
  <row Id="1597" PostHistoryTypeId="5" PostId="617" RevisionGUID="74e8b163-0e19-4334-bddf-44bdedf8adf4" CreationDate="2014-06-27T16:49:26.077" UserId="97" Comment="Add more details to answer." Text="What do you think is distance measure in your case?&#xD;&#xA;&#xD;&#xA;I assume there are three dimensions here:&#xD;&#xA;&#xD;&#xA; - `RowN` (row number)&#xD;&#xA; - `ColN` (column number)&#xD;&#xA; - `Value` (value: A, B or C)&#xD;&#xA;&#xD;&#xA;That means data you get from `4x5` matrix looks like:&#xD;&#xA;&#xD;&#xA;    Sample1 -&gt; (1, 1, A)&#xD;&#xA;    Sample2 -&gt; (1, 2, B)&#xD;&#xA;    ...&#xD;&#xA;    Sample5 -&gt; (1, 5, A)&#xD;&#xA;    Sample6 -&gt; (2, 1, A)&#xD;&#xA;    ...&#xD;&#xA;    Sample15 -&gt; (3, 5, C)&#xD;&#xA;    ...&#xD;&#xA;    Sample20 -&gt; (4, 5, A)&#xD;&#xA;&#xD;&#xA;Is `value` scaled? In other words, is `A &lt; B &lt; C`? &#xD;&#xA;&#xD;&#xA;If yes, then&#xD;&#xA;&#xD;&#xA; - you can replace `{A, B, C}` with `{0, 1, 2}` (or may be `{10, 11, 12}`, if you want this difference be less important than RowN and ColN attributes)&#xD;&#xA; - normalize your data&#xD;&#xA; - use, for example, K-Means clustering algorithm (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html) from `stats` R package&#xD;&#xA;&#xD;&#xA;In that case the distance between two will be:&#xD;&#xA;&#xD;&#xA;    Sqrt( (RowN1-RowN2)^2 + (ColN1-ColN2)^2 + (Value1-Value2)^2 )&#xD;&#xA;&#xD;&#xA;If `value` is not scaled (regular categorical variable), use some [modifications of K-Means that work with categorical data][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data&#xD;&#xA;&#xD;&#xA;So in case of 100x100 matrix you have 10000 observations and three variables, which is pretty trivial sample size." />
  <row Id="1598" PostHistoryTypeId="5" PostId="616" RevisionGUID="314e6abb-697f-4148-a6e3-3988326ea02b" CreationDate="2014-06-27T17:06:52.027" UserId="322" Comment="code blocks instead of quotes for fixed width alignment; &quot;clustering&quot; is a better tag than &quot;clusters&quot;" Text="I have a matrix that is populated with discrete elements, and I need to cluster them (using R) into intact groups. So, for example, take this matrix:&#xD;&#xA;&#xD;&#xA;    [A B B C A]  &#xD;&#xA;    [A A B A A]  &#xD;&#xA;    [A B B C C]  &#xD;&#xA;    [A A A A A]  &#xD;&#xA;&#xD;&#xA;There would be two separate clusters for A, two separate clusters for C, and one cluster for B.&#xD;&#xA;&#xD;&#xA;The output I'm looking for would ideally assign a unique ID to each cluster, something like this:&#xD;&#xA;&#xD;&#xA;    [1 2 2 3 4]  &#xD;&#xA;    [1 1 2 4 4]  &#xD;&#xA;    [1 2 2 5 5]  &#xD;&#xA;    [1 1 1 1 1]&#xD;&#xA;&#xD;&#xA;Right now I wrote a code that does this recursively by just iteratively checking nearest neighbor, but it quickly overflows when the matrix gets large (i.e., 100x100).&#xD;&#xA;&#xD;&#xA;Is there a built in function in R that can do this? I looked into raster and image processing, but no luck. I'm convinced it must be out there." />
  <row Id="1599" PostHistoryTypeId="6" PostId="616" RevisionGUID="314e6abb-697f-4148-a6e3-3988326ea02b" CreationDate="2014-06-27T17:06:52.027" UserId="322" Comment="code blocks instead of quotes for fixed width alignment; &quot;clustering&quot; is a better tag than &quot;clusters&quot;" Text="&lt;r&gt;&lt;clustering&gt;" />
  <row Id="1600" PostHistoryTypeId="24" PostId="616" RevisionGUID="314e6abb-697f-4148-a6e3-3988326ea02b" CreationDate="2014-06-27T17:06:52.027" Comment="Proposed by 322 approved by 84 edit id of 100" />
  <row Id="1601" PostHistoryTypeId="4" PostId="511" RevisionGUID="7405d122-2937-442e-90e6-95b797fe6ae4" CreationDate="2014-06-27T17:36:39.530" UserId="97" Comment="Get header back" Text="Cross-validation: K-fold vs Repeated random sub-sampling" />
  <row Id="1602" PostHistoryTypeId="2" PostId="618" RevisionGUID="aaac8eb5-39e8-4358-b4ed-83f1bc7de52e" CreationDate="2014-06-27T19:18:11.433" UserId="1193" Text="Although adesantos has already given a good answer, I would like to add a little background information.&#xD;&#xA;&#xD;&#xA;The name for the problem you are looking at is &quot;imputation&quot;. As adesantos already said, one of the possibilities is to fit a distribution. For example, you could fit a multivariate Gaussian to the data. You will get the mean only from the samples you know and you calculate the covariances only from the samples you know. You can then use standard MVG results to impute the missing data linearly.&#xD;&#xA;&#xD;&#xA;This is probably the simplest probabilistic method of imputation and it is already quite involved. If you are a neural networks, a recently proposed method that can do so are deep latent gaussian models by Rezende et al. However, understand the method will require a lot of neural net knowledge, quite some variational Bayes knowledge about Markov chains.&#xD;&#xA;&#xD;&#xA;Another method, which I have hear to work well is to train a generative stochastic network (Bengio et al). This is done by training a denoising auto encoder on the data you have (neglecting missing values in the reconstruction loss). Say you have a reconstruction function f and a input x. Then you will reconstruct it via x' = f(x). You then reset the values of x' with those you know from x. (I.e. you only keep the values that were missing before reconstruction.) If you do so many times, you are guaranteed to sample from the distribution given the values you know.&#xD;&#xA;&#xD;&#xA;But in either case, these methods require quite some knowledge about statistics and neural nets." />
  <row Id="1603" PostHistoryTypeId="2" PostId="619" RevisionGUID="4f13ec19-758e-47d9-a8f8-fee61c3ba14b" CreationDate="2014-06-27T19:37:44.847" UserId="21" Text="[rhadoop][1] (the part you are interested in is now called [rmr2][2]) is simply a client API for MapReduce written in R. You invoke MapReduce using R package APIs, and send an R function to the workers, where it is executed by an R interpreter locally. But it is otherwise exactly the same MapReduce.&#xD;&#xA;&#xD;&#xA;You can call anything you like in R this way, but no R functions are themselves parallelized to use MapReduce in this way. The point is simply that you can invoke M/R from R. I don't think it somehow lets you do anything more magical than that.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/RevolutionAnalytics/RHadoop&#xD;&#xA;  [2]: https://github.com/RevolutionAnalytics/rmr2" />
  <row Id="1604" PostHistoryTypeId="5" PostId="613" RevisionGUID="5dd8aaad-b72d-48ec-a979-15e7336226b6" CreationDate="2014-06-27T19:39:00.247" UserId="322" Comment="took my best shot... can't parse the third sentence" Text="I understand Hadoop MapReduce and its features but I am confused about R MapReduce.&#xD;&#xA;&#xD;&#xA;One difference I have read is that R utilizes maximum RAM. So do perform parallel processing integrated R with Hadoop.&#xD;&#xA;&#xD;&#xA;My doubt is:&#xD;&#xA;-&#xD;&#xA;&#xD;&#xA;1.  R can do all stats, math and data science related stuff, but why R MapReduce?&#xD;&#xA;2.  Is there any new task I can achieve by using R MapReduce instead of Hadoop MapReduce? If yes, please specify.&#xD;&#xA;3.  We can achieve the task by using R with Hadoop (directly) but what is the importance of MapReduce in R and how it is different from normal MapReduce?" />
  <row Id="1605" PostHistoryTypeId="24" PostId="613" RevisionGUID="5dd8aaad-b72d-48ec-a979-15e7336226b6" CreationDate="2014-06-27T19:39:00.247" Comment="Proposed by 322 approved by 21 edit id of 99" />
  <row Id="1609" PostHistoryTypeId="5" PostId="609" RevisionGUID="636ec2b7-b9a6-4055-a37f-b2be7745c8f9" CreationDate="2014-06-27T20:17:32.853" UserId="322" Comment="prob -&gt; propagation, some grammar" Text="Below is a very good note (page 12) on learning rate in Neural Nets (Back Propagation) by Andrew Ng. You will find details relating to learning rate. &#xD;&#xA;&#xD;&#xA;http://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf&#xD;&#xA;&#xD;&#xA;For your 4th point, you're right that normally one has to choose a &quot;balanced&quot; learning rate, that should neither overshoot nor converge too slowly. One can plot the learning rate w.r.t. the descent of the cost function to diagnose/fine tune. In practice, Andrew normally uses the L-BFGS algorithm (mentioned in page 12) to get a &quot;good enough&quot; learning rate.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1610" PostHistoryTypeId="24" PostId="609" RevisionGUID="636ec2b7-b9a6-4055-a37f-b2be7745c8f9" CreationDate="2014-06-27T20:17:32.853" Comment="Proposed by 322 approved by 84 edit id of 101" />
  <row Id="1612" PostHistoryTypeId="2" PostId="620" RevisionGUID="9c092d89-12d8-49f7-821d-4b561a05ef28" CreationDate="2014-06-27T21:45:23.237" UserId="953" Text="Yes, it's problematic.  If you oversample the minority, you risk overfitting.  If you undersample the majority, you risk missing aspects of the majority class.  Stratified sampling, btw, is the equivalent to assigning non-uniform misclassification costs.  &#xD;&#xA;&#xD;&#xA;Alternatives:&#xD;&#xA;&#xD;&#xA;(1) Independently sampling several subsets from the majority class and making multiple classifiers by combining each subset with all the minority class data, as suggested in the answer from @Debasis and described in this [EasyEnsemble paper][1], &#xD;&#xA;&#xD;&#xA;(2) [SMOTE (Synthetic Minority Oversampling Technique)][2] or [SMOTEBoost, (combining SMOTE with boosting)][3] to create synthetic instances of the minority class by making nearest neighbors in the feature space.  SMOTE is implemented in R in [the DMwR package][4].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cse.seu.edu.cn/people/xyliu/publication/tsmcb09.pdf&#xD;&#xA;  [2]: http://arxiv.org/pdf/1106.1813.pdf&#xD;&#xA;  [3]: http://www3.nd.edu/~nchawla/papers/ECML03.pdf&#xD;&#xA;  [4]: http://cran.r-project.org/web/packages/DMwR/index.html" />
  <row Id="1616" PostHistoryTypeId="5" PostId="461" RevisionGUID="df7dc38b-7574-4a47-99ef-78f05d1338ee" CreationDate="2014-06-27T22:58:13.613" UserId="21" Comment="appended answer 505 as supplemental" Text="There's this side project I'm working on where I need to structure a solution to the following problem.&#xD;&#xA;&#xD;&#xA;I have two groups of people (clients). Group `A` intends to buy and group `B` intends to sell a determined product `X`. The product has a series of attributes `x_i`, and my objective is to facilitate the transaction between `A` and `B` by matching their preferences. The main idea is to point out to each member of `A` a corresponding in `B` whose product better suits his needs, and vice versa. &#xD;&#xA;&#xD;&#xA;Some complicating aspects of the problem:&#xD;&#xA;&#xD;&#xA;1. The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can't predict. Can't previously list all the attributes;&#xD;&#xA;	 &#xD;&#xA;2. Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design);&#xD;&#xA;&#xD;&#xA;Any suggestion on how to approach this problem and solve it in an automated way? The idea is to really think out of the box here, so feel free to &quot;go wild&quot; on your suggestions.&#xD;&#xA;&#xD;&#xA;I would also appreciate some references to other similar problems if possible. &#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;&#xD;&#xA;Great suggestions! Many similarities in to the way I’m thinking of approaching the problem.&#xD;&#xA; &#xD;&#xA;The main issue on mapping the attributes is that the level of detail to which the product should be described depends on each buyers. Let’s take an example of a car. The product “car” has lots and lots of attributes that range from its performance, mechanical structure, price etc.&#xD;&#xA; &#xD;&#xA;Suppose I just want a cheap car, or an electric car. Ok, that’s easy to map because they represent main features of this product. But let’s say, for instance, that I want a car with Dual-Clutch transmission or Xenon headlights. Well there might be many cars on the data base with this attributes but I wouldn’t ask the seller to fill in this level of detail to their product prior to the information that there is someone looking them.  Such a procedure would require every seller fill a complex, very detailed, form just try to sell his car on the platform. Just wouldn’t work. &#xD;&#xA;&#xD;&#xA;But still, my challenge is to try to be as detailed as necessary in the search to make a good match. So the way I’m thinking is mapping main aspects of the product, those that are probably relevant to everyone, to narrow down de group of potential sellers.&#xD;&#xA;Next step would be a “refined search”. In order to avoid creating a too detailed form I could ask buyers and sellers to write a free text of their specification. And then use some word matching algorithm to find possible matches. Although I understand that this is not a proper solution to the problem because the seller cannot “guess” what the buyer needs.  But might get me close.  &#xD;&#xA;  &#xD;&#xA;The weighting criteria suggested is great.  It allows me to quantify the level to which the seller matches the buyer’s needs.  The scaling part might be a problem though, because the importance of each attribute varies from client to client. I’m thinking of using some kind of pattern recognition or just asking de buyer to input the level of importance of each attribute. &#xD;&#xA;  &#xD;&#xA;I’m not a native English speaker so hope I’m being able to express my ideas properly. &#xD;&#xA;" />
  <row Id="1618" PostHistoryTypeId="2" PostId="621" RevisionGUID="92b10a37-208a-412e-8df3-f6351c448268" CreationDate="2014-06-28T01:07:27.263" UserId="553" Text="There is a bunch of datasets made free by UC Irvine [to play with here](https://archive.ics.uci.edu/ml/datasets.html). Among those datasets, [there are a few dozen textual datasets](https://archive.ics.uci.edu/ml/datasets.html?format=&amp;task=&amp;att=&amp;area=&amp;numAtt=&amp;numIns=&amp;type=text&amp;sort=nameUp&amp;view=table) that might help you guys with your task.&#xD;&#xA;&#xD;&#xA;Those are kind of generic datasets, so depending on your purpose they should not be used as the only data to train your models, or else your model -- while it might work -- will not produce quality results." />
  <row Id="1619" PostHistoryTypeId="2" PostId="622" RevisionGUID="e1162f38-43db-4df2-bbd9-03d282411671" CreationDate="2014-06-28T14:34:17.487" UserId="1085" Text="I'm not sure if your question classifies as a clustering problem. In clustering you are trying to discover clusters of similar examples using unlabelled data. Here, it seems you wish to enumerate existing &quot;clusters&quot; of nearby nodes. &#xD;&#xA;&#xD;&#xA;To be honest, I have no idea of such a function in R. But, as far as the algorithm is concerned, I believe what you are looking for is [Connected-Component Labeling][1]. Kind of a bucket fill, for matrices.&#xD;&#xA;&#xD;&#xA;The wikipedia article is linked above. One of the algorithms presented there, termed as single-pass algorithm, is as follows:&#xD;&#xA;&#xD;&#xA;    One-Pass(Image)&#xD;&#xA;            [M, N]=size(Image);&#xD;&#xA;            Connected = zeros(M,N);&#xD;&#xA;            Mark = Value;&#xD;&#xA;            Difference = Increment;&#xD;&#xA;            Offsets = [-1; M; 1; -M];&#xD;&#xA;            Index = [];&#xD;&#xA;            No_of_Objects = 0; &#xD;&#xA;   &#xD;&#xA;       for i: 1:M :&#xD;&#xA;           for j: 1:N:&#xD;&#xA;                if(Image(i,j)==1)            &#xD;&#xA;                     No_of_Objects = No_of_Objects +1;            &#xD;&#xA;                     Index = [((j-1)*M + i)];           &#xD;&#xA;                     Connected(Index)=Mark;            &#xD;&#xA;                     while ~isempty(Index)                &#xD;&#xA;                          Image(Index)=0;                &#xD;&#xA;                          Neighbors = bsxfun(@plus, Index, Offsets');&#xD;&#xA;                          Neighbors = unique(Neighbors(:));                &#xD;&#xA;                          Index = Neighbors(find(Image(Neighbors)));                                &#xD;&#xA;                          Connected(Index)=Mark;&#xD;&#xA;                     end            &#xD;&#xA;                     Mark = Mark + Difference;&#xD;&#xA;                end&#xD;&#xA;          end&#xD;&#xA;      end&#xD;&#xA;&#xD;&#xA;I guess it'd be easy to roll your own using the above.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Connected-component_labeling" />
  <row Id="1620" PostHistoryTypeId="2" PostId="623" RevisionGUID="1f36761d-906f-40fa-9da2-acb698f82ec9" CreationDate="2014-06-28T14:57:18.177" UserId="474" Text="I'm currently using several different classifiers on various entities extracted from text, and using precision/recall as a summary of how well each separate classifier performs across a given dataset.&#xD;&#xA;&#xD;&#xA;I'm wondering if there's a meaningful way of comparing the performance of these classifiers in a similar way, but which also takes into account the total numbers of each entity in the test data that's being classified?&#xD;&#xA;&#xD;&#xA;Currently, I'm using precision/recall as a measure of performance, so might have something like:&#xD;&#xA;&#xD;&#xA;                        Precision Recall&#xD;&#xA;    Person classifier   65%       40%&#xD;&#xA;    Company classifier  98%       90%&#xD;&#xA;    Cheese classifier   10%       50%&#xD;&#xA;    Egg classifier      100%      100%&#xD;&#xA;&#xD;&#xA;However, the dataset I'm running these on might contain 100k people, 5k companies, 500 cheeses, and 1 egg.&#xD;&#xA;&#xD;&#xA;So is there a summary statistic I can add to the above table which also takes into account the total number of each item? Or is there some way of measuring the fact that e.g. 100% prec/rec on the Egg classifier might not be meaningful with only 1 data item?&#xD;&#xA;&#xD;&#xA;Let's say we had hundreds of such classifiers, I guess I'm looking for a good way to answer questions like &quot;Which classifiers are underperforming? Which classifiers lack sufficient test data to tell whether they're underperforming?&quot;. &#xD;&#xA;&#xD;&#xA;" />
  <row Id="1621" PostHistoryTypeId="1" PostId="623" RevisionGUID="1f36761d-906f-40fa-9da2-acb698f82ec9" CreationDate="2014-06-28T14:57:18.177" UserId="474" Text="Measuring performance of different classifiers with different sample sizes" />
  <row Id="1622" PostHistoryTypeId="3" PostId="623" RevisionGUID="1f36761d-906f-40fa-9da2-acb698f82ec9" CreationDate="2014-06-28T14:57:18.177" UserId="474" Text="&lt;classification&gt;" />
  <row Id="1623" PostHistoryTypeId="5" PostId="607" RevisionGUID="ec352044-d2c2-4779-b84f-954508811d4a" CreationDate="2014-06-28T20:49:26.073" UserId="1223" Comment="added 38 characters in body; edited title" Text="I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven't been able to find any side-by-side comparison.  &#xD;&#xA;&#xD;&#xA;Here's a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:&#xD;&#xA;&#xD;&#xA;datasciencetoolKIT.org -&gt; vm is on vagrant cloud (4 GB) and seems to be more &quot;hip&quot; with R, iPython notebook, and other useful command-line tools (html-&gt;txt, json-&gt;xml, etc). There is a book being released in August with detail.&#xD;&#xA;&#xD;&#xA;datasciencetoolBOX.org -&gt; vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1624" PostHistoryTypeId="4" PostId="607" RevisionGUID="ec352044-d2c2-4779-b84f-954508811d4a" CreationDate="2014-06-28T20:49:26.073" UserId="1223" Comment="added 38 characters in body; edited title" Text="Do you need a virtual machine as an instrument for your data science practice?" />
  <row Id="1627" PostHistoryTypeId="2" PostId="624" RevisionGUID="d252d701-2b9e-40b9-be98-5f1d07df7b1d" CreationDate="2014-06-29T07:06:00.430" UserId="454" Text="In most cases a practicing data scientist creates his own working environment on personal computed installing preferred software packages. Normally it is sufficient and efficient use of computing resources, because to run a virtual machine (VM) on your main machine you have to allocate a significant portion of RAM for it. The software will run noticeably slower on both the main and the virtual machine unless a lot of RAM. &#xD;&#xA;&#xD;&#xA;Due to this impact on speed it is not common to use VMs as main working environment but they are a good solution in several cases when there is a need of additional working environment. &#xD;&#xA;&#xD;&#xA;The VMs be considered when:&#xD;&#xA;&#xD;&#xA; 1. There is a need to easily replicate a number of identical computing&#xD;&#xA;    environments when teaching a course or doing a presentation on a&#xD;&#xA;    conference.&#xD;&#xA; 2. There is a need to save and recreate an exact environment for an experiment or a calculation.&#xD;&#xA; 3. There is a need to run a different OS or to test a solution on a tool that runs on a different OS. &#xD;&#xA; 4. One wants to try out a bundle of software tools before installing&#xD;&#xA;    them on the main machine. E.g. there is an opportunity to instal an instance of CHS on a VM during an [Intro to Hadoop][1] course on Udacity. &#xD;&#xA; 5. VMs are sometimes used for fast deployment in the cloud like AWS EC, Rackspace etc.&#xD;&#xA;&#xD;&#xA;The VMs mentioned in the original question are made as easily installable data science software bundles. There are more than these two. This [blog post][2] by Jeroen Janssens gives a comparison of at least four:&#xD;&#xA;&#xD;&#xA;1. Data Science Toolbox &#xD;&#xA;2. Mining the Social Web &#xD;&#xA;3. Data Science Toolkit &#xD;&#xA;4. Data Science Box  &#xD;&#xA;&#xD;&#xA;       &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.udacity.com/course/ud617&#xD;&#xA;  [2]: http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html" />
  <row Id="1628" PostHistoryTypeId="5" PostId="624" RevisionGUID="a3fc09cb-4e31-431d-bc4d-63848aaaf2d9" CreationDate="2014-06-29T07:20:51.053" UserId="454" Comment="Appended Hadoop server name to CDH" Text="In most cases a practicing data scientist creates his own working environment on personal computed installing preferred software packages. Normally it is sufficient and efficient use of computing resources, because to run a virtual machine (VM) on your main machine you have to allocate a significant portion of RAM for it. The software will run noticeably slower on both the main and the virtual machine unless a lot of RAM. &#xD;&#xA;&#xD;&#xA;Due to this impact on speed it is not common to use VMs as main working environment but they are a good solution in several cases when there is a need of additional working environment. &#xD;&#xA;&#xD;&#xA;The VMs be considered when:&#xD;&#xA;&#xD;&#xA; 1. There is a need to easily replicate a number of identical computing&#xD;&#xA;    environments when teaching a course or doing a presentation on a&#xD;&#xA;    conference.&#xD;&#xA; 2. There is a need to save and recreate an exact environment for an experiment or a calculation.&#xD;&#xA; 3. There is a need to run a different OS or to test a solution on a tool that runs on a different OS. &#xD;&#xA; 4. One wants to try out a bundle of software tools before installing&#xD;&#xA;    them on the main machine. E.g. there is an opportunity to instal an instance of Hadoop (CDH) on a VM during an [Intro to Hadoop][1] course on Udacity. &#xD;&#xA; 5. VMs are sometimes used for fast deployment in the cloud like AWS EC, Rackspace etc.&#xD;&#xA;&#xD;&#xA;The VMs mentioned in the original question are made as easily installable data science software bundles. There are more than these two. This [blog post][2] by Jeroen Janssens gives a comparison of at least four:&#xD;&#xA;&#xD;&#xA;1. Data Science Toolbox &#xD;&#xA;2. Mining the Social Web &#xD;&#xA;3. Data Science Toolkit &#xD;&#xA;4. Data Science Box  &#xD;&#xA;&#xD;&#xA;       &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.udacity.com/course/ud617&#xD;&#xA;  [2]: http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html" />
  <row Id="1629" PostHistoryTypeId="2" PostId="625" RevisionGUID="a190b0eb-5442-4aa2-987b-28712fc8bb0c" CreationDate="2014-06-29T12:48:08.840" UserId="816" Text="Maybe you can check here - http://snap.stanford.edu/data/&#xD;&#xA;&#xD;&#xA;For each data set you will also see references of the works where they have been used" />
  <row Id="1630" PostHistoryTypeId="2" PostId="626" RevisionGUID="779c614a-2d2f-410a-830d-bbac8ea98969" CreationDate="2014-06-29T21:11:47.727" UserId="1155" Text="In my opinion, it is difficult to compare the performance when there is such a big difference of size. On this link, (please check it out here in Wikipedia http://en.wikipedia.org/wiki/Effect_size), you may see different strategies.&#xD;&#xA;&#xD;&#xA;The one I suggest is one related to the variance. For instance, consider the performance of the classifier (100%) and the person classifier (65%). The minimum error you commit with the former classifier is 100%. However, the minimum error you can commit with the latter classifier is 10e-5.&#xD;&#xA;&#xD;&#xA;So one way to compare classifier is to have on mind this **Rule of Three** (http://en.wikipedia.org/wiki/Rule_of_three_(statistics) where you can compare the performance and its variability.&#xD;&#xA;&#xD;&#xA;Other possibility is **F-measure** which is a combination of Precision and Recall and it is somehow independent to the effect size." />
  <row Id="1631" PostHistoryTypeId="2" PostId="627" RevisionGUID="fa6baaab-f6d1-4f18-bc04-af01d8975a3b" CreationDate="2014-06-29T22:44:02.560" UserId="843" Text="Roughly speaking, over-fitting typically occurs when the ratio&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;is too high.&#xD;&#xA;&#xD;&#xA;Think of over-fitting as a situation where your model learn the training data by heart instead of learning the big pictures which prevent it from being able to generalized to the test data: this happens when the model is too complex with respect to the size of the training data, that is to say when the size of the training data is to small in comparison with the model complexity.&#xD;&#xA;&#xD;&#xA;Examples: &#xD;&#xA;&#xD;&#xA;- if your data is in two dimensions, you have 10000 points in the training set and the model is a line, you are likely to *under*-fit.&#xD;&#xA;- if your data is in two dimensions, you have 10 points in the training set and the model is 100-degree polynomial, you are likely to *over*-fit.&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;From a theoretical standpoint, the amount of data you need to properly train your model is a crucial yet far-to-be-answered question in machine learning. One such approach to answer this question is the [VC dimension][3]. Another is the [bias-variance tradeoff](https://www.quora.com/Machine-Learning/What-is-an-intuitive-explanation-for-bias-variance-tradeoff/answer/Franck-Dernoncourt?share=1).&#xD;&#xA;&#xD;&#xA;From an empirical standpoint, people typically plot the training error and the test error on the same plot and make sure that they don't reduce the training error at the expense of the test error:&#xD;&#xA;&#xD;&#xA;![enter image description here][4]&#xD;&#xA;&#xD;&#xA;I would advise to watch [Coursera' Machine Learning course](https://www.coursera.org/course/ml), section &quot;10: Advice for applying Machine Learning&quot;.&#xD;&#xA;&#xD;&#xA;(PS: please go [here](http://meta.datascience.stackexchange.com/q/6/843) to ask for TeX support on this SE.) &#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/AVVpB.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/HH23h.png&#xD;&#xA;  [3]: http://math.stackexchange.com/a/656167/24265&#xD;&#xA;  [4]: http://i.stack.imgur.com/I7LiT.png" />
  <row Id="1632" PostHistoryTypeId="2" PostId="628" RevisionGUID="887b5083-cf6e-46d8-b6f3-8433378626dd" CreationDate="2014-06-29T23:14:39.513" UserId="843" Text="A model underfits when it is too simple with regards to the data it is trying to model.&#xD;&#xA;&#xD;&#xA;One way to detect such situation is to use the [bias–variance approach](http://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma), which can represented like this:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;Your model is underfitted when you have a high bias.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To know whether you have a too high bias or a too high variance, you view the phenomenon in terms of training and test errors:&#xD;&#xA;&#xD;&#xA;High bias: This learning curve shows high error on both the training and test sets, so the algorithm is suffering from high bias:&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;High variance: This learning curve shows a large gap between training and test set errors, so the algorithm is suffering from high variance.&#xD;&#xA;&#xD;&#xA;![enter image description here][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If an algorithm is suffering from high variance:&#xD;&#xA;&#xD;&#xA;- more data will probably help&#xD;&#xA;- otherwise reduce the model complexity&#xD;&#xA;&#xD;&#xA;If an algorithm is suffering from high bias:&#xD;&#xA;&#xD;&#xA;- increase reduce the model complexity&#xD;&#xA;&#xD;&#xA;I would advise to watch [Coursera' Machine Learning course](https://www.coursera.org/course/ml), section &quot;10: Advice for applying Machine Learning&quot;, from which I took the above graphs.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/t0zit.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/KFjM4.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/Y9bpL.png&#xD;&#xA;  [4]: http://i.stack.imgur.com/Ypj9y.png&#xD;&#xA;  [5]: http://i.stack.imgur.com/xtDWb.png" />
  <row Id="1633" PostHistoryTypeId="2" PostId="629" RevisionGUID="30078f3c-8485-4b61-8e57-fd185f89fc39" CreationDate="2014-06-30T01:51:11.027" UserId="62" Text="#Do you need a VM?&#xD;&#xA;&#xD;&#xA;You need to keep in mind that a virtual machine is a software emulation of your own or another machine hardware configuration that can run an operating systems. In most basic terms, it acts as a layer interfacing between the virtual OS, and your own OS which then communicates with the lower level hardware to provide support to the virtual OS. What this means for you is:&#xD;&#xA;&#xD;&#xA;#Cons&#xD;&#xA;&#xD;&#xA;###Hardware Support &#xD;&#xA;&#xD;&#xA;A drawback of virtual machine technology is that it supports only the hardware that both the virtual machine hypervisor and the guest operating system support. Even if the guest operating system supports the physical hardware, it sees only the virtual hardware presented by the virtual machine. &#xD;&#xA;The second aspect of virtual machine hardware support is the hardware presented to the guest operating system. No matter the hardware in the host, the hardware presented to the guest environment is usually the same (with the exception of the CPU, which shows through). For example, VMware GSX Server presents an AMD PCnet32 Fast Ethernet card or an optimized VMware-proprietary network card, depending on which you choose. The network card in the host machine does not matter. VMware GSX Server performs the translation between the guest environment's network card and the host environment's network card. This is great for standardization, but it also means that host hardware that VMware does not understand will not be present in the guest environment.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;###Performance Penalty&#xD;&#xA;&#xD;&#xA;Virtual machine technology imposes a performance penalty from running an additional layer above the physical hardware but beneath the guest operating system. The performance penalty varies based on the virtualization software used and the guest software being run. This is significant.&#xD;&#xA;&#xD;&#xA;#Pros&#xD;&#xA;&#xD;&#xA;###Isolation&#xD;&#xA;&#xD;&#xA;&gt; One of the key reasons to employ virtualization is to isolate applications from each other. Running everything on one machine would be great if it all worked, but many times it results in undesirable interactions or even outright conflicts. The cause often is software problems or business requirements, such as the need for isolated security. Virtual machines allow you to isolate each application (or group of applications) in its own sandbox environment. The virtual machines can run on the same physical machine (simplifying IT hardware management), yet appear as independent machines to the software you are running. For all intents and purposes—except performance, the virtual machines are independent machines. If one virtual machine goes down due to application or operating system error, the others continue running, providing services your business needs to function smoothly.&#xD;&#xA;&#xD;&#xA;###Standardization&#xD;&#xA;&#xD;&#xA;&gt; Another key benefit virtual machines provide is standardization. The hardware that is presented to the guest operating system is uniform for the most part, usually with the CPU being the only component that is &quot;pass-through&quot; in the sense that the guest sees what is on the host. A standardized hardware platform reduces support costs and increases the share of IT resources that you can devote to accomplishing goals that give your business a competitive advantage. The host machines can be different (as indeed they often are when hardware is acquired at different times), but the virtual machines will appear to be the same across all of them.&#xD;&#xA;&#xD;&#xA;###Ease of Testing&#xD;&#xA;&#xD;&#xA;&gt; Virtual machines let you test scenarios easily. Most virtual machine software today provides snapshot and rollback capabilities. This means you can stop a virtual machine, create a snapshot, perform more operations in the virtual machine, and then roll back again and again until you have finished your testing. This is very handy for software development, but it is also useful for system administration. Admins can snapshot a system and install some software or make some configuration changes that they suspect may destabilize the system. If the software installs or changes work, then the admin can commit the updates. If the updates damage or destroy the system, the admin can roll them back.&#xD;&#xA;Virtual machines also facilitate scenario testing by enabling virtual networks. In VMware Workstation, for example, you can set up multiple virtual machines on a virtual network with configurable parameters, such as packet loss from congestion and latency. You can thus test timing-sensitive or load-sensitive applications to see how they perform under the stress of a simulated heavy workload.&#xD;&#xA;&#xD;&#xA;###Mobility&#xD;&#xA;&#xD;&#xA;&gt; Virtual machines are easy to move between physical machines. Most of the virtual machine software on the market today stores a whole disk in the guest environment as a single file in the host environment. Snapshot and rollback capabilities are implemented by storing the change in state in a separate file in the host information. Having a single file represent an entire guest environment disk promotes the mobility of virtual machines. Transferring the virtual machine to another physical machine is as easy as moving the virtual disk file and some configuration files to the other physical machine. Deploying another copy of a virtual machine is the same as transferring a virtual machine, except that instead of moving the files, you copy them.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;#Which VM should I use if I am starting out?&#xD;&#xA;&#xD;&#xA;The Data Science Box or the Data Science Toolbox are your best bets if you just getting into data science. They have the basic software that you will need, with the primary difference being the virtual environment in which each of these can run. The DSB can run on AWS while the DST can run on Virtual Box (which is the most common tool used for VMs).&#xD;&#xA;&#xD;&#xA;#Sources&#xD;&#xA;&#xD;&#xA;- http://www.devx.com/vmspecialreport/Article/30383&#xD;&#xA;- http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html&#xD;&#xA;" />
  <row Id="1634" PostHistoryTypeId="2" PostId="630" RevisionGUID="7c3aba73-2309-4f8f-8319-c9b11fd214ef" CreationDate="2014-06-30T04:00:35.077" UserId="609" Text="Here's a crazy idea: talk to the volunteers who know the neighborhoods and who have done door-to-door work before.  Get their advice and ideas.  They will probably have insights that no algorithm will produce, and those modifications will be valuable to any computer-generated route list.  One example: Avoiding crossing heavily traveled streets with slow lights or no lights.  Another example: pairs of volunteers working on opposite sides of the same street will feel safer than a volunteer working that street alone." />
  <row Id="1639" PostHistoryTypeId="5" PostId="535" RevisionGUID="60e32443-2106-4d9f-8059-39a789379fe9" CreationDate="2014-06-30T05:35:26.340" UserId="1107" Comment="Added a python version, not sure it works as intended" Text="As I understood, Document 1 and Document 2 may have different number of keys. And you wand to get final similarity evaluation between 0 and 1. If so, I would propose following algorithm:&#xD;&#xA;&#xD;&#xA;1. Sum of max. vals is equal to 0.&#xD;&#xA;2. Select maximum value from doc-doc matrix and add it to Sum of max. vals.&#xD;&#xA;3. Remove row and column with maximum value from the matrix.&#xD;&#xA;4. Repeat steps 2-3 until rows or columns are ended.&#xD;&#xA;5. Denominate Sum of max. vals by average number of key words in two texts.&#xD;&#xA;&#xD;&#xA;Final estimation would be equal to 1, if both documents have identical length, and every word from Doc 1 has equivalent in Doc 2.&#xD;&#xA;&#xD;&#xA;You haven't mentioned software, you are using, but here is **R** example of function, computing such similarity (it takes object of class matrix as input):&#xD;&#xA;&#xD;&#xA;    eval.sim &lt;- function(sim.matrix){&#xD;&#xA;      similarity &lt;- 0&#xD;&#xA;      denominator &lt;- sum(dim(sim.matrix)) / 2&#xD;&#xA;      for(i in 1:(min(c(nrow(sim.matrix), ncol(sim.matrix))) - 1)){&#xD;&#xA;        extract &lt;- which(sim.matrix == max(sim.matrix), arr.ind=T)[1, ]&#xD;&#xA;        similarity &lt;- similarity + sim.matrix[extract[1], extract[2]]&#xD;&#xA;        sim.matrix &lt;- sim.matrix[-extract[1], -extract[2]]&#xD;&#xA;      }&#xD;&#xA;      similarity &lt;- similarity + max(sm.copy)&#xD;&#xA;      similarity &lt;- similarity / denominator&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;In python - &#xD;&#xA;&#xD;&#xA;    def score_matrix(sim_matrix):&#xD;&#xA;&#xD;&#xA;	   similarity = 0&#xD;&#xA;	   denominator = sum(sim_matrix.shape) / 2&#xD;&#xA;	   max_value = 0&#xD;&#xA;&#xD;&#xA;	   for i in range(1, min(sim_matrix.shape) + 1):&#xD;&#xA;&#xD;&#xA;		  x, y = np.where(sim_matrix == np.max(sim_matrix))&#xD;&#xA;	&#xD;&#xA;		  if i == 1:&#xD;&#xA;			max_value = sim_matrix[(x,y)][0]&#xD;&#xA;&#xD;&#xA;		  if any(j &lt;= 1 for j in sim_matrix.shape):&#xD;&#xA;			break&#xD;&#xA;&#xD;&#xA;		  similarity = similarity + sim_matrix[(x,y)][0]&#xD;&#xA;		  sim_matrix = np.delete(sim_matrix,(x),axis=0)&#xD;&#xA;		  sim_matrix = np.delete(sim_matrix,(y),axis=1)&#xD;&#xA;&#xD;&#xA;	   similarity = similarity + max_value&#xD;&#xA;	   similarity = similarity / denominator&#xD;&#xA;&#xD;&#xA;	   return similarity" />
  <row Id="1640" PostHistoryTypeId="24" PostId="535" RevisionGUID="60e32443-2106-4d9f-8059-39a789379fe9" CreationDate="2014-06-30T05:35:26.340" Comment="Proposed by 1107 approved by -1 edit id of 102" />
  <row Id="1641" PostHistoryTypeId="5" PostId="535" RevisionGUID="4b6f5cc6-0d2a-4b57-b4bc-32c3ebe11fa5" CreationDate="2014-06-30T05:35:26.340" UserId="941" Comment="Added a python version, not sure it works as intended, simplified python version" Text="As I understood, Document 1 and Document 2 may have different number of keys. And you wand to get final similarity evaluation between 0 and 1. If so, I would propose following algorithm:&#xD;&#xA;&#xD;&#xA;1. Sum of max. vals is equal to 0.&#xD;&#xA;2. Select maximum value from doc-doc matrix and add it to Sum of max. vals.&#xD;&#xA;3. Remove row and column with maximum value from the matrix.&#xD;&#xA;4. Repeat steps 2-3 until rows or columns are ended.&#xD;&#xA;5. Denominate Sum of max. vals by average number of key words in two texts.&#xD;&#xA;&#xD;&#xA;Final estimation would be equal to 1, if both documents have identical length, and every word from Doc 1 has equivalent in Doc 2.&#xD;&#xA;&#xD;&#xA;You haven't mentioned software, you are using, but here is **R** example of function, computing such similarity (it takes object of class matrix as input):&#xD;&#xA;&#xD;&#xA;    eval.sim &lt;- function(sim.matrix){&#xD;&#xA;      similarity &lt;- 0&#xD;&#xA;      denominator &lt;- sum(dim(sim.matrix)) / 2&#xD;&#xA;      for(i in 1:(min(c(nrow(sim.matrix), ncol(sim.matrix))) - 1)){&#xD;&#xA;        extract &lt;- which(sim.matrix == max(sim.matrix), arr.ind=T)[1, ]&#xD;&#xA;        similarity &lt;- similarity + sim.matrix[extract[1], extract[2]]&#xD;&#xA;        sim.matrix &lt;- sim.matrix[-extract[1], -extract[2]]&#xD;&#xA;      }&#xD;&#xA;      similarity &lt;- similarity + max(sm.copy)&#xD;&#xA;      similarity &lt;- similarity / denominator&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;In python - &#xD;&#xA;&#xD;&#xA;    import numpy as np&#xD;&#xA;&#xD;&#xA;    def score_matrix(sim_matrix):&#xD;&#xA;        similarity = 0&#xD;&#xA;        denominator = sum(sim_matrix.shape) / 2&#xD;&#xA;        for i in range(min(sim_matrix.shape)):&#xD;&#xA;            x, y = np.where(sim_matrix == np.max(sim_matrix))[0][0], np.where(sim_matrix == np.max(sim_matrix))[1][0]&#xD;&#xA;            similarity += sim_matrix[x, y]&#xD;&#xA;            sim_matrix = np.delete(sim_matrix,(x),axis=0)&#xD;&#xA;            sim_matrix = np.delete(sim_matrix,(y),axis=1)&#xD;&#xA;        return similarity / denominator&#xD;&#xA;" />
  <row Id="1643" PostHistoryTypeId="2" PostId="634" RevisionGUID="123d3a1f-5f76-42f5-b1f1-f1fd766523ae" CreationDate="2014-06-30T09:43:01.940" UserId="1271" Text="I'm working on a fraud detection system. In this field, new frauds appear regularly, so that new features have to be added to the model on ongoing basis. &#xD;&#xA;&#xD;&#xA;I wonder what is the best way to handle it (from the development process perspective)? Just adding a new feature into the feature vector and re-training the classifier seems to be a naive approach, because too much time will be spent for re-learning of the old features.&#xD;&#xA;&#xD;&#xA;I'm thinking along the way of training a classifier for each feature (or a couple of related features), and then combining the results of those classifiers with an overall classifier. Are there any drawbacks of this approach? How to choose the classification algorithm for the overall classifier?" />
  <row Id="1644" PostHistoryTypeId="1" PostId="634" RevisionGUID="123d3a1f-5f76-42f5-b1f1-f1fd766523ae" CreationDate="2014-06-30T09:43:01.940" UserId="1271" Text="Handling flexible feature set" />
  <row Id="1645" PostHistoryTypeId="3" PostId="634" RevisionGUID="123d3a1f-5f76-42f5-b1f1-f1fd766523ae" CreationDate="2014-06-30T09:43:01.940" UserId="1271" Text="&lt;machine-learning&gt;&lt;bigdata&gt;" />
  <row Id="1646" PostHistoryTypeId="2" PostId="635" RevisionGUID="46f5b0b4-5c9e-4725-9b38-8ec829eeda64" CreationDate="2014-06-30T10:47:06.453" UserId="21" Text="In an ideal world, you retain all of your historical data, and do indeed run a new model with the new feature extracted retroactively from historical data. I'd argue that the computing resource spent on this is quite useful actually. Is it really a problem?&#xD;&#xA;&#xD;&#xA;Yes, it's a widely accepted technique to build an ensemble of classifiers and combine their results. You can build a new model in parallel just on new features and average in its prediction. This should add value, but, you will never capture interaction between the new and old features this way, since they will never appear together in a classifier. &#xD;&#xA;" />
  <row Id="1647" PostHistoryTypeId="2" PostId="636" RevisionGUID="3abc3f93-272c-416c-9896-30e7b5c35970" CreationDate="2014-06-30T12:05:38.597" UserId="1273" Text="I am an ml noob. I have a task at hand of predicting click probability given user information like city, state, os version, os family, device, browser family browser version, city, etc. I have been recommended to try logit since logit seems to be what MS and Google are using too. I have some questions regarding logistic regression like:&#xD;&#xA;&#xD;&#xA;Click and non click is a very very unbalanced class and the simple glm predictions do not look good. How to make the data work through this?&#xD;&#xA;&#xD;&#xA;All variables I have are categorical and things like device and city can be numerous. Also the frequency of occurrence of some devices or some cities can be very very low. So how to deal with what I can say is a very random variety of categorical variables?&#xD;&#xA;&#xD;&#xA;One of the variables that we get is device id also. This is a very unique feature that can be translated to a user's identity. How to make use of it in logit, or should it be used in a completely different model based on a user identity?" />
  <row Id="1648" PostHistoryTypeId="1" PostId="636" RevisionGUID="3abc3f93-272c-416c-9896-30e7b5c35970" CreationDate="2014-06-30T12:05:38.597" UserId="1273" Text="Data preparation and machine algo for click prediction" />
  <row Id="1649" PostHistoryTypeId="3" PostId="636" RevisionGUID="3abc3f93-272c-416c-9896-30e7b5c35970" CreationDate="2014-06-30T12:05:38.597" UserId="1273" Text="&lt;machine-learning&gt;&lt;bigdata&gt;&lt;data-mining&gt;&lt;dataset&gt;&lt;data-cleaning&gt;" />
  <row Id="1650" PostHistoryTypeId="2" PostId="637" RevisionGUID="0ffc2231-a1ed-48f6-9c2d-ce366b6a0bd5" CreationDate="2014-06-30T13:56:10.753" UserId="1085" Text="Here's an idea that just popped out of the blue – what if you make use of [Random Subspace Sampling][1] (as in fact Sean Owen already suggested) to train a bunch of new classifiers every time a new feature appears (using a random feature subset, including the new set of features). You could train those models on a subset of samples as well to save some training time.&#xD;&#xA;&#xD;&#xA;This way you can have new classifiers possibly taking on both new and old features, and at the same time keeping your old classifiers. You might even, perhaps using a cross validation technique to measure each classifier's performance, be able to kill-off the worst performing ones after a while, to avoid a bloated model.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Random_subspace_method" />
  <row Id="1653" PostHistoryTypeId="2" PostId="640" RevisionGUID="c8afdff9-81c5-4b02-b18d-0e2586eeb2fc" CreationDate="2014-06-30T20:51:58.640" UserId="684" Text="I'm currently working on a project that would benefit from personalized predictions.  Given an input document, a set of output documents, and a history of user behavior, I'd like to predict which of the output documents are clicked.  &#xD;&#xA;&#xD;&#xA;In short, I'm wondering what the typical approach to this kind of personalization problem is.  Are models trained per user, or does a single global model take in summary statistics of past user behavior to help inform that decision?  Per user models won't be accurate until the user has been active for a while, while most global models have to take in a fixed length feature vector (meaning we more or less have to compress a stream of past events into a smaller number of summary statistics).  " />
  <row Id="1654" PostHistoryTypeId="1" PostId="640" RevisionGUID="c8afdff9-81c5-4b02-b18d-0e2586eeb2fc" CreationDate="2014-06-30T20:51:58.640" UserId="684" Text="Large Scale Personalization - Per User vs Global Models" />
  <row Id="1655" PostHistoryTypeId="3" PostId="640" RevisionGUID="c8afdff9-81c5-4b02-b18d-0e2586eeb2fc" CreationDate="2014-06-30T20:51:58.640" UserId="684" Text="&lt;classification&gt;" />
  <row Id="1656" PostHistoryTypeId="2" PostId="641" RevisionGUID="eac507ab-c47b-429c-ba38-53705d1c9ed9" CreationDate="2014-06-30T21:02:05.053" UserId="684" Text="I'm currently searching for labeled datasets to use to train a model to extract named entities from informal text (think something similar to tweets). Because capitalization and grammar are often lacking in the documents in my dataset, I'm looking for out of domain data that's a bit more &quot;informal&quot; than the news articles and journal entries that many of today's state of the art named entity recognition systems are trained on.  Any recommendations?  So far I've only been able to locate 50k tokens from twitter published here: https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt" />
  <row Id="1657" PostHistoryTypeId="1" PostId="641" RevisionGUID="eac507ab-c47b-429c-ba38-53705d1c9ed9" CreationDate="2014-06-30T21:02:05.053" UserId="684" Text="Dataset for Named Entity Recognition on Informal Text" />
  <row Id="1658" PostHistoryTypeId="3" PostId="641" RevisionGUID="eac507ab-c47b-429c-ba38-53705d1c9ed9" CreationDate="2014-06-30T21:02:05.053" UserId="684" Text="&lt;dataset&gt;&lt;nlp&gt;" />
  <row Id="1659" PostHistoryTypeId="2" PostId="642" RevisionGUID="e91f1737-5d4b-43ff-84cf-027a8b26336d" CreationDate="2014-06-30T23:10:53.397" UserId="159" Text="The answer to this question is going to vary pretty wildly depending on the size and nature of your data. At a high level, you could think of it as a special case of multilevel models; you have the option of estimating a model with complete pooling (i.e., a universal model that doesn't distinguish between users), models with no pooling (a separate model for each user), and partially pooled models (a mixture of the two). You should really read Andrew Gelman on this topic if you're interested.&#xD;&#xA;&#xD;&#xA;You can also think of this as a learning-to-rank problem that either tries to produce point-wise estimates using a single function or instead tries to optimize on some list-wise loss function (e.g., NDCG). &#xD;&#xA;&#xD;&#xA;As with most machine learning problems, it all depends on what kind of data you have, the quality of it, the sparseness of it, and what kinds of features you are able to extract from it. If you have reason to believe that each and every user is going to be pretty unique in their behavior, you might want to build a per-user model, but that's going to be unwieldy fast -- and what do you do when you are faced with a new user?" />
  <row Id="1660" PostHistoryTypeId="2" PostId="643" RevisionGUID="1b0d02a3-470b-4590-b904-06b9e16f99c6" CreationDate="2014-07-01T08:04:30.127" UserId="127" Text="I guess you say that you want to use 3-fold cross-validation because you know something about your data (that using k=10 would cause overfitting? I'm curious to your reasoning). I am not sure that you know this, if not then you can simply use a larger k.&#xD;&#xA;&#xD;&#xA;If you still think that you cannot use standard k-fold cross-validation, then you could modify the algorithm a bit: say that you split the data into 30 folds and each time use 20 for training and 10 for evaluation (and then shift up one fold and use the first and the last 9 as evaluation and the rest as training). This means that you're able to use all your data.&#xD;&#xA;&#xD;&#xA;When I use k-fold cross-validation I usually run the process multiple times with a different randomisation to make sure that I have sufficient data, if you don't you will see different performances depending on the randomisation. In such cases I would suggest sampling. The trick then is to do it often enough." />
  <row Id="1661" PostHistoryTypeId="2" PostId="644" RevisionGUID="4df24e06-da1a-433e-bf14-43ade7e858ee" CreationDate="2014-07-01T13:44:23.290" UserId="754" Text="For a recommendation system I'm using cosign similarity to compute similarities between items. However, for items with small amounts of data I'd like to bin them under a general &quot;average&quot; category (in the general not mathematical sense). To accomplish this I'm currently trying to create a synthetic observation to represent that middle of the road point.&#xD;&#xA;&#xD;&#xA;So for example if these were my observations (rows are observations, cols are features):&#xD;&#xA;&#xD;&#xA;    [[0, 0, 0, 1, 1, 1, 0, 1, 0],&#xD;&#xA;     [1, 0, 1, 0, 0, 0, 1, 0, 0],&#xD;&#xA;     [1, 1, 1, 1, 0, 1, 0, 1, 1],&#xD;&#xA;     [0, 0, 1, 0, 0, 1, 0, 1, 0]]&#xD;&#xA;&#xD;&#xA;A strategy where I'd simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I'd then append to the matrix before doing the similarity calculation.&#xD;&#xA;&#xD;&#xA;    [ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]&#xD;&#xA;&#xD;&#xA;While this might work well with certain similarity metrics (e.g. L1 distance) I'm sure there are much better ways for cosign similarity. Though, at the moment, I'm having trouble reasoning through way through angles between lines in high dimensional space.&#xD;&#xA;&#xD;&#xA;Any ideas?" />
  <row Id="1662" PostHistoryTypeId="1" PostId="644" RevisionGUID="4df24e06-da1a-433e-bf14-43ade7e858ee" CreationDate="2014-07-01T13:44:23.290" UserId="754" Text="Create most &quot;average&quot; cosign similarity observation" />
  <row Id="1663" PostHistoryTypeId="3" PostId="644" RevisionGUID="4df24e06-da1a-433e-bf14-43ade7e858ee" CreationDate="2014-07-01T13:44:23.290" UserId="754" Text="&lt;recommendation&gt;&lt;similarity&gt;" />
  <row Id="1664" PostHistoryTypeId="5" PostId="644" RevisionGUID="36d186ae-bc83-4cf7-8d6f-20f30d81b275" CreationDate="2014-07-01T14:18:38.743" UserId="754" Comment="edited body; edited title" Text="For a recommendation system I'm using cosine similarity to compute similarities between items. However, for items with small amounts of data I'd like to bin them under a general &quot;average&quot; category (in the general not mathematical sense). To accomplish this I'm currently trying to create a synthetic observation to represent that middle of the road point.&#xD;&#xA;&#xD;&#xA;So for example if these were my observations (rows are observations, cols are features):&#xD;&#xA;&#xD;&#xA;    [[0, 0, 0, 1, 1, 1, 0, 1, 0],&#xD;&#xA;     [1, 0, 1, 0, 0, 0, 1, 0, 0],&#xD;&#xA;     [1, 1, 1, 1, 0, 1, 0, 1, 1],&#xD;&#xA;     [0, 0, 1, 0, 0, 1, 0, 1, 0]]&#xD;&#xA;&#xD;&#xA;A strategy where I'd simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I'd then append to the matrix before doing the similarity calculation.&#xD;&#xA;&#xD;&#xA;    [ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]&#xD;&#xA;&#xD;&#xA;While this might work well with certain similarity metrics (e.g. L1 distance) I'm sure there are much better ways for cosine similarity. Though, at the moment, I'm having trouble reasoning through way through angles between lines in high dimensional space.&#xD;&#xA;&#xD;&#xA;Any ideas?" />
  <row Id="1665" PostHistoryTypeId="4" PostId="644" RevisionGUID="36d186ae-bc83-4cf7-8d6f-20f30d81b275" CreationDate="2014-07-01T14:18:38.743" UserId="754" Comment="edited body; edited title" Text="Create most &quot;average&quot; cosine similarity observation" />
  <row Id="1666" PostHistoryTypeId="5" PostId="644" RevisionGUID="c570ee47-b873-4369-9692-3f987929be0c" CreationDate="2014-07-01T15:33:10.697" UserId="754" Comment="deleted 5 characters in body" Text="For a recommendation system I'm using cosine similarity to compute similarities between items. However, for items with small amounts of data I'd like to bin them under a general &quot;average&quot; category (in the general not mathematical sense). To accomplish this I'm currently trying to create a synthetic observation to represent that middle of the road point.&#xD;&#xA;&#xD;&#xA;So for example if these were my observations (rows are observations, cols are features):&#xD;&#xA;&#xD;&#xA;    [[0, 0, 0, 1, 1, 1, 0, 1, 0],&#xD;&#xA;     [1, 0, 1, 0, 0, 0, 1, 0, 0],&#xD;&#xA;     [1, 1, 1, 1, 0, 1, 0, 1, 1],&#xD;&#xA;     [0, 0, 1, 0, 0, 1, 0, 1, 0]]&#xD;&#xA;&#xD;&#xA;A strategy where I'd simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I'd then append to the matrix before doing the similarity calculation.&#xD;&#xA;&#xD;&#xA;    [ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]&#xD;&#xA;&#xD;&#xA;While this might work well with certain similarity metrics (e.g. L1 distance) I'm sure there are much better ways for cosine similarity. Though, at the moment, I'm having trouble reasoning my way through angles between lines in high dimensional space.&#xD;&#xA;&#xD;&#xA;Any ideas?" />
  <row Id="1667" PostHistoryTypeId="2" PostId="645" RevisionGUID="e0afa250-206c-4168-99c4-e55405112a5b" CreationDate="2014-07-01T17:01:12.263" UserId="178" Text="You need to look at the confidence interval of the statistic.  This helps measure how much uncertainty in the statistic, which is largely a function of sample size." />
  <row Id="1668" PostHistoryTypeId="2" PostId="646" RevisionGUID="b8f4207e-b819-4fe9-b8b3-784a6e284e36" CreationDate="2014-07-01T17:03:31.907" UserId="1283" Text="I'm trying to build a nonparametric density function for a fairly large dataset that can be evaluated efficently, and can be updated efficiently when new points are added. There will only ever be a maximum of 4 independent variables, but we can start off with 2. Lets use a gaussian kernel. Let the result be a probability density function, i.e. its volume will be 1.&#xD;&#xA;&#xD;&#xA;In each evaluation, we can omit all points for which the evaluation point is outside a certain ellipsoid corresponding to the minimum gaussian value we care about. We can change this threshold for accuracy or performance, and the maximum number of points inside the threshold will depend on the chosen covariance matrix of the kernel. Then, we can evaluate the distribution approximately using the subset of points.&#xD;&#xA;&#xD;&#xA;If we use a fixed kernel, then we can use the eigenvalues and eigenvectors we get from the covariance matrix to transform each point so that the threshold ellipsoid is a fixed circle. We can then shove all the transformed points into a spatial index, and efficiently find all points within the required radius of the evaluation point.&#xD;&#xA;&#xD;&#xA;However, we would the kernel to be variable for two reasons. (1) to fit the data better, and (2) because adding or modifying points would require the fixed kernel to be updated, which would mean that the entire data set would need to be reindexed. With a variable kernel, we could make new/updated points only affect the closest points.&#xD;&#xA;&#xD;&#xA;Specifically, is there a spatial index that can efficiently find ellipses surrounding a given point from a set of around 10 million ellipses of different shapes and sizes?&#xD;&#xA;&#xD;&#xA;More generally though, does my approach look sound? I am open to answers like &quot;give up and precalculate a grid of results&quot;. Answers much appreciated!" />
  <row Id="1669" PostHistoryTypeId="1" PostId="646" RevisionGUID="b8f4207e-b819-4fe9-b8b3-784a6e284e36" CreationDate="2014-07-01T17:03:31.907" UserId="1283" Text="Spatial index for variable kernel nonparametric density" />
  <row Id="1670" PostHistoryTypeId="3" PostId="646" RevisionGUID="b8f4207e-b819-4fe9-b8b3-784a6e284e36" CreationDate="2014-07-01T17:03:31.907" UserId="1283" Text="&lt;data-indexing-techniques&gt;" />
  <row Id="1674" PostHistoryTypeId="2" PostId="648" RevisionGUID="0c6ad330-0ae0-4d62-ab98-23fdbb0c93c4" CreationDate="2014-07-01T18:48:13.757" UserId="984" Text="Singular Value Decomposition is a linear algebraic technique as a result of which the notion of normalization is hard to define. In principle, you can do this normalization by dividing each element A(i,j) of the matrix by the sum (or max) of the elements in that particular (ith) row, i.e. A(i,j) = A(i, j) / \sum_{k=1}^{n} A(i,k)&#xD;&#xA;&#xD;&#xA;However, a more elegant way to achieve this would be to apply a probabilistic dimensionality reduction technique such as [PLSA][1] or [LDA][2]. These dimensionality reduction techniques ensure that you always end up with probability values strictly between 0 and 1.  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" />
  <row Id="1675" PostHistoryTypeId="2" PostId="649" RevisionGUID="a55c2eb9-62a7-4b05-abea-59e2a9359fcf" CreationDate="2014-07-01T23:11:35.627" UserId="984" Text="You are doing the correct thing. Technically, this averaging leads to computing the [centroid][1] in the Euclidean space of a set of N points. The centroid works pretty well with cosine similarities (cosine of the angles between normalized vectors), e.g. [the Rocchio algorithm][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Centroid&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Rocchio_algorithm" />
  <row Id="1676" PostHistoryTypeId="2" PostId="650" RevisionGUID="cf39b222-ad94-474f-91f6-6bdc38fca16b" CreationDate="2014-07-02T08:40:09.773" UserId="172" Text="As @Debasis writes, what you are doing is correct. Do not worry about the relative values as the cosine similarity focusses on the angle, not the length of the vector. &#xD;&#xA;&#xD;&#xA;Note that the averages that you compute actually are the proportion of observations that have that feature in your subset. If the subset is sufficiently large, you can interpret them as probabilities.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1677" PostHistoryTypeId="5" PostId="536" RevisionGUID="5a234d8b-c9c3-41af-9ea9-b6170ed14a12" CreationDate="2014-07-02T08:48:41.517" UserId="24" Comment="Just tightening things up a bit to improve readability" Text="More often than not, data I am working with is not 100% clean. Even if it is reasonably clean, still there are portions that need to be fixed. &#xD;&#xA;&#xD;&#xA;When a fraction of data needs it, I write a script and incorporate it in data processing.&#xD;&#xA;&#xD;&#xA;But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let's focus on &quot;small data&quot;,  such as that in CSV files or a relational database.&#xD;&#xA;&#xD;&#xA;The practical problems I encountered:&#xD;&#xA;&#xD;&#xA;* Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).&#xD;&#xA;* Copying and modifying data may make a mess, as:&#xD;&#xA;  * Generating it again will destroy all fixes.&#xD;&#xA;  * When there are more errors of different kinds, too many copies of the same file result, and it is hard to keep track of them all.&#xD;&#xA;* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.&#xD;&#xA;&#xD;&#xA;What are the best practices in such a case as this?&#xD;&#xA;&#xD;&#xA;EDIT: The question is on the workflow, not whether to use it or not.&#xD;&#xA;&#xD;&#xA;(In my particular case I don't want the end-user to see misspelled cities and, even worse, see two points of data, for the same city but with different spelling; the data is small, ~500 different cities, so manual corrections do make sense.)" />
  <row Id="1678" PostHistoryTypeId="24" PostId="536" RevisionGUID="5a234d8b-c9c3-41af-9ea9-b6170ed14a12" CreationDate="2014-07-02T08:48:41.517" Comment="Proposed by 24 approved by 289 edit id of 105" />
  <row Id="1679" PostHistoryTypeId="2" PostId="651" RevisionGUID="9ce5fb04-3080-4787-8461-5a274c742513" CreationDate="2014-07-02T08:50:55.980" UserId="172" Text="The number of data in the class is sometimes referred to as the `support` of the classifier. It tells how much you can trust your result, like a p-value would allow you to trust or distrust some test.&#xD;&#xA;&#xD;&#xA;One approach you can use is to compute several classifier performance measures, not only precision and recall, but also true positive rate, false positive rate, specificity, sensitivity, positive likelihood, negative likelihood, etc. and see whether they are consistent with one another. If one of the measure maxes out (100%) and the other do not, it is often, in my experience, indicative of something went wrong (e.g. poor support, trivial classifier, biased classifier, etc.). See [this][1] for a list of classifier performance measures.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf" />
  <row Id="1680" PostHistoryTypeId="2" PostId="652" RevisionGUID="175deb96-e187-41a5-97a2-069500723adc" CreationDate="2014-07-02T08:58:25.413" UserId="172" Text="Simply, one common approach is to increase the complexity of the model, making it simple, and most probably underfitting at first, and increasing the complexity of the model  until early signs of overfitting are witnessed using a resampling technique such as cross validation, bootstrap, etc.&#xD;&#xA;&#xD;&#xA;You increase the complexity either by adding parameters (number of hidden neurons for artificial neural networks, number of trees in a random forest) or by relaxing the regularization (often named lambda, or C for support vector machines) term in your model." />
  <row Id="1681" PostHistoryTypeId="2" PostId="653" RevisionGUID="83be9aa6-45f6-4580-9795-d27756f36aec" CreationDate="2014-07-02T13:40:27.000" UserId="133" Text="I am trying to find which classification methods, that do not use a training phase, are available. &#xD;&#xA;&#xD;&#xA;The scenario is gene expression based classification, in which you have a matrix of gene expression of m genes (features) and n samples (observations).&#xD;&#xA;A signature for each class is also provided (that is a list of the features to consider to define to which class belongs a sample).&#xD;&#xA;&#xD;&#xA;An application (non-training) is the [Nearest Template Prediction][1] method. In this case it is computed the cosine distance between each sample and each signature (on the common set of features). Then each sample is assigned to the nearest class (the sample-class comparison resulting in a smaller distance). No already classified samples are needed in this case.&#xD;&#xA;&#xD;&#xA;A different application (training) is the [kNN][2] method, in which we have a set of already labeled samples. Then, each new sample is labeled depending on how are labeled the k nearest samples.&#xD;&#xA;&#xD;&#xA;Are there any other non-training methods?&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;  [1]: http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0015543&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" />
  <row Id="1682" PostHistoryTypeId="1" PostId="653" RevisionGUID="83be9aa6-45f6-4580-9795-d27756f36aec" CreationDate="2014-07-02T13:40:27.000" UserId="133" Text="Which non-training classification methods are available?" />
  <row Id="1683" PostHistoryTypeId="3" PostId="653" RevisionGUID="83be9aa6-45f6-4580-9795-d27756f36aec" CreationDate="2014-07-02T13:40:27.000" UserId="133" Text="&lt;classification&gt;" />
  <row Id="1684" PostHistoryTypeId="2" PostId="654" RevisionGUID="aee08f32-496d-4adc-91ef-9d4f34c35da2" CreationDate="2014-07-02T14:14:22.593" UserId="964" Text="What you are asking about is [Instance-Based Learning](http://en.wikipedia.org/wiki/Instance-based_learning). k-Nearest Neighbors (kNN) appears to be the most popular of these methods and is applicable to a wide variety of problem domains. Another general type of instance-based learning is [Analogical Modeling](http://en.wikipedia.org/wiki/Analogical_modeling), which uses instances as exemplars for comparison with new data.&#xD;&#xA;&#xD;&#xA;You referred to kNN as an application that uses training but that is not correct (the Wikipedia entry you linked is somewhat misleading in that regard). Yes, there are &quot;training examples&quot; (labeled instances) but the classifier doesn't learn/train from these data. Rather, they are only used whenever you actually want to classify a new instance, which is why it is considered a &quot;lazy&quot; learner.&#xD;&#xA;&#xD;&#xA;Note that the Nearest Template Prediction method you mention effectively is a form of kNN with `k=1` cosine distance as the distance measure." />
  <row Id="1685" PostHistoryTypeId="2" PostId="655" RevisionGUID="7d33531f-48f8-4537-9e37-4ed85f39e383" CreationDate="2014-07-02T14:40:55.413" UserId="1300" Text="I would like to use ANN in order to automate trading forex, preferebly USD/EUR or USD/GBP. I know this is hard and may not be straightforward. I have already read some parers, did some experiments but no much luck still. I would like to get advice from EXPERTS to make this work.&#xD;&#xA;&#xD;&#xA;Here is what I did so far:&#xD;&#xA;1.I got tick by tick data for the month of july 2013. It has bid/ask/bid volume/ask volume.&#xD;&#xA;2.Extracted all ticks for the time frame 12PM to 14PM for all days.&#xD;&#xA;3.From this data, created a data set where each entry consists of n bid values in sequence. &#xD;&#xA;4. Used that data to train an ANN with n-1 inputs and the output is the forecasted nth bid value.&#xD;&#xA;5. The ANN had n-1 inputs neurons, (n-1)*2 + 1 hidden and 1 out put neuron. Input layer had linear TF, hidden had log TF and output had linear TF.&#xD;&#xA;6. Trained the network with back propagation with n-125 first and then 10.&#xD;&#xA;&#xD;&#xA;For both n, the MSE did not drop below .5 and stayed at that value during full training. Assumes that this could be due to the time series being totally random, so used the R package to find partial autocorrelation on the data set. (pacf). This gave non zero values for 2 and 3 lags only. &#xD;&#xA;&#xD;&#xA;Question 1: What does this mean exactly? &#xD;&#xA;&#xD;&#xA;Then  used hurst exponent to evaluate the randomness.&#xD;&#xA;in R:&#xD;&#xA;hurst(values) showed values above .9? &#xD;&#xA;Question 2: It supposed to be nearly random and should have a value closer to .5?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Repeated the training of the ANN with n=3. The ANN was trained and was able to obtain a pretty low value for MSE. However, the calculated output from this ANN does not differ much from the n-1 th bid value. It looks like ANN just takes the last bid as the next bid! Tries different networks structures (All multilayer perceptions), different training params, etc, but results are same.&#xD;&#xA;&#xD;&#xA;Question 3: Any suggestions to improve the accuracy? Any other training methods than back propogation? &#xD;&#xA;&#xD;&#xA;" />
  <row Id="1686" PostHistoryTypeId="1" PostId="655" RevisionGUID="7d33531f-48f8-4537-9e37-4ed85f39e383" CreationDate="2014-07-02T14:40:55.413" UserId="1300" Text="Forex forecasting with neural networks" />
  <row Id="1687" PostHistoryTypeId="3" PostId="655" RevisionGUID="7d33531f-48f8-4537-9e37-4ed85f39e383" CreationDate="2014-07-02T14:40:55.413" UserId="1300" Text="&lt;neuralnetwork&gt;" />
  <row Id="1688" PostHistoryTypeId="5" PostId="654" RevisionGUID="7b2b43b6-98e0-45ad-bd14-bc5652ae0826" CreationDate="2014-07-02T14:46:55.010" UserId="964" Comment="added 4 characters in body" Text="What you are asking about is [Instance-Based Learning](http://en.wikipedia.org/wiki/Instance-based_learning). k-Nearest Neighbors (kNN) appears to be the most popular of these methods and is applicable to a wide variety of problem domains. Another general type of instance-based learning is [Analogical Modeling](http://en.wikipedia.org/wiki/Analogical_modeling), which uses instances as exemplars for comparison with new data.&#xD;&#xA;&#xD;&#xA;You referred to kNN as an application that uses training but that is not correct (the Wikipedia entry you linked is somewhat misleading in that regard). Yes, there are &quot;training examples&quot; (labeled instances) but the classifier doesn't learn/train from these data. Rather, they are only used whenever you actually want to classify a new instance, which is why it is considered a &quot;lazy&quot; learner.&#xD;&#xA;&#xD;&#xA;Note that the Nearest Template Prediction method you mention effectively is a form of kNN with `k=1` and cosine distance as the distance measure." />
  <row Id="1689" PostHistoryTypeId="2" PostId="656" RevisionGUID="bdde9df7-5e92-4b05-a292-876aa9a1e41b" CreationDate="2014-07-02T15:52:44.470" UserId="1301" Text="Some suggestions&#xD;&#xA;&#xD;&#xA;1. Remove items appearing too infrequently in the data. That will reduce the dimensionality by several orders of magnitude. If a feature occurs less than say 10 times, it's likely that it's not adding any predictive value, and it may lead to overfitting due to low frequency&#xD;&#xA;2. Try a Linear SVM instead. They handle large dimensional data very well in terms of not overfitting. They also often have the option to assign relative weights to different classes, which may help address your unbalanced problem above. The sklearn svm (which simply wraps some other packages such as libsvm) has this option.&#xD;&#xA;3. Don't use the ID column. Producing a model per user will most probably lead to overfitting. Instead, feed in attributes that describe the user that allows the model to generalize over similar users. You could try fitting a separate model per user, but you need a lot of data per user to do this well.&#xD;&#xA;4. It sounds like you really need to try some feature selection here, to reduce the dimensionality of the problem. But try 1 and 2 first, as they may give you good results sooner (although the end solution may still work better with some good feature selection). Sklearn again has a number of options for feature selection.&#xD;&#xA;" />
  <row Id="1690" PostHistoryTypeId="2" PostId="657" RevisionGUID="28a76c3d-79e5-4f91-905b-fc45382e9d30" CreationDate="2014-07-02T16:41:47.467" UserId="133" Text="Suppose I want to use CART as classification tree (I want a categorical response). I have the training set, and I split it using observation labels.&#xD;&#xA;&#xD;&#xA;Now, to build the decision tree (classification tree) how are selected the features to decide which label apply to testing observations?&#xD;&#xA;&#xD;&#xA;Supposing we are working on gene expression matrix, in which each element is a real number, is that done using features that are more distant between classes?&#xD;&#xA;" />
  <row Id="1691" PostHistoryTypeId="1" PostId="657" RevisionGUID="28a76c3d-79e5-4f91-905b-fc45382e9d30" CreationDate="2014-07-02T16:41:47.467" UserId="133" Text="How are selected the features for a decision tree in CART?" />
  <row Id="1692" PostHistoryTypeId="3" PostId="657" RevisionGUID="28a76c3d-79e5-4f91-905b-fc45382e9d30" CreationDate="2014-07-02T16:41:47.467" UserId="133" Text="&lt;classification&gt;" />
  <row Id="1693" PostHistoryTypeId="5" PostId="628" RevisionGUID="bf053754-ae18-4cac-bfb9-f1721e0c5335" CreationDate="2014-07-02T16:55:27.367" UserId="843" Comment="deleted 7 characters in body" Text="A model underfits when it is too simple with regards to the data it is trying to model.&#xD;&#xA;&#xD;&#xA;One way to detect such situation is to use the [bias–variance approach](http://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma), which can represented like this:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;Your model is underfitted when you have a high bias.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To know whether you have a too high bias or a too high variance, you view the phenomenon in terms of training and test errors:&#xD;&#xA;&#xD;&#xA;High bias: This learning curve shows high error on both the training and test sets, so the algorithm is suffering from high bias:&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;High variance: This learning curve shows a large gap between training and test set errors, so the algorithm is suffering from high variance.&#xD;&#xA;&#xD;&#xA;![enter image description here][4]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If an algorithm is suffering from high variance:&#xD;&#xA;&#xD;&#xA;- more data will probably help&#xD;&#xA;- otherwise reduce the model complexity&#xD;&#xA;&#xD;&#xA;If an algorithm is suffering from high bias:&#xD;&#xA;&#xD;&#xA;- increase the model complexity&#xD;&#xA;&#xD;&#xA;I would advise to watch [Coursera' Machine Learning course](https://www.coursera.org/course/ml), section &quot;10: Advice for applying Machine Learning&quot;, from which I took the above graphs.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/t0zit.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/KFjM4.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/Y9bpL.png&#xD;&#xA;  [4]: http://i.stack.imgur.com/Ypj9y.png&#xD;&#xA;  [5]: http://i.stack.imgur.com/xtDWb.png" />
  <row Id="1694" PostHistoryTypeId="2" PostId="658" RevisionGUID="8a0d0ec8-456a-4d03-8628-10730ae66039" CreationDate="2014-07-02T20:54:53.767" UserId="1306" Text="I would like to use another type of data, not atomic data, as a feature for a prediction. &#xD;&#xA;Suppose I have a Table with those Features:&#xD;&#xA;- Column 1: Categorical - House&#xD;&#xA;- Column 2: Numerical - 23.22&#xD;&#xA;- Column 3: A Vector - [ 12, 22, 32 ]&#xD;&#xA;- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]&#xD;&#xA;- Column 5: A List [ 122, Boolean ]&#xD;&#xA;I would like to predict/classify ... Columns 2 ... for example....&#xD;&#xA;&#xD;&#xA;I am making a Software to automatically respond questions... Any type...like &quot;Where Foo was Born ?&quot; ...&#xD;&#xA;&#xD;&#xA;I first make a query to a search engine ---&gt;&gt;&gt; then I get some Text data as a Result.&#xD;&#xA;So I do all the Parsing Staff... Tagging, Stemming, Parsing, Splitting... &#xD;&#xA;My first approach was to make a table, each row with a line of text.. and a lot of Features...like ... First Word ... Tag of First Word.. Chunks, etc..&#xD;&#xA;But with this approach I am missing the relationships between the Sentences. &#xD;&#xA;&#xD;&#xA;I would like to know if there is an algorithm that look inside the Tree Structures... Vectors... and make the relations and extract whatever is relevant for predicting/classifying.&#xD;&#xA;I rather know a library that does that then an algorithm that I have to implement...&#xD;&#xA;&#xD;&#xA;Thank you very much !" />
  <row Id="1695" PostHistoryTypeId="1" PostId="658" RevisionGUID="8a0d0ec8-456a-4d03-8628-10730ae66039" CreationDate="2014-07-02T20:54:53.767" UserId="1306" Text="Prediction with not atomic features" />
  <row Id="1696" PostHistoryTypeId="3" PostId="658" RevisionGUID="8a0d0ec8-456a-4d03-8628-10730ae66039" CreationDate="2014-07-02T20:54:53.767" UserId="1306" Text="&lt;machine-learning&gt;" />
  <row Id="1697" PostHistoryTypeId="5" PostId="655" RevisionGUID="6ab92a34-0931-445d-94b7-ba705626aaa1" CreationDate="2014-07-02T22:18:40.597" UserId="322" Comment="formatting, grammar, spelling" Text="I would like to use ANN to automate trading currencies, preferably USD/EUR or USD/GBP. I know this is hard and may not be straightforward. I have already read some papers and done some experiments but without much luck. I would like to get advice from EXPERTS to make this work.&#xD;&#xA;&#xD;&#xA;Here is what I did so far:&#xD;&#xA;&#xD;&#xA;1. I got tick by tick data for the month of july 2013. It has bid/ask/bid volume/ask volume.&#xD;&#xA;2. Extracted all ticks for the time frame 12PM to 14PM for all days.&#xD;&#xA;3. From this data, created a data set where each entry consists of n bid values in sequence. &#xD;&#xA;4. Used that data to train an ANN with n-1 inputs and the output is the forecasted nth bid value.&#xD;&#xA;5. The ANN had n-1 inputs neurons, (n-1)*2 + 1 hidden and 1 output neuron. Input layer had linear TF, hidden had log TF and output had linear TF.&#xD;&#xA;6. Trained the network with back propagation with n-125 first and then 10.&#xD;&#xA;&#xD;&#xA;For both n, the MSE did not drop below 0.5 and stayed at that value during full training. Assuming that this could be due to the time series being totally random, I used the R package to find partial autocorrelation on the data set (pacf). This gave non zero values for 2 and 3 lags only. &#xD;&#xA;&#xD;&#xA;Question 1: What does this mean exactly? &#xD;&#xA;&#xD;&#xA;Then I used hurst exponent to evaluate the randomness. In R, hurst(values) showed values above 0.9.&#xD;&#xA;&#xD;&#xA;Question 2: It is supposed to be nearly random. Should it have a value closer to 0.5?&#xD;&#xA;&#xD;&#xA;I repeated the training of the ANN with n=3. The ANN was trained and was able to obtain a pretty low value for MSE. However, the calculated output from this ANN does not differ much from the (n-1)th bid value. It looks like ANN just takes the last bid as the next bid! I tried different network structures (all multilayer perceptions), different training parameters, etc, but results are same.&#xD;&#xA;&#xD;&#xA;Question 3: How can I improve the accuracy? Are there any other training methods than backpropagation?" />
  <row Id="1698" PostHistoryTypeId="4" PostId="655" RevisionGUID="6ab92a34-0931-445d-94b7-ba705626aaa1" CreationDate="2014-07-02T22:18:40.597" UserId="322" Comment="formatting, grammar, spelling" Text="Foreign exchange market forecasting with neural networks" />
  <row Id="1699" PostHistoryTypeId="24" PostId="655" RevisionGUID="6ab92a34-0931-445d-94b7-ba705626aaa1" CreationDate="2014-07-02T22:18:40.597" Comment="Proposed by 322 approved by 1300 edit id of 106" />
  <row Id="1700" PostHistoryTypeId="2" PostId="659" RevisionGUID="d0fc3a83-0ca9-4fcc-875f-a77d805275df" CreationDate="2014-07-03T08:00:10.823" UserId="1289" Text="As Steve Kallestad has said, this is a TSP problem, and there are wonderful free solvers to find approximate solutions.&#xD;&#xA;&#xD;&#xA;It may be too much work for what you are looking for, but you may try use one of those solvers in combination with the Google Maps API, to find real walking distances beetwen your coordinates:&#xD;&#xA;https://developers.google.com/maps/documentation/directions/#DirectionsRequests&#xD;&#xA;&#xD;&#xA;(I have never used this API, so I don't know how easy or effective it would be)&#xD;&#xA;" />
  <row Id="1702" PostHistoryTypeId="2" PostId="660" RevisionGUID="541c90b5-99c9-42af-8fac-925323a10df2" CreationDate="2014-07-03T10:49:50.993" UserId="1235" Text="I m new to RHadoop and also to RMR...&#xD;&#xA;I had an requirement to write a Mapreduce Job in R Mapreduce. I have Tried writing but While executing this it gives an Error.&#xD;&#xA;Tring to read the file from hdfs&#xD;&#xA;Error:&#xD;&#xA;------------&#xD;&#xA;&#xD;&#xA;    Error in mr(map = map, reduce = reduce, combine = combine, vectorized.reduce,  : &#xD;&#xA;       hadoop streaming failed with error code 1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Code :&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    Sys.setenv(HADOOP_HOME=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop&quot;)&#xD;&#xA;    Sys.setenv(HADOOP_CMD=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/bin/hadoop&quot;)&#xD;&#xA;&#xD;&#xA;    Sys.setenv(HADOOP_STREAMING=&quot;/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar&quot;)&#xD;&#xA;    library(rmr2)&#xD;&#xA;    library(rhdfs)&#xD;&#xA;    hdfs.init()&#xD;&#xA;    day_file = hdfs.file(&quot;/hdfs/bikes_LR/day.csv&quot;,&quot;r&quot;)&#xD;&#xA;    day_read = hdfs.read(day_file)&#xD;&#xA;    c = rawToChar(day_read)&#xD;&#xA;&#xD;&#xA;    XtX =&#xD;&#xA;      values(from.dfs(&#xD;&#xA;        mapreduce(&#xD;&#xA;          input = &quot;/hdfs/bikes_LR/day.csv&quot;,&#xD;&#xA;          map=&#xD;&#xA;            function(.,Xi){&#xD;&#xA;             yi =c[Xi[,1],]&#xD;&#xA;             Xi = Xi[,-1]&#xD;&#xA;             keyval(1,list(t(Xi)%*%Xi))&#xD;&#xA;           },&#xD;&#xA;      reduce = function(k,v )&#xD;&#xA;      {&#xD;&#xA;        vals =as.numeric(v)&#xD;&#xA;        keyval(k,sum(vals))&#xD;&#xA;      } ,&#xD;&#xA;      combine = TRUE)))[[1]]&#xD;&#xA;&#xD;&#xA;    XtY =&#xD;&#xA;     values(from.dfs(&#xD;&#xA;        mapreduce(&#xD;&#xA;         input = &quot;/hdfs/bikes_LR/day.csv&quot;,&#xD;&#xA;         map=&#xD;&#xA;           function(.,Xi){&#xD;&#xA;             yi =c[Xi[,1],]&#xD;&#xA;             Xi = Xi[,-1]&#xD;&#xA;            keyval(1,list(t(Xi)%*%yi))&#xD;&#xA;           },&#xD;&#xA;         reduce = TRUE ,&#xD;&#xA;         combine = TRUE)))[[1]]&#xD;&#xA;    solve(XtX,XtY)&#xD;&#xA;  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    Input:&#xD;&#xA;    ------------&#xD;&#xA;&#xD;&#xA;    instant,dteday,season,yr,mnth,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt&#xD;&#xA;    1,2011-01-01,1,0,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446,331,654,985&#xD;&#xA;    2,2011-01-02,1,0,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539,131,670,801&#xD;&#xA;    3,2011-01-03,1,0,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309,120,1229,1349&#xD;&#xA;    4,2011-01-04,1,0,1,0,2,1,1,0.2,0.212122,0.590435,0.160296,108,1454,1562&#xD;&#xA;    5,2011-01-05,1,0,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869,82,1518,1600&#xD;&#xA;    6,2011-01-06,1,0,1,0,4,1,1,0.204348,0.233209,0.518261,0.0895652,88,1518,1606&#xD;&#xA;    7,2011-01-07,1,0,1,0,5,1,2,0.196522,0.208839,0.498696,0.168726,148,1362,1510&#xD;&#xA;    8,2011-01-08,1,0,1,0,6,0,2,0.165,0.162254,0.535833,0.266804,68,891,959&#xD;&#xA;    9,2011-01-09,1,0,1,0,0,0,1,0.138333,0.116175,0.434167,0.36195,54,768,822&#xD;&#xA;    10,2011-01-10,1,0,1,0,1,1,1,0.150833,0.150888,0.482917,0.223267,41,1280,1321&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;     Please Suggest me any mistakes.&#xD;&#xA;" />
  <row Id="1703" PostHistoryTypeId="1" PostId="660" RevisionGUID="541c90b5-99c9-42af-8fac-925323a10df2" CreationDate="2014-07-03T10:49:50.993" UserId="1235" Text="Linear Regression in R Mapreduce(RHadoop)" />
  <row Id="1704" PostHistoryTypeId="3" PostId="660" RevisionGUID="541c90b5-99c9-42af-8fac-925323a10df2" CreationDate="2014-07-03T10:49:50.993" UserId="1235" Text="&lt;machine-learning&gt;&lt;r&gt;&lt;hadoop&gt;&lt;map-reduce&gt;" />
  <row Id="1705" PostHistoryTypeId="2" PostId="661" RevisionGUID="73689669-b92f-4765-8a8e-f38b4b931ade" CreationDate="2014-07-03T14:18:14.387" UserId="1314" Text="I Download and Installed CDH 5 Package succesfully on a Single Linux Node in Pseudo-distributed Mode on my CentOS 6.5&#xD;&#xA;&#xD;&#xA;Starting Hadoop and Verifying it is Working Properly as in the below link&#xD;&#xA;&#xD;&#xA;http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Quick-Start/cdh5qs_prereq.html#../CDH5-Quick-Start/../CDH5-Requirements-and-Supported-Versions/../CDH5-Requirements-and-Supported-Versions/../CDH5-Requirements-and-Supported-Versions/../CDH5-Requirements-and-Supported-Versions/../CDH5-Quick-Start/../CDH5-Quick-Start/../CDH5-Quick-Start/../CDH5-Quick-Start/../CDH5-Quick-Start/cdh5qs_mrv1_pseudo.html&#xD;&#xA;&#xD;&#xA;I succesfully finished the following steps&#xD;&#xA;&#xD;&#xA;Step 1: Format the NameNode.&#xD;&#xA;&#xD;&#xA;Step 2: Start HDFS&#xD;&#xA;&#xD;&#xA;Step 3: Create the /tmp Directory&#xD;&#xA;&#xD;&#xA;Step 4: Create the MapReduce system directories:&#xD;&#xA;&#xD;&#xA;Step 5: Verify the HDFS File Structure&#xD;&#xA;&#xD;&#xA;Step 6: Start MapReduce&#xD;&#xA;&#xD;&#xA;while following command in step 7 I get the following error.&#xD;&#xA;&#xD;&#xA;Step 7: Create User Directories&#xD;&#xA;&#xD;&#xA;***$ sudo -u hdfs hadoop fs -mkdir -p /user/hadoopuser***&#xD;&#xA;&#xD;&#xA;***mkdir: '/user/hadoopuser': No such file or directory***&#xD;&#xA;&#xD;&#xA;(where hadoopuser is my linux login username)&#xD;&#xA;&#xD;&#xA;If I create the directory manually as /user/hadoopuser in the filesystem, it is not accepting.&#xD;&#xA;&#xD;&#xA;How to success the step 7:?&#xD;&#xA;&#xD;&#xA;Please provide the sloution to procced the remaining installation." />
  <row Id="1706" PostHistoryTypeId="1" PostId="661" RevisionGUID="73689669-b92f-4765-8a8e-f38b4b931ade" CreationDate="2014-07-03T14:18:14.387" UserId="1314" Text="Challenge in Cloudera Hadoop CDH5 Installation" />
  <row Id="1707" PostHistoryTypeId="3" PostId="661" RevisionGUID="73689669-b92f-4765-8a8e-f38b4b931ade" CreationDate="2014-07-03T14:18:14.387" UserId="1314" Text="&lt;bigdata&gt;&lt;hadoop&gt;" />
  <row Id="1708" PostHistoryTypeId="2" PostId="662" RevisionGUID="2d4b4082-f46e-4bfe-b376-12b3de493edb" CreationDate="2014-07-03T16:11:22.637" UserId="1315" Text="Note that I am doing everything in R. &#xD;&#xA;&#xD;&#xA;The problem goes as follow: &#xD;&#xA;&#xD;&#xA;Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don't. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation . &#xD;&#xA;&#xD;&#xA;Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem. &#xD;&#xA;&#xD;&#xA;My original idea:  *make this a supervised learning problem*. &#xD;&#xA;Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those *unlabelled data*, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to. &#xD;&#xA;&#xD;&#xA;Question 2: The tricky part is: how to **identify and extract the keywords** ? Using the `tm` package in R ? what algorithm is the `tm`   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Any ideas would be great. Thanks&#xD;&#xA;&#xD;&#xA; " />
  <row Id="1709" PostHistoryTypeId="1" PostId="662" RevisionGUID="2d4b4082-f46e-4bfe-b376-12b3de493edb" CreationDate="2014-07-03T16:11:22.637" UserId="1315" Text="What algorithms should I use to perform job classification based on resume data?" />
  <row Id="1710" PostHistoryTypeId="3" PostId="662" RevisionGUID="2d4b4082-f46e-4bfe-b376-12b3de493edb" CreationDate="2014-07-03T16:11:22.637" UserId="1315" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;nlp&gt;&lt;text-mining&gt;" />
  <row Id="1711" PostHistoryTypeId="2" PostId="663" RevisionGUID="717d0fe6-4273-4e4c-b252-8ba9abcbf5ac" CreationDate="2014-07-03T17:06:13.143" UserId="375" Text="Not enough rep for a comment, but check out: http://www.rdatamining.com/examples/text-mining&#xD;&#xA;&#xD;&#xA;Here, they will take you through loading unstructured text to creating a wordcloud.  You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used.  The idea is to take the unstructured text and structure it somehow.  You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices.  You also have the option of stemming the words.  If you stem words you will be able to detect different forms of words as the same word.  For example, 'programed' and 'programming' could be stemmed to 'program'.  You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training.&#xD;&#xA;&#xD;&#xA;You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function.&#xD;&#xA;&#xD;&#xA;I would post an example, but I don't have access to my text mining code right now." />
  <row Id="1712" PostHistoryTypeId="2" PostId="664" RevisionGUID="39fde80d-951d-41ad-b3bf-1a2443a25a40" CreationDate="2014-07-03T17:11:01.770" UserId="1319" Text="R base function glm() uses Fishers Scoring for MLE, while the glmnet uses the coordinate descent method to solve the same equation ? Coordinate descent is more time efficient than Fisher Scoring as fisher scoring calculates the second order derivative matrix and some other matrix operation which makes it space and time expensive, while coordinate descent can do the same task in O(np) time.&#xD;&#xA;&#xD;&#xA;Why R base function uses Fisher Scoring or this method has advantage over other optimization methods? What will be comparison between coordinate descent and Fisher Scoring ? I am relatively new to do this field so any help or resource will be helpfu" />
  <row Id="1713" PostHistoryTypeId="1" PostId="664" RevisionGUID="39fde80d-951d-41ad-b3bf-1a2443a25a40" CreationDate="2014-07-03T17:11:01.770" UserId="1319" Text="Fisher Scoring v/s Coordinate Descent for MLE in R" />
  <row Id="1714" PostHistoryTypeId="3" PostId="664" RevisionGUID="39fde80d-951d-41ad-b3bf-1a2443a25a40" CreationDate="2014-07-03T17:11:01.770" UserId="1319" Text="&lt;machine-learning&gt;&lt;r&gt;&lt;algorithms&gt;&lt;optimization&gt;" />
  <row Id="1715" PostHistoryTypeId="5" PostId="661" RevisionGUID="371aca4d-4046-44d1-a758-a60fbd0694cc" CreationDate="2014-07-03T17:45:59.843" UserId="434" Comment="deleted 306 characters in body; edited title" Text="I download and installed CDH 5 package succesfully on a single linux node in pseudo-distributed Mode on my CentOS 6.5&#xD;&#xA;&#xD;&#xA;Starting Hadoop and verifying it is Working Properly as in [this link][1]&#xD;&#xA;&#xD;&#xA;I succesfully finished the following steps&#xD;&#xA;&#xD;&#xA;Step 1: Format the NameNode.&#xD;&#xA;&#xD;&#xA;Step 2: Start HDFS&#xD;&#xA;&#xD;&#xA;Step 3: Create the /tmp Directory&#xD;&#xA;&#xD;&#xA;Step 4: Create the MapReduce system directories:&#xD;&#xA;&#xD;&#xA;Step 5: Verify the HDFS File Structure&#xD;&#xA;&#xD;&#xA;Step 6: Start MapReduce&#xD;&#xA;&#xD;&#xA;while following command in step 7 I get the following error.&#xD;&#xA;&#xD;&#xA;Step 7: Create User Directories&#xD;&#xA;&#xD;&#xA;***$ sudo -u hdfs hadoop fs -mkdir -p /user/hadoopuser***&#xD;&#xA;&#xD;&#xA;***mkdir: '/user/hadoopuser': No such file or directory***&#xD;&#xA;&#xD;&#xA;(where hadoopuser is my linux login username)&#xD;&#xA;&#xD;&#xA;If I create the directory manually as /user/hadoopuser in the filesystem, it is not accepting.&#xD;&#xA;&#xD;&#xA;How to success the step 7:?&#xD;&#xA;&#xD;&#xA;Please provide the sloution to procced the remaining installation.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Quick-Start/cdh5qs_mrv1_pseudo.html" />
  <row Id="1716" PostHistoryTypeId="4" PostId="661" RevisionGUID="371aca4d-4046-44d1-a758-a60fbd0694cc" CreationDate="2014-07-03T17:45:59.843" UserId="434" Comment="deleted 306 characters in body; edited title" Text="Cannot make user directory on a new CDH5 installation (Hadoop)" />
  <row Id="1717" PostHistoryTypeId="2" PostId="665" RevisionGUID="3fca4e87-895d-4016-9127-0139be01f2df" CreationDate="2014-07-03T17:52:00.963" UserId="1111" Text="At each split point, CART will choose the feature which &quot;best&quot; splits the observations. What qualifies as best varies, but generally the split is done so that the subsequent nodes are more homogenous/pure with respect to the target. There are different ways of measuring homogeneity, for example Gini, Entropy, Chi-square. If you are using software, it may allow you to choose the measure of homogenity that the tree algorithm will use.&#xD;&#xA;&#xD;&#xA;Distance is not a factor with trees - what matters is whether the value is greater than or less than the split point, not the distance from the split point." />
  <row Id="1718" PostHistoryTypeId="2" PostId="666" RevisionGUID="03bb1ff3-2c4e-4148-bc5e-7997f3a57301" CreationDate="2014-07-03T18:36:36.050" UserId="786" Text="As far as gathering data goes, you can check out **Quandl** (there's a tutorial on using it with **R** on [DataCamp](https://www.datacamp.com/courses/how-to-work-with-quandl-in-r) if you're interested).&#xD;&#xA;&#xD;&#xA;In addition, Aswath Damodaran's [site](http://people.stern.nyu.edu/adamodar/New_Home_Page/data.html) contains a lot of helpful datasets. Though they aren't updated that frequently, they may still be useful, especially as a benchmark for comparing your own output (from the scripts you will inevitably need to write to calculate the necessary metrics).  &#xD;&#xA;&#xD;&#xA;And, again, **Quant SE** is probably a better place to be looking..." />
  <row Id="1720" PostHistoryTypeId="2" PostId="667" RevisionGUID="5e6e8972-786e-4f5e-9300-7b870fa55476" CreationDate="2014-07-03T20:47:20.147" UserId="984" Text="This is a tricky problem. There are many ways to handle it. I guess, resumes can be treated as semi-structured documents. Sometimes, it's beneficial to have some minimal structure in the documents. I believe, in resumes you would see some tabular data. You might want to treat these as attribute value pairs. For example, you would get a list of terms for the attribute &quot;Skill set&quot;.&#xD;&#xA;&#xD;&#xA;The key idea is to manually configure a list of key phrases such as &quot;skill&quot;, &quot;education&quot;, &quot;publication&quot; etc. The next step is to extract terms which pertain to these key phrases either by exploiting the structure in some way (such as tables) or by utilizing the proximity of terms around these key phrases, e.g. the fact that the word &quot;Java&quot; is in close proximity to the term &quot;skill&quot; might indicate that the person is skilled in Java.&#xD;&#xA;&#xD;&#xA;After you extract these information, the next step could be to build up a feature vector for each of these key phrases. You can then represent a document as a vector with different fields (one each for a key phrase). For example, consider the following two resumes represented with two fields, namely *project* and *education*.&#xD;&#xA;&#xD;&#xA;Doc1: {project: (java, 3) (c, 4)}, {education: (computer, 2), (physics, 1)}&#xD;&#xA;&#xD;&#xA;Doc2: {project: (java, 3) (python, 2)}, {education: (maths, 3), (computer, 2)}&#xD;&#xA;&#xD;&#xA;In the above example, I show a term with the frequency. Of course, while extracting the terms you need to stem and remove stop-words. It is clear from the examples that the person whose resume is Doc1 is more skilled in C than that of D2. Implementation wise, it's very easy to represent documents as field vectors in Lucene.&#xD;&#xA;&#xD;&#xA;Now, the next step is to retrieve a ranked list of resumes given a job specification. In fact, that's fairly straight forward if you represent queries (job specs) as field vectors as well. You just need to retrieve a ranked list of candidates (resumes) using Lucene from a collection of indexed resumes.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1721" PostHistoryTypeId="2" PostId="668" RevisionGUID="073477ea-7830-4101-9574-0d7294b5865b" CreationDate="2014-07-03T20:51:59.510" UserId="1329" Text="I've done some research in this area. I've found first order Markov chains work well for predicting within game scoring dynamics across a variety of sports.&#xD;&#xA;&#xD;&#xA;You can read in more detail here:&#xD;&#xA;http://www.epjdatascience.com/content/3/1/4" />
  <row Id="1722" PostHistoryTypeId="5" PostId="662" RevisionGUID="d65e227a-e75c-4316-b010-03ab1307e6fd" CreationDate="2014-07-03T22:10:08.953" UserId="1315" Comment="Added new question: question 2" Text="Note that I am doing everything in R. &#xD;&#xA;&#xD;&#xA;The problem goes as follow: &#xD;&#xA;&#xD;&#xA;Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don't. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation . &#xD;&#xA;&#xD;&#xA;Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem. &#xD;&#xA;&#xD;&#xA;My original idea:  *make this a supervised learning problem*. &#xD;&#xA;Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those *unlabelled data*, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to. &#xD;&#xA;&#xD;&#xA;**Update**&#xD;&#xA;Question 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? Is this approach wrong ? Please correct me if you think my approach is wrong. &#xD;&#xA;&#xD;&#xA;Question 3: The tricky part is: how to **identify and extract the keywords** ? Using the `tm` package in R ? what algorithm is the `tm`   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Any ideas would be great. Thanks&#xD;&#xA;&#xD;&#xA; " />
  <row Id="1723" PostHistoryTypeId="5" PostId="663" RevisionGUID="f0e4f213-3511-48f3-b6ea-124c6851cdf6" CreationDate="2014-07-03T22:52:29.253" UserId="375" Comment="Added Example" Text="Not enough rep for a comment, but check out: http://www.rdatamining.com/examples/text-mining&#xD;&#xA;&#xD;&#xA;Here, they will take you through loading unstructured text to creating a wordcloud.  You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used.  The idea is to take the unstructured text and structure it somehow.  You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices.  You also have the option of stemming the words.  If you stem words you will be able to detect different forms of words as the same word.  For example, 'programed' and 'programming' could be stemmed to 'program'.  You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training.&#xD;&#xA;&#xD;&#xA;You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function.&#xD;&#xA;&#xD;&#xA;Example:&#xD;&#xA;&#xD;&#xA;1) Load libraries and build the example data&#xD;&#xA;&#xD;&#xA;    library(tm)&#xD;&#xA;    library(SnowballC)&#xD;&#xA;&#xD;&#xA;    doc1 = &quot;I am highly skilled in Java Programming.  I have spent 5 years developing bug-tracking systems and creating data managing system applications in C.&quot;&#xD;&#xA;    job1 = &quot;Software Engineer&quot;&#xD;&#xA;    doc2 = &quot;Tested new software releases for major program enhancements.  Designed and executed test procedures and worked with relational databases.  I helped organize and lead meetings and work independently and in a group setting.&quot;&#xD;&#xA;    job2 = &quot;Quality Assurance&quot;&#xD;&#xA;    doc3 = &quot;Developed large and complex web applications for client service center. Lead projects for upcoming releases and interact with consumers.  Perform database design and debugging of current releases.&quot;&#xD;&#xA;    job3 = &quot;Software Engineer&quot;&#xD;&#xA;    jobInfo = data.frame(&quot;text&quot; = c(doc1,doc2,doc3),&#xD;&#xA;                         &quot;job&quot; = c(job1,job2,job3))&#xD;&#xA;&#xD;&#xA;2) Now we do some text structuring. I am positive there are quicker/shorter ways to do the following.&#xD;&#xA;&#xD;&#xA;    # Convert to lowercase&#xD;&#xA;    jobInfo$text = sapply(jobInfo$text,tolower)&#xD;&#xA;&#xD;&#xA;    # Remove Punctuation&#xD;&#xA;    jobInfo$text = sapply(jobInfo$text,function(x) gsub(&quot;[[:punct:]]&quot;,&quot; &quot;,x))&#xD;&#xA;&#xD;&#xA;    # Remove extra white space&#xD;&#xA;    jobInfo$text = sapply(jobInfo$text,function(x) gsub(&quot;[ ]+&quot;,&quot; &quot;,x))&#xD;&#xA;&#xD;&#xA;    # Remove stop words&#xD;&#xA;    jobInfo$text = sapply(jobInfo$text, function(x){&#xD;&#xA;      paste(setdiff(strsplit(x,&quot; &quot;)[[1]],stopwords()),collapse=&quot; &quot;)&#xD;&#xA;    })&#xD;&#xA;&#xD;&#xA;    # Stem words (Also try without stemming?)&#xD;&#xA;    jobInfo$text = sapply(jobInfo$text, function(x)  {&#xD;&#xA;      paste(setdiff(wordStem(strsplit(x,&quot; &quot;)[[1]]),&quot;&quot;),collapse=&quot; &quot;)&#xD;&#xA;    })&#xD;&#xA;&#xD;&#xA;3) Make a corpus source and document term matrix.&#xD;&#xA;&#xD;&#xA;    # Create Corpus Source&#xD;&#xA;    jobCorpus = Corpus(VectorSource(jobInfo$text))&#xD;&#xA;&#xD;&#xA;    # Create Document Term Matrix&#xD;&#xA;    jobDTM = DocumentTermMatrix(jobCorpus)&#xD;&#xA;&#xD;&#xA;    # Create Term Frequency Matrix&#xD;&#xA;    jobFreq = as.matrix(jobDTM)&#xD;&#xA;&#xD;&#xA;Now we have the frequency matrix, jobFreq, that is a (3 by x) matrix, 3 entries and X number of words.&#xD;&#xA;&#xD;&#xA;Where you go from here is up to you.  You can keep only specific (more common) words and use them as features in your model.  Another way is to keep it simple and have a percentage of words used in each job description, say &quot;java&quot; would have 80% occurrence in 'software engineer' and only 50% occurrence in 'quality assurance'.&#xD;&#xA;&#xD;&#xA;Now it's time to go look up why 'assurance' has 1 'r' and 'occurrence' has 2 'r's.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1724" PostHistoryTypeId="2" PostId="669" RevisionGUID="11ff1261-e5fe-4ad3-8245-865435ad02bb" CreationDate="2014-07-03T23:00:42.350" UserId="548" Text="The results you're seeing aren't a byproduct of your training product, but rather that `neural nets` are not a great choice for this task. `Neural nets` are effectively a means to create a high order non-linear function by composing a number of simpler functions. This is often a really good thing, because it allows neural nets to fit very complex patterns.&#xD;&#xA;&#xD;&#xA;However, in a stock exchange any complex pattern, when traded upon will quickly decay. Detecting a complicated pattern will generally not generate useful results, because it is typically complex patterns in the short term. Additionally, depending on the metric you choose, there are a number of ways of performing well that actually won't pay off in investing (such as just predicting the last value in your example).&#xD;&#xA;&#xD;&#xA;In addition the stock market is startlingly chaotic which can result in a `neural net` overfitting. This means that the patterns it learns will generalize poorly. Something along the lines of just seeing a stock decrease over a day and uniformly deciding that the stock will always decrease just because it was seen on a relatively short term. Instead techniques like `ridge` and `robust regression`, which will identify more general, less complex patterns, do better.&#xD;&#xA;&#xD;&#xA;The winner of a similar Kaggle competition used `robust regression` for this very reason. You are likely to see better results if you switch to a shallow learning model that will find functions of a lower polynomial order, over the deep complex functions of a neural net." />
  <row Id="1725" PostHistoryTypeId="5" PostId="662" RevisionGUID="6e222939-6f23-4da8-b227-00577075d170" CreationDate="2014-07-04T00:14:16.620" UserId="1315" Comment="added 240 characters in body" Text="Note that I am doing everything in R. &#xD;&#xA;&#xD;&#xA;The problem goes as follow: &#xD;&#xA;&#xD;&#xA;Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don't. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation . &#xD;&#xA;&#xD;&#xA;Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem. &#xD;&#xA;&#xD;&#xA;My original idea:  *make this a supervised learning problem*. &#xD;&#xA;Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those *unlabelled data*, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to. &#xD;&#xA;&#xD;&#xA;**Update**&#xD;&#xA;Question 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? For example, the text file may look something like this:&#xD;&#xA;&#xD;&#xA;`I deployed ML algorithm in this project and... Skills: Java, Python, c++ ...`&#xD;&#xA; &#xD;&#xA;This is what I meant by 'unstructured', i.e. collapsing everything into a single line string.&#xD;&#xA;&#xD;&#xA;Is this approach wrong ? Please correct me if you think my approach is wrong. &#xD;&#xA;&#xD;&#xA;Question 3: The tricky part is: how to **identify and extract the keywords** ? Using the `tm` package in R ? what algorithm is the `tm`   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Any ideas would be great. Thanks&#xD;&#xA;&#xD;&#xA; " />
  <row Id="1726" PostHistoryTypeId="2" PostId="670" RevisionGUID="ff925324-4438-4f63-9f25-d78aac34e6cb" CreationDate="2014-07-04T02:31:42.080" UserId="1334" Text="I was building a model that predicts user churn for a website, where I have data on all users, both past and present.&#xD;&#xA;&#xD;&#xA;I can build a model that only uses those users that have left, but then I'm leaving 2/3 of the total user population unused.&#xD;&#xA;&#xD;&#xA;Is there a good way to incorporate data from these users into a model from a conceptual standpoint?" />
  <row Id="1727" PostHistoryTypeId="1" PostId="670" RevisionGUID="ff925324-4438-4f63-9f25-d78aac34e6cb" CreationDate="2014-07-04T02:31:42.080" UserId="1334" Text="Dealing with events that have not yet happened" />
  <row Id="1728" PostHistoryTypeId="3" PostId="670" RevisionGUID="ff925324-4438-4f63-9f25-d78aac34e6cb" CreationDate="2014-07-04T02:31:42.080" UserId="1334" Text="&lt;data-mining&gt;" />
  <row Id="1729" PostHistoryTypeId="4" PostId="670" RevisionGUID="65a6139d-dfd8-4f85-b14c-3279f112dbc6" CreationDate="2014-07-04T02:38:12.523" UserId="1334" Comment="edited title" Text="Dealing with events that have not yet happened when building a model" />
  <row Id="1730" PostHistoryTypeId="2" PostId="671" RevisionGUID="9b662400-be8f-4464-bd7a-40ed3c03212b" CreationDate="2014-07-04T05:12:44.707" UserId="870" Text="I've a linearly increasing time series dataset of a sensor with value ranges between 50 to 150. On which I've implemented [Simple Linear Regression][1] algorithm to fit a regression line, and I'm predicting the date when the series would reach 120. &#xD;&#xA;&#xD;&#xA;All works fine when the series move upwards. But, there are cases when the sensor reaches around 110 or 115, the sensor would be reset and the values start over again at say 50 or 60... &#xD;&#xA;&#xD;&#xA;This is where I start facing issues with the regression line as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I'm trying to understand if there are any algorithm available that considers this case.&#xD;&#xA;&#xD;&#xA;I'm new to data science, would appreciate any pointers to move further.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;ArunDhaJ&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Simple_linear_regression" />
  <row Id="1731" PostHistoryTypeId="1" PostId="671" RevisionGUID="9b662400-be8f-4464-bd7a-40ed3c03212b" CreationDate="2014-07-04T05:12:44.707" UserId="870" Text="Linearly increasing data with manual reset" />
  <row Id="1732" PostHistoryTypeId="3" PostId="671" RevisionGUID="9b662400-be8f-4464-bd7a-40ed3c03212b" CreationDate="2014-07-04T05:12:44.707" UserId="870" Text="&lt;machine-learning&gt;&lt;statistics&gt;&lt;time-series&gt;" />
  <row Id="1733" PostHistoryTypeId="2" PostId="672" RevisionGUID="c9120954-597c-435b-9a20-ad8ec13c867e" CreationDate="2014-07-04T06:46:19.180" UserId="609" Text="This setting is common in reliability, health care, and mortality. The statistical analysis method is called [Survival Analysis](http://en.wikipedia.org/wiki/Survival_analysis). All users are coded according to their start date (or week or month).  You use the empirical data to estimate the survival function, which is the probability that the time of defection is later than some specified time ***t***.&#xD;&#xA;&#xD;&#xA;Your baseline model will estimate survival function for all users.  Then you can do more sophisticated modeling to estimate what factors or behaviors might predict defection (churn), given your baseline survival function.  Basically, any model that is predictive will yield a survival probability that is significantly lower than the baseline.&#xD;&#xA;&#xD;&#xA;----&#xD;&#xA;&#xD;&#xA;There's another approach which involves attempting to identify precursor events patterns or user behavior pattern that foreshadow defection. Any given event/behavior pattern might occur for users that defect, or for users that stay. For this analysis, you may need to censor your data to only include users that have been members for some minimum period of time. The minimum time period can be estimated using your estimate of survival function, or even simple histogram analysis of the distribution of membership period for users who have defected. " />
  <row Id="1741" PostHistoryTypeId="2" PostId="675" RevisionGUID="9650a3d2-211e-494c-801a-9983e19a98c0" CreationDate="2014-07-04T22:46:32.130" UserId="1279" Text="Just extract **keywords** and train **classifier** on them. That's all, really. &#xD;&#xA;&#xD;&#xA;Most of the text in CVs is not actually related to skills. E.g. consider sentense &quot;I'm experienced and highly efficient in Java&quot;. Here only 1 out of 7 words is a skill name, the rest is just a noise that's going to put your classification accuracy down. &#xD;&#xA;&#xD;&#xA;Most of CVs are not really structured. Or structured too freely. Or use unusual names for sections. Or file formats that don't preserve structure when translated to text. I have experience extracting dates, times, names, addresses and even people intents from unstructured text, but not skill (or university or anything) list, not even closely. &#xD;&#xA;&#xD;&#xA;So just tokenize (and possibly [stem](http://stackoverflow.com/questions/16069406/text-mining-with-the-tm-package-word-stemming)) your CVs, select only words from predefined list (you can use LinkedIn or something similar to grab this list), create feature vector and try out a couple of classifiers (say, SVM and Naive Bayes). &#xD;&#xA;&#xD;&#xA;(Note: I used similar approach to classify LinkedIn profiles into more than 50 classes with accuracy &gt; 90%, so I'm pretty sure even naive implementation will work well.)" />
  <row Id="1744" PostHistoryTypeId="5" PostId="675" RevisionGUID="7d0161a8-539d-496c-8fcd-6e5a23ce36fa" CreationDate="2014-07-05T08:31:38.717" UserId="434" Comment="minor grammar and spelling" Text="Just extract **keywords** and train a **classifier** on them. That's all, really. &#xD;&#xA;&#xD;&#xA;Most of the text in CVs is not actually related to skills. E.g. consider sentence &quot;I'm experienced and highly efficient in Java&quot;. Here only 1 out of 7 words is a skill name, the rest is just a noise that's going to put your classification accuracy down. &#xD;&#xA;&#xD;&#xA;Most of CVs are not really structured. Or structured too freely. Or use unusual names for sections. Or file formats that don't preserve structure when translated to text. I have experience extracting dates, times, names, addresses and even people intents from unstructured text, but not a skill (or university or anything) list, not even closely. &#xD;&#xA;&#xD;&#xA;So just tokenize (and possibly [stem](http://stackoverflow.com/questions/16069406/text-mining-with-the-tm-package-word-stemming)) your CVs, select only words from predefined list (you can use LinkedIn or something similar to grab this list), create a feature vector and try out a couple of classifiers (say, SVM and Naive Bayes). &#xD;&#xA;&#xD;&#xA;(Note: I used a similar approach to classify LinkedIn profiles into more than 50 classes with accuracy &gt; 90%, so I'm pretty sure even naive implementation will work well.)" />
  <row Id="1745" PostHistoryTypeId="2" PostId="676" RevisionGUID="2e9c5e37-cdef-4ee0-9916-0864b58b1505" CreationDate="2014-07-05T14:48:10.570" UserId="1355" Text="In general regression models (any) can behave in an arbitrary way beyond the domain spanned by training samples. In particular, they are free to assume linearity of the modeled function, so if you for instance train a regression model with points:&#xD;&#xA;&#xD;&#xA;    X     Y&#xD;&#xA;    10    0&#xD;&#xA;    20    1&#xD;&#xA;    30    2&#xD;&#xA;&#xD;&#xA;it is reasonable to build a model `f(x) = x/10-1`, which for `x&lt;10` returns negative values.&#xD;&#xA;&#xD;&#xA;The same applies &quot;in between&quot; your data points, it is always possible that due to the assumed famility of functions (which can be modeled by particular method) you will get values &quot;out of your training samples&quot;. &#xD;&#xA;&#xD;&#xA;You can think about this in another way - &quot;what is so special about negative values?&quot;, why do you find existance of negative values weird (if not provided in training set) while you don't get alarmed by existance of lets say... value 2131.23? Unless developed in such a way, no model will treat negative values &quot;different&quot; than positive ones. This is just a natural element of the real values which can be attained as any other value." />
  <row Id="1746" PostHistoryTypeId="2" PostId="677" RevisionGUID="61c782b1-3f8a-49cc-9b17-52aa706c32b6" CreationDate="2014-07-05T15:01:43.940" UserId="974" Text="I'm trying to use the sklearn_pandas module to extend the work I do in pandas and dip a toe into machine learning but I'm struggling with an error I don't really understand how to fix.&#xD;&#xA;&#xD;&#xA;I was working through the following dataset on [Kaggle][1].&#xD;&#xA;&#xD;&#xA;It's essentially an unheadered table (1000 rows, 40 features) with floating point values.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    import pandas as pdfrom sklearn import neighbors&#xD;&#xA;    from sklearn_pandas import DataFrameMapper, cross_val_score&#xD;&#xA;    path_train =&quot;../kaggle/scikitlearn/train.csv&quot;&#xD;&#xA;    path_labels =&quot;../kaggle/scikitlearn/trainLabels.csv&quot;&#xD;&#xA;    path_test = &quot;../kaggle/scikitlearn/test.csv&quot;&#xD;&#xA;&#xD;&#xA;    train = pd.read_csv(path_train, header=None)&#xD;&#xA;    labels = pd.read_csv(path_labels, header=None)&#xD;&#xA;    test = pd.read_csv(path_test, header=None)&#xD;&#xA;    mapper_train = DataFrameMapper([(list(train.columns),neighbors.KNeighborsClassifier(n_neighbors=3))])&#xD;&#xA;    mapper_train&#xD;&#xA;&#xD;&#xA;Output:&#xD;&#xA;    &#xD;&#xA;    DataFrameMapper(features=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',&#xD;&#xA;           n_neighbors=3, p=2, weights='uniform'))])&#xD;&#xA;&#xD;&#xA;So far so good. But then I try the fit&#xD;&#xA;&#xD;&#xA;    mapper_train.fit_transform(train, labels)&#xD;&#xA;&#xD;&#xA;Output:&#xD;&#xA;&#xD;&#xA;    ---------------------------------------------------------------------------&#xD;&#xA;    TypeError                                 Traceback (most recent call last)&#xD;&#xA;    &lt;ipython-input-6-e3897d6db1b5&gt; in &lt;module&gt;()&#xD;&#xA;    ----&gt; 1 mapper_train.fit_transform(train, labels)&#xD;&#xA;&#xD;&#xA;    //anaconda/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y,     **fit_params)&#xD;&#xA;        409         else:&#xD;&#xA;        410             # fit method of arity 2 (supervised transformation)&#xD;&#xA;    --&gt; 411             return self.fit(X, y, **fit_params).transform(X)&#xD;&#xA;        412 &#xD;&#xA;        413 &#xD;&#xA;&#xD;&#xA;    //anaconda/lib/python2.7/site-packages/sklearn_pandas/__init__.pyc in fit(self, X, y)&#xD;&#xA;        116         for columns, transformer in self.features:&#xD;&#xA;        117             if transformer is not None:&#xD;&#xA;    --&gt; 118                 transformer.fit(self._get_col_subset(X, columns))&#xD;&#xA;        119         return self&#xD;&#xA;        120 &#xD;&#xA;&#xD;&#xA;    TypeError: fit() takes exactly 3 arguments (2 given)`&#xD;&#xA;&#xD;&#xA;What am I doing wrong? While the data in this case is all the same, I'm planning to work up a workflow for mixtures categorical, nominal and floating point features and sklearn_pandas seemed to be a logical fit.&#xD;&#xA;&#xD;&#xA;  [1]: https://www.kaggle.com/c/data-science-london-scikit-learn/data" />
  <row Id="1747" PostHistoryTypeId="1" PostId="677" RevisionGUID="61c782b1-3f8a-49cc-9b17-52aa706c32b6" CreationDate="2014-07-05T15:01:43.940" UserId="974" Text="Struggling to integrate sklearn and pandas in simple Kaggle task" />
  <row Id="1748" PostHistoryTypeId="3" PostId="677" RevisionGUID="61c782b1-3f8a-49cc-9b17-52aa706c32b6" CreationDate="2014-07-05T15:01:43.940" UserId="974" Text="&lt;python&gt;&lt;pandas&gt;&lt;sklearn&gt;" />
  <row Id="1749" PostHistoryTypeId="2" PostId="678" RevisionGUID="ceaeca18-ecfe-4157-87f2-2aa3dd7e53a5" CreationDate="2014-07-05T16:10:21.580" UserId="1097" Text="When I say &quot;document&quot;, I have in mind web pages like Wikipedia articles and news stories.  I prefer answers giving either vanilla lexical distance metrics or state-of-the-art semantic distance metrics, with stronger preference for the latter." />
  <row Id="1750" PostHistoryTypeId="1" PostId="678" RevisionGUID="ceaeca18-ecfe-4157-87f2-2aa3dd7e53a5" CreationDate="2014-07-05T16:10:21.580" UserId="1097" Text="What are some standard ways of computing the distance between documents?" />
  <row Id="1751" PostHistoryTypeId="3" PostId="678" RevisionGUID="ceaeca18-ecfe-4157-87f2-2aa3dd7e53a5" CreationDate="2014-07-05T16:10:21.580" UserId="1097" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;nlp&gt;&lt;text-mining&gt;" />
  <row Id="1752" PostHistoryTypeId="2" PostId="679" RevisionGUID="3d013f83-dfe3-4367-9d9c-44ebaa18a290" CreationDate="2014-07-05T16:20:17.963" UserId="1097" Text="I made a similar question asking about distance between &quot;documents&quot; (Wikipedia articles, news stories, etc.).  I made this a separate question because search queries are considerably smaller than documents and are considerably noisier.  I hence don't know (and doubt) if the same distance metrics would be used here.  &#xD;&#xA;&#xD;&#xA;Either vanilla lexical distance metrics or state-of-the-art semantic distance metrics are preferred, with stronger preference for the latter." />
  <row Id="1753" PostHistoryTypeId="1" PostId="679" RevisionGUID="3d013f83-dfe3-4367-9d9c-44ebaa18a290" CreationDate="2014-07-05T16:20:17.963" UserId="1097" Text="What are some standard ways of computing the distance between individual search queries?" />
  <row Id="1754" PostHistoryTypeId="3" PostId="679" RevisionGUID="3d013f83-dfe3-4367-9d9c-44ebaa18a290" CreationDate="2014-07-05T16:20:17.963" UserId="1097" Text="&lt;machine-learning&gt;&lt;nlp&gt;&lt;search&gt;" />
  <row Id="1755" PostHistoryTypeId="2" PostId="680" RevisionGUID="ad960bc3-af51-4904-b4b3-1b12b1df6cd8" CreationDate="2014-07-06T00:01:41.327" UserId="1279" Text="There's a number of semantic distance measures, each with its pros and cons. Here are just a few of them: &#xD;&#xA;&#xD;&#xA; * [**cosine distance**](http://en.wikipedia.org/wiki/Cosine_similarity), inner product between document feature vectors;&#xD;&#xA; * [**LSA**](http://en.wikipedia.org/wiki/Latent_semantic_analysis), another vector-based model, but utilizing SVD for de-noising original term-document matrix;&#xD;&#xA; * [**WordNet**](http://en.wikipedia.org/wiki/WordNet)-based, human verified, though hardly extensible.&#xD;&#xA;&#xD;&#xA;Start with a simplest approach and then move further based on issues for your specific case. " />
  <row Id="1759" PostHistoryTypeId="2" PostId="682" RevisionGUID="3e39a53a-203a-4ad0-8c4a-0fa791491591" CreationDate="2014-07-06T14:01:57.467" UserId="1360" Text="Michael Maouboussin, in his book, &quot;The Success Equation,&quot; looks at differentiating luck from skill in various endeavors, including sports.  He actually ranks sports by the amount of luck that contributes to performance in the different sports (p. 23) and about 2/3 of performance in football is attributable to skill.  By contrast, I used MM's technique to analyze performance in Formula 1 racing, and found that 60% is attributable to skill (less than I was expecting.)&#xD;&#xA;&#xD;&#xA;That said, it seems this kind of analysis would imply that a sufficiently detailed and crafted feature set would allow ML algorithms to predict performance of NFL teams, perhaps even to the play level, with the caveat that significant variance will still exist because of the influence of luck in the game." />
  <row Id="1767" PostHistoryTypeId="2" PostId="683" RevisionGUID="20a707b9-769d-4c75-a090-0d1a79f655a9" CreationDate="2014-07-07T09:34:36.950" UserId="906" Text="I have 2 datasets, one with positive instances of what I would like to detect, and one with unlabeled instances. What methods can I use ?&#xD;&#xA;&#xD;&#xA;As an example, suppose we want to understand detect spam email based on a few structured email characteristics. We have one dataset of 10000 spam emails, and one dataset of 100000 emails for which we don't know whether they are spam or not.&#xD;&#xA;&#xD;&#xA;How can we tackle this problem (without labeling manually any of the unlabeled data) ?&#xD;&#xA;&#xD;&#xA;What can we do if we have additional information about the proportion of spam in the unlabeled data (i.e. what if we estimate that between 20-40% of the 100000 unlabeled emails are spam) ?&#xD;&#xA;" />
  <row Id="1768" PostHistoryTypeId="1" PostId="683" RevisionGUID="20a707b9-769d-4c75-a090-0d1a79f655a9" CreationDate="2014-07-07T09:34:36.950" UserId="906" Text="Build a binary classifier with only positive and unlabeled data" />
  <row Id="1769" PostHistoryTypeId="3" PostId="683" RevisionGUID="20a707b9-769d-4c75-a090-0d1a79f655a9" CreationDate="2014-07-07T09:34:36.950" UserId="906" Text="&lt;classification&gt;" />
  <row Id="1770" PostHistoryTypeId="2" PostId="684" RevisionGUID="ba3db5d5-da3d-43aa-962e-cc58e8799bb7" CreationDate="2014-07-07T10:25:55.323" UserId="1367" Text="I have never used `sklearn_pandas`, but from reading their source code, it looks like this is a bug on their side. If you look for [the function that is throwing the exception][1], you can notice that they are discarding the `y` argument (it does not even survive until the docstring), and the inner `fit` function expects one argument more, which is probably `y`:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: python --&gt;&#xD;&#xA;&#xD;&#xA;    def fit(self, X, y=None):&#xD;&#xA;        '''&#xD;&#xA;        Fit a transformation from the pipeline&#xD;&#xA;&#xD;&#xA;        X       the data to fit&#xD;&#xA;        '''&#xD;&#xA;        for columns, transformer in self.features:&#xD;&#xA;            if transformer is not None:&#xD;&#xA;                transformer.fit(self._get_col_subset(X, columns))&#xD;&#xA;        return self&#xD;&#xA;&#xD;&#xA;I would recommend that you open an issue in [their bug tracker][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/paulgb/sklearn-pandas/blob/master/sklearn_pandas/__init__.py&#xD;&#xA;  [2]: https://github.com/paulgb/sklearn-pandas/issues" />
  <row Id="1771" PostHistoryTypeId="2" PostId="685" RevisionGUID="3c17c75c-dd17-490a-a11d-6d7126a6628d" CreationDate="2014-07-07T11:43:48.430" UserId="1273" Text="i am very new to machine learning and in my first project have stumbled across a lot of issues which i really want to get through.&#xD;&#xA;&#xD;&#xA;i m using logistic regression using R's glmnet package with alpha = 0 for ridge regression&#xD;&#xA;&#xD;&#xA;i m using ridge regression actually since lasso deleted all my variables and gave very low Area under Curve (.52)&#xD;&#xA;&#xD;&#xA;but with ridge also, there aint much of a difference (.61)&#xD;&#xA;&#xD;&#xA;my dependent variable/output is probability of click, based on if there a click or not in historical data&#xD;&#xA;&#xD;&#xA;the independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version, OS family&#xD;&#xA;&#xD;&#xA;of these, in prediction i m using state, device, user age, user gender, IP carrier, browser version, browser family, OS version, OS family, I am not using keyword or template since we want to reject a user request before deep diving in our system and select a keyword or template. I am not using city because they are too many, or mobile manufacturer cuz they are too few&#xD;&#xA;&#xD;&#xA;is it okay or should i be using these the rejected variables?&#xD;&#xA;&#xD;&#xA;to go about, i create a sparse matrix from my variables which are mapped against the column of clicks that has yes or no yes or no.&#xD;&#xA;&#xD;&#xA;so after training the model, i save the coefficients and intercept. these are used for new incoming requests using the formula for logistic that is &#xD;&#xA;1/(1+e^-1*sum(a+k(ith)*x(ith)))&#xD;&#xA;where a is intercept, k is the ith  coefficient and x is the ith variable value&#xD;&#xA;&#xD;&#xA;please let me know if my approach is correct so far.&#xD;&#xA;&#xD;&#xA;now simple glm in R (that is where there is no regularised regression, right?) gave me .56 AUC &#xD;&#xA;with regularization i get .61 but there is no distinct threshold that we could say that okay above 0.xx its mostly 1s and below it most 0s are covered, actually max probability of where click didnt happened is almost always &gt; max probability where click happened&#xD;&#xA;&#xD;&#xA;so basically what should i do?&#xD;&#xA;&#xD;&#xA;i have read how stochastic gradient descent is an effective technique in logit&#xD;&#xA;so how to implement stochastic gradient descent in R? &#xD;&#xA;if its not straightforward, is there a way to implement this system in python?&#xD;&#xA;is SGD implemented after generating a regularized logistic regression model or is it a different process all together?&#xD;&#xA;&#xD;&#xA;Also there is an algo called follow the regularized leader (FTRL) that is used in ctr prediction. is there a sample code and use of that it i could go through?" />
  <row Id="1772" PostHistoryTypeId="1" PostId="685" RevisionGUID="3c17c75c-dd17-490a-a11d-6d7126a6628d" CreationDate="2014-07-07T11:43:48.430" UserId="1273" Text="SGD in logistic regression" />
  <row Id="1773" PostHistoryTypeId="3" PostId="685" RevisionGUID="3c17c75c-dd17-490a-a11d-6d7126a6628d" CreationDate="2014-07-07T11:43:48.430" UserId="1273" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;logistic-regression&gt;" />
  <row Id="1774" PostHistoryTypeId="5" PostId="685" RevisionGUID="0ccc1bc8-7b2a-4541-93a7-209110965fe9" CreationDate="2014-07-07T11:54:21.357" UserId="1273" Comment="deleted 12 characters in body" Text="i am very new to machine learning and in my first project have stumbled across a lot of issues which i really want to get through.&#xD;&#xA;&#xD;&#xA;i m using logistic regression using R's glmnet package with alpha = 0 for ridge regression&#xD;&#xA;&#xD;&#xA;i m using ridge regression actually since lasso deleted all my variables and gave very low Area under Curve (.52)&#xD;&#xA;&#xD;&#xA;but with ridge also, there aint much of a difference (.61)&#xD;&#xA;&#xD;&#xA;my dependent variable/output is probability of click, based on if there a click or not in historical data&#xD;&#xA;&#xD;&#xA;the independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version, OS family&#xD;&#xA;&#xD;&#xA;of these, in prediction i m using state, device, user age, user gender, IP carrier, browser version, browser family, OS version, OS family, I am not using keyword or template since we want to reject a user request before deep diving in our system and select a keyword or template. I am not using city because they are too many, or mobile manufacturer cuz they are too few&#xD;&#xA;&#xD;&#xA;is it okay or should i be using these the rejected variables?&#xD;&#xA;&#xD;&#xA;to go about, i create a sparse matrix from my variables which are mapped against the column of clicks that have yes or no values.&#xD;&#xA;&#xD;&#xA;so after training the model, i save the coefficients and intercept. these are used for new incoming requests using the formula for logistic that is 1/(1+e^-1*sum(a+k(ith)*x(ith))) where a is intercept, k is the ith coefficient and x is the ith variable value&#xD;&#xA;&#xD;&#xA;please let me know if my approach is correct so far.&#xD;&#xA;&#xD;&#xA;now simple glm in R (that is where there is no regularised regression, right?) gave me .56 AUC with regularization i get .61 but there is no distinct threshold that we could say that okay above 0.xx its mostly 1s and below it most 0s are covered, actually max probability of where click didnt happened is almost always &gt; max probability where click happened&#xD;&#xA;&#xD;&#xA;so basically what should i do?&#xD;&#xA;&#xD;&#xA;i have read how stochastic gradient descent is an effective technique in logit so how to implement stochastic gradient descent in R? if its not straightforward, is there a way to implement this system in python? is SGD implemented after generating a regularized logistic regression model or is it a different process all together?&#xD;&#xA;&#xD;&#xA;Also there is an algo called follow the regularized leader (FTRL) that is used in ctr prediction. is there a sample code and use of it that i could go through?" />
  <row Id="1775" PostHistoryTypeId="2" PostId="686" RevisionGUID="cd2a34ea-23be-450e-8b8a-194bb19d56e7" CreationDate="2014-07-07T13:23:28.220" UserId="574" Text="State of the art appears to be &quot;paragraph vectors&quot; introduced in a recent paper: http://cs.stanford.edu/~quocle/paragraph_vector.pdf. Cosine/Euclidean distance between paragraph vectors would likely work better than any other approach. This probably isn't feasible yet due to lack of open source implementations.&#xD;&#xA;&#xD;&#xA;Next best thing is cosine distance between LSA vectors or cosine distance between raw BOW vectors. Sometimes it works better to choose different weighting schemes, like TF-IDF. " />
  <row Id="1778" PostHistoryTypeId="5" PostId="684" RevisionGUID="c267f08b-18f1-4655-956d-a0a95a3ec575" CreationDate="2014-07-07T13:42:31.827" UserId="1367" Comment="Show debugging session" Text="I have never used `sklearn_pandas`, but from reading their source code, it looks like this is a bug on their side. If you look for [the function that is throwing the exception][1], you can notice that they are discarding the `y` argument (it does not even survive until the docstring), and the inner `fit` function expects one argument more, which is probably `y`:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: python --&gt;&#xD;&#xA;&#xD;&#xA;    def fit(self, X, y=None):&#xD;&#xA;        '''&#xD;&#xA;        Fit a transformation from the pipeline&#xD;&#xA;&#xD;&#xA;        X       the data to fit&#xD;&#xA;        '''&#xD;&#xA;        for columns, transformer in self.features:&#xD;&#xA;            if transformer is not None:&#xD;&#xA;                transformer.fit(self._get_col_subset(X, columns))&#xD;&#xA;        return self&#xD;&#xA;&#xD;&#xA;I would recommend that you open an issue in [their bug tracker][2].&#xD;&#xA;&#xD;&#xA;**UPDATE**:&#xD;&#xA;&#xD;&#xA;You can test this if you run your code from IPython. To summarize, if you use the `%pdb on` magic right before you run the problematic call, the exception is captured by the Python debugger, so you can play around a bit and see that calling the `fit` function with the label values `y[0]` does work  -- see the last line with the `pdb&gt; ` prompt. (The CSV files are downloaded from Kaggle, except for the largest one which is just a part of the real file).&#xD;&#xA;&#xD;&#xA;    In [1]: import pandas as pd&#xD;&#xA;    &#xD;&#xA;    In [2]: from sklearn import neighbors&#xD;&#xA;    &#xD;&#xA;    In [3]: from sklearn_pandas import DataFrameMapper, cross_val_score&#xD;&#xA;    &#xD;&#xA;    In [4]: path_train =&quot;train.csv&quot;&#xD;&#xA;    &#xD;&#xA;    In [5]: path_labels =&quot;trainLabels.csv&quot;&#xD;&#xA;    &#xD;&#xA;    In [6]: path_test = &quot;test.csv&quot;&#xD;&#xA;    &#xD;&#xA;    In [7]: train = pd.read_csv(path_train, header=None)&#xD;&#xA;    &#xD;&#xA;    In [8]: labels = pd.read_csv(path_labels, header=None)&#xD;&#xA;    &#xD;&#xA;    In [9]: test = pd.read_csv(path_test, header=None)&#xD;&#xA;    &#xD;&#xA;    In [10]: mapper_train = DataFrameMapper([(list(train.columns),neighbors.KNeighborsClassifier(n_neighbors=3))])&#xD;&#xA;    &#xD;&#xA;    In [13]: %pdb on&#xD;&#xA;    &#xD;&#xA;    In [14]: mapper_train.fit_transform(train, labels)&#xD;&#xA;    ---------------------------------------------------------------------------&#xD;&#xA;    TypeError                                 Traceback (most recent call last)&#xD;&#xA;    &lt;ipython-input-14-e3897d6db1b5&gt; in &lt;module&gt;()&#xD;&#xA;    ----&gt; 1 mapper_train.fit_transform(train, labels)&#xD;&#xA;    &#xD;&#xA;    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y, **fit_params)&#xD;&#xA;        409         else:&#xD;&#xA;        410             # fit method of arity 2 (supervised transformation)&#xD;&#xA;    --&gt; 411             return self.fit(X, y, **fit_params).transform(X)&#xD;&#xA;        412&#xD;&#xA;        413&#xD;&#xA;    &#xD;&#xA;    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn_pandas/__init__.pyc in fit(self, X, y)&#xD;&#xA;        116         for columns, transformer in self.features:&#xD;&#xA;        117             if transformer is not None:&#xD;&#xA;    --&gt; 118                 transformer.fit(self._get_col_subset(X, columns))&#xD;&#xA;        119         return self&#xD;&#xA;        120&#xD;&#xA;    &#xD;&#xA;    TypeError: fit() takes exactly 3 arguments (2 given)&#xD;&#xA;    &gt; /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn_pandas/__init__.py(118)fit()&#xD;&#xA;        117             if transformer is not None:&#xD;&#xA;    --&gt; 118                 transformer.fit(self._get_col_subset(X, columns))&#xD;&#xA;        119         return self&#xD;&#xA;    &#xD;&#xA;    ipdb&gt; l&#xD;&#xA;        113&#xD;&#xA;        114         X       the data to fit&#xD;&#xA;        115         '''&#xD;&#xA;        116         for columns, transformer in self.features:&#xD;&#xA;        117             if transformer is not None:&#xD;&#xA;    --&gt; 118                 transformer.fit(self._get_col_subset(X, columns))&#xD;&#xA;        119         return self&#xD;&#xA;        120&#xD;&#xA;        121&#xD;&#xA;        122     def transform(self, X):&#xD;&#xA;        123         '''&#xD;&#xA;    ipdb&gt; transformer.fit(self._get_col_subset(X, columns), y[0])&#xD;&#xA;    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',&#xD;&#xA;               n_neighbors=3, p=2, weights='uniform')&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/paulgb/sklearn-pandas/blob/master/sklearn_pandas/__init__.py&#xD;&#xA;  [2]: https://github.com/paulgb/sklearn-pandas/issues" />
  <row Id="1779" PostHistoryTypeId="2" PostId="687" RevisionGUID="c478b097-e601-4002-aded-0c3ac6f211f1" CreationDate="2014-07-07T14:15:51.117" UserId="548" Text="My suggestion would be to attempt to build some kind of clustering on your unlabeled data that somewhat approximates a labelled dataset. The rationale is more or less as follows:&#xD;&#xA;&#xD;&#xA; - You have some feature vector for representing your documents&#xD;&#xA; - Based on that feature vector, you can come up with a number of different clusterings, with either fuzzy, rough, or class-based clustering methods&#xD;&#xA; - Knowing what a positive example looks like, you can quickly evaluate the overall similarity of a cluster to your positive cluster&#xD;&#xA; - Knowing that there should really only be two clusters, you can adjust the hyperparameters on your clustering method so that the above two metrics are closer and closer to satisfaction&#xD;&#xA; - With the two clusters, you have what is likely a close approximation of a labelled dataset, which you can then use as a silver-standard corpus of sorts to actually train your model&#xD;&#xA;&#xD;&#xA;Hope that makes sense, if you're specifically looking for clustering algorithms, a few that I personally enjoy that might be good in this scenario are [FLAME][1] and [tsne][2]. Alternately, looking at the spectacular [gensim][3] library in python will get you a long way toward the clustering you're looking for.&#xD;&#xA;&#xD;&#xA;Hope that helps and makes sense, leave a comment if you've got any questions.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/FLAME_clustering&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding&#xD;&#xA;  [3]: http://radimrehurek.com/gensim/" />
  <row Id="1780" PostHistoryTypeId="2" PostId="688" RevisionGUID="3eb6736b-bf64-4197-8f8c-3d48342217fe" CreationDate="2014-07-07T14:41:59.523" UserId="1367" Text="As @SpacedMan has noted [in a comment][1], the street layout will have a massive influence on the optimization of the walk list.  You have included only &quot;latitude and longitude&quot; in your question's title; but solving that problem does not lead to a &quot;walk list&quot;, but to a &quot;as-the-crow-flies list&quot;.&#xD;&#xA;&#xD;&#xA;Looking at your street layout as a graph, with edge weights describing distances, and trying to find the shortest traversal between all required addresses, will lead you to think of your problem as a &quot;[Shortest path problem][2]&quot;. [Dijkstra's algorithm][3] is the best known solution (there are others); in its naive implementation it converges in *O(n&lt;sup&gt;2&lt;/sup&gt;)*, which may be acceptable if your lists of addresses are moderate in size.  Otherwise, look for optimized versions in the above links.&#xD;&#xA;&#xD;&#xA;As for libraries and resources to start tackling the problem, since you do not specify languages or platforms, let me point to the [compilation of routing solvers in the Open Street Maps wiki][4] and in general [their frameworks and libraries page][5].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/600/how-do-you-create-an-optimized-walk-list-given-longitude-and-latitude-coordinate#comment1670_600&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Shortest_path_problem&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm&#xD;&#xA;  [4]: http://wiki.openstreetmap.org/wiki/Routing#Developers&#xD;&#xA;  [5]: http://wiki.openstreetmap.org/wiki/Frameworks" />
  <row Id="1781" PostHistoryTypeId="2" PostId="689" RevisionGUID="98e684c7-917c-422d-ba44-881560cdad32" CreationDate="2014-07-07T15:36:40.960" UserId="548" Text="There's a number of different ways of going about this depending on exactly how much semantic information you want to retain and how easy your documents are to tokenize (html documents would probably be pretty difficult to tokenize, but you could conceivably do something with tags and context.)&#xD;&#xA;&#xD;&#xA;Some of them have been mentioned by ffriend, and the paragraph vectors by user1133029 is a really solid one, but I just figured I would go into some more depth about plusses and minuses of different approaches.&#xD;&#xA;&#xD;&#xA; - [Cosine Distance][1] - Tried a true, cosine distance is probably the most common distance metric used generically across multiple domains. With that said, there's very little information in cosine distance that can actually be mapped back to anything semantic, which seems to be non-ideal for this situation.&#xD;&#xA; - [Levenshtein Distance][2] - Also known as `edit distance`, this is usually just used on the individual token level (words, bigrams, etc...). In general I wouldn't recommend this metric as it not only discards any semantic information, but also tends to treat very different word alterations very similarly, but it is an extremely common metric for this kind of thing&#xD;&#xA; - [LSA][3] - Is a part of a large arsenal of techniques when it comes to evaluating document similarity called `topic modeling`. LSA has gone out of fashion pretty recently, and in my experience, it's not quite the strongest topic modeling approach, but it is relatively straightforward to implement and has a few open source implementations&#xD;&#xA; - [LDA][4] - Is also a technique used for `topic modeling`, but it's different from `LSA` in that it actually learns internal representations that tend to be more smooth and intuitive. In general, the results you get from `LDA` are better for modeling document similarity than `LSA`, but not quite as good for learning how to discriminate strongly between topics.&#xD;&#xA; - [Pachinko Allocation][5] - Is a really neat extension on top of LDA. In general, this is just a significantly improved version of `LDA`, with the only downside being that it takes a bit longer to train and open-source implementations are a little harder to come by&#xD;&#xA; - [word2vec][6] - Google has been working on a series of techniques for intelligently reducing words and documents to more reasonable vectors than the sparse vectors yielded by techniques such as `Count Vectorizers` and `TF-IDF`. Word2vec is great because it has a number of open source implementations. Once you have the vector, any other similarity metric (like cosine distance) can be used on top of it with significantly more efficacy.&#xD;&#xA; - [doc2vec][7] - Also known as `paragraph vectors`, this is the latest and greatest in a series of papers by Google, looking into dense vector representations of documents. The `gensim` library in python has an implementation of `word2vec` that is straightforward enough that it can pretty reasonably be leveraged to build `doc2vec`, but make sure to keep the license in mind if you want to go down this route&#xD;&#xA;&#xD;&#xA;Hope that helps, let me know if you've got any questions.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Cosine_similarity&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Levenshtein_distance&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/Latent_semantic_analysis&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/Pachinko_allocation&#xD;&#xA;  [6]: https://code.google.com/p/word2vec/&#xD;&#xA;  [7]: http://cs.stanford.edu/~quocle/paragraph_vector.pdf" />
  <row Id="1782" PostHistoryTypeId="2" PostId="690" RevisionGUID="b0c3915c-31f5-4152-8738-5feebbed4571" CreationDate="2014-07-07T18:06:24.583" UserId="1301" Text="Train 2 generative models, one for each dataset (spam only, spam plus ham), that will give you the probability that a datapoint is drawn from the same probability distribution of the training data. Assign emails as spam or ham based on which model gives you the highest probability of the document arising from the training data used to train it. Example generative models are RBM's, autoencoders (in that case, which model has the lowest reconstruction error). There are likely some bayesian generative models also that will assign a probability to a data point based on some training data.&#xD;&#xA;&#xD;&#xA;The best option though would be to take time to curate a second dataset containing only ham. That will give you higher classification accuracy. Assuming a lower proportion of spam to ham emails, that should not be too hard. You can even use Mechanical Turk if you lack the time or resources (or interns \ graduates students or other cheap labor)." />
  <row Id="1783" PostHistoryTypeId="2" PostId="691" RevisionGUID="2c9db33a-02b0-4a13-99cd-c574373e8dea" CreationDate="2014-07-07T18:20:36.090" UserId="1301" Text="Empirically I have found LSA vastly superior to LDA every time and on every dataset I have tried it on. I have talked to other people who have said the same thing. It's also been used to win a number of the SemEval competitions for measuring semantic similarity between documents, often in combinations with a wordnet based measure, so I wouldn't say it's going out of fashion, or is definitely inferior to LDA, which is better for topic modelling and not semantic similarity in my experience, contrary to what some responders have stated. &#xD;&#xA;&#xD;&#xA;If you use gensim (a python library), it has LSA, LDA and word2vec, so you can easily compare the 3. doc2vec is a cool idea, but does not scale very well and you will likely have to implement it yourself as I am unaware of any open source implementations. It does not scale well as for each document, a new and separate model has to be built using SGD, a slow machine learning algorithm. But it will probably give you the most accurate results. LSA and LDA also don't scale well (word2vec does however), LDA scales worse in general. Gensim's implementations are very fast however, as it uses iterative SVD.&#xD;&#xA;&#xD;&#xA;One other note, if you use word2vec, you will still have to determine a way to compose vectors from documents, as it gives you a different vector per word. The simplest way to do this is to normalize each vector and take the mean over all word vectors in the document, or take a weighted mean by idf weighting of each word. So it's not as simple as 'use word2vec', you will need to do something further to compute document similarity. &#xD;&#xA;&#xD;&#xA;I would personally go with LSA, as I have seen it work well empirically, and gensim's library scales very well. However, there's no free lunch, so preferably try each method and see which works better for your data." />
  <row Id="1784" PostHistoryTypeId="2" PostId="692" RevisionGUID="572437c1-bf5a-4519-8e48-6e82800c6470" CreationDate="2014-07-07T18:28:50.420" UserId="1301" Text="The cosine similarity metric does a good (if not perfect) job of controlling for the document length, so comparing the similarity of 2 documents or 2 queries using the cosine metric and tf idf weights for the words should work well in either case. I would also recommend doing LSA first on tf idf weights, and then computing the cosine distance\similarities.&#xD;&#xA;&#xD;&#xA;If you are trying to build a search engine, I would recommend using a free open source search engine like solr or elastic search, or just the raw lucene libraries, as they do most of the work for you, and have good built in methods for handling the query to document similarity problem." />
  <row Id="1785" PostHistoryTypeId="2" PostId="693" RevisionGUID="9ce5a69f-7baa-4368-8688-1f6e6e9f1209" CreationDate="2014-07-07T18:36:23.430" UserId="1301" Text="I work for an online jobs site and we build solutions to recommend jobs based on resumes. Our approach take's a person's job title (or desired job title if a student and known), along with skills we extract from their resume, and their location (which is very important to most people) and find matches with jobs based on that. &#xD;&#xA;&#xD;&#xA;in terms of document classification, I would take a similar approach. I would recommend computing a tf idf matrix for each resume as a standard bag of words model, extracting just the person's job title and skills (for which you will need to define a list of skills to look for), and feed that into a ML algorithm. I would recommend trying knn, and an SVM, the latter works very well with high dimensional text data. Linear SVM's tend to do better than non-linear (e.g. using RBf kernels). If you have that outputting reasonable results, I would then play with extracting features using a natural language parser \ chunker, and also some custom built phrases matched by regex's." />
  <row Id="1786" PostHistoryTypeId="2" PostId="694" RevisionGUID="b07cc531-109e-4364-a55f-e945d118d409" CreationDate="2014-07-07T19:17:04.973" UserId="989" Text="I'm using Neural Networks to solve different Machine learning problems. I'm using Python and [pybrain][1] but this library is almost discontinued. Are there other good alternatives in Python?&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://pybrain.org/" />
  <row Id="1787" PostHistoryTypeId="1" PostId="694" RevisionGUID="b07cc531-109e-4364-a55f-e945d118d409" CreationDate="2014-07-07T19:17:04.973" UserId="989" Text="Best python library for neural networks" />
  <row Id="1788" PostHistoryTypeId="3" PostId="694" RevisionGUID="b07cc531-109e-4364-a55f-e945d118d409" CreationDate="2014-07-07T19:17:04.973" UserId="989" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;neuralnetwork&gt;" />
  <row Id="1789" PostHistoryTypeId="2" PostId="695" RevisionGUID="83fad451-79eb-4c22-8607-18fbf0af94b3" CreationDate="2014-07-07T19:55:51.057" UserId="684" Text="[Pylearn2](http://deeplearning.net/software/pylearn2/) is generally considered the library of choice for neural networks and deep learning in python.  Its designed for easy scientific experimentation rather than ease of use, so the learning curve is rather steep, but if you take your time and follow the tutorials I think you'll be happy with the functionality it provides.  Everything from standard Multilayer Perceptrons to Restricted Boltzmann Machines to Convolutional Nets to Autoencoders are provided.  There's great GPU support and everything is built on top of Theano, so performance is typically quite good.&#xD;&#xA;&#xD;&#xA;Be aware that Pylearn2 has the opposite problem of pybrain at the moment -- rather than being abandoned, Pylearn2 is under active development and is subject to frequent changes." />
  <row Id="1791" PostHistoryTypeId="5" PostId="685" RevisionGUID="0cdf5161-92ad-4c9d-8234-2a8f0363f85b" CreationDate="2014-07-07T20:02:08.777" UserId="322" Comment="grammar overhaul, well-formatted formula, add tags" Text="I am very new to machine learning and in my first project have stumbled across a lot of issues which I really want to get through.&#xD;&#xA;&#xD;&#xA;I'm using logistic regression with R's `glmnet` package and alpha = 0 for ridge regression.&#xD;&#xA;&#xD;&#xA;I'm using ridge regression actually since lasso deleted all my variables and gave very low area under curve (0.52) but with ridge there isn't much of a difference (0.61).&#xD;&#xA;&#xD;&#xA;My dependent variable/output is probability of click, based on if there is a click or not in historical data.&#xD;&#xA;&#xD;&#xA;The independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version and OS family.&#xD;&#xA;&#xD;&#xA;Of these, for prediction I'm using state, device, user age, user gender, IP carrier, browser version, browser family, OS version and OS family; I am not using keyword or template since we want to reject a user request before deep diving in our system and selecting a keyword or template. I am not using city because they are too many or mobile manufacturer because they are too few.&#xD;&#xA;&#xD;&#xA;**Is that okay or should I be using the rejected variables?**&#xD;&#xA;&#xD;&#xA;To start, I create a sparse matrix from my variables which are mapped against the column of clicks that have yes or no values.&#xD;&#xA;&#xD;&#xA;After training the model, I save the coefficients and intercept. These are used for new incoming requests using the formula for logistic regression:&#xD;&#xA;&#xD;&#xA;&gt; ![1 / (1+e^-1*sum(a+k(ith)*x(ith)))][1]&#xD;&#xA;&#xD;&#xA;Where `a` is intercept, `k` is the `i`th coefficient and `x` is the `i`th variable value.&#xD;&#xA;&#xD;&#xA;**Is my approach correct so far?**&#xD;&#xA;&#xD;&#xA;Simple GLM in R (that is where there is no regularized regression, right?) gave me 0.56 AUC. With regularization I get 0.61 but there is no distinct threshold where we could say that above 0.xx its mostly ones and below it most zeros are covered; actually, the max probability that a click didn't happen is almost always greater than the max probability that a click happened.&#xD;&#xA;&#xD;&#xA;**So basically what should I do?**&#xD;&#xA;&#xD;&#xA;I have read how stochastic gradient descent is an effective technique in logit so how do I implement stochastic gradient descent in R? If it's not straightforward, is there a way to implement this system in Python? Is SGD implemented after generating a regularized logistic regression model or is it a different process altogether?&#xD;&#xA;&#xD;&#xA;Also there is an algorithm called follow the regularized leader (FTRL) that is used in click-through rate prediction. Is there a sample code and use of FTRL that I could go through?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/AZBRq.png" />
  <row Id="1792" PostHistoryTypeId="4" PostId="685" RevisionGUID="0cdf5161-92ad-4c9d-8234-2a8f0363f85b" CreationDate="2014-07-07T20:02:08.777" UserId="322" Comment="grammar overhaul, well-formatted formula, add tags" Text="Stochastic gradient descent in logistic regression" />
  <row Id="1793" PostHistoryTypeId="6" PostId="685" RevisionGUID="0cdf5161-92ad-4c9d-8234-2a8f0363f85b" CreationDate="2014-07-07T20:02:08.777" UserId="322" Comment="grammar overhaul, well-formatted formula, add tags" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;r&gt;&lt;logistic-regression&gt;&lt;gradient-descent&gt;" />
  <row Id="1794" PostHistoryTypeId="24" PostId="685" RevisionGUID="0cdf5161-92ad-4c9d-8234-2a8f0363f85b" CreationDate="2014-07-07T20:02:08.777" Comment="Proposed by 322 approved by 434, 1273 edit id of 109" />
  <row Id="1796" PostHistoryTypeId="2" PostId="697" RevisionGUID="34a1f41f-2ec8-4b3d-a5c9-15ede509c5ef" CreationDate="2014-07-07T21:26:36.830" UserId="199" Text="I'm trying to run some analysis with some big datasets (eg 400k rows vs. 400 columns) with R (e.g. using neural networks and recommendation systems).&#xD;&#xA;But, it's taking too long to process the data (with huge matrices, e.g. 400k rows vs. 400k columns).&#xD;&#xA;What are some free/cheap ways to improve R performance?&#xD;&#xA;&#xD;&#xA;I'm accepting packages or web services suggestions (other options are welcome).&#xD;&#xA;" />
  <row Id="1797" PostHistoryTypeId="1" PostId="697" RevisionGUID="34a1f41f-2ec8-4b3d-a5c9-15ede509c5ef" CreationDate="2014-07-07T21:26:36.830" UserId="199" Text="Running huge datasets with R" />
  <row Id="1798" PostHistoryTypeId="3" PostId="697" RevisionGUID="34a1f41f-2ec8-4b3d-a5c9-15ede509c5ef" CreationDate="2014-07-07T21:26:36.830" UserId="199" Text="&lt;bigdata&gt;&lt;r&gt;&lt;optimization&gt;&lt;processing&gt;" />
  <row Id="1800" PostHistoryTypeId="2" PostId="698" RevisionGUID="46435fa7-87b4-47b5-a43d-5d7e32d2bbd8" CreationDate="2014-07-08T02:08:04.560" UserId="1379" Text="They can't predict, but they can tell you the most likely result. There's an study about this kind of approach from **Etienne** - [Predicting Who Will Win the World Cup with Wolfram Language][1]. This is a very detailed study, so you can check all the methodology used to get the predictions. &#xD;&#xA;&#xD;&#xA; &#xD;&#xA;Interesting enough, 11 from 15 matches were correct!&#xD;&#xA;&#xD;&#xA;&gt; As one might expect, Brazil is the favorite, with a probability to win of 42.5%. This striking result is due to the fact that Brazil has both the highest Elo ranking and plays at home.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;(Let's go Brazil!)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://blog.wolfram.com/2014/06/20/predicting-who-will-win-the-world-cup-with-wolfram-language/" />
  <row Id="1801" PostHistoryTypeId="2" PostId="699" RevisionGUID="7a197e5c-12e6-4386-b021-f1fa7ae9aa21" CreationDate="2014-07-08T06:40:36.923" UserId="1384" Text="From my experience only some classes of queries can be classified on lexical features (due to ambiguity of natural language). Instead you can try to use boolean search results (sites or segments of sites, not documents, without ranking) as features for classification (instead on words). This approach works well in classes where there is a big lexical ambiguity in a query but exists a lot of good sites relevant to the query (e.g. movies, music, commercial queries and so on).&#xD;&#xA;&#xD;&#xA;Also, for offline classification you can do LSI on query-site matrix. See &quot;Introduction to Information Retrieval&quot; book for details." />
  <row Id="1802" PostHistoryTypeId="2" PostId="700" RevisionGUID="fc33a836-7cb9-4a5d-8e6d-8cbe6a6e8e66" CreationDate="2014-07-08T07:29:34.167" UserId="1386" Text="I have a set of datapoints from the unit interval (i.e. 1-dimensional dataset with numerical values). I receive some additional datapoints online, and moreover the value of some datapoints might change dynamically. I'm looking for an ideal clustering algorithm which can handle these issues efficiently.&#xD;&#xA;&#xD;&#xA;I know [sequential k-means clustering](https://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm) copes with the addition of new instances, and I suppose with minor modification it can work with dynamic instance values (i.e. first taking the modified instance from the respective cluster, then updating the mean of the cluster and finally giving the modified instance as an input to the algorithm just as the addition of an unseen instance).&#xD;&#xA;&#xD;&#xA;My concern with using the k-means algorithm is the requirement of supplying the number of clusters as an input. I know that they beat other clustering algorithms (GAs, MSTs, Hierarchical Methods etc.) in time&amp;space complexity. Honestly I'm not sure, but maybe I can get away with using one of the aforementioned algorithms. Even that my datasets are relatively large, the existence of a single dimension makes me wonder.&#xD;&#xA;&#xD;&#xA;More specifically a typical test case of mine would contain about 10K-200K 1-dimensional datapoints. I would like to complete the clustering preferably under a second. The dynamic changes in the value points are assumed to be smooth, i.e. relatively small. Thus being able to use existing solutions (i.e. being able to continue clustering on the existing one when a value is changed or new one is added) is highly preferred. &#xD;&#xA;&#xD;&#xA;So all in all:&#xD;&#xA;&gt; Can you think of an algorithm which will provide a sweet spot between computational efficiency and the accuracy of clusters wrt. the problem defined above?&#xD;&#xA;&#xD;&#xA;&gt; Are there some nice heuristics for the k-means algorithm to automatically compute the value of K beforehand?" />
  <row Id="1803" PostHistoryTypeId="1" PostId="700" RevisionGUID="fc33a836-7cb9-4a5d-8e6d-8cbe6a6e8e66" CreationDate="2014-07-08T07:29:34.167" UserId="1386" Text="Efficient dynamic clustering" />
  <row Id="1804" PostHistoryTypeId="3" PostId="700" RevisionGUID="fc33a836-7cb9-4a5d-8e6d-8cbe6a6e8e66" CreationDate="2014-07-08T07:29:34.167" UserId="1386" Text="&lt;machine-learning&gt;&lt;algorithms&gt;&lt;clustering&gt;&lt;k-means&gt;&lt;hierarchical-data-format&gt;" />
  <row Id="1805" PostHistoryTypeId="2" PostId="701" RevisionGUID="62f04c14-e734-47e7-a561-32e97ef813a4" CreationDate="2014-07-08T07:37:57.123" UserId="645" Text="I have generated a dataset of pairwise distances as follows:&#xD;&#xA;&#xD;&#xA;    id_1 id_2 dist_12&#xD;&#xA;    id_2 id_3 dist_23&#xD;&#xA;&#xD;&#xA;I want to cluster this data so as to identify the pattern. I have been looking at Spectral clustering and DBSCAN, but I haven't been able to come to a conclusion and have been ambiguous on how to make use of the existing implementations of these algorithms. I have been looking at Python and Java implementations so far.&#xD;&#xA;&#xD;&#xA;Could anyone point me to a tutorial or demo on how to make use of these clustering algorithms to handle the situation in hand?&#xD;&#xA;&#xD;&#xA;Thanks,&#xD;&#xA;TM" />
  <row Id="1806" PostHistoryTypeId="1" PostId="701" RevisionGUID="62f04c14-e734-47e7-a561-32e97ef813a4" CreationDate="2014-07-08T07:37:57.123" UserId="645" Text="Clustering pair-wise distance dataset" />
  <row Id="1807" PostHistoryTypeId="3" PostId="701" RevisionGUID="62f04c14-e734-47e7-a561-32e97ef813a4" CreationDate="2014-07-08T07:37:57.123" UserId="645" Text="&lt;clustering&gt;" />
  <row Id="1808" PostHistoryTypeId="2" PostId="702" RevisionGUID="7ff769a1-d669-47d3-8b50-84d6a1513863" CreationDate="2014-07-08T09:18:17.990" UserId="1367" Text="In the scikit-learn implementation of Spectral clustering and DBSCAN you do not need to precompute the distances, you should input the sample coordinates for all `id_1` ... `id_n`.  Here is a simplification of the [documented example comparison of clustering algorithms][1]:&#xD;&#xA;&#xD;&#xA;    import numpy as np&#xD;&#xA;    from sklearn import cluster&#xD;&#xA;    from sklearn.preprocessing import StandardScaler&#xD;&#xA;    &#xD;&#xA;    ## Prepare the data&#xD;&#xA;    X = np.random.rand(1500, 2)&#xD;&#xA;    # When reading from a file of the form: `id_n coord_x coord_y`&#xD;&#xA;    # you will need this call instead:&#xD;&#xA;    # X = np.loadtxt('coords.csv', usecols=(1, 2))&#xD;&#xA;    X = StandardScaler().fit_transform(X)&#xD;&#xA;    &#xD;&#xA;    ## Instantiate the algorithms&#xD;&#xA;    spectral = cluster.SpectralClustering(n_clusters=2,&#xD;&#xA;                                          eigen_solver='arpack',&#xD;&#xA;                                          affinity=&quot;nearest_neighbors&quot;)&#xD;&#xA;    dbscan = cluster.DBSCAN(eps=.2)&#xD;&#xA;    &#xD;&#xA;    ## Use the algorithms&#xD;&#xA;    spectral_labels = spectral.fit_predict(X)&#xD;&#xA;    dbscan_labels = dbscan.fit_predict(X)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html" />
  <row Id="1809" PostHistoryTypeId="2" PostId="703" RevisionGUID="ffb23650-6e09-4487-9613-c1120bd62f20" CreationDate="2014-07-08T10:36:44.220" UserId="1390" Text="Pylearn is relies on Theano and as mentioned in other answer to use the library is really complicated, until you get the hold of it.&#xD;&#xA;&#xD;&#xA;In the meantime I would suggest using [Theanets][1]. It aslo built on top of Theano, but is much more easier to work with. It might be true, that it doesn't have all the features of Pylearn, but for the basic work it's sufficient.&#xD;&#xA;&#xD;&#xA;Also it's open source, so you can add custom networks on the fly, if you dare. :)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/lmjohns3/theano-nets/" />
  <row Id="1811" PostHistoryTypeId="2" PostId="704" RevisionGUID="859d33ca-9a84-472b-b8c1-dc0ccf9b932a" CreationDate="2014-07-08T13:45:07.583" UserId="728" Text="In my Uni we have an HPC computing clusters. I use the cluster to train classifiers and so on. So usually to send a job to the cluster, (e.g. python scikit-learn script), I need to write a Bash scrip file that contains (among other lines) a line like this `qsub script.py`.&#xD;&#xA;&#xD;&#xA;However I find this process very very frustrating. Usually what happens is that I write the python script on my laptop and then I login to the server and update the SVN repository so I get the same python script there. Then I write that Bash script or edit it. Then I run the bash script. As you see this is really frustrating, since for every little update for the python script I need to do many steps in order to have it executed at the computing clusters. Of course the task gets even more complicated when I have to put the data on the server and use the datasets' path on the server.&#xD;&#xA;&#xD;&#xA;I'm sure many people here are using computing clusters for their data science tasks. I just want to know how you guys manage sending the jobs to the clusters?" />
  <row Id="1812" PostHistoryTypeId="1" PostId="704" RevisionGUID="859d33ca-9a84-472b-b8c1-dc0ccf9b932a" CreationDate="2014-07-08T13:45:07.583" UserId="728" Text="Working with HPC clusters" />
  <row Id="1813" PostHistoryTypeId="3" PostId="704" RevisionGUID="859d33ca-9a84-472b-b8c1-dc0ccf9b932a" CreationDate="2014-07-08T13:45:07.583" UserId="728" Text="&lt;bigdata&gt;&lt;data-mining&gt;" />
  <row Id="1814" PostHistoryTypeId="2" PostId="705" RevisionGUID="b5a5f8d2-3f45-46c9-a4bf-e61abfb4ca37" CreationDate="2014-07-08T14:32:55.827" UserId="1399" Text="Stochastic gradient descent is a method of setting the parameters of the regressor; since the objective for logistic regression is convex (has only one maximum), this won't be an issue and SGD is generally only needed to improve convergence speed with masses of training data.&#xD;&#xA;&#xD;&#xA;What your numbers suggest to me is that your features are not adequate to separate the classes. Consider adding extra features if you can think any any that are useful. You might also consider interactions and quadratic features in your original feature space." />
  <row Id="1815" PostHistoryTypeId="5" PostId="695" RevisionGUID="035aa9ff-6a0d-4e41-8cdf-bf779a2a3e04" CreationDate="2014-07-08T14:39:21.820" UserId="684" Comment="added 89 characters in body" Text="[Pylearn2](http://deeplearning.net/software/pylearn2/) is generally considered the library of choice for neural networks and deep learning in python.  Its designed for easy scientific experimentation rather than ease of use, so the learning curve is rather steep, but if you take your time and follow the tutorials I think you'll be happy with the functionality it provides.  Everything from standard Multilayer Perceptrons to Restricted Boltzmann Machines to Convolutional Nets to Autoencoders are provided.  There's great GPU support and everything is built on top of Theano, so performance is typically quite good.  The source for Pylearn2 is available [on github](https://github.com/lisa-lab/pylearn2).&#xD;&#xA;&#xD;&#xA;Be aware that Pylearn2 has the opposite problem of pybrain at the moment -- rather than being abandoned, Pylearn2 is under active development and is subject to frequent changes." />
  <row Id="1820" PostHistoryTypeId="5" PostId="701" RevisionGUID="b9b4ae3c-5425-4d32-a9fb-febf7d8c9654" CreationDate="2014-07-09T00:13:52.097" UserId="84" Comment="Improving formatting." Text="I have generated a dataset of pairwise distances as follows:&#xD;&#xA;&#xD;&#xA;    id_1 id_2 dist_12&#xD;&#xA;    id_2 id_3 dist_23&#xD;&#xA;&#xD;&#xA;I want to cluster this data so as to identify the pattern. I have been looking at Spectral clustering and DBSCAN, but I haven't been able to come to a conclusion and have been ambiguous on how to make use of the existing implementations of these algorithms. I have been looking at Python and Java implementations so far.&#xD;&#xA;&#xD;&#xA;Could anyone point me to a tutorial or demo on how to make use of these clustering algorithms to handle the situation in hand?" />
  <row Id="1821" PostHistoryTypeId="6" PostId="701" RevisionGUID="b9b4ae3c-5425-4d32-a9fb-febf7d8c9654" CreationDate="2014-07-09T00:13:52.097" UserId="84" Comment="Improving formatting." Text="&lt;data-mining&gt;&lt;clustering&gt;" />
  <row Id="1822" PostHistoryTypeId="5" PostId="662" RevisionGUID="86406d8f-6544-45fc-83c4-a378dddf2e57" CreationDate="2014-07-09T00:19:01.640" UserId="1352" Comment="removed &quot;thanks&quot;" Text="Note that I am doing everything in R. &#xD;&#xA;&#xD;&#xA;The problem goes as follow: &#xD;&#xA;&#xD;&#xA;Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don't. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation . &#xD;&#xA;&#xD;&#xA;Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem. &#xD;&#xA;&#xD;&#xA;My original idea:  *make this a supervised learning problem*. &#xD;&#xA;Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those *unlabelled data*, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to. &#xD;&#xA;&#xD;&#xA;**Update**&#xD;&#xA;Question 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? For example, the text file may look something like this:&#xD;&#xA;&#xD;&#xA;`I deployed ML algorithm in this project and... Skills: Java, Python, c++ ...`&#xD;&#xA; &#xD;&#xA;This is what I meant by 'unstructured', i.e. collapsing everything into a single line string.&#xD;&#xA;&#xD;&#xA;Is this approach wrong ? Please correct me if you think my approach is wrong. &#xD;&#xA;&#xD;&#xA;Question 3: The tricky part is: how to **identify and extract the keywords** ? Using the `tm` package in R ? what algorithm is the `tm`   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Any ideas would be great.&#xD;&#xA;&#xD;&#xA; " />
  <row Id="1823" PostHistoryTypeId="24" PostId="662" RevisionGUID="86406d8f-6544-45fc-83c4-a378dddf2e57" CreationDate="2014-07-09T00:19:01.640" Comment="Proposed by 1352 approved by 84 edit id of 108" />
  <row Id="1824" PostHistoryTypeId="5" PostId="636" RevisionGUID="9634e5fa-6841-4049-a47d-ba5de4758c2a" CreationDate="2014-07-09T00:19:12.683" UserId="322" Comment="proofreading grammar" Text="I am new to machine learning. I have a task at hand of predicting click probability given user information like city, state, OS version, OS family, device, browser family, browser version, etc. I have been advised to try logit since logit seems to be what MS and Google are using. I have some questions regarding logistic regression:&#xD;&#xA;&#xD;&#xA;Click and non click is a very very unbalanced class and the simple GLM predictions do not look good. How can I make the data work better with the GLM?&#xD;&#xA;&#xD;&#xA;All the variables I have are categorical and things like device and city can be numerous. Also the frequency of occurrence of some devices or some cities can be very very low. How can I deal with this distribution of categorical variables?&#xD;&#xA;&#xD;&#xA;One of the variables that we get is device ID. This is a very unique feature that can be translated to a user's identity. How can I make use of it in logit, or should it be used in a completely different model based on user identity?" />
  <row Id="1825" PostHistoryTypeId="4" PostId="636" RevisionGUID="9634e5fa-6841-4049-a47d-ba5de4758c2a" CreationDate="2014-07-09T00:19:12.683" UserId="322" Comment="proofreading grammar" Text="Data preparation and machine learning algorithm for click prediction" />
  <row Id="1826" PostHistoryTypeId="24" PostId="636" RevisionGUID="9634e5fa-6841-4049-a47d-ba5de4758c2a" CreationDate="2014-07-09T00:19:12.683" Comment="Proposed by 322 approved by 434, 84 edit id of 103" />
  <row Id="1827" PostHistoryTypeId="5" PostId="634" RevisionGUID="80e9a95e-b9f3-4f6d-bf96-025d103ca3bd" CreationDate="2014-07-09T00:19:42.423" UserId="322" Comment="More explicit title, some grammar" Text="I'm working on a fraud detection system. In this field, new frauds appear regularly, so that new features have to be added to the model on ongoing basis. &#xD;&#xA;&#xD;&#xA;I wonder what is the best way to handle it (from the development process perspective)? Just adding a new feature into the feature vector and re-training the classifier seems to be a naive approach, because too much time will be spent for re-learning of the old features.&#xD;&#xA;&#xD;&#xA;I'm thinking along the way of training a classifier for each feature (or a couple of related features), and then combining the results of those classifiers with an overall classifier. Are there any drawbacks of this approach? How can I choose an algorithm for the overall classifier?" />
  <row Id="1828" PostHistoryTypeId="4" PostId="634" RevisionGUID="80e9a95e-b9f3-4f6d-bf96-025d103ca3bd" CreationDate="2014-07-09T00:19:42.423" UserId="322" Comment="More explicit title, some grammar" Text="Handling a regularly increasing feature set" />
  <row Id="1829" PostHistoryTypeId="24" PostId="634" RevisionGUID="80e9a95e-b9f3-4f6d-bf96-025d103ca3bd" CreationDate="2014-07-09T00:19:42.423" Comment="Proposed by 322 approved by 434, 84 edit id of 104" />
  <row Id="1830" PostHistoryTypeId="5" PostId="658" RevisionGUID="157056ee-5e53-4de5-bcc1-9b4fd009751f" CreationDate="2014-07-09T00:20:49.460" UserId="1163" Comment="Formatted text" Text="I would like to use another type of data, not atomic data, as a feature for a prediction. &#xD;&#xA;Suppose I have a Table with those Features:&#xD;&#xA;&lt;pre&gt;&#xD;&#xA;- Column 1: Categorical - House&#xD;&#xA;- Column 2: Numerical - 23.22&#xD;&#xA;- Column 3: A Vector - [ 12, 22, 32 ]&#xD;&#xA;- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]&#xD;&#xA;- Column 5: A List [ 122, Boolean ]&#xD;&#xA;&lt;/pre&gt;&#xD;&#xA;I would like to predict/classify ... Columns 2 ... for example....&#xD;&#xA;&#xD;&#xA;I am making a Software to automatically respond questions... Any type...like &quot;Where Foo was Born ?&quot; ...&#xD;&#xA;&#xD;&#xA;I first make a query to a search engine ---&gt;&gt;&gt; then I get some Text data as a Result.&#xD;&#xA;So I do all the Parsing Staff... Tagging, Stemming, Parsing, Splitting... &#xD;&#xA;My first approach was to make a table, each row with a line of text.. and a lot of Features...like ... First Word ... Tag of First Word.. Chunks, etc..&#xD;&#xA;But with this approach I am missing the relationships between the Sentences. &#xD;&#xA;&#xD;&#xA;I would like to know if there is an algorithm that look inside the Tree Structures... Vectors... and make the relations and extract whatever is relevant for predicting/classifying.&#xD;&#xA;I rather know a library that does that then an algorithm that I have to implement...&#xD;&#xA;&#xD;&#xA;Thank you very much !" />
  <row Id="1831" PostHistoryTypeId="24" PostId="658" RevisionGUID="157056ee-5e53-4de5-bcc1-9b4fd009751f" CreationDate="2014-07-09T00:20:49.460" Comment="Proposed by 1163 approved by 434, -1 edit id of 107" />
  <row Id="1832" PostHistoryTypeId="5" PostId="658" RevisionGUID="a63486a3-ea30-49c0-9dd3-7afe49823480" CreationDate="2014-07-09T00:20:49.460" UserId="84" Comment="Formatted text" Text="I would like to use another type of data, not atomic data, as a feature for a prediction. &#xD;&#xA;Suppose I have a Table with those Features:&#xD;&#xA;&lt;pre&gt;&#xD;&#xA;- Column 1: Categorical - House&#xD;&#xA;- Column 2: Numerical - 23.22&#xD;&#xA;- Column 3: A Vector - [ 12, 22, 32 ]&#xD;&#xA;- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]&#xD;&#xA;- Column 5: A List [ 122, Boolean ]&#xD;&#xA;&lt;/pre&gt;&#xD;&#xA;I would like to predict/classify ... Columns 2 ... for example....&#xD;&#xA;&#xD;&#xA;I am making a Software to automatically respond questions... Any type...like &quot;Where Foo was Born ?&quot; ...&#xD;&#xA;&#xD;&#xA;I first make a query to a search engine ---&gt;&gt;&gt; then I get some Text data as a Result.&#xD;&#xA;So I do all the Parsing Staff... Tagging, Stemming, Parsing, Splitting... &#xD;&#xA;My first approach was to make a table, each row with a line of text.. and a lot of Features...like ... First Word ... Tag of First Word.. Chunks, etc..&#xD;&#xA;But with this approach I am missing the relationships between the Sentences. &#xD;&#xA;&#xD;&#xA;I would like to know if there is an algorithm that look inside the Tree Structures... Vectors... and make the relations and extract whatever is relevant for predicting/classifying. I'd prefer to know about a library that does that than an algorithm that I have to implement." />
  <row Id="1833" PostHistoryTypeId="5" PostId="704" RevisionGUID="91f6ce21-8205-425d-8e7b-9be38d1b3fcf" CreationDate="2014-07-09T00:25:47.190" UserId="84" Comment="Improving formatting." Text="In my university, we have an HPC computing cluster. I use the cluster to train classifiers and so on. So, usually, to send a job to the cluster, (e.g. python scikit-learn script), I need to write a Bash script that contains (among others) a command like `qsub script.py`.&#xD;&#xA;&#xD;&#xA;However, I find this process very very frustrating. Usually what happens is that I write the python script on my laptop and then I login to the server and update the SVN repository, so I get the same python script there. Then I write that Bash script or edit it, so I can run the bash script.&#xD;&#xA;&#xD;&#xA;As you see this is really frustrating since, for every little update for the python script, I need to do many steps to have it executed at the computing cluster. Of course the task gets even more complicated when I have to put the data on the server and use the datasets' path on the server.&#xD;&#xA;&#xD;&#xA;I'm sure many people here are using computing clusters for their data science tasks. I just want to know how you guys manage sending the jobs to the clusters?" />
  <row Id="1834" PostHistoryTypeId="2" PostId="707" RevisionGUID="7eed256f-7943-4bfc-a8fa-8ec1ce8f3168" CreationDate="2014-07-09T00:37:51.167" UserId="684" Text="Try a recurrent neural network, a model well suited for time series data. They're notoriously difficult to train, but seem to perform well when trained properly: http://cs229.stanford.edu/proj2012/BernalFokPidaparthi-FinancialMarketTimeSeriesPredictionwithRecurrentNeural.pdf" />
  <row Id="1835" PostHistoryTypeId="2" PostId="708" RevisionGUID="8cbda1f9-fe35-4c68-b83b-702ac15495a1" CreationDate="2014-07-09T00:44:28.207" UserId="84" Text="There are many solutions to ease the burden of copying the file from a local machine to the computing nodes in the clusters. A simple approach is to use an interface that allows multi-access to the machines in the cluster, like [clusterssh](http://sourceforge.net/projects/clusterssh/) (cssh). It allows you to type commands to multiple machines at once via a set of terminal screens (each one a ssh connection to a different machine in the cluster).&#xD;&#xA;&#xD;&#xA;Since your cluster seem to have `qsub` set up, your problem may be rather related to *replicating* the data along the machines (other than simply running a command in each node). So, to address this point, you may either write an `scp` script, to copy things to and from each node in the cluster (which is surely better addressed with SVN), or you may set up a NFS. This would allow for a simple and transparent access to the data, and also reduce the need for replicating unnecessary data.&#xD;&#xA;&#xD;&#xA;For example, you could access a node, copy the data to such place, and simply use the data *remotely*, via network communication. I'm not acquainted with how to set up a NFS, but you already have access to it (in case your home folder is the same across the machines you access). Then, the scripts and data could be sent to a single place, and later accessed from others. This is akin to the SVN approach, except it's more transparent/straightforward." />
  <row Id="1836" PostHistoryTypeId="5" PostId="671" RevisionGUID="11ade90a-b22d-4f11-9c47-f8fbe0b9700a" CreationDate="2014-07-09T06:38:11.923" UserId="84" Comment="Improving writing." Text="I have a linearly increasing time series dataset of a sensor, with value ranges between 50 and 150. I've implemented a [Simple Linear Regression][1] algorithm to fit a regression line on such data, and I'm predicting the date when the series would reach 120.&#xD;&#xA;&#xD;&#xA;All works fine when the series move upwards. But, there are cases in which the sensor reaches around 110 or 115, and it is reset; in such cases the values would start over again at, say, 50 or 60.&#xD;&#xA;&#xD;&#xA;This is where I start facing issues with the regression line, as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I'm trying to understand if there are any algorithms available that consider this case.&#xD;&#xA;&#xD;&#xA;I'm new to data science, would appreciate any pointers to move further.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Simple_linear_regression" />
  <row Id="1837" PostHistoryTypeId="2" PostId="709" RevisionGUID="78b49eef-7e20-44b6-aba2-4fb8438d59c2" CreationDate="2014-07-09T07:08:34.740" UserId="1131" Text="Since you mention you are building a recommendation system, I believe you have a sparse matrix which you are working on. Check [sparseMatrix](http://stat.ethz.ch/R-manual/R-devel/library/Matrix/html/sparseMatrix.html) from Matrix package. This should be able to help you with storing your large size matrix in memory and train your model. " />
  <row Id="1838" PostHistoryTypeId="2" PostId="710" RevisionGUID="b7a45c64-cab7-4f3d-811b-a65e90176df1" CreationDate="2014-07-09T11:05:40.813" UserId="979" Text="I'm looking for commercial text summarization tools (APIs, Libraries,...) which are able to perform any of the following tasks:&#xD;&#xA;&#xD;&#xA;1. Extractive Multi-Document Summarization (Generic or query-based)&#xD;&#xA;2. Extractive Single-Document Summarization (Generic or query-based)&#xD;&#xA;3. Generative Single-Document Summarization (Generic or query-based)&#xD;&#xA;4. Generative Multi-Document Summarization (Generic or query-based)&#xD;&#xA;&#xD;&#xA;Thank you in advance.&#xD;&#xA;&#xD;&#xA;Regards,&#xD;&#xA;pasmod&#xD;&#xA;" />
  <row Id="1839" PostHistoryTypeId="1" PostId="710" RevisionGUID="b7a45c64-cab7-4f3d-811b-a65e90176df1" CreationDate="2014-07-09T11:05:40.813" UserId="979" Text="Commercial Text Summarization Tools" />
  <row Id="1840" PostHistoryTypeId="3" PostId="710" RevisionGUID="b7a45c64-cab7-4f3d-811b-a65e90176df1" CreationDate="2014-07-09T11:05:40.813" UserId="979" Text="&lt;text-mining&gt;" />
  <row Id="1841" PostHistoryTypeId="2" PostId="711" RevisionGUID="ede783de-aeca-4026-8bae-e5e20cb9476c" CreationDate="2014-07-09T12:22:22.400" UserId="836" Text="This question is in response to a comment I saw on another question.&#xD;&#xA;&#xD;&#xA;The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of &quot;SVMs are not used so much nowadays&quot;.&#xD;&#xA;&#xD;&#xA;I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a &quot;niche&quot; covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.&#xD;&#xA;&#xD;&#xA;So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM's &quot;sweet spot&quot; just as well, better CPUs meaning SVM's computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?&#xD;&#xA;&#xD;&#xA;I tried a search for e.g. &quot;are support vector machines out of fashion&quot; and found nothing to imply they were being dropped in favour of anything else.&#xD;&#xA;&#xD;&#xA;And Wikipedia has this: http://en.wikipedia.org/wiki/Support_vector_machine#Issues . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights." />
  <row Id="1842" PostHistoryTypeId="1" PostId="711" RevisionGUID="ede783de-aeca-4026-8bae-e5e20cb9476c" CreationDate="2014-07-09T12:22:22.400" UserId="836" Text="Are Support Vector Machines still considered &quot;state of the art&quot; in their niche?" />
  <row Id="1843" PostHistoryTypeId="3" PostId="711" RevisionGUID="ede783de-aeca-4026-8bae-e5e20cb9476c" CreationDate="2014-07-09T12:22:22.400" UserId="836" Text="&lt;svm&gt;" />
  <row Id="1844" PostHistoryTypeId="5" PostId="711" RevisionGUID="b8ca092a-8fdb-4c24-b577-2353092b2a85" CreationDate="2014-07-09T12:32:51.360" UserId="836" Comment="added 187 characters in body" Text="This question is in response to a comment I saw on another question.&#xD;&#xA;&#xD;&#xA;The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of &quot;SVMs are not used so much nowadays&quot;.&#xD;&#xA;&#xD;&#xA;I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a &quot;niche&quot; covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.&#xD;&#xA;&#xD;&#xA;So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM's &quot;sweet spot&quot; just as well, better CPUs meaning SVM's computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?&#xD;&#xA;&#xD;&#xA;I tried a search for e.g. &quot;are support vector machines out of fashion&quot; and found nothing to imply they were being dropped in favour of anything else.&#xD;&#xA;&#xD;&#xA;And Wikipedia has this: http://en.wikipedia.org/wiki/Support_vector_machine#Issues . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights. I don't see that as a major issue, just another minor thing to take into account when picking the right tool for the job (along with nature of the training data and learning task etc).&#xD;&#xA;" />
  <row Id="1845" PostHistoryTypeId="2" PostId="712" RevisionGUID="21036688-6a74-4f08-a3c2-6424df094b3a" CreationDate="2014-07-09T13:07:13.303" UserId="984" Text="SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:&#xD;&#xA;&#xD;&#xA;  - Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn't be stored in memory.&#xD;&#xA;  - The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.&#xD;&#xA;&#xD;&#xA;There are some significant disadvantages as well.&#xD;&#xA;&#xD;&#xA;   -  Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.&#xD;&#xA;   -  Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.&#xD;&#xA;&#xD;&#xA;SVMs generally belong to the class of &quot;Sparse Kernel Machines&quot;. The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the Relevance Vector Machine (RVM) perform better than SVM. The following figure shows a comparative performance of the two.&#xD;&#xA;&#xD;&#xA;![RVM vs. SVM][1] &#xD;&#xA;&#xD;&#xA;A very effective classifier, which is very popular nowadays is the Random Forest. The main advantages are:&#xD;&#xA;&#xD;&#xA;   -  Only one parameter to tune (i.e. the number of trees in the forest)&#xD;&#xA;   -  Not utterly parameter sensitive&#xD;&#xA;   -  Can easily be extended to multiple classes&#xD;&#xA;   -  Is based on probabilistic principles (maximizing mutual information gain with the help of decision trees)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/zNYbt.png" />
  <row Id="1847" PostHistoryTypeId="5" PostId="509" RevisionGUID="df635743-4b70-469d-9336-aec3a86dd31b" CreationDate="2014-07-09T17:18:44.043" UserId="1015" Comment="deleted 72 characters in body" Text="For example...if u have an agricultural land then selecting one particular area of that land would be feature selection.If u aim to find the affected plants in that area den u need to observe each plant based on a particular feature that is common in each plant so as to find the abnormalities...for this u would be considering feature extraction.In this example the original agricultural land corresponds to Dimensionality reduction." />
  <row Id="1848" PostHistoryTypeId="5" PostId="712" RevisionGUID="8233ce02-9555-4320-b6a3-8e54f1c0d566" CreationDate="2014-07-09T17:43:29.320" UserId="984" Comment="Explained the figure" Text="SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:&#xD;&#xA;&#xD;&#xA;  - Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn't be stored in memory.&#xD;&#xA;  - The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.&#xD;&#xA;&#xD;&#xA;There are some significant disadvantages as well.&#xD;&#xA;&#xD;&#xA;   -  Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.&#xD;&#xA;   -  Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.&#xD;&#xA;&#xD;&#xA;SVMs generally belong to the class of &quot;Sparse Kernel Machines&quot;. The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the Relevance Vector Machine (RVM) perform better than SVM. The following figure shows a comparative performance of the two. In the figure, the x-axis shows one dimensional data from two classes y={0,1}. The mixture model is defined as P(x|y=0)=Unif(0,1) and P(x|y=1)=Unif(.5,1.5) (Unif denotes uniform distribution). 1000 points were sampled from this mixture and an SVM and an RVM were used to estimate the posterior. The problem of SVM is that the predicted values are far off from the true log odds.  &#xD;&#xA;&#xD;&#xA;![RVM vs. SVM][1] &#xD;&#xA;&#xD;&#xA;A very effective classifier, which is very popular nowadays is the Random Forest. The main advantages are:&#xD;&#xA;&#xD;&#xA;   -  Only one parameter to tune (i.e. the number of trees in the forest)&#xD;&#xA;   -  Not utterly parameter sensitive&#xD;&#xA;   -  Can easily be extended to multiple classes&#xD;&#xA;   -  Is based on probabilistic principles (maximizing mutual information gain with the help of decision trees)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/zNYbt.png" />
  <row Id="1849" PostHistoryTypeId="2" PostId="713" RevisionGUID="97e3729d-e807-4749-b176-3245766341b5" CreationDate="2014-07-09T17:51:40.583" UserId="1314" Text="I have installed cloudera CDH5 Quick start VM on VM player. When I login through HUE in the first page I am the following error&#xD;&#xA;&#xD;&#xA;“Potential misconfiguration detected. Fix and restart Hue.”![Potential misconfiguration detected. Fix and restart Hue][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/vnq5P.png&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;How to solve this issue.&#xD;&#xA;&#xD;&#xA;Thanks,&#xD;&#xA;Green" />
  <row Id="1850" PostHistoryTypeId="1" PostId="713" RevisionGUID="97e3729d-e807-4749-b176-3245766341b5" CreationDate="2014-07-09T17:51:40.583" UserId="1314" Text="Cloudera QuickStart VM Error" />
  <row Id="1851" PostHistoryTypeId="3" PostId="713" RevisionGUID="97e3729d-e807-4749-b176-3245766341b5" CreationDate="2014-07-09T17:51:40.583" UserId="1314" Text="&lt;hadoop&gt;" />
  <row Id="1854" PostHistoryTypeId="5" PostId="712" RevisionGUID="4543c6ea-281b-446c-a23f-34467672157c" CreationDate="2014-07-09T20:23:12.380" UserId="984" Comment="added 9 characters in body" Text="SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:&#xD;&#xA;&#xD;&#xA;  - Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn't be stored in memory.&#xD;&#xA;  - The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.&#xD;&#xA;&#xD;&#xA;There are some significant disadvantages as well.&#xD;&#xA;&#xD;&#xA;   -  Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.&#xD;&#xA;   -  Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.&#xD;&#xA;&#xD;&#xA;SVMs generally belong to the class of &quot;Sparse Kernel Machines&quot;. The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the **Relevance Vector Machine** (RVM) perform better than SVM. The following figure shows a comparative performance of the two. In the figure, the x-axis shows one dimensional data from two classes y={0,1}. The mixture model is defined as P(x|y=0)=Unif(0,1) and P(x|y=1)=Unif(.5,1.5) (Unif denotes uniform distribution). 1000 points were sampled from this mixture and an SVM and an RVM were used to estimate the posterior. The problem of SVM is that the predicted values are far off from the true log odds.  &#xD;&#xA;&#xD;&#xA;![RVM vs. SVM][1] &#xD;&#xA;&#xD;&#xA;A very effective classifier, which is very popular nowadays, is the **Random Forest**. The main advantages are:&#xD;&#xA;&#xD;&#xA;   -  Only one parameter to tune (i.e. the number of trees in the forest)&#xD;&#xA;   -  Not utterly parameter sensitive&#xD;&#xA;   -  Can easily be extended to multiple classes&#xD;&#xA;   -  Is based on probabilistic principles (maximizing mutual information gain with the help of decision trees)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/zNYbt.png" />
  <row Id="1855" PostHistoryTypeId="2" PostId="714" RevisionGUID="e9221279-67ef-49c8-9a52-d434f47b4f69" CreationDate="2014-07-10T08:38:43.353" UserId="2433" Text="Although your question is not very specific so I'll try to give you some generic solutions. There are couple of things you can do here:&#xD;&#xA;&#xD;&#xA; - Check sparseMatrix from Matrix package as mentioned by @Sidhha&#xD;&#xA; - Try running your model in parallel using packages like snowfall, [Parallel](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf). Check this [list of packages on Cran](http://cran.r-project.org/web/views/HighPerformanceComputing.html) which can help you runnning your model in multicore parallel mode.&#xD;&#xA; - You can also try [data.table](http://datatable.r-forge.r-project.org/) package. It is quite phenomenal in speed.&#xD;&#xA;&#xD;&#xA;Good reads:&#xD;&#xA;&#xD;&#xA; 1. [11 Tips on How to Handle Big Data in R (and 1 Bad Pun)](http://theodi.org/blog/fig-data-11-tips-how-handle-big-data-r-and-1-bad-pun)&#xD;&#xA; 2. [Why R is slow &amp; how to improve its Performance?](http://adv-r.had.co.nz/Performance.html)" />
  <row Id="1856" PostHistoryTypeId="2" PostId="715" RevisionGUID="a2b48fe4-2c77-4513-b0c2-a484cc06d4cb" CreationDate="2014-07-10T09:16:39.937" UserId="434" Text="Go into the other link from home - to the cloudera manager.&#xD;&#xA;&#xD;&#xA;From there, you'll see Hue can be restarted, but there is probably an alert that needs to be resolved in there first.&#xD;&#xA;&#xD;&#xA;If I remember right there's some initial configuration that needs to be done on the quickstart VM that's spelled out as soon as you log into the manager application." />
  <row Id="1857" PostHistoryTypeId="2" PostId="716" RevisionGUID="9946cab7-331f-4408-b610-641d3a0207bb" CreationDate="2014-07-10T10:07:13.523" UserId="989" Text="I know that there is no a clear answer for this question, but let's suppose that I have a huge neural network, with a lot of data and I want to add a new feature in input. The &quot;best&quot; way would be to test the network with the new feature and see the results, but is there a method to test if the feature IS UNLIKELY helpful? Like correlation measures (http://www3.nd.edu/~mclark19/learn/CorrelationComparison.pdf) etc?&#xD;&#xA;" />
  <row Id="1858" PostHistoryTypeId="1" PostId="716" RevisionGUID="9946cab7-331f-4408-b610-641d3a0207bb" CreationDate="2014-07-10T10:07:13.523" UserId="989" Text="How to choose the features for a neural network?" />
  <row Id="1859" PostHistoryTypeId="3" PostId="716" RevisionGUID="9946cab7-331f-4408-b610-641d3a0207bb" CreationDate="2014-07-10T10:07:13.523" UserId="989" Text="&lt;machine-learning&gt;&lt;neuralnetwork&gt;" />
  <row Id="1860" PostHistoryTypeId="2" PostId="717" RevisionGUID="6f040623-8a64-420c-99da-b33c7055a239" CreationDate="2014-07-10T11:55:49.637" UserId="133" Text="I am searching for the correct definition for an experimental design I am using to test the robustness of different classification methods.&#xD;&#xA;&#xD;&#xA;I am creating different subsets of the full dataset by cutting away some samples. Each subset is created independently with respect to the others. &#xD;&#xA;&#xD;&#xA;Then I run each classification method on every subset.&#xD;&#xA;&#xD;&#xA;Finally, I estimate the accuracy of each method as how many classification on subsets are in agreement with the classification on full dataset. &#xD;&#xA;&#xD;&#xA;Example:&#xD;&#xA;&#xD;&#xA;    Classification-full     1    2    3    2    1    1    2&#xD;&#xA;&#xD;&#xA;    Classification-subset1  1    2         2    3    1   &#xD;&#xA;    Classification-subset2       2    3         1    1    2&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;    Accuracy                1    1    1    1  0.5    1    1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Is there a correct name to this methodology? I thought it can fall under [bootstrapping][1] but I am unsure about this.&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Bootstrapping_(statistics)" />
  <row Id="1861" PostHistoryTypeId="1" PostId="717" RevisionGUID="6f040623-8a64-420c-99da-b33c7055a239" CreationDate="2014-07-10T11:55:49.637" UserId="133" Text="How to define a custom methodology" />
  <row Id="1862" PostHistoryTypeId="3" PostId="717" RevisionGUID="6f040623-8a64-420c-99da-b33c7055a239" CreationDate="2014-07-10T11:55:49.637" UserId="133" Text="&lt;classification&gt;&lt;definitions&gt;&lt;accuracy&gt;&lt;sampling&gt;" />
  <row Id="1863" PostHistoryTypeId="4" PostId="717" RevisionGUID="e39b7863-1347-472f-bc5d-75581b022ff6" CreationDate="2014-07-10T12:08:34.847" UserId="133" Comment="edited title" Text="How to define a custom resampling methodology" />
  <row Id="1864" PostHistoryTypeId="2" PostId="718" RevisionGUID="eefeae51-bc95-4315-aeac-6824e3dfe867" CreationDate="2014-07-10T15:43:53.177" UserId="684" Text="A very strong correlation between the new feature and an existing feature is a fairly good sign that the new feature provides little new information.  A low correlation between the new feature and existing features is likely preferable.&#xD;&#xA;&#xD;&#xA;If the new feature was manually constructed from a combination of existing features, consider leaving it out.  The beauty of neural networks is that little feature engineering and preprocessing is required -- features are instead learned by intermediate layers.  Whenever possible, prefer learning features to engineering them." />
  <row Id="1865" PostHistoryTypeId="6" PostId="716" RevisionGUID="e497a222-a887-4807-918d-071ebf6495c2" CreationDate="2014-07-10T17:51:00.230" UserId="97" Comment="More relevant tags." Text="&lt;machine-learning&gt;&lt;neuralnetwork&gt;&lt;feature-selection&gt;&lt;feature-extraction&gt;" />
  <row Id="1866" PostHistoryTypeId="24" PostId="716" RevisionGUID="e497a222-a887-4807-918d-071ebf6495c2" CreationDate="2014-07-10T17:51:00.230" Comment="Proposed by 97 approved by 989 edit id of 110" />
  <row Id="1867" PostHistoryTypeId="5" PostId="717" RevisionGUID="a5f01dae-52c3-4a86-b6db-2c488ccf25f4" CreationDate="2014-07-10T18:04:59.523" UserId="84" Comment="Improving writing." Text="I'm using an experimental design to test the robustness of different classification methods, and now I'm searching for the correct definition of such design.&#xD;&#xA;&#xD;&#xA;I'm creating different subsets of the full dataset by cutting away some samples. Each subset is created independently with respect to the others. Then, I run each classification method on every subset. Finally, I estimate the accuracy of each method as how many classifications on subsets are in agreement with the classification on the full dataset. For example:&#xD;&#xA;&#xD;&#xA;    Classification-full     1    2    3    2    1    1    2&#xD;&#xA;&#xD;&#xA;    Classification-subset1  1    2         2    3    1   &#xD;&#xA;    Classification-subset2       2    3         1    1    2&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;    Accuracy                1    1    1    1  0.5    1    1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Is there a correct name to this methodology? I thought it can fall under [bootstrapping][1] but I'm not sure about this.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Bootstrapping_(statistics)" />
  <row Id="1868" PostHistoryTypeId="5" PostId="718" RevisionGUID="4374816c-37c2-4456-a45c-f2d236e3698b" CreationDate="2014-07-10T19:18:05.697" UserId="684" Comment="added 299 characters in body" Text="A very strong correlation between the new feature and an existing feature is a fairly good sign that the new feature provides little new information.  A low correlation between the new feature and existing features is likely preferable.&#xD;&#xA;&#xD;&#xA;A strong linear correlation between the new feature and the predicted variable is an good sign that a new feature will be valuable, but the absence of a high correlation is not necessary a sign of a poor feature, because neural networks are not restricted to linear combinations of variables.   &#xD;&#xA;&#xD;&#xA;If the new feature was manually constructed from a combination of existing features, consider leaving it out.  The beauty of neural networks is that little feature engineering and preprocessing is required -- features are instead learned by intermediate layers.  Whenever possible, prefer learning features to engineering them." />
  <row Id="1869" PostHistoryTypeId="2" PostId="719" RevisionGUID="08015365-f35e-4d92-8d8c-acde8d5a0ad8" CreationDate="2014-07-10T22:42:13.720" UserId="2443" Text="nsl-&#xD;&#xA;I'm a beginner at machine learning, so forgive the lay-like description here, but it sounds like you might be able to use topic modelling, like latent dirichlet analysis (LDA). It's an algorithm widely used to classify documents, according to what topics they are about, based on the words found and the relative frequencies of those words in the overall corpus.  I bring it up mainly because, in LDA it's not necessary to define the topics in advance.&#xD;&#xA;&#xD;&#xA;Since the help pages on LDA are mostly written for text analysis, the analogy I would use, in order to apply it to your question, is:&#xD;&#xA;- Treat each gene expression, or feature, as a 'word' (sometimes called a token in typical LDA text-classification applications)&#xD;&#xA;- Treat each sample as a document (ie it contains an assortment of words, or gene expressions)&#xD;&#xA;- Treat the signatures as pre-existing topics&#xD;&#xA;&#xD;&#xA;If I'm not mistaken, LDA should give weighted probabilities for each topic, as to how strongly it is present in each document.&#xD;&#xA;" />
  <row Id="1870" PostHistoryTypeId="2" PostId="720" RevisionGUID="b7a7136d-0895-4aed-8c9b-4bc3d0416cb4" CreationDate="2014-07-10T23:38:58.153" UserId="434" Text="There are a couple of open source options I know of - &#xD;&#xA;&#xD;&#xA;LibOTS - http://libots.sourceforge.net/&#xD;&#xA;&#xD;&#xA;DocSum - http://docsum.sourceforge.net/docsum/web/about.php&#xD;&#xA;&#xD;&#xA;A couple of commercial solutions - &#xD;&#xA;&#xD;&#xA;Intellix Summarizer Pro - http://summarizer.intellexer.com/order_summarizer_pro.php&#xD;&#xA;&#xD;&#xA;Copernic Summarizer - http://www.copernic.com/en/products/summarizer/&#xD;&#xA;&#xD;&#xA;And this one is a web service - &#xD;&#xA;&#xD;&#xA;TextTeaser - http://www.textteaser.com/&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I'm sure there are plenty of others out there.  I have used Copernic a good deal and it's pretty good, but I was hoping it could be automated easily, which it can't - at least it couldn't when I used it.  " />
  <row Id="1871" PostHistoryTypeId="2" PostId="721" RevisionGUID="b62069ff-953a-4753-83fd-bc1247a3b630" CreationDate="2014-07-11T01:28:24.957" UserId="548" Text="When it comes to dealing with many disparate kinds of data, especially when the relationships between them are unclear, I would strongly recommend a technique based on [decision trees][1], the most popular ones today to the best of my knowledge are [random forest][2], and [extremely randomized trees][3].&#xD;&#xA;&#xD;&#xA;Both have implementations in [sklearn][4], and they are pretty straightforward to use. At a very high level, the reason that a `decision tree`-based approach is advantageous for multiple disparate kinds of data is because decision trees are largely independent from the specific data they are dealing with, just so long as they are capable of understanding your representation.&#xD;&#xA;&#xD;&#xA;You'll still have to fit your data into a feature vector of some kind, but based on your example that seems like a pretty straightforward task, and if you're willing to go a little deeper on your implementation you could certainly come up with a custom tree-splitting rule without actually having to change anything in the underlying algorithm. The [original paper][5] is a pretty decent place to start if you want to give that a shot.&#xD;&#xA;&#xD;&#xA;If you want pseudo-structural data from your text data though, I might suggest looking into `doc2vec`, recently developed by Google. I don't think there are any good open-source implementations now, but it's a pretty straightforward improvement on the `word2vec` algorithm, which has implementations in at least `C` and `python`.&#xD;&#xA;&#xD;&#xA;Hope that helps! Let me know if you've got any more questions.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Decision_tree_learning&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Random_forest&#xD;&#xA;  [3]: http://www.montefiore.ulg.ac.be/~ernst/uploads/news/id63/extremely-randomized-trees.pdf&#xD;&#xA;  [4]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit_transform&#xD;&#xA;  [5]: http://www.cs.nyu.edu/~roweis/csc2515-2006/readings/quinlan.pdf" />
  <row Id="1872" PostHistoryTypeId="2" PostId="722" RevisionGUID="7bce2775-9100-4376-b411-62ae4d3eb15f" CreationDate="2014-07-11T09:49:32.933" UserId="2452" Text="R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**, **reshape2**, and **plyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.&#xD;&#xA;&#xD;&#xA;Additionally, R offers some packages specifically *focused* on data cleaning and transformation (my current reputation on this site limits number of links that I can post within an answer, so please see corresponding URLs in the comments below):&#xD;&#xA;&#xD;&#xA;- **editrules**&#xD;&#xA;- **deducorrect**&#xD;&#xA;- **StatMatch**&#xD;&#xA;- **MatchIt**&#xD;&#xA;&#xD;&#xA;A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf." />
  <row Id="1873" PostHistoryTypeId="5" PostId="722" RevisionGUID="ea0aaa55-b9f2-490f-ba8a-469d13c22371" CreationDate="2014-07-11T10:10:41.697" UserId="2452" Comment="Added info on DataCombine package." Text="R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**, **reshape2**, and **plyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.&#xD;&#xA;&#xD;&#xA;Additionally, R offers some packages specifically *focused* on data cleaning and transformation (my current reputation on this site limits number of links that I can post within an answer, so please see corresponding URLs in the comments below):&#xD;&#xA;&#xD;&#xA;- **editrules**&#xD;&#xA;- **deducorrect**&#xD;&#xA;- **StatMatch**&#xD;&#xA;- **MatchIt**&#xD;&#xA;- **DataCombine**&#xD;&#xA;&#xD;&#xA;A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf." />
  <row Id="1874" PostHistoryTypeId="2" PostId="723" RevisionGUID="9f1d7eac-76ff-42d3-aaf8-306721fef026" CreationDate="2014-07-11T11:32:13.550" UserId="127" Text="Random subsampling seems appropriate, bootstrapping is a bit more generic, but also correct.&#xD;&#xA;&#xD;&#xA;Here are some references and synonyms: http://www.frank-dieterle.com/phd/2_4_3.html" />
  <row Id="1875" PostHistoryTypeId="2" PostId="724" RevisionGUID="b63867ea-94d1-4d37-96ce-5f3a1d8812b4" CreationDate="2014-07-11T14:13:30.403" UserId="172" Text="Your approach of using a source version repository is a good one and it actually allows you also working on the cluster and then copying everything back.&#xD;&#xA;&#xD;&#xA;If you find yourself making minor edits to your Python script on your laptop, then updating your SVN directory on the cluster, why not work directly on the cluster frontend, make all needed minor edits, and then, at the end of the day, commit everything there and update on your laptop? &#xD;&#xA;&#xD;&#xA;All you need is to get familiar with the environment there (OS, editor, etc.) or install your own environment (I usually install in my home directory the latest version of [Vim][1], [Tmux][2], etc. with the proper dotfiles so I feel at home there.)&#xD;&#xA;&#xD;&#xA;Also, you can version your data, and even your intermediate results if size permits. My repositories often comprise code, data (original and cleaned versions), documentation, and paper sources for publishing (latex)&#xD;&#xA;&#xD;&#xA;Finally, you can script your job submission to avoid modifying scripts manually. `qsub` accepts a script from stdin and also accepts all `#$` comments as command-line arguments. &#xD;&#xA;&#xD;&#xA;  [1]: http://www.vim.org/&#xD;&#xA;  [2]: http://tmux.sourceforge.net/" />
  <row Id="1876" PostHistoryTypeId="2" PostId="725" RevisionGUID="aa15d9e3-f0c3-4541-ba2d-7b84886f0ed7" CreationDate="2014-07-11T14:27:01.603" UserId="172" Text="What you describe falls in the category of [concept drift][1] in machine learning.&#xD;&#xA;You might find interesting and actionable ideas in this [summary paper][2] and you'll find a taxonomy of the possible approaches in [these slides][3].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Concept_drift&#xD;&#xA;  [2]: http://arxiv.org/pdf/1010.4784.pdf&#xD;&#xA;  [3]: http://www.cs.waikato.ac.nz/~abifet/PAKDD2011/PAKDD11Tutorial_Handling_Concept_Drift.pdf" />
  <row Id="1877" PostHistoryTypeId="2" PostId="726" RevisionGUID="7463cf8c-1ddf-4e61-bd9b-676e8e7e7cb1" CreationDate="2014-07-11T21:09:58.873" UserId="2458" Text="I am trying to understand a neuroscience article by Karl Friston. In it he gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear as equation (5) in the article at http://www.fil.ion.ucl.ac.uk/spm/doc/papers/Action_and_behavior_A_free-energy_formulation.pdf (DOI 10.1007/s00422-010-0364-z). Here they are (The tilda in &quot;˜s&quot; below is supposed to be over the s rather than in front of it.):&#xD;&#xA;&#xD;&#xA;• Energy minus entropy: F = −{ln p(˜s,Ψ|m)}q + {ln q(Ψ|μ)}q • Divergence plus surprise: = D(q(Ψ|μ)||p(Ψ|˜s,m)) − ln p (˜s|m) • Complexity minus accuracy: = D(q(Ψ|μ)||p(Ψ|m)) − {ln p(˜s|Ψ,m)}q&#xD;&#xA;&#xD;&#xA;The things I am struggling with at this point are 1) the meaning of the || in the 2nd and 3rd versions of the equations, 2) the negative logs. Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?" />
  <row Id="1878" PostHistoryTypeId="1" PostId="726" RevisionGUID="7463cf8c-1ddf-4e61-bd9b-676e8e7e7cb1" CreationDate="2014-07-11T21:09:58.873" UserId="2458" Text="Trying to understand the equations in an Karl Friston article" />
  <row Id="1879" PostHistoryTypeId="3" PostId="726" RevisionGUID="7463cf8c-1ddf-4e61-bd9b-676e8e7e7cb1" CreationDate="2014-07-11T21:09:58.873" UserId="2458" Text="&lt;neuralnetwork&gt;&lt;accuracy&gt;" />
  <row Id="1880" PostHistoryTypeId="5" PostId="722" RevisionGUID="fd5f3cbf-db83-4153-a6f9-a04b5cbbc18f" CreationDate="2014-07-11T21:34:06.603" UserId="2452" Comment="Added info on DataCombine. Moved URLs from comments to the answer." Text="R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**, **reshape2**, and **plyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.&#xD;&#xA;&#xD;&#xA;Additionally, R offers some packages specifically *focused* on data cleaning and transformation:&#xD;&#xA;&#xD;&#xA;- **editrules** (http://cran.r-project.org/web/packages/editrules/index.html)&#xD;&#xA;- **deducorrect** (http://cran.r-project.org/web/packages/deducorrect/index.html)&#xD;&#xA;- **StatMatch** (http://cran.r-project.org/web/packages/StatMatch/index.html)&#xD;&#xA;- **MatchIt** (http://cran.r-project.org/web/packages/MatchIt/index.html)&#xD;&#xA;- **DataCombine** (http://cran.r-project.org/web/packages/DataCombine)&#xD;&#xA;&#xD;&#xA;A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf." />
  <row Id="1881" PostHistoryTypeId="2" PostId="727" RevisionGUID="ea42f294-4762-447c-b3d3-2daa45318f10" CreationDate="2014-07-11T21:35:22.677" UserId="2459" Text="Your problem is that the resets aren't part of your linear model. You either have to cut your data into different fragments at the resets, so that no reset occurs within each fragment, and you can fit a linear model to each fragment. Or you can build a more complicated model that allows for resets. In this case, either the time of occurrence of the resets has to be put into the model manually, or the time of resets has to be a free parameter in the model that is determined by fitting the model to the data." />
  <row Id="1882" PostHistoryTypeId="2" PostId="728" RevisionGUID="0bd550f6-10c1-4138-aebd-09bfc34c7816" CreationDate="2014-07-11T22:03:31.950" UserId="2452" Text="In addition to excellent previous answers, I'd like to recommend two papers on **data cleaning**. They are not specific to *manual* data cleaning, but, considering the benefits and advice (which I completely agree with) of expressing even **manual** data transformations **in code**, these resources can be as valuable. Also, despite the fact that following papers are somewhat R-focused, I believe that general *ideas* and *workflows* for data cleaning can be easily extracted and are equally applicable to non-R environments, as well.&#xD;&#xA;&#xD;&#xA;The first paper presents the concept of *tidy data*, as well as examples and best practices of use of standard and specific R packages in data cleaning: http://vita.had.co.nz/papers/tidy-data.pdf.&#xD;&#xA;&#xD;&#xA;A comprehensive and coherent approach to data cleaning in R, including examples, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf." />
  <row Id="1883" PostHistoryTypeId="2" PostId="729" RevisionGUID="ca6bf796-44f2-40d7-a7c1-d3915c766dce" CreationDate="2014-07-11T23:08:16.267" UserId="375" Text="I thought this was an interesting problem, so I wrote a sample data set and a linear slope estimator in R.  I hope it helps you with your problem.  I'm going to make some assumptions, the biggest is that you want to estimate a constant slope, given by some segments in your data.&#xD;&#xA;&#xD;&#xA;This code creates the sample data set.  It will consist of 100 points, random noise with a slope of 4 (Hopefully we will estimate this).  When the y-values reach a cutoff, they reset to 50.  The cutoff is randomly chosen between 115 and 120 for each reset.  Here is the R code to create the data set.&#xD;&#xA;&#xD;&#xA;    # Create Sample Data&#xD;&#xA;    set.seed(1001)&#xD;&#xA;    x_data = 1:100 # x-data&#xD;&#xA;    y_data = rep(0,length(x_data)) # Initialize y-data&#xD;&#xA;    y_data[1] = 50 &#xD;&#xA;    reset_level = sample(115:120,1) # Select initial cutoff&#xD;&#xA;    for (i in x_data[-1]){ # Loop through rest of x-data&#xD;&#xA;      if(y_data[i-1]&gt;reset_level){ # check if y-value is above cutoff&#xD;&#xA;        y_data[i] = 50             # Reset if it is and&#xD;&#xA;        reset_level = sample(115:120,1) # rechoose cutoff&#xD;&#xA;      }else {&#xD;&#xA;        y_data[i] = y_data[i-1] + 4 + (10*runif(1)-5) # Or just increment y with random noise&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;    plot(x_data,y_data) # Plot data&#xD;&#xA;![Sample Data][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/2dC1w.png&#xD;&#xA;&#xD;&#xA;Now we find the breaks and fit each set of y-values and record the slopes.&#xD;&#xA;&#xD;&#xA;    # Find the differences between adjacent points&#xD;&#xA;    diffs = y_data[-1] - y_data[-length(y_data)]&#xD;&#xA;    # Find the break points (here I use 4 s.d.'s)&#xD;&#xA;    break_points = c(0,which(diffs &lt; (mean(diffs) - 4*sd(diffs))),length(y_data))&#xD;&#xA;    # Create the lists of y-values&#xD;&#xA;    y_lists = sapply(1:(length(break_points)-1),function(x){&#xD;&#xA;      y_data[(break_points[x]+1):(break_points[x+1])]&#xD;&#xA;    })&#xD;&#xA;    # Create the lists of x-values&#xD;&#xA;    x_lists = lapply(y_lists,function(x) 1:length(x))&#xD;&#xA;    #Find all the slopes for the lists of points&#xD;&#xA;    slopes = unlist(lapply(1:length(y_lists), function(x) lm(y_lists[[x]] ~ x_lists[[x]])$coefficients[2]))&#xD;&#xA;&#xD;&#xA;Here are the slopes:&#xD;&#xA;(3.309110, 4.419178, 3.292029, 4.531126, 3.675178, 4.294389)&#xD;&#xA;&#xD;&#xA;And we can just take the mean to find the expected slope (3.920168)." />
  <row Id="1884" PostHistoryTypeId="5" PostId="726" RevisionGUID="902a48e0-3043-4274-9ae8-6037896845de" CreationDate="2014-07-12T03:18:03.923" UserId="2458" Comment="added 4 characters in body" Text="I am trying to understand a neuroscience article by Karl Friston. In it he gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear as equation (5) in the article at http://www.fil.ion.ucl.ac.uk/spm/doc/papers/Action_and_behavior_A_free-energy_formulation.pdf (DOI 10.1007/s00422-010-0364-z). Here they are (The tilda in &quot;˜s&quot; below is supposed to be over the s rather than in front of it.):&#xD;&#xA;&#xD;&#xA;• Energy minus entropy: F = −{ln p(˜s,Ψ|m)}q + {ln q(Ψ|μ)}q &#xD;&#xA;&#xD;&#xA;• Divergence plus surprise: = D(q(Ψ|μ)||p(Ψ|˜s,m)) − ln p (˜s|m) &#xD;&#xA;&#xD;&#xA;• Complexity minus accuracy: = D(q(Ψ|μ)||p(Ψ|m)) − {ln p(˜s|Ψ,m)}q&#xD;&#xA;&#xD;&#xA;The things I am struggling with at this point are 1) the meaning of the || in the 2nd and 3rd versions of the equations, 2) the negative logs. Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?" />
  <row Id="1885" PostHistoryTypeId="5" PostId="726" RevisionGUID="db1cc94b-5947-4910-b83b-f1dbff405dd0" CreationDate="2014-07-12T03:38:07.083" UserId="2458" Comment="added 49 characters in body" Text="I am trying to understand a neuroscience article by Karl Friston. In it he gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear as equation (5) in the article at http://www.fil.ion.ucl.ac.uk/spm/doc/papers/Action_and_behavior_A_free-energy_formulation.pdf (DOI 10.1007/s00422-010-0364-z). Here they are (The tilda in &quot;˜s&quot; below is supposed to be over the s rather than in front of it AND the { &amp; } are angle brackets in the original.):&#xD;&#xA;&#xD;&#xA;• Energy minus entropy: F = −{ln p(˜s,Ψ|m)}q + {ln q(Ψ|μ)}q &#xD;&#xA;&#xD;&#xA;• Divergence plus surprise: = D(q(Ψ|μ)||p(Ψ|˜s,m)) − ln p (˜s|m) &#xD;&#xA;&#xD;&#xA;• Complexity minus accuracy: = D(q(Ψ|μ)||p(Ψ|m)) − {ln p(˜s|Ψ,m)}q&#xD;&#xA;&#xD;&#xA;The things I am struggling with at this point are 1) the meaning of the || in the 2nd and 3rd versions of the equations, 2) the negative logs. Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?" />
  <row Id="1886" PostHistoryTypeId="2" PostId="730" RevisionGUID="c4180c50-4e0c-4032-9f32-58d9c747eb53" CreationDate="2014-07-12T17:25:52.907" UserId="84" Text="As far as I know the development of algorithms to solve the [Frequent Pattern Mining]() (FPM) problem, the road of improvements have some main checkpoints. Firstly, the [Apriori](http://en.wikipedia.org/wiki/Apriori_algorithm) algorithm was proposed in 1993, by [Agrawal et al.](http://dl.acm.org/citation.cfm?id=170072), along with the formalization of the problem. The algorithm was able to *strip-off* some sets from the `2^n - 1` sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.&#xD;&#xA;&#xD;&#xA;Later, on year 1997, [Zaki et al.](http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html) proposed the algorithm [Eclat](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm), which *inserted* the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.&#xD;&#xA;&#xD;&#xA;In 2000, [Han et al.](http://dl.acm.org/citation.cfm?doid=335191.335372) proposed an algorithm named [FPGrowth](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm), along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to *strip-off* non-frequent itemsets.&#xD;&#xA;&#xD;&#xA;As far as I know, this may be considered a state-of-the-art algorithm, but I'd like to know about other proposed solutions. What algorithms for FPM are considered &quot;state-of-the-art&quot;? What is the *intuition*/*main-contribution* of such algorithms?" />
  <row Id="1887" PostHistoryTypeId="1" PostId="730" RevisionGUID="c4180c50-4e0c-4032-9f32-58d9c747eb53" CreationDate="2014-07-12T17:25:52.907" UserId="84" Text="State-of-the-art algorithms in frequent pattern mining" />
  <row Id="1888" PostHistoryTypeId="3" PostId="730" RevisionGUID="c4180c50-4e0c-4032-9f32-58d9c747eb53" CreationDate="2014-07-12T17:25:52.907" UserId="84" Text="&lt;bigdata&gt;&lt;data-mining&gt;&lt;efficiency&gt;&lt;fpm&gt;&lt;state-of-the-art&gt;" />
  <row Id="1889" PostHistoryTypeId="6" PostId="711" RevisionGUID="cf62dc8b-9ab4-4fa5-92a3-d80e263c4eff" CreationDate="2014-07-12T17:26:32.727" UserId="84" Comment="Adding tags" Text="&lt;data-mining&gt;&lt;svm&gt;&lt;state-of-the-art&gt;" />
  <row Id="1890" PostHistoryTypeId="5" PostId="730" RevisionGUID="5be84dc7-6d0c-4248-b462-c7405547a993" CreationDate="2014-07-12T17:31:42.567" UserId="84" Comment="added 6 characters in body" Text="As far as I know the development of algorithms to solve the [Frequent Pattern Mining]() (FPM) problem, the road of improvements have some main checkpoints. Firstly, the [Apriori](http://en.wikipedia.org/wiki/Apriori_algorithm) algorithm was proposed in 1993, by [Agrawal et al.](http://dl.acm.org/citation.cfm?id=170072), along with the formalization of the problem. The algorithm was able to *strip-off* some sets from the `2^n - 1` sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.&#xD;&#xA;&#xD;&#xA;Later, on year 1997, [Zaki et al.](http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html) proposed the algorithm [Eclat](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm), which *inserted* the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.&#xD;&#xA;&#xD;&#xA;In 2000, [Han et al.](http://dl.acm.org/citation.cfm?doid=335191.335372) proposed an algorithm named [FPGrowth](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm), along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to *strip-off* non-frequent itemsets.&#xD;&#xA;&#xD;&#xA;As far as I know, this may be considered a state-of-the-art algorithm, but I'd like to know about other proposed solutions. What other algorithms for FPM are considered &quot;state-of-the-art&quot;? What is the *intuition*/*main-contribution* of such algorithms?" />
  <row Id="1893" PostHistoryTypeId="5" PostId="730" RevisionGUID="eaf0d598-a638-4c85-b245-7429719a0b7c" CreationDate="2014-07-13T03:05:46.660" UserId="84" Comment="Repharsing the question." Text="As far as I know the development of algorithms to solve the [Frequent Pattern Mining]() (FPM) problem, the road of improvements have some main checkpoints. Firstly, the [Apriori](http://en.wikipedia.org/wiki/Apriori_algorithm) algorithm was proposed in 1993, by [Agrawal et al.](http://dl.acm.org/citation.cfm?id=170072), along with the formalization of the problem. The algorithm was able to *strip-off* some sets from the `2^n - 1` sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.&#xD;&#xA;&#xD;&#xA;Later, on year 1997, [Zaki et al.](http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html) proposed the algorithm [Eclat](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm), which *inserted* the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.&#xD;&#xA;&#xD;&#xA;In 2000, [Han et al.](http://dl.acm.org/citation.cfm?doid=335191.335372) proposed an algorithm named [FPGrowth](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm), along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to *strip-off* non-frequent itemsets.&#xD;&#xA;&#xD;&#xA;**Edit**:&#xD;&#xA;&#xD;&#xA;&lt;strike&gt;As far as I know, this may be considered a state-of-the-art algorithm, but I'd like to know about other proposed solutions. What other algorithms for FPM are considered &quot;state-of-the-art&quot;? What is the *intuition*/*main-contribution* of such algorithms?&lt;/strike&gt;&#xD;&#xA;&#xD;&#xA;Is the FPGrowth algorithm still considered &quot;state of the art&quot; in frequent pattern mining? If not, what algorithm(s) may extract frequent itemsets from large datasets more efficiently?" />
  <row Id="1894" PostHistoryTypeId="4" PostId="730" RevisionGUID="eaf0d598-a638-4c85-b245-7429719a0b7c" CreationDate="2014-07-13T03:05:46.660" UserId="84" Comment="Repharsing the question." Text="Is FPGrowth still considered &quot;state of the art&quot; in frequent pattern mining?" />
  <row Id="1895" PostHistoryTypeId="2" PostId="731" RevisionGUID="787a51d5-1db3-414f-8662-5688a55483fc" CreationDate="2014-07-13T09:04:39.703" UserId="2471" Text="When I started with ANN I thought I'd have to fight overfitting as the main problem. But in practice I can't even get my NN to pass the 20% error rate barrier. I can't even nearly beat my score on random forest!&#xD;&#xA;&#xD;&#xA;I'm seeking some very general or not so general advice on what should one do to make his NN start capturing trends in data.&#xD;&#xA;&#xD;&#xA;For implementing NN I use Theano Stacked Auto Encoder, [the code from tutorial][1] that works great (less than 5% error rate) on classifying MNIST dataset.&#xD;&#xA;It is a multilayer perceptron, with softmax layer on top with each hidden later being pre-trained as autoencoder (fully described at [tutorial][2], chapter 8).&#xD;&#xA;The number of input features are ~50 and output classes ~10. NN has sigmoid neurons and all data normalized to [0,1]. I tried lots of different configurations: number of hidden layers and neurons in them (100-&gt;100-&gt;100, 60-&gt;60-&gt;60, 60-&gt;30-&gt;15, etc,etc), different learning and pre-train rates, etc.&#xD;&#xA;&#xD;&#xA;And the best thing I can get is %20 error rate on validation set and %40 error rate on test set.&#xD;&#xA;&#xD;&#xA;On the contrary, when I try to use Random Forest (from scikit-learn) I easily get %12 error rate on validation set and %25(!) on test set.&#xD;&#xA;&#xD;&#xA;How can that be that my deep NN with pre-training behaves so badly? What should I try? &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/SdA.py&#xD;&#xA;  [2]: http://deeplearning.net/tutorial/deeplearning.pdf" />
  <row Id="1896" PostHistoryTypeId="1" PostId="731" RevisionGUID="787a51d5-1db3-414f-8662-5688a55483fc" CreationDate="2014-07-13T09:04:39.703" UserId="2471" Text="How to fight underfitting in deep neural net" />
  <row Id="1897" PostHistoryTypeId="3" PostId="731" RevisionGUID="787a51d5-1db3-414f-8662-5688a55483fc" CreationDate="2014-07-13T09:04:39.703" UserId="2471" Text="&lt;neuralnetwork&gt;" />
  <row Id="1898" PostHistoryTypeId="2" PostId="732" RevisionGUID="585d734d-cd1c-465e-bc7b-974d4cb0269b" CreationDate="2014-07-13T09:58:16.387" UserId="1279" Text="The problem with deep networks is that they have lots of hyperparameters to tune and very small solution space. Thus, finding good ones is more like an art rather than engineering task. I would start with working example from tutorial and play around with its parameters to see how results change - this give a good intuition (though not formal explanation) about dependencies between parameters and results (both - final and intermediate). &#xD;&#xA;&#xD;&#xA;Also I found following papers very useful: &#xD;&#xA;&#xD;&#xA; * [Visually Debugging Restricted Boltzmann Machine Training&#xD;&#xA;with a 3D Example](http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf)&#xD;&#xA; * [A Practical Guide to Training Restricted Boltzmann&#xD;&#xA;Machines](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)&#xD;&#xA;&#xD;&#xA;They both describe RBMs, but contain some insights on deep networks in general. For example, one of key points is that networks need to be debugged layer-wise - if previous layer doesn't provide good representation of features, further layers have almost no chance to fix it. " />
  <row Id="1902" PostHistoryTypeId="2" PostId="733" RevisionGUID="817f05d4-f07c-424c-88e1-8e7254e9ebfe" CreationDate="2014-07-13T12:24:36.430" UserId="471" Text="Ask your grid administrator to add your local machine as a &quot;submit host&quot;, and install SGE (which we assume you are using, you don't actually say) so then you can `qsub` from your machine.&#xD;&#xA;&#xD;&#xA;OR....&#xD;&#xA;&#xD;&#xA;Use emacs, then you can edit on your HPC via emacs's &quot;tramp&quot; ssh-connection facilities, and keep a shell open in another emacs window. You don't say what editor/operating system you like to use. You can even configure emacs to save a file in two places, so you could save to your local machine for running tests and to the HPC file system simultaneously for big jobs.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1903" PostHistoryTypeId="5" PostId="658" RevisionGUID="298e1ff7-4a4b-4161-bf40-5ad17a3b4fd9" CreationDate="2014-07-13T15:45:48.960" UserId="548" Comment="General Grammar and punctuation clean up" Text="I would like to use non-atomic data, as a feature for a prediction. &#xD;&#xA;Suppose I have a Table with these features:&#xD;&#xA;&lt;pre&gt;&#xD;&#xA;- Column 1: Categorical - House&#xD;&#xA;- Column 2: Numerical - 23.22&#xD;&#xA;- Column 3: A Vector - [ 12, 22, 32 ]&#xD;&#xA;- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]&#xD;&#xA;- Column 5: A List [ 122, Boolean ]&#xD;&#xA;&lt;/pre&gt;&#xD;&#xA;I would like to predict/classify ... Column 2 ... for example....&#xD;&#xA;&#xD;&#xA;I am making something to automatically respond to questions, any type of question, like &quot;Where was Foo Born?&quot; ...&#xD;&#xA;&#xD;&#xA;I first make a query to a search engine ---&gt;&gt;&gt; then I get some Text data as a Result.&#xD;&#xA;So I do all the Parsing Staff... Tagging, Stemming, Parsing, Splitting... &#xD;&#xA;My first approach was to make a table, each row with a line of text.. and a lot of Features...like ... First Word ... Tag of First Word.. Chunks, etc..&#xD;&#xA;But with this approach I am missing the relationships between the Sentences. &#xD;&#xA;&#xD;&#xA;I would like to know if there is an algorithm that look inside the Tree Structures... Vectors... and make the relations and extract whatever is relevant for predicting/classifying. I'd prefer to know about a library that does that than an algorithm that I have to implement." />
  <row Id="1904" PostHistoryTypeId="4" PostId="658" RevisionGUID="298e1ff7-4a4b-4161-bf40-5ad17a3b4fd9" CreationDate="2014-07-13T15:45:48.960" UserId="548" Comment="General Grammar and punctuation clean up" Text="Prediction with non-atomic features" />
  <row Id="1905" PostHistoryTypeId="24" PostId="658" RevisionGUID="298e1ff7-4a4b-4161-bf40-5ad17a3b4fd9" CreationDate="2014-07-13T15:45:48.960" Comment="Proposed by 548 approved by 434, 84 edit id of 111" />
  <row Id="1907" PostHistoryTypeId="5" PostId="732" RevisionGUID="b34521d7-fad9-4f08-814b-1eeceed396ac" CreationDate="2014-07-13T16:46:23.883" UserId="1279" Comment="added 1 character in body" Text="The problem with deep networks is that they have lots of hyperparameters to tune and very small solution space. Thus, finding good ones is more like an art rather than engineering task. I would start with working example from tutorial and play around with its parameters to see how results change - this gives a good intuition (though not formal explanation) about dependencies between parameters and results (both - final and intermediate). &#xD;&#xA;&#xD;&#xA;Also I found following papers very useful: &#xD;&#xA;&#xD;&#xA; * [Visually Debugging Restricted Boltzmann Machine Training&#xD;&#xA;with a 3D Example](http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf)&#xD;&#xA; * [A Practical Guide to Training Restricted Boltzmann&#xD;&#xA;Machines](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)&#xD;&#xA;&#xD;&#xA;They both describe RBMs, but contain some insights on deep networks in general. For example, one of key points is that networks need to be debugged layer-wise - if previous layer doesn't provide good representation of features, further layers have almost no chance to fix it. " />
  <row Id="1908" PostHistoryTypeId="2" PostId="734" RevisionGUID="40f0f1be-ab99-4b72-9a72-c43fa372bb8a" CreationDate="2014-07-14T12:08:13.373" UserId="21" Text="I'm not qualified to understand almost all of that paper, but, I might be able to give some intuitions from information theory that help you parse the paper.&#xD;&#xA;&#xD;&#xA;`||` denotes the [Kullback-Leibler divergence][1]. It measures an information gain between two distributions. I suppose you could say it indicates the information in the real distribution of data that a model fails to capture.&#xD;&#xA;&#xD;&#xA;When you see &quot;negative log&quot; think [&quot;entropy&quot;][2].&#xD;&#xA;&#xD;&#xA;In the first equation, think of it as &quot;-ln(...) - -ln(...)&quot;. This may help think of it as the difference of entropies. Likewise in the second, read it as &quot;D(...) + -ln(...)&quot;. This may help think of it as &quot;plus entropy&quot;.&#xD;&#xA;&#xD;&#xA;If you look at the divergence definition, you'll see it is defined as the log of the ratio of the PDFs. This may help connect it to logs and negative logs. Look at the definition that writes it as cross-entropy minus entropy. Then this is all a question of differences of entropies of things which may be clearer.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Information_entropy" />
  <row Id="1909" PostHistoryTypeId="2" PostId="735" RevisionGUID="b70f286d-7614-4f2d-8602-ea7660b13b8d" CreationDate="2014-07-14T13:06:05.523" UserId="1367" Text="From your question's wording I assume that you have a local machine and a remote machine where you update two files &amp;mdash; a Python script and a Bash script.  Both files are under SVN control, and both machines have access to the same SVN server.&#xD;&#xA;&#xD;&#xA;I am sorry I do not have any advice specific to your grid system, but let me list some general points I have found important for any deployment.&#xD;&#xA;&#xD;&#xA;**Keep production changes limited to configuration changes**. You write that you have to &quot;use the datasets' path on the server&quot;; this sounds to me like you have the paths hardcoded into your Python script.  This is not a good idea, precisely because you will need to change those paths in every other machine where you move the script to.  If you commit those changes back to SVN, then on your local machine you will have the remote paths, and on and on ...  (What if there are not only paths, but also passwords?  You should not have production passwords in an SVN server.)&#xD;&#xA;&#xD;&#xA;So, keep paths and other setup informations in a `.ini` file and use [ConfigParser][1] to read it, or use a `.json` file and use the [json][2] module. Keep one copy of the file locally and one remotely, both under the same path, both without SVN control, and just keep the path to that configuration file in the Python script (or get it from the command line if you can't keep both configurations under the same path).&#xD;&#xA;&#xD;&#xA;**Keep configuration as small as possible**. Any configuration is a &quot;moving part&quot; of your application, and any system is more robust the less it has moving parts.  A good indicator of something that belongs into configuration is exactly that you have to edit it every time you move the code; things that have not needed editing can remain as constants in the code.&#xD;&#xA;&#xD;&#xA;**Automate your deployment**.  You can do it via a Bash script on your local machine; note that you can [run any command on a remote machine][3] through `ssh`.  For instance:&#xD;&#xA;&#xD;&#xA;&lt;!-- language: bash --&gt;&#xD;&#xA;&#xD;&#xA;    svn export yourprojectpath /tmp/exportedproject&#xD;&#xA;    tar czf /tmp/yourproject.tgz /tmp/exportedproject&#xD;&#xA;    scp /tmp/myproject.tgz youruser@remotemachine:~/dev&#xD;&#xA;&#xD;&#xA;    ## Remote commands are in the right hand side, between ''&#xD;&#xA;    ssh youruser@remotemachine 'tar xzf ~/dev/yourproject.tgz'&#xD;&#xA;    ssh youruser@remotemachine 'qsub ~/dev/yourproject/script.py'&#xD;&#xA;&#xD;&#xA;For this to work, you need of course to have a [passwordless login][4], based on public/private keys, set up between your local and the remote machine.&#xD;&#xA;&#xD;&#xA;If you need more than this, you can think of using Python's [Fabric][5] or the higher-level [cuisine][6].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://docs.python.org/2/library/configparser.html&#xD;&#xA;  [2]: https://docs.python.org/2/library/json.html&#xD;&#xA;  [3]: http://malcontentcomics.com/systemsboy/2006/07/send-remote-commands-via-ssh.html&#xD;&#xA;  [4]: http://www.linuxproblem.org/art_9.html&#xD;&#xA;  [5]: http://www.fabfile.org&#xD;&#xA;  [6]: https://github.com/sebastien/cuisine" />
  <row Id="1910" PostHistoryTypeId="2" PostId="736" RevisionGUID="5e27e7a2-35b5-43cf-bffa-458bb818b5a3" CreationDate="2014-07-14T13:53:28.437" UserId="802" Text="I have a dataset which contains ~100,000 samples of 50 classes. I have been using SVM with an RBF kernel to train and predict new data. The problem though is the dataset is skewed towards different classes. &#xD;&#xA;&#xD;&#xA;For example, Class 1 - 30 (~3% each), Class 31 - 45 (~0.6% each), Class 46 - 50 (~0.2% each)&#xD;&#xA;&#xD;&#xA;I see that the model tends to very rarely predict the classes which occur less frequent in the training set, even though the test set has the same class distribution as the training set. &#xD;&#xA;&#xD;&#xA;I am aware that there are technique such as 'undersampling' where the majority class is scaled down to the minor class. However, is this applicable here where there are so many different classes? Are there other methods to help handle this case?" />
  <row Id="1911" PostHistoryTypeId="1" PostId="736" RevisionGUID="5e27e7a2-35b5-43cf-bffa-458bb818b5a3" CreationDate="2014-07-14T13:53:28.437" UserId="802" Text="Skewed multi-class data" />
  <row Id="1912" PostHistoryTypeId="3" PostId="736" RevisionGUID="5e27e7a2-35b5-43cf-bffa-458bb818b5a3" CreationDate="2014-07-14T13:53:28.437" UserId="802" Text="&lt;classification&gt;&lt;svm&gt;" />
  <row Id="1913" PostHistoryTypeId="5" PostId="729" RevisionGUID="be757d7e-d405-4d1c-abb5-5ad60b7d3f46" CreationDate="2014-07-14T15:21:03.373" UserId="375" Comment="Suggested Edits" Text="I thought this was an interesting problem, so I wrote a sample data set and a linear slope estimator in R.  I hope it helps you with your problem.  I'm going to make some assumptions, the biggest is that you want to estimate a constant slope, given by some segments in your data.  Another assumption to separate the blocks of linear data is that the natural 'reset' will be found by comparing consecutive differences and finding ones that are X-standard deviations below the mean. (I chose 4 sd's, but this can be changed)&#xD;&#xA;&#xD;&#xA;Here is a plot of the data, and the code to generating it is at the bottom.&#xD;&#xA;![Sample Data][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/2dC1w.png&#xD;&#xA;&#xD;&#xA;For starters, we find the breaks and fit each set of y-values and record the slopes.&#xD;&#xA;&#xD;&#xA;    # Find the differences between adjacent points&#xD;&#xA;    diffs = y_data[-1] - y_data[-length(y_data)]&#xD;&#xA;    # Find the break points (here I use 4 s.d.'s)&#xD;&#xA;    break_points = c(0,which(diffs &lt; (mean(diffs) - 4*sd(diffs))),length(y_data))&#xD;&#xA;    # Create the lists of y-values&#xD;&#xA;    y_lists = sapply(1:(length(break_points)-1),function(x){&#xD;&#xA;      y_data[(break_points[x]+1):(break_points[x+1])]&#xD;&#xA;    })&#xD;&#xA;    # Create the lists of x-values&#xD;&#xA;    x_lists = lapply(y_lists,function(x) 1:length(x))&#xD;&#xA;    #Find all the slopes for the lists of points&#xD;&#xA;    slopes = unlist(lapply(1:length(y_lists), function(x) lm(y_lists[[x]] ~ x_lists[[x]])$coefficients[2]))&#xD;&#xA;&#xD;&#xA;Here are the slopes:&#xD;&#xA;(3.309110, 4.419178, 3.292029, 4.531126, 3.675178, 4.294389)&#xD;&#xA;&#xD;&#xA;And we can just take the mean to find the expected slope (3.920168).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;How the sample data was created:&#xD;&#xA;&#xD;&#xA;The sample data will consist of 100 points, random noise with a slope of 4 (Hopefully we will estimate this).  When the y-values reach a cutoff, they reset to 50.  The cutoff is randomly chosen between 115 and 120 for each reset.  Here is the R code to create the data set.&#xD;&#xA;&#xD;&#xA;    # Create Sample Data&#xD;&#xA;    set.seed(1001)&#xD;&#xA;    x_data = 1:100 # x-data&#xD;&#xA;    y_data = rep(0,length(x_data)) # Initialize y-data&#xD;&#xA;    y_data[1] = 50 &#xD;&#xA;    reset_level = sample(115:120,1) # Select initial cutoff&#xD;&#xA;    for (i in x_data[-1]){ # Loop through rest of x-data&#xD;&#xA;      if(y_data[i-1]&gt;reset_level){ # check if y-value is above cutoff&#xD;&#xA;        y_data[i] = 50             # Reset if it is and&#xD;&#xA;        reset_level = sample(115:120,1) # rechoose cutoff&#xD;&#xA;      }else {&#xD;&#xA;        y_data[i] = y_data[i-1] + 4 + (10*runif(1)-5) # Or just increment y with random noise&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;    plot(x_data,y_data) # Plot data&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1914" PostHistoryTypeId="2" PostId="737" RevisionGUID="d14cf11b-b7ec-4a1f-a812-0fa9fb34bd03" CreationDate="2014-07-14T15:58:42.517" UserId="2485" Text="I have faced this problem many times while using SVM with Rbf kernel. Using Linear kernel instead of Rbf kernel solved my problem, but I dealt with lesser number of classes. The results were less skewed and more accurate with the linear kernel. Hope this solves your problem." />
  <row Id="1918" PostHistoryTypeId="2" PostId="739" RevisionGUID="90d1fb48-d16c-4b0a-937c-5c94383ca0b9" CreationDate="2014-07-14T19:02:01.670" UserId="2489" Text="I am an MSc student at the University of Edinburgh, specialized in machine learning and natural language processing. I had some practical courses focused on data mining, and others dealing with machine learning, bayesian statistics and graphical models. My background is a BSc in Computer Science.&#xD;&#xA;&#xD;&#xA;I did some software engineering and I learnt the basic concepts, such as design patterns, but I have never been involved in a large software development project. However, I had a data mining project in my MSc. My question is, if I want to go for a career as Data Scientist, should I apply for a graduate data scientist position first, or should I get a position as graduate software engineer first, maybe something related to data science, such as big data infrastructure or machine learning software development?&#xD;&#xA;&#xD;&#xA;My concern is that I might need good software engineering skills for data science, and I am not sure if these can be obtained by working as a graduate data scientist directly.&#xD;&#xA;&#xD;&#xA;Moreover, at the moment I like Data Mining, but what if I want to change my career to software engineering in the future? It might be difficult if I specialised so much in data science.&#xD;&#xA;&#xD;&#xA;I have not been employed yet, so my knowledge is still limited. Any clarification or advice are welcome, as I am about to finish my MSc and I want to start applying for graduate positions in early October.&#xD;&#xA;&#xD;&#xA;Thank you very much" />
  <row Id="1919" PostHistoryTypeId="1" PostId="739" RevisionGUID="90d1fb48-d16c-4b0a-937c-5c94383ca0b9" CreationDate="2014-07-14T19:02:01.670" UserId="2489" Text="Starting my career as Data Scientist, is Software Engineering experience required?" />
  <row Id="1920" PostHistoryTypeId="3" PostId="739" RevisionGUID="90d1fb48-d16c-4b0a-937c-5c94383ca0b9" CreationDate="2014-07-14T19:02:01.670" UserId="2489" Text="&lt;data-mining&gt;" />
  <row Id="1921" PostHistoryTypeId="2" PostId="740" RevisionGUID="5b1fb498-371f-4734-acb5-974c4e45eed4" CreationDate="2014-07-14T19:06:44.090" UserId="1350" Text="I would suggest you to use libsvm, which already has adjustable class weights implemented in it. Rather than replicating the training samples, one modifies the C parameter for different classes in the SVM optimization. For example if your data has 2 classes, and the first class is only 10% of the data, you would choose class weights to be 10 and 1 for class 1 and 2 respectively. Therefore, margin violations of the first class would cost 10 times more than the margin violations for second class, and per-class accuracies would be more balanced." />
  <row Id="1923" PostHistoryTypeId="2" PostId="741" RevisionGUID="65a57967-596e-4e5d-a212-365c61b47ca3" CreationDate="2014-07-15T04:39:11.670" UserId="381" Text="Absolutely. Keep your software skills sharp. You can do this in an academic program if you simply implement by yourself all the algorithms you learn about. &#xD;&#xA;&#xD;&#xA;Good selection of courses, btw. Consider getting an internship too." />
  <row Id="1924" PostHistoryTypeId="2" PostId="742" RevisionGUID="224d77bc-9fd1-4bff-9bf9-4a440237ce8b" CreationDate="2014-07-15T06:19:31.820" UserId="2452" Text="1) I think that there's no need to question whether your background is adequate for a career in data science. CS degree IMHO is **more than enough** for data scientist from software engineering point of view. Having said that, theoretical knowledge is not very helpful without matching **practical experience**, so I would definitely try to **enrich** my experience through participating in *additional school projects, internships or open source projects* (maybe ones, focused on data science / machine learning / artificial intelligence).&#xD;&#xA;&#xD;&#xA;2) I believe your concern about **focusing** on data science **too early** is unfounded, as long as you will be practicing software engineering either as a part of your data science job, or additionally in your spare time.&#xD;&#xA;&#xD;&#xA;3) I find the following **definition of a data scientist** rather accurate and hope it will be helpful in your future career success:&#xD;&#xA;&#xD;&#xA;&gt; A *data scientist* is someone who is better at statistics than any&#xD;&#xA;&gt; software engineer and better at software engineering than any&#xD;&#xA;&gt; statistician.&#xD;&#xA;&#xD;&#xA;P.S. Today's **enormous** number of various resources on data science topics is mind-blowing, but this **open source curriculum for learning data science** might fill some gaps between your BSc/MSc respective curricula and reality of the data science career (or, at least, provide some direction for further research and maybe answer some of your concerns): http://datasciencemasters.org, or on GitHub: https://github.com/datasciencemasters/go." />
  <row Id="1925" PostHistoryTypeId="2" PostId="743" RevisionGUID="87d2dbb3-a36c-4665-a8f4-5b24bcf6b3b7" CreationDate="2014-07-15T09:30:02.183" UserId="791" Text="From the job ads I have seen, the answer depends: There are jobs which are more technical in nature (designing big data projects, doing some analysis) or the exact opposite (doing analysis, storage etc. is someone elses job).&#xD;&#xA;&#xD;&#xA;So I would say that SOME software design skills are extremely useful , but you don't need the abillity to build a huge program in C# / Java or whatever.&#xD;&#xA;Why I like some SW skills is simply that your code probably looks way better than code from someone who never programmed for the sake of programming. Most of the time, the latter code is very hard do understand / debug for outsiders. Also, sometimes your analysis needs to be integrated in a bigger program,an understand of the needs of the programms certainly helps." />
  <row Id="1926" PostHistoryTypeId="2" PostId="744" RevisionGUID="733dabc8-cad4-4937-8eec-e59a366bbb69" CreationDate="2014-07-15T21:30:11.600" UserId="2507" Text="It looks like the cosine similarity of two features is just their dot product scaled by the product of their magnitudes. When does cosine similarity make a better distance metric than the dot product? I.e. do the dot product and cosine similarity have different strengths or weaknesses in different situations?&#xD;&#xA;" />
  <row Id="1927" PostHistoryTypeId="1" PostId="744" RevisionGUID="733dabc8-cad4-4937-8eec-e59a366bbb69" CreationDate="2014-07-15T21:30:11.600" UserId="2507" Text="Cosine similarity versus dot product as distance metrics" />
  <row Id="1928" PostHistoryTypeId="3" PostId="744" RevisionGUID="733dabc8-cad4-4937-8eec-e59a366bbb69" CreationDate="2014-07-15T21:30:11.600" UserId="2507" Text="&lt;classification&gt;" />
  <row Id="1929" PostHistoryTypeId="2" PostId="745" RevisionGUID="57c151aa-bb71-4a3e-9014-84e5d42bc383" CreationDate="2014-07-16T00:06:02.160" UserId="989" Text="I'm trying to use ARMA/ARIMA with the [statsmodel Python package][1], in order to predict the gas consumption. I tried with [a dataset][2] of this format:&#xD;&#xA;&#xD;&#xA;![with this format](https://i.imgur.com/ZUvBlUP.png)&#xD;&#xA;&#xD;&#xA;Using only the gas column.&#xD;&#xA;&#xD;&#xA;    from pandas.tseries.offsets import *&#xD;&#xA;&#xD;&#xA;    arma_mod20 = sm.tsa.ARMA(januaryFeb[['gas [m3]']], (5,3)).fit()&#xD;&#xA;    predict_sunspots = arma_mod20.predict('2012-01-13', '2012-01-14', dynamic=True)&#xD;&#xA;    ax = januaryFeb.ix['2012-01-13 00:00:00':'2012-01-15 22:00:00']['gas [m3]'].plot(figsize=(12,8))&#xD;&#xA;    ax = predict_sunspots.plot(ax=ax, style='r--', label='Dynamic Prediction');&#xD;&#xA;    ax.legend();&#xD;&#xA;&#xD;&#xA;![result](https://i.imgur.com/oCPonu7.png)&#xD;&#xA;&#xD;&#xA;Why is the prediction so bad?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://statsmodels.sourceforge.net/devel/tsa.html#descriptive-statistics-and-tests&#xD;&#xA;  [2]: https://github.com/denadai2/Gas-consumption-outliers/blob/master/exportWeb.csv" />
  <row Id="1930" PostHistoryTypeId="1" PostId="745" RevisionGUID="57c151aa-bb71-4a3e-9014-84e5d42bc383" CreationDate="2014-07-16T00:06:02.160" UserId="989" Text="ARMA/ARIMA on energy forecasts timeseries: strange prediction" />
  <row Id="1931" PostHistoryTypeId="3" PostId="745" RevisionGUID="57c151aa-bb71-4a3e-9014-84e5d42bc383" CreationDate="2014-07-16T00:06:02.160" UserId="989" Text="&lt;machine-learning&gt;&lt;python&gt;" />
  <row Id="1932" PostHistoryTypeId="2" PostId="746" RevisionGUID="5b5748da-abad-483c-8df0-859048bae0f6" CreationDate="2014-07-16T03:32:16.270" UserId="2452" Text="I'm not an expert on time series, but I have a **general advice**: may I suggest you to try other packages (and various parameters) to see, if there are any differences in results.&#xD;&#xA;&#xD;&#xA;Also, unless you have to use `Python`, I'd recommend to take a look at the `R`'s extensive *ecosystem* for **time series analysis**: see http://www.statmethods.net/advstats/timeseries.html and http://cran.r-project.org/web/views/TimeSeries.html.&#xD;&#xA;&#xD;&#xA;In particular, you may want to check the standard `stats` package (including functions `arima()` and `arima0`), as well as some other packages: `FitARMA` (http://cran.r-project.org/web/packages/FitARMA), `forecast` (http://cran.r-project.org/web/packages/forecast) and education-focused `fArma` (cran.r-project.org/web/packages/fArma), to mention just a few. I hope this is helpful." />
  <row Id="1933" PostHistoryTypeId="2" PostId="747" RevisionGUID="cbf2d712-1519-473d-8b3b-19706117bc15" CreationDate="2014-07-16T06:45:36.740" UserId="791" Text="About automatic cleaning: You really cannot clean data automatically, because the number of errors and the definition of an error is often dependent on the data. E.g.: Your column &quot;Income&quot; might contain negative values, which are an error - you have to do something about the cases. On the other hand a column &quot;monthly savings&quot; could reasonably contain negative values. &#xD;&#xA;&#xD;&#xA;Such errors are highly domain dependent - so to find them, you must have domain knowledge, something at which humans excel, automated processes not so much.&#xD;&#xA;&#xD;&#xA;Where you can and should automate is repeated projects. E.g. a report which has to produced monthly. If you spot errors, you should place some automated process which can spot  these kinds of errors in subsequent months, freeing your time. " />
  <row Id="1935" PostHistoryTypeId="2" PostId="748" RevisionGUID="930bf6c7-a160-4474-8b64-1a7fcdd1ee29" CreationDate="2014-07-16T07:47:48.603" UserId="2511" Text="I asked a data science question regarding how to decide on the best variation of a split test on the Statistics section of StackExchange. I hope I will have better luck here. The question is basically, &quot;Why is mean revenue per user the best metric to make your decision on in a split test?&quot;&#xD;&#xA;&#xD;&#xA;The original question is here: http://stats.stackexchange.com/questions/107599/better-estimator-of-expected-sum-than-mean&#xD;&#xA;&#xD;&#xA;Since it was not well received/understood I simplified the problem to a discrete set of purchases and phrased it as a classical probability problem. That question is here: http://stats.stackexchange.com/questions/107848/drawing-numbered-balls-from-an-urn&#xD;&#xA;&#xD;&#xA;The mean may be the best metric for such a decision but I am not convinced. We often have a lot of prior information so a Bayesian method would likely improve our estimates. I realize that this is a difficult question but Data Scientists are doing such split tests everyday. " />
  <row Id="1936" PostHistoryTypeId="1" PostId="748" RevisionGUID="930bf6c7-a160-4474-8b64-1a7fcdd1ee29" CreationDate="2014-07-16T07:47:48.603" UserId="2511" Text="Why use mean revinue in a split test?" />
  <row Id="1937" PostHistoryTypeId="3" PostId="748" RevisionGUID="930bf6c7-a160-4474-8b64-1a7fcdd1ee29" CreationDate="2014-07-16T07:47:48.603" UserId="2511" Text="&lt;research&gt;&lt;cross-validation&gt;" />
  <row Id="1938" PostHistoryTypeId="2" PostId="749" RevisionGUID="cb66685e-13ef-4cde-b8d9-f5494dff8013" CreationDate="2014-07-16T09:24:51.780" UserId="728" Text="I'm learning about matrix factorization for recommending systems and I'm seeing the term `latent features` occurring too frequently but I'm unable to understand what it means. I know what a feature is but I don't understand the idea of latent features. Could please explain it? Or at least point me to a paper/place where I can read about it?" />
  <row Id="1939" PostHistoryTypeId="1" PostId="749" RevisionGUID="cb66685e-13ef-4cde-b8d9-f5494dff8013" CreationDate="2014-07-16T09:24:51.780" UserId="728" Text="Meaning of latent features?" />
  <row Id="1940" PostHistoryTypeId="3" PostId="749" RevisionGUID="cb66685e-13ef-4cde-b8d9-f5494dff8013" CreationDate="2014-07-16T09:24:51.780" UserId="728" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;recommendation&gt;" />
  <row Id="1941" PostHistoryTypeId="2" PostId="750" RevisionGUID="4b358d4a-5056-4e53-8857-11b5944ad03a" CreationDate="2014-07-16T09:49:15.933" UserId="1387" Text="I am using OpenCV letter_recog.cpp example to experiment on random trees and other classifiers. This example has implementations of six classifiers - random trees, boosting, MLP, kNN, naive Bayes and SVM. UCI letter recognition dataset with 20000 instances and 16 features is used, which I split in half for training and testing. I have experience with SVM so I quickly set its recognition error to 3.3%. After some experimentation what I got was:&#xD;&#xA;&#xD;&#xA;UCI letter recognition:&#xD;&#xA;&#xD;&#xA; - RTrees -	5.3% &#xD;&#xA; - Boost -	13% &#xD;&#xA; - MLP -	7.9% &#xD;&#xA; - kNN(k=3) -	6.5%  &#xD;&#xA; - Bayes -	11.5%  &#xD;&#xA; - SVM -	3.3%&#xD;&#xA;&#xD;&#xA;Parameters used:&#xD;&#xA;&#xD;&#xA; - RTrees -	max_num_of_trees_in_the_forrest=200, max_depth=20,&#xD;&#xA;   min_sample_count=1&#xD;&#xA;   &#xD;&#xA; - Boost -	boost_type=REAL, weak_count=200, weight_trim_rate=0.95,&#xD;&#xA;   max_depth=7&#xD;&#xA;   &#xD;&#xA; - MLP -	method=BACKPROP, param=0.001, max_iter=300 (default values - too&#xD;&#xA;   slow to experiment) &#xD;&#xA;   &#xD;&#xA; - kNN(k=3) -	k=3&#xD;&#xA;   &#xD;&#xA; - Bayes -	none&#xD;&#xA;   &#xD;&#xA; - SVM -	RBF kernel, C=10, gamma=0.01&#xD;&#xA;&#xD;&#xA;After that I used same parameters and tested on Digits and MNIST datasets by extracting gradient features first (vector size 200 elements):&#xD;&#xA;&#xD;&#xA;Digits:&#xD;&#xA;&#xD;&#xA; - RTrees -	5.1%&#xD;&#xA; - Boost -	23.4%&#xD;&#xA; - MLP -	4.3%&#xD;&#xA; - kNN(k=3) -	7.3%&#xD;&#xA; - Bayes -	17.7%&#xD;&#xA; - SVM -	4.2%&#xD;&#xA;&#xD;&#xA;MNIST:&#xD;&#xA;&#xD;&#xA; - RTrees -	1.4%&#xD;&#xA; - Boost -	out of memory&#xD;&#xA; - MLP -	in progress&#xD;&#xA; - kNN(k=3) -	1.2%&#xD;&#xA; - Bayes -	34.33%&#xD;&#xA; - SVM -	0.6%&#xD;&#xA;&#xD;&#xA;I am new to all classifiers except SVM and kNN, for these two I can say the results seem fine. What about others? I expected more from random trees, on MNIST kNN gives better accuracy, any ideas how to get it higher? Boost and Bayes give very low accuracy. In the end I'd like to use these classifiers to make a multiple classifier system. Any advice?" />
  <row Id="1942" PostHistoryTypeId="1" PostId="750" RevisionGUID="4b358d4a-5056-4e53-8857-11b5944ad03a" CreationDate="2014-07-16T09:49:15.933" UserId="1387" Text="How to increase accuracy of classifiers?" />
  <row Id="1943" PostHistoryTypeId="3" PostId="750" RevisionGUID="4b358d4a-5056-4e53-8857-11b5944ad03a" CreationDate="2014-07-16T09:49:15.933" UserId="1387" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;&lt;random-forest&gt;" />
  <row Id="1944" PostHistoryTypeId="2" PostId="751" RevisionGUID="a7b20b81-bf9d-4081-8a0d-a88f2cbec909" CreationDate="2014-07-16T10:42:55.793" UserId="941" Text="You are right, cosine similarity has a lot of common with dot product of vectors. Indeed, it is a dot product, scaled by magnitude. And because of scaling it is normalized between 0 and 1. CS is preferable because it takes into account variability of data and features' relative frequencies. On the other hand, plain dot product is a little bit &quot;cheaper&quot; (in terms of complexity and implementation)." />
  <row Id="1945" PostHistoryTypeId="33" PostId="224" RevisionGUID="4cfef48a-36ee-4f64-a513-2b281056b3b8" CreationDate="2014-07-16T11:00:39.170" UserId="122" Comment="5" />
  <row Id="1946" PostHistoryTypeId="2" PostId="752" RevisionGUID="d3a1c4e4-6c7b-40ca-a2c8-4f3ada7b9927" CreationDate="2014-07-16T11:07:56.467" UserId="2452" Text="It seems to me that *latent features* is a term used to describe criteria for **classifying entities** by their structure, in other words, by *features* (traits) they contain, instead of *classes* they belong to. Meaning of the word &quot;latent&quot; here is most likely similar to its meaning in social sciences, where very popular term latent variable (http://en.wikipedia.org/wiki/Latent_variable) means unobservable variable (concept).&#xD;&#xA;&#xD;&#xA;Section &quot;Introduction&quot; in the following paper provides a good explanation of latent features' **meaning** and use in **modeling** of social sciences phenomena: http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf." />
  <row Id="1947" PostHistoryTypeId="2" PostId="753" RevisionGUID="ddc29c6a-717b-48dd-965e-e2b54a9544bb" CreationDate="2014-07-16T13:05:41.123" UserId="2489" Text="I could try to explain you with words, but these slides explain it very well with pictures. Hope it helps.&#xD;&#xA;http://www.inf.ed.ac.uk/teaching/courses/mt/lectures/phrase-model.pdf&#xD;&#xA;&#xD;&#xA;Note this slides correspond to the chapter 5 of &quot;Statistical Machine Translation&quot; by Philipp Koehn, highly recommended if you are working on machine translation, and it is easy to read." />
  <row Id="1948" PostHistoryTypeId="2" PostId="754" RevisionGUID="012f8664-0245-46bf-aad3-9ffaf6cc2c3c" CreationDate="2014-07-16T14:22:38.677" UserId="2513" Text="Most of the recent Frequent Pattern approaches that I've seen in the literature are based on optimizing FPGrowth. I have to admit, I haven't seen many developments within the literature in FPM in many years.&#xD;&#xA;&#xD;&#xA;[This wikibook](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm) highlights many of the variants on FPGrowth that are out there." />
  <row Id="1950" PostHistoryTypeId="5" PostId="750" RevisionGUID="00d3584e-9215-48b0-8e12-0d18c61f6aa7" CreationDate="2014-07-16T15:09:44.907" UserId="1387" Comment="deleted 7 characters in body; edited tags" Text="I am using OpenCV letter_recog.cpp example to experiment on random trees and other classifiers. This example has implementations of six classifiers - random trees, boosting, MLP, kNN, naive Bayes and SVM. UCI letter recognition dataset with 20000 instances and 16 features is used, which I split in half for training and testing. I have experience with SVM so I quickly set its recognition error to 3.3%. After some experimentation what I got was:&#xD;&#xA;&#xD;&#xA;UCI letter recognition:&#xD;&#xA;&#xD;&#xA; - RTrees -	5.3% &#xD;&#xA; - Boost -	13% &#xD;&#xA; - MLP -	7.9% &#xD;&#xA; - kNN(k=3) -	6.5%  &#xD;&#xA; - Bayes -	11.5%  &#xD;&#xA; - SVM -	3.3%&#xD;&#xA;&#xD;&#xA;Parameters used:&#xD;&#xA;&#xD;&#xA; - RTrees -	max_num_of_trees_in_the_forrest=200, max_depth=20,&#xD;&#xA;   min_sample_count=1&#xD;&#xA;   &#xD;&#xA; - Boost -	boost_type=REAL, weak_count=200, weight_trim_rate=0.95,&#xD;&#xA;   max_depth=7&#xD;&#xA;   &#xD;&#xA; - MLP -	method=BACKPROP, param=0.001, max_iter=300 (default values - too&#xD;&#xA;   slow to experiment) &#xD;&#xA;   &#xD;&#xA; - kNN(k=3) -	k=3&#xD;&#xA;   &#xD;&#xA; - Bayes -	none&#xD;&#xA;   &#xD;&#xA; - SVM -	RBF kernel, C=10, gamma=0.01&#xD;&#xA;&#xD;&#xA;After that I used same parameters and tested on Digits and MNIST datasets by extracting gradient features first (vector size 200 elements):&#xD;&#xA;&#xD;&#xA;Digits:&#xD;&#xA;&#xD;&#xA; - RTrees -	5.1%&#xD;&#xA; - Boost -	23.4%&#xD;&#xA; - MLP -	4.3%&#xD;&#xA; - kNN(k=3) -	7.3%&#xD;&#xA; - Bayes -	17.7%&#xD;&#xA; - SVM -	4.2%&#xD;&#xA;&#xD;&#xA;MNIST:&#xD;&#xA;&#xD;&#xA; - RTrees -	1.4%&#xD;&#xA; - Boost -	out of memory&#xD;&#xA; - MLP -	1.0%&#xD;&#xA; - kNN(k=3) -	1.2%&#xD;&#xA; - Bayes -	34.33%&#xD;&#xA; - SVM -	0.6%&#xD;&#xA;&#xD;&#xA;I am new to all classifiers except SVM and kNN, for these two I can say the results seem fine. What about others? I expected more from random trees, on MNIST kNN gives better accuracy, any ideas how to get it higher? Boost and Bayes give very low accuracy. In the end I'd like to use these classifiers to make a multiple classifier system. Any advice?" />
  <row Id="1951" PostHistoryTypeId="6" PostId="750" RevisionGUID="00d3584e-9215-48b0-8e12-0d18c61f6aa7" CreationDate="2014-07-16T15:09:44.907" UserId="1387" Comment="deleted 7 characters in body; edited tags" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;svm&gt;&lt;accuracy&gt;&lt;random-forest&gt;" />
  <row Id="1952" PostHistoryTypeId="6" PostId="739" RevisionGUID="f9d259f3-e80c-4704-9133-56a222e8ca35" CreationDate="2014-07-16T15:35:15.697" UserId="97" Comment="Re-tag to more relevant." Text="&lt;education&gt;&lt;definitions&gt;" />
  <row Id="1953" PostHistoryTypeId="24" PostId="739" RevisionGUID="f9d259f3-e80c-4704-9133-56a222e8ca35" CreationDate="2014-07-16T15:35:15.697" Comment="Proposed by 97 approved by 434, -1 edit id of 114" />
  <row Id="1954" PostHistoryTypeId="5" PostId="739" RevisionGUID="87a0b5ea-32f4-4c45-aeab-f03a56202725" CreationDate="2014-07-16T15:35:15.697" UserId="84" Comment="Re-tag to more relevant." Text="I am an MSc student at the University of Edinburgh, specialized in machine learning and natural language processing. I had some practical courses focused on data mining, and others dealing with machine learning, bayesian statistics and graphical models. My background is a BSc in Computer Science.&#xD;&#xA;&#xD;&#xA;I did some software engineering and I learnt the basic concepts, such as design patterns, but I have never been involved in a large software development project. However, I had a data mining project in my MSc. My question is, if I want to go for a career as Data Scientist, should I apply for a graduate data scientist position first, or should I get a position as graduate software engineer first, maybe something related to data science, such as big data infrastructure or machine learning software development?&#xD;&#xA;&#xD;&#xA;My concern is that I might need good software engineering skills for data science, and I am not sure if these can be obtained by working as a graduate data scientist directly.&#xD;&#xA;&#xD;&#xA;Moreover, at the moment I like Data Mining, but what if I want to change my career to software engineering in the future? It might be difficult if I specialised so much in data science.&#xD;&#xA;&#xD;&#xA;I have not been employed yet, so my knowledge is still limited. Any clarification or advice are welcome, as I am about to finish my MSc and I want to start applying for graduate positions in early October." />
  <row Id="1955" PostHistoryTypeId="5" PostId="658" RevisionGUID="c3f1660e-97e3-4f1f-86ca-525f65381312" CreationDate="2014-07-16T15:35:31.410" UserId="1367" Comment="Remove capitals and simplify the formulation" Text="I would like to use non-atomic data, as a feature for a prediction. &#xD;&#xA;Suppose I have a Table with these features:&#xD;&#xA;&#xD;&#xA;    - Column 1: Categorical - House&#xD;&#xA;    - Column 2: Numerical - 23.22&#xD;&#xA;    - Column 3: A Vector - [ 12, 22, 32 ]&#xD;&#xA;    - Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]&#xD;&#xA;    - Column 5: A List [ 122, Boolean ]&#xD;&#xA;&#xD;&#xA;I would like to predict/classify, for instance, Column 2.&#xD;&#xA;&#xD;&#xA;I am making something to automatically respond to questions, any type of question, like &quot;Where was Foo Born?&quot; ...&#xD;&#xA;&#xD;&#xA;I first make a query to a search engine, then I get some text data as a result, then I do all the parsing stuff (tagging, stemming, parsing, splitting ... )&#xD;&#xA; &#xD;&#xA;My first approach was to make a table, each row with a line of text and a lot of features, like &quot;First Word&quot;, &quot;Tag of First Word&quot;, &quot;Chunks&quot;, etc...&#xD;&#xA;&#xD;&#xA;But with this approach I am missing the relationships between the sentences. &#xD;&#xA;&#xD;&#xA;I would like to know if there is an algorithm that looks inside the tree structures (or vectors) and makes the relations and extract whatever is relevant for predicting/classifying. I'd prefer to know about a library that does that than an algorithm that I have to implement." />
  <row Id="1956" PostHistoryTypeId="24" PostId="658" RevisionGUID="c3f1660e-97e3-4f1f-86ca-525f65381312" CreationDate="2014-07-16T15:35:31.410" Comment="Proposed by 1367 approved by 434, 84 edit id of 112" />
  <row Id="1957" PostHistoryTypeId="5" PostId="726" RevisionGUID="666b2591-8722-48a2-8842-464f0a678ac4" CreationDate="2014-07-16T16:18:18.573" UserId="322" Comment="make the title, citation, and quoted material all explicit and internal; fix equation formatting" Text="I am trying to understand a neuroscience article:&#xD;&#xA;&#xD;&#xA;- Friston, Karl J., et al. &quot;Action and behavior: a free-energy formulation.&quot; *Biological cybernetics* 102.3 (2010): 227-260. ([DOI 10.1007/s00422-010-0364-z](http://link.springer.com/article/10.1007%2Fs00422-010-0364-z))&#xD;&#xA;&#xD;&#xA;In this article, Friston gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear on page 231 of the article as equation (5):&#xD;&#xA;&#xD;&#xA;&gt;The resulting expression for free-energy can be expressed in three ways (with the use of the Bayes rules and simple rearrangements):&#xD;&#xA;&gt;&#xD;&#xA;&gt;• Energy minus entropy&#xD;&#xA;&gt;&#xD;&#xA;&gt;• Divergence plus surprise&#xD;&#xA;&gt;&#xD;&#xA;&gt;• Complexity minus accuracy&#xD;&#xA;&gt;&#xD;&#xA;&gt;Mathematically, these correspond to:&#xD;&#xA;&gt;&#xD;&#xA;&gt;![\begin {aligned} F &amp;= - \langle \ln p(\tilde s, \Psi \vert m) \rangle _q + \langle \ln q(\Psi \vert \mu) \rangle _q \\ &amp;= D(q(\Psi \vert \mu) \parallel p(\Psi \vert \tilde s, m)) - \ln p(\tilde s \vert m) &amp; \qquad \qquad &amp; (5) \\ &amp;= D(q(\Psi \vert \mu) \parallel p(\Psi \vert m)) - \langle \ln p(\tilde s \vert \Psi, m) \rangle _q \end{aligned}][1]&#xD;&#xA;&#xD;&#xA;The things I am struggling with at this point are 1) the meaning of the || in the 2nd and 3rd versions of the equations and 2) the negative logs. Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/0eQgg.png" />
  <row Id="1958" PostHistoryTypeId="4" PostId="726" RevisionGUID="666b2591-8722-48a2-8842-464f0a678ac4" CreationDate="2014-07-16T16:18:18.573" UserId="322" Comment="make the title, citation, and quoted material all explicit and internal; fix equation formatting" Text="Trying to understand free-energy equations in a Karl Friston neuroscience article" />
  <row Id="1959" PostHistoryTypeId="6" PostId="726" RevisionGUID="666b2591-8722-48a2-8842-464f0a678ac4" CreationDate="2014-07-16T16:18:18.573" UserId="322" Comment="make the title, citation, and quoted material all explicit and internal; fix equation formatting" Text="&lt;neuralnetwork&gt;" />
  <row Id="1960" PostHistoryTypeId="24" PostId="726" RevisionGUID="666b2591-8722-48a2-8842-464f0a678ac4" CreationDate="2014-07-16T16:18:18.573" Comment="Proposed by 322 approved by 434, -1 edit id of 113" />
  <row Id="1961" PostHistoryTypeId="5" PostId="726" RevisionGUID="1e001584-ee66-4c21-af7c-f0ebe3f67488" CreationDate="2014-07-16T16:18:18.573" UserId="84" Comment="make the title, citation, and quoted material all explicit and internal; fix equation formatting" Text="I am trying to understand a neuroscience article:&#xD;&#xA;&#xD;&#xA;- Friston, Karl J., et al. &quot;Action and behavior: a free-energy formulation.&quot; *Biological cybernetics* 102.3 (2010): 227-260. ([DOI 10.1007/s00422-010-0364-z](http://link.springer.com/article/10.1007%2Fs00422-010-0364-z))&#xD;&#xA;&#xD;&#xA;In this article, Friston gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear on page 231 of the article as equation (5):&#xD;&#xA;&#xD;&#xA;&gt;The resulting expression for free-energy can be expressed in three ways (with the use of the Bayes rules and simple rearrangements):&#xD;&#xA;&gt;&#xD;&#xA;&gt;• Energy minus entropy&#xD;&#xA;&gt;&#xD;&#xA;&gt;• Divergence plus surprise&#xD;&#xA;&gt;&#xD;&#xA;&gt;• Complexity minus accuracy&#xD;&#xA;&gt;&#xD;&#xA;&gt;Mathematically, these correspond to:&#xD;&#xA;&gt;&#xD;&#xA;&gt;![\begin {aligned} F &amp;= - \langle \ln p(\tilde s, \Psi \vert m) \rangle _q + \langle \ln q(\Psi \vert \mu) \rangle _q \\ &amp;= D(q(\Psi \vert \mu) \parallel p(\Psi \vert \tilde s, m)) - \ln p(\tilde s \vert m) &amp; \qquad \qquad &amp; (5) \\ &amp;= D(q(\Psi \vert \mu) \parallel p(\Psi \vert m)) - \langle \ln p(\tilde s \vert \Psi, m) \rangle _q \end{aligned}][1]&#xD;&#xA;&#xD;&#xA;The things I am struggling with at this point are:&#xD;&#xA;&#xD;&#xA;1. the meaning of the || in the 2nd and 3rd versions of the equations;&#xD;&#xA;2. and the negative logs.&#xD;&#xA;&#xD;&#xA;Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/0eQgg.png" />
  <row Id="1962" PostHistoryTypeId="5" PostId="729" RevisionGUID="8daca80c-9e82-496b-9c48-1ff3148973fa" CreationDate="2014-07-16T16:36:02.310" UserId="375" Comment="Answered the Question" Text="I thought this was an interesting problem, so I wrote a sample data set and a linear slope estimator in R.  I hope it helps you with your problem.  I'm going to make some assumptions, the biggest is that you want to estimate a constant slope, given by some segments in your data.  Another assumption to separate the blocks of linear data is that the natural 'reset' will be found by comparing consecutive differences and finding ones that are X-standard deviations below the mean. (I chose 4 sd's, but this can be changed)&#xD;&#xA;&#xD;&#xA;Here is a plot of the data, and the code to generating it is at the bottom.&#xD;&#xA;![Sample Data][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;For starters, we find the breaks and fit each set of y-values and record the slopes.&#xD;&#xA;&#xD;&#xA;    # Find the differences between adjacent points&#xD;&#xA;    diffs = y_data[-1] - y_data[-length(y_data)]&#xD;&#xA;    # Find the break points (here I use 4 s.d.'s)&#xD;&#xA;    break_points = c(0,which(diffs &lt; (mean(diffs) - 4*sd(diffs))),length(y_data))&#xD;&#xA;    # Create the lists of y-values&#xD;&#xA;    y_lists = sapply(1:(length(break_points)-1),function(x){&#xD;&#xA;      y_data[(break_points[x]+1):(break_points[x+1])]&#xD;&#xA;    })&#xD;&#xA;    # Create the lists of x-values&#xD;&#xA;    x_lists = lapply(y_lists,function(x) 1:length(x))&#xD;&#xA;    #Find all the slopes for the lists of points&#xD;&#xA;    slopes = unlist(lapply(1:length(y_lists), function(x) lm(y_lists[[x]] ~ x_lists[[x]])$coefficients[2]))&#xD;&#xA;&#xD;&#xA;Here are the slopes:&#xD;&#xA;(3.309110, 4.419178, 3.292029, 4.531126, 3.675178, 4.294389)&#xD;&#xA;&#xD;&#xA;And we can just take the mean to find the expected slope (3.920168).&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;**Edit: Predicting when series reaches 120**&#xD;&#xA;&#xD;&#xA;I realized I didn't finish predicted when series reaches 120.  If we estimate the slope to be m and we see a reset at time t to a value x (x&lt;120), we can predict how much longer it would take to reach 120 by some simple algebra.&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;Here, t is the time it would take to reach 120 after a reset, x is what it resets to, and m is the estimated slope.  I'm not going to even touch the subject of units here, but it's good practice to work them out and make sure everything makes sense.&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;**Edit: Creating The Sample Data**&#xD;&#xA;&#xD;&#xA;The sample data will consist of 100 points, random noise with a slope of 4 (Hopefully we will estimate this).  When the y-values reach a cutoff, they reset to 50.  The cutoff is randomly chosen between 115 and 120 for each reset.  Here is the R code to create the data set.&#xD;&#xA;&#xD;&#xA;    # Create Sample Data&#xD;&#xA;    set.seed(1001)&#xD;&#xA;    x_data = 1:100 # x-data&#xD;&#xA;    y_data = rep(0,length(x_data)) # Initialize y-data&#xD;&#xA;    y_data[1] = 50 &#xD;&#xA;    reset_level = sample(115:120,1) # Select initial cutoff&#xD;&#xA;    for (i in x_data[-1]){ # Loop through rest of x-data&#xD;&#xA;      if(y_data[i-1]&gt;reset_level){ # check if y-value is above cutoff&#xD;&#xA;        y_data[i] = 50             # Reset if it is and&#xD;&#xA;        reset_level = sample(115:120,1) # rechoose cutoff&#xD;&#xA;      }else {&#xD;&#xA;        y_data[i] = y_data[i-1] + 4 + (10*runif(1)-5) # Or just increment y with random noise&#xD;&#xA;      }&#xD;&#xA;    }&#xD;&#xA;    plot(x_data,y_data) # Plot data&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/2dC1w.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/DixZv.gif" />
  <row Id="1963" PostHistoryTypeId="2" PostId="755" RevisionGUID="63c5c61f-4267-46ca-a17e-4cdcfe7b0cc1" CreationDate="2014-07-16T17:34:24.880" UserId="984" Text="*I expected more from random trees*:&#xD;&#xA;&#xD;&#xA;  - With random forests, typically for N features, sqrt(N) features are used for each decision tree construction. Since in your case *N*=20, you could try setting *max_depth* (the number of sub-features to construct each decision tree) to 5.&#xD;&#xA;&#xD;&#xA;  - Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes. This might improve your accuracy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;*On MNIST kNN gives better accuracy, any ideas how to get it higher?*&#xD;&#xA;&#xD;&#xA;  - Try with a higher value of *K* (say 5 or 7). A higher value of K would give you more supportive evidence about the class label of a point.&#xD;&#xA;  - You could run PCA or Fisher's Linear Discriminant Analysis before running k-nearest neighbour. By this you could potentially get rid of correlated features while computing distances between the points, and hence your k neighbours would be more robust.&#xD;&#xA;  - Try different K values for different points based on the variance in the distances between the K neighbours.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1964" PostHistoryTypeId="2" PostId="756" RevisionGUID="cb1999e1-135d-4522-9369-8f5d109fc2ff" CreationDate="2014-07-16T18:15:42.343" UserId="2515" Text="At the expense of over-simplication, latent features are 'hidden' features to distinguish them from observed features. Latent features are computed from observed features using matrix factorization. An example would be text document analysis. 'words' extracted from the documents are features. If you factorize the data of words you can find 'topics', where 'topic' is a group of words with semantic relevance. Low-rank matrix factorization maps several rows (observed features) to a smaller set of rows (latent features).&#xD;&#xA;To elaborate, the document could have observed features (words) like [sail-boat, schooner, yatch, steamer, cruiser] which would 'factorize' to latent feature (topic) like 'ship' and 'boat'. &#xD;&#xA;&#xD;&#xA;[sail-boat, schooner, yatch, steamer, cruiser, ...] -&gt; [ship, boat]&#xD;&#xA;&#xD;&#xA;The underlying idea is that latent features are semantically relevant 'aggregates' of observered features. When you have large-scale, high-dimensional, and noisy observered features, it makes sense to build your classifier on latent features.&#xD;&#xA;&#xD;&#xA;This is a of course a simplified description to elucidate the concept. You can read the details on Latent Dirichlet Allocation (LDA) or probabilistic Latent Semantic Analysis (pLSA) models for an accurate description." />
  <row Id="1965" PostHistoryTypeId="2" PostId="757" RevisionGUID="f6fa3a99-56be-4e6c-8ef5-2e9fc6bca998" CreationDate="2014-07-16T18:26:54.233" UserId="325" Text="Gas usage has a daily cycle but there are also secondary weekly and annual cycles that the ARIMA may not be able to capture.&#xD;&#xA;&#xD;&#xA;There is a very noticeable difference between the weekday and Saturday data. Try creating a subset of the data for each day of the week or splitting the data into weekday and weekend and applying the model.&#xD;&#xA;&#xD;&#xA;If you can obtain temperature data for the same period check if there is a correlation between the temperature and gas usage.&#xD;&#xA;&#xD;&#xA;As @Aleksandr Blekh said R does have good packages for ARIMA models" />
  <row Id="1966" PostHistoryTypeId="2" PostId="758" RevisionGUID="c9f8a8ad-657c-4431-9ad3-a00f5d595dd0" CreationDate="2014-07-16T20:09:08.640" UserId="895" Text="I am working on a data science project using Python.&#xD;&#xA;The project has several stages.&#xD;&#xA;Each stage comprises of taking a data set, using Python scripts, auxiliary data, configuration and parameters, and creating another data set.&#xD;&#xA;I store the code in git, so that part is covered.&#xD;&#xA;I would like to hear about:&#xD;&#xA;&#xD;&#xA; 1. Tools for data version control.&#xD;&#xA; 2. Tools enabling to reproduce stages and experiments.&#xD;&#xA; 3. Protocol and suggested directory structure for such a project.&#xD;&#xA; 4. Automated build/run tools.&#xD;&#xA;&#xD;&#xA; " />
  <row Id="1967" PostHistoryTypeId="1" PostId="758" RevisionGUID="c9f8a8ad-657c-4431-9ad3-a00f5d595dd0" CreationDate="2014-07-16T20:09:08.640" UserId="895" Text="Tools and protocol for reproducible data science using Python" />
  <row Id="1968" PostHistoryTypeId="3" PostId="758" RevisionGUID="c9f8a8ad-657c-4431-9ad3-a00f5d595dd0" CreationDate="2014-07-16T20:09:08.640" UserId="895" Text="&lt;python&gt;&lt;tools&gt;" />
  <row Id="1969" PostHistoryTypeId="2" PostId="759" RevisionGUID="f1bb9bb1-291f-4656-924b-2285cfd56666" CreationDate="2014-07-17T06:02:04.813" UserId="2452" Text="This topic of *reproducible research* (RR) is **very popular** today and, consequently, is **huge**, but I hope that my answer will be **comprehensive enough** as an answer and will provide enough information for **further research**, should you decide to do so.&#xD;&#xA;&#xD;&#xA;While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more **universal tools** (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let's take a look what tools are available per your list.&#xD;&#xA;&#xD;&#xA;1) **Tools for data version control**. Unless you plan to work with (very) *big data*, I guess, it would make sense to use the same `git`, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git.&#xD;&#xA;&#xD;&#xA;2) **Tools for managing RR workflows and experiments**. Here's a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):&#xD;&#xA;&#xD;&#xA;- *Taverna Workflow Management System* (http://www.taverna.org.uk) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal myExperiment (http://www.myexperiment.org), where user can store and share their RR workflows. Web-based RR portal, fully compatible with Taverna is called Taverna Online, but it is being developed and maintained by totally different organization in Russia (referred there to as OnlineHPC: http://onlinehpc.com).&#xD;&#xA;&#xD;&#xA;- *The Kepler Project* (https://kepler-project.org)&#xD;&#xA;&#xD;&#xA;- *VisTrails* (http://vistrails.org)&#xD;&#xA;&#xD;&#xA;- *Madagascar* (http://www.reproducibility.org)&#xD;&#xA;&#xD;&#xA;**EXAMPLE**. Here's an interesting article on scientific workflows with an example of the **real** workflow design and data analysis, based on using *Kepler* and *myExperiment* projects: http://f1000research.com/articles/3-110/v1.&#xD;&#xA;&#xD;&#xA;There are many RR tools that implement *literate programming* paradigm, exemplified by `LaTeX` software family. Tools that help in report generation and presentation is also a large category, where `Sweave` and `knitr` are probably the most well-known ones. `Sweave` is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (http://stackoverflow.com/questions/2161152/sweave-for-python). I think that `knitr` might be a better option, as it's modern, has extensive support by popular tools (such as `RStudio`) and is language-neutral (http://yihui.name/knitr/demo/engines).&#xD;&#xA;&#xD;&#xA;3) **Protocol and suggested directory structure**. If I understood correctly what you implied by using term *protocol* (*workflow*), generally I think that standard RR data analysis workflow consists of the following sequential phases: *data collection* =&gt; *data preparation* (cleaning, transformation, merging, sampling) =&gt; *data analysis* =&gt; *presentation of results* (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.&#xD;&#xA;&#xD;&#xA;For sample directory structure, you may take a look at documentation for R package `ProjectTemplate` (http://projecttemplate.net), as an attempt to automate data analysis workflows and projects:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;4) **Automated build/run tools**. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is `make`. Read the following article for some reasons to use `make` as the preferred RR workflow automation tool: http://bost.ocks.org/mike/make. Certainly, there are other **similar** tools, which either improve some aspects of `make`, or add some additional features. For example: `ant` (officially, Apache Ant: http://ant.apache.org), `Maven` (&quot;next generation `ant`&quot;: http://maven.apache.org), `rake` (https://github.com/ruby/rake), `Makepp` (http://makepp.sourceforge.net). For a comprehensive list of such tools, see Wikipedia: http://en.wikipedia.org/wiki/List_of_build_automation_software.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/0B2vo.png" />
  <row Id="1970" PostHistoryTypeId="2" PostId="760" RevisionGUID="db27118c-6e1e-4e07-a928-14244d2ea693" CreationDate="2014-07-17T09:26:11.833" UserId="1315" Text="I am given a time series data vector (ordered by months and years),which contains only `0`s and `1`s. `1` s represent a person changes his job at a particular a month. &#xD;&#xA;&#xD;&#xA;**Questions:** What model can i use to determine model how frequently this person change his job ? In addition, this model should be able to predict the probability of this person changing his in the next 6 months.  &#xD;&#xA;&#xD;&#xA;A poisson process ? (I have studied poisson process before however I have no idea when and how to apply it). Any assumptions that data need to meet before applying the poisson process ? &#xD;&#xA;&#xD;&#xA;Would love to gather more information on how to model something like this. Thanks" />
  <row Id="1971" PostHistoryTypeId="1" PostId="760" RevisionGUID="db27118c-6e1e-4e07-a928-14244d2ea693" CreationDate="2014-07-17T09:26:11.833" UserId="1315" Text="Given time series data, how to model the frequency of someone changes his job?" />
  <row Id="1972" PostHistoryTypeId="3" PostId="760" RevisionGUID="db27118c-6e1e-4e07-a928-14244d2ea693" CreationDate="2014-07-17T09:26:11.833" UserId="1315" Text="&lt;data-mining&gt;&lt;time-series&gt;" />
  <row Id="1973" PostHistoryTypeId="2" PostId="761" RevisionGUID="6f98ad34-c53f-4499-918b-251754f61341" CreationDate="2014-07-17T09:50:41.437" UserId="2533" Text="What is the right approach and clustering algorithm for geo location clustering?&#xD;&#xA;&#xD;&#xA;I'm using the following code to cluster geolocation coordinates :&#xD;&#xA;&#xD;&#xA;    import numpy as np&#xD;&#xA;    import matplotlib.pyplot as plt&#xD;&#xA;    from scipy.cluster.vq import kmeans2, whiten&#xD;&#xA;    &#xD;&#xA;    coordinates= np.array([&#xD;&#xA;               [lat, long],&#xD;&#xA;               [lat, long],&#xD;&#xA;                ...&#xD;&#xA;               [lat, long]&#xD;&#xA;               ])&#xD;&#xA;    x, y = kmeans2(whiten(coordinates), 3, iter = 20)  &#xD;&#xA;    plt.scatter(coordinates[:,0], coordinates[:,1], c=y);&#xD;&#xA;    plt.show()&#xD;&#xA;&#xD;&#xA;Is it right to use Kmeans for location clustering, as it uses Euclidean distance and not Haversine formula as a distance function?" />
  <row Id="1974" PostHistoryTypeId="1" PostId="761" RevisionGUID="6f98ad34-c53f-4499-918b-251754f61341" CreationDate="2014-07-17T09:50:41.437" UserId="2533" Text="Clustering geo location coordinates (lat,long pairs)" />
  <row Id="1975" PostHistoryTypeId="3" PostId="761" RevisionGUID="6f98ad34-c53f-4499-918b-251754f61341" CreationDate="2014-07-17T09:50:41.437" UserId="2533" Text="&lt;machine-learning&gt;&lt;python&gt;&lt;clustering&gt;&lt;k-means&gt;&lt;clusters&gt;" />
  <row Id="1976" PostHistoryTypeId="2" PostId="762" RevisionGUID="2412e6b0-2c0d-4625-9b7a-eb265718cd57" CreationDate="2014-07-17T10:04:29.797" UserId="131" Text="t-SNE, as in [1], works by progressively reducing the Kullback-Leibler (KL) divergence, until a certain condition is met.&#xD;&#xA;The creators of t-SNE suggests to use KL divergence as a performance criterion for the visualizations:&#xD;&#xA;&#xD;&#xA;&gt; you can compare the Kullback-Leibler divergences that t-SNE reports. It is perfectly fine to run t-SNE ten times, and select the solution with the lowest KL divergence [2]&#xD;&#xA;&#xD;&#xA;I tried two implementations of t-SNE:&#xD;&#xA;&#xD;&#xA;* **python**: sklearn.manifold.TSNE().&#xD;&#xA;* **R**: tsne, from library(tsne).&#xD;&#xA;&#xD;&#xA;Both these implementations, when verbosity is set, print the error (Kullback-Leibler divergence) for each iteration. However, they don't allow the user to get this information, which looks a bit strange to me.&#xD;&#xA;&#xD;&#xA;For example, the code:&#xD;&#xA;&#xD;&#xA;    import numpy as np&#xD;&#xA;    from sklearn.manifold import TSNE&#xD;&#xA;    X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])&#xD;&#xA;    model = TSNE(n_components=2, verbose=2, n_iter=200)&#xD;&#xA;    t = model.fit_transform(X)&#xD;&#xA;produces:&#xD;&#xA;&#xD;&#xA;    [t-SNE] Computing pairwise distances...&#xD;&#xA;    [t-SNE] Computed conditional probabilities for sample 4 / 4&#xD;&#xA;    [t-SNE] Mean sigma: 1125899906842624.000000&#xD;&#xA;    [t-SNE] Iteration 10: error = 6.7213750, gradient norm = 0.0012028&#xD;&#xA;    [t-SNE] Iteration 20: error = 6.7192064, gradient norm = 0.0012062&#xD;&#xA;    [t-SNE] Iteration 30: error = 6.7178683, gradient norm = 0.0012114&#xD;&#xA;    ...&#xD;&#xA;    [t-SNE] Error after 200 iterations: 0.270186&#xD;&#xA;&#xD;&#xA;Now, as far as I understand, **0.270186** should be the KL divergence. However i cannot get this information, neither from **model** nor from **t** (which is a simple numpy.ndarray).&#xD;&#xA;&#xD;&#xA;To solve this problem I could: i) Calculate KL divergence by my self, ii) Do something nasty in python for capturing and parsing TSNE() function's output [3]. However: i) would be quite stupid to re-calculate KL divergence, when TSNE() has already computed it, ii) would be a bit unusual in terms of code.&#xD;&#xA;&#xD;&#xA;Do you have any other suggestion? Is there a standard way to get this information using this library?&#xD;&#xA;&#xD;&#xA;I mentioned I tried *R*'s tsne library, but I'd prefer the answers to focus on the *python* sklearn implementation.&#xD;&#xA;&#xD;&#xA;------------&#xD;&#xA;References&#xD;&#xA;&#xD;&#xA;[1] http://nbviewer.ipython.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb&#xD;&#xA;&#xD;&#xA;[2] http://homepage.tudelft.nl/19j49/t-SNE.html&#xD;&#xA;&#xD;&#xA;[3] http://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call" />
  <row Id="1977" PostHistoryTypeId="1" PostId="762" RevisionGUID="2412e6b0-2c0d-4625-9b7a-eb265718cd57" CreationDate="2014-07-17T10:04:29.797" UserId="131" Text="t-SNE Python/R implementations getting final divergence" />
  <row Id="1978" PostHistoryTypeId="3" PostId="762" RevisionGUID="2412e6b0-2c0d-4625-9b7a-eb265718cd57" CreationDate="2014-07-17T10:04:29.797" UserId="131" Text="&lt;machine-learning&gt;&lt;python&gt;" />
  <row Id="1979" PostHistoryTypeId="2" PostId="763" RevisionGUID="2d5339aa-cf02-4853-a15d-923726711ef7" CreationDate="2014-07-17T10:26:45.547" UserId="1329" Text="A simple and perhaps somewhat naive approach would be to assume that a person changes jobs at a constant rate and that previous job changes have no influence on future ones. Under these assumptions you could model the job changes as a Poisson process and estimate the rate parameter using MLE (http://en.wikipedia.org/wiki/Poisson_process and http://en.wikipedia.org/wiki/Poisson_distribution).&#xD;&#xA;&#xD;&#xA;Of course one should explore how well these assumptions hold in the data. To do this, you could study whether or not job changes are independent of one another by computing the correlation between events at various lags (http://en.wikipedia.org/wiki/Correlation). You could also plot the distribution of time between job change events. If the process is Poisson-like then you should observe little to no correlation between events at any number of lags and the distribution of time between job change events should be exponentially distributed (http://en.wikipedia.org/wiki/Exponential_distribution).&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1980" PostHistoryTypeId="5" PostId="759" RevisionGUID="626937c4-d005-44d1-84ac-8a6d97cdcbd1" CreationDate="2014-07-17T11:28:13.340" UserId="2452" Comment="Minor updates: added emphasis (italics) for some software project titles." Text="This topic of *reproducible research* (RR) is **very popular** today and, consequently, is **huge**, but I hope that my answer will be **comprehensive enough** as an answer and will provide enough information for **further research**, should you decide to do so.&#xD;&#xA;&#xD;&#xA;While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more **universal tools** (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let's take a look what tools are available per your list.&#xD;&#xA;&#xD;&#xA;1) **Tools for data version control**. Unless you plan to work with (very) *big data*, I guess, it would make sense to use the same `git`, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git.&#xD;&#xA;&#xD;&#xA;2) **Tools for managing RR workflows and experiments**. Here's a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):&#xD;&#xA;&#xD;&#xA;- *Taverna Workflow Management System* (http://www.taverna.org.uk) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal *myExperiment* (http://www.myexperiment.org), where user can store and share their RR workflows. Web-based RR portal, fully compatible with *Taverna* is called *Taverna Online*, but it is being developed and maintained by totally different organization in Russia (referred there to as *OnlineHPC*: http://onlinehpc.com).&#xD;&#xA;&#xD;&#xA;- *The Kepler Project* (https://kepler-project.org)&#xD;&#xA;&#xD;&#xA;- *VisTrails* (http://vistrails.org)&#xD;&#xA;&#xD;&#xA;- *Madagascar* (http://www.reproducibility.org)&#xD;&#xA;&#xD;&#xA;**EXAMPLE**. Here's an interesting article on scientific workflows with an example of the **real** workflow design and data analysis, based on using *Kepler* and *myExperiment* projects: http://f1000research.com/articles/3-110/v1.&#xD;&#xA;&#xD;&#xA;There are many RR tools that implement *literate programming* paradigm, exemplified by `LaTeX` software family. Tools that help in report generation and presentation is also a large category, where `Sweave` and `knitr` are probably the most well-known ones. `Sweave` is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (http://stackoverflow.com/questions/2161152/sweave-for-python). I think that `knitr` might be a better option, as it's modern, has extensive support by popular tools (such as `RStudio`) and is language-neutral (http://yihui.name/knitr/demo/engines).&#xD;&#xA;&#xD;&#xA;3) **Protocol and suggested directory structure**. If I understood correctly what you implied by using term *protocol* (*workflow*), generally I think that standard RR data analysis workflow consists of the following sequential phases: *data collection* =&gt; *data preparation* (cleaning, transformation, merging, sampling) =&gt; *data analysis* =&gt; *presentation of results* (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.&#xD;&#xA;&#xD;&#xA;For sample directory structure, you may take a look at documentation for R package `ProjectTemplate` (http://projecttemplate.net), as an attempt to automate data analysis workflows and projects:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;4) **Automated build/run tools**. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is `make`. Read the following article for some reasons to use `make` as the preferred RR workflow automation tool: http://bost.ocks.org/mike/make. Certainly, there are other **similar** tools, which either improve some aspects of `make`, or add some additional features. For example: `ant` (officially, Apache Ant: http://ant.apache.org), `Maven` (&quot;next generation `ant`&quot;: http://maven.apache.org), `rake` (https://github.com/ruby/rake), `Makepp` (http://makepp.sourceforge.net). For a comprehensive list of such tools, see Wikipedia: http://en.wikipedia.org/wiki/List_of_build_automation_software.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/0B2vo.png" />
  <row Id="1981" PostHistoryTypeId="5" PostId="759" RevisionGUID="48b149f5-3bcf-4963-b76b-57182cf8f7bd" CreationDate="2014-07-17T11:42:25.753" UserId="2452" Comment="Fixed typo." Text="The topic of *reproducible research* (RR) is **very popular** today and, consequently, is **huge**, but I hope that my answer will be **comprehensive enough** as an answer and will provide enough information for **further research**, should you decide to do so.&#xD;&#xA;&#xD;&#xA;While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more **universal tools** (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let's take a look what tools are available per your list.&#xD;&#xA;&#xD;&#xA;1) **Tools for data version control**. Unless you plan to work with (very) *big data*, I guess, it would make sense to use the same `git`, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git.&#xD;&#xA;&#xD;&#xA;2) **Tools for managing RR workflows and experiments**. Here's a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):&#xD;&#xA;&#xD;&#xA;- *Taverna Workflow Management System* (http://www.taverna.org.uk) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal *myExperiment* (http://www.myexperiment.org), where user can store and share their RR workflows. Web-based RR portal, fully compatible with *Taverna* is called *Taverna Online*, but it is being developed and maintained by totally different organization in Russia (referred there to as *OnlineHPC*: http://onlinehpc.com).&#xD;&#xA;&#xD;&#xA;- *The Kepler Project* (https://kepler-project.org)&#xD;&#xA;&#xD;&#xA;- *VisTrails* (http://vistrails.org)&#xD;&#xA;&#xD;&#xA;- *Madagascar* (http://www.reproducibility.org)&#xD;&#xA;&#xD;&#xA;**EXAMPLE**. Here's an interesting article on scientific workflows with an example of the **real** workflow design and data analysis, based on using *Kepler* and *myExperiment* projects: http://f1000research.com/articles/3-110/v1.&#xD;&#xA;&#xD;&#xA;There are many RR tools that implement *literate programming* paradigm, exemplified by `LaTeX` software family. Tools that help in report generation and presentation is also a large category, where `Sweave` and `knitr` are probably the most well-known ones. `Sweave` is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (http://stackoverflow.com/questions/2161152/sweave-for-python). I think that `knitr` might be a better option, as it's modern, has extensive support by popular tools (such as `RStudio`) and is language-neutral (http://yihui.name/knitr/demo/engines).&#xD;&#xA;&#xD;&#xA;3) **Protocol and suggested directory structure**. If I understood correctly what you implied by using term *protocol* (*workflow*), generally I think that standard RR data analysis workflow consists of the following sequential phases: *data collection* =&gt; *data preparation* (cleaning, transformation, merging, sampling) =&gt; *data analysis* =&gt; *presentation of results* (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.&#xD;&#xA;&#xD;&#xA;For sample directory structure, you may take a look at documentation for R package `ProjectTemplate` (http://projecttemplate.net), as an attempt to automate data analysis workflows and projects:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;4) **Automated build/run tools**. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is `make`. Read the following article for some reasons to use `make` as the preferred RR workflow automation tool: http://bost.ocks.org/mike/make. Certainly, there are other **similar** tools, which either improve some aspects of `make`, or add some additional features. For example: `ant` (officially, Apache Ant: http://ant.apache.org), `Maven` (&quot;next generation `ant`&quot;: http://maven.apache.org), `rake` (https://github.com/ruby/rake), `Makepp` (http://makepp.sourceforge.net). For a comprehensive list of such tools, see Wikipedia: http://en.wikipedia.org/wiki/List_of_build_automation_software.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/0B2vo.png" />
  <row Id="1982" PostHistoryTypeId="4" PostId="762" RevisionGUID="bf161695-5ab1-4334-8169-c3f64220b70a" CreationDate="2014-07-17T12:20:20.850" UserId="131" Comment="edited title" Text="t-SNE Python implementation: Kullback-Leibler divergence" />
  <row Id="1983" PostHistoryTypeId="2" PostId="764" RevisionGUID="5bbff1e7-9e39-453f-a5dd-5a54f13fe870" CreationDate="2014-07-17T12:34:11.397" UserId="802" Text="K-means should be right in this case. Since k-means tries to group based solely on euclidean distance between objects you will get back clusters of locations that are close to each other. &#xD;&#xA;&#xD;&#xA;To find the optimal number of clusters you can try making an 'elbow' plot of the within group sum of square distance. This may be helpful (http://nbviewer.ipython.org/github/nborwankar/LearnDataScience/blob/master/notebooks/D3.%20K-Means%20Clustering%20Analysis.ipynb)" />
  <row Id="1984" PostHistoryTypeId="2" PostId="765" RevisionGUID="0e9c3d8d-81c5-4178-a688-c7770b7ee5d8" CreationDate="2014-07-17T13:19:46.817" UserId="2543" Text="I've seen the same thing: My target variables are non-negative, but sklearn's GBR sometimes makes negative predictions.  To grossly oversimplify, GBR is just using averages of subsets of the target variable to make predictions, so if the target variables are non-negative, then the predictions should be non-negative. &#xD;&#xA;&#xD;&#xA;Also, I can provide an example, if anyone's interested.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;I'd make this a comment, but I don't have enough reputation.&#xD;&#xA;" />
  <row Id="1985" PostHistoryTypeId="2" PostId="766" RevisionGUID="152aca45-5368-44c0-bf89-c6b9beb063ad" CreationDate="2014-07-17T14:07:12.643" UserId="159" Text="The TSNE source in scikit-learn is in pure Python. Fit `fit_transform()` method is actually calling a private `_fit()` function which then calls a private `_tsne()` function. That `_tsne()` function has a local variable `error` which is printed out at the end of the fit. Seems like you could pretty easily change one or two lines of source code to have that value returned to `fit_transform()`." />
  <row Id="1986" PostHistoryTypeId="2" PostId="767" RevisionGUID="a61030fd-4b33-49f9-9653-8207b19035d3" CreationDate="2014-07-17T14:28:38.207" UserId="2544" Text="A great reproducibility tool for Python is of course [IPython Notebook][1] (don't forget the [%logon and %logstart][2] magics to keep a history of all your experiments).&#xD;&#xA;&#xD;&#xA;Another more general tool working with any language (with a Python API on [pypi][3]) is [Sumatra][4], which is specifically designed to produce **replicable** research (which aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://ipython.org/&#xD;&#xA;  [2]: https://damontallen.github.io/IPython-quick-ref-sheets/&#xD;&#xA;  [3]: https://pypi.python.org/pypi/Sumatra&#xD;&#xA;  [4]: http://neuralensemble.org/sumatra/" />
  <row Id="1987" PostHistoryTypeId="5" PostId="767" RevisionGUID="e1670543-a442-4974-93c2-357075788172" CreationDate="2014-07-17T14:35:57.760" UserId="2544" Comment="Added more infos about Sumatra" Text="A great reproducibility tool for Python with a low learning curve is of course [**IPython Notebook**][1] (don't forget the [%logon and %logstart][2] magics to keep a history of all your experiments).&#xD;&#xA;&#xD;&#xA;Another more general tool working with any language (with a Python API on [pypi][3]) is [**Sumatra**][4], which is specifically designed to produce **replicable** research (which aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium). Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a &quot;save game state&quot; often found in videogames. More precisely, it will will save:&#xD;&#xA;&#xD;&#xA;- all the parameters you provided;&#xD;&#xA;- the exact sourcecode state of your whole experimental application and config files;&#xD;&#xA;- the output/plots/results and also any file produced by your experimental application.&#xD;&#xA;&#xD;&#xA;It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don't want to save everything everytime).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://ipython.org/&#xD;&#xA;  [2]: https://damontallen.github.io/IPython-quick-ref-sheets/&#xD;&#xA;  [3]: https://pypi.python.org/pypi/Sumatra&#xD;&#xA;  [4]: http://neuralensemble.org/sumatra/" />
  <row Id="1988" PostHistoryTypeId="2" PostId="768" RevisionGUID="8f174ddc-0ef1-437a-91b7-76e8f046aa81" CreationDate="2014-07-17T15:19:32.237" UserId="2556" Text="Since I started doing research in academia I was constantly looking for a satisfactory workflow.&#xD;&#xA;I think that I finally found something I am happy with:&#xD;&#xA;&#xD;&#xA;1) Put everything under version control, e.g., Git:&#xD;&#xA;&#xD;&#xA;For hobby research projects I use GitHub, for research at work I use the private GitLab server that is provided by our university. I also keep my datasets there.&#xD;&#xA;&#xD;&#xA;2) I do most of my analyses along with the documentation on IPython notebooks. It is very organized (for me) to have the code, the plots, and the discussion/conclusion all in one document&#xD;&#xA;If I am running larger scripts, I would usually put them into separate script .py files, but I would still execute them from the IPython notebook via the %run magic to add information about the purpose, outcome, and other parameters.&#xD;&#xA;&#xD;&#xA;I have written a small cell-magic extension for IPython and IPython notebooks, called &quot;watermark&quot; that I use to conveniently create time stamps and keep track of the different package versions I used and also Git hashs&#xD;&#xA;&#xD;&#xA;For example&#xD;&#xA;&#xD;&#xA;&lt;br&gt;&#xD;&#xA;&#xD;&#xA;    %watermark&#xD;&#xA;&#xD;&#xA;    29/06/2014 01:19:10&#xD;&#xA;    &#xD;&#xA;    CPython 3.4.1&#xD;&#xA;    IPython 2.1.0&#xD;&#xA;    &#xD;&#xA;    compiler   : GCC 4.2.1 (Apple Inc. build 5577)&#xD;&#xA;    system     : Darwin&#xD;&#xA;    release    : 13.2.0&#xD;&#xA;    machine    : x86_64&#xD;&#xA;    processor  : i386&#xD;&#xA;    CPU cores  : 2&#xD;&#xA;    interpreter: 64bit&#xD;&#xA;&lt;br&gt;&#xD;&#xA;&#xD;&#xA;    %watermark -d -t&#xD;&#xA;&#xD;&#xA;    29/06/2014 01:19:11 &#xD;&#xA;&#xD;&#xA;&lt;br&gt;&#xD;&#xA;&#xD;&#xA;    %watermark -v -m -p numpy,scipy&#xD;&#xA;&#xD;&#xA;    CPython 3.4.1&#xD;&#xA;    IPython 2.1.0&#xD;&#xA;    &#xD;&#xA;    numpy 1.8.1&#xD;&#xA;    scipy 0.14.0&#xD;&#xA;    &#xD;&#xA;    compiler   : GCC 4.2.1 (Apple Inc. build 5577)&#xD;&#xA;    system     : Darwin&#xD;&#xA;    release    : 13.2.0&#xD;&#xA;    machine    : x86_64&#xD;&#xA;    processor  : i386&#xD;&#xA;    CPU cores  : 2&#xD;&#xA;    interpreter: 64bit&#xD;&#xA;&#xD;&#xA;For more info, see the [documentation here](http://nbviewer.ipython.org/github/rasbt/python_reference/blob/master/ipython_magic/watermark.ipynb).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="1989" PostHistoryTypeId="2" PostId="769" RevisionGUID="59b43a3b-dff1-41ee-bbda-ee3f23dbe722" CreationDate="2014-07-17T15:28:25.940" UserId="2556" Text="**Dimensionality Reduction**&#xD;&#xA;&#xD;&#xA;Another important procedure is to compare the error rates on training and test dataset to see if you are overfitting (due to the &quot;curse of dimensionality&quot;). E.g., if your error rate on the test dataset is much larger than the error on the training data set, this would be one indicator.   &#xD;&#xA;In this case, you could try dimensionality reduction techniques, such as PCA or LDA.&#xD;&#xA;&#xD;&#xA;If you are interested, I have written about PCA, LDA and some other techniques here: http://sebastianraschka.com/index.html#machine_learning and in my GitHub repo here: https://github.com/rasbt/pattern_classification&#xD;&#xA;&#xD;&#xA;**Cross validation**&#xD;&#xA;&#xD;&#xA;Also you may want to take a look at cross-validation techniques in order to evaluate the performance of your classifiers in a more objective manner" />
  <row Id="1990" PostHistoryTypeId="2" PostId="770" RevisionGUID="306f4428-c77d-4e1c-abac-dd6fce2e2988" CreationDate="2014-07-17T15:35:17.487" UserId="2556" Text="I am not an export in using SVMs, but usually (if you are using a machine learning library like Python's `scikit-learn` or R's `libsvm`, there is the `class_weight` parameter, or `class.weights`, respectively. &#xD;&#xA;&#xD;&#xA;Or if you'd use a Bayes classifier, you would take this &quot;skew&quot; into account via the &quot;prior (class) probabilities&quot; P(&amp;omega;&lt;sub&gt;j&lt;/sub&gt;)" />
  <row Id="1992" PostHistoryTypeId="2" PostId="771" RevisionGUID="7d554780-209e-48e4-847c-8f20378551d3" CreationDate="2014-07-17T18:25:44.683" UserId="2556" Text="From what I heard, Pylearn2 might be currently the library of choice for most people. This reminds me of a recent blog post a few month ago that lists all the different machine learning libraries with a short explanation&#xD;&#xA;&#xD;&#xA;https://www.cbinsights.com/blog/python-tools-machine-learning&#xD;&#xA;&#xD;&#xA;The section you might be interested in here would be &quot;Deep Learning&quot;. About Pylearn2, he writes&#xD;&#xA;&#xD;&#xA;&gt; PyLearn2&#xD;&#xA;&gt; &#xD;&#xA;&gt; There is another library built on top of Theano, called PyLearn2 which&#xD;&#xA;&gt; brings modularity and configurability to Theano where you could create&#xD;&#xA;&gt; your neural network through different configuration files so that it&#xD;&#xA;&gt; would be easier to experiment different parameters. Arguably, it&#xD;&#xA;&gt; provides more modularity by separating the parameters and properties&#xD;&#xA;&gt; of neural network to the configuration file." />
  <row Id="1995" PostHistoryTypeId="4" PostId="748" RevisionGUID="34bb614a-4a9c-4314-9dc5-20eb6a093fbc" CreationDate="2014-07-17T19:58:22.467" UserId="1333" Comment="spelling error in title" Text="Why use mean revenue in a split test?" />
  <row Id="1996" PostHistoryTypeId="24" PostId="748" RevisionGUID="34bb614a-4a9c-4314-9dc5-20eb6a093fbc" CreationDate="2014-07-17T19:58:22.467" Comment="Proposed by 1333 approved by 434, 2511 edit id of 116" />
  <row Id="1997" PostHistoryTypeId="2" PostId="773" RevisionGUID="c228f7c0-338f-4c3e-b625-2d35f500bea0" CreationDate="2014-07-17T20:02:11.227" UserId="154" Text="Think geometrically. Cosine similarity only cares about angle difference, while dot product cares about angle and magnitude. If you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. Note that neither of them is a &quot;distance metric&quot;." />
  <row Id="1999" PostHistoryTypeId="2" PostId="774" RevisionGUID="c396573e-97ba-47cf-82d6-46663fdf3c07" CreationDate="2014-07-18T04:50:58.287" UserId="2452" Text="The following **general** answer is my uneducated guess, so take it with grain of salt. Hopefully, it makes sense. I think that the best way to describe or analyze *experiments* (as any other *systems*, in general) is to build their **statistical (multivariate) models** and evaluate them. Depending on whether *environments* for your set of experiments are represented by the **same model or different**, I see the following **approaches**:&#xD;&#xA;&#xD;&#xA;1) **Single model approach.** *Define* experiments' statistical model for all environments (dependent and independent variables, data types, assumptions, constraints). *Analyze* it (most likely, using *regression analysis*). *Compare* results **across variables**, which determine (influence) different environments.&#xD;&#xA;&#xD;&#xA;2) **Multiple models approach.** The same steps as previous case, but *compare* results **across models**, corresponding to different environments." />
  <row Id="2000" PostHistoryTypeId="2" PostId="775" RevisionGUID="08922425-28ee-4535-887a-0c6ea6e098cd" CreationDate="2014-07-18T07:43:56.823" UserId="2575" Text="Be sure to check out [docker][1]! And in general, all the other good things that software engineering has created along decades for ensuring isolation and reproductibility. &#xD;&#xA;&#xD;&#xA;I would like to stress that it is not enough to have *just* reproducible workflows, but also *easy* to reproduce workflows. Let me show what I mean. Suppose that your project uses Python, a database X and Scipy. Most surely you will be using a specific library to connect to your database from Python, and Scipy will be in turn using some sparse algebraic routines. This is by all means a very simple setup, but not entirely simple to setup, pun intended. If somebody wants to execute your scripts, she will have to install all the dependencies. Or worse, she might have incompatible versions of it already installed. Fixing those things takes time. It will also take time to you if you at some moment need to move your computations to a cluster, to a different cluster, or to some cloud servers. &#xD;&#xA;&#xD;&#xA;Here is where I find docker useful. Docker is a way to formalize and compile recipes for binary environments. You can write the following in a dockerfile (I'm using here plain English instead of the Dockerfile syntax):&#xD;&#xA;&#xD;&#xA;* Start with a basic binary environment, like Ubuntu's&#xD;&#xA;* Install libsparse-dev&#xD;&#xA;* (Pip) Install numpy and scipy&#xD;&#xA;* Install X&#xD;&#xA;* Install libX-dev&#xD;&#xA;* (Pip) Install python-X&#xD;&#xA;* Install IPython-Notebook&#xD;&#xA;* Copy my python scripts/notebooks to my binary environment, these datafiles, and these configurations to do other miscellaneous things. To ensure reproductibility, copy them from a named url instead of a local file. &#xD;&#xA;* Maybe run IPython-Notebook.&#xD;&#xA;&#xD;&#xA;Some of the lines will be installing things in Python using pip, since pip can do a very clean work in selecting specific package versions. Check it out too!&#xD;&#xA;&#xD;&#xA;And that's it. If after you create your Dockerfile it can be built, then it can be built anywhere, by anybody (provided they also have access to your project-specific files, e.g. because you put them in a public url referenced from the Dockerfile). What is best, you can upload the resulting environment (called an &quot;image&quot;) to a public or private server (called a &quot;register&quot;) for other people to use. So, when you publish your workflow, you have both a fully reproducible recipe in the form of a Dockerfile, and an easy way for you or other people to reproduce what you do:&#xD;&#xA;&#xD;&#xA;    docker run dockerregistery.thewheezylab.org/nowyouwillbelieveme&#xD;&#xA;&#xD;&#xA;Or if they want to poke around in your scripts and so forth:&#xD;&#xA;&#xD;&#xA;    docker run -i -t dockerregistery.thewheezylab.org/nowyouwillbelieveme /bin/bash&#xD;&#xA;&#xD;&#xA;  [1]: https://www.docker.com/" />
  <row Id="2001" PostHistoryTypeId="5" PostId="767" RevisionGUID="bc139639-9bcc-4f2c-91b1-0f79c5b2c592" CreationDate="2014-07-18T09:47:43.920" UserId="2544" Comment="Added talk about setup replicability" Text="A great reproducibility tool for Python with a low learning curve is of course [**IPython Notebook**][1] (don't forget the [%logon and %logstart][2] magics and/or [Git][3] to keep a history of all your experiments).&#xD;&#xA;&#xD;&#xA;Another more general tool working with any language (with a Python API on [pypi][4]) is [**Sumatra**][5], which is specifically designed to produce **replicable** research (which aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium). Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a &quot;save game state&quot; often found in videogames. More precisely, it will will save:&#xD;&#xA;&#xD;&#xA;- all the parameters you provided;&#xD;&#xA;- the exact sourcecode state of your whole experimental application and config files;&#xD;&#xA;- the output/plots/results and also any file produced by your experimental application.&#xD;&#xA;&#xD;&#xA;It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don't want to save everything everytime).&#xD;&#xA;&#xD;&#xA;/EDIT: [dsign][6] touched a very important point here: the replicability of your setup is as important as the replicability of your application. In other words, you should at least provide a **full list of the libraries and compilers** you used along with their exact **versions** and the details of your **platform**.&#xD;&#xA;&#xD;&#xA;Personally, in scientific computing with Python, I have found that packaging an application along with the libraries is just too painful, thus I now just use an all-in-one scientific python package such as [Anaconda][7] (with the great package manager [conda][8]), and just advise users to use the same package. Another solution could be to provide a script to automatically generate a [virtualenv][9], or to package everything using the [Docker application as cited by dsign][10].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://ipython.org/&#xD;&#xA;  [2]: https://damontallen.github.io/IPython-quick-ref-sheets/&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/Git_(software)&#xD;&#xA;  [4]: https://pypi.python.org/pypi/Sumatra&#xD;&#xA;  [5]: http://neuralensemble.org/sumatra/&#xD;&#xA;  [6]: http://datascience.stackexchange.com/a/775&#xD;&#xA;  [7]: https://store.continuum.io/cshop/anaconda/&#xD;&#xA;  [8]: http://www.continuum.io/blog/conda&#xD;&#xA;  [9]: http://docs.python-guide.org/en/latest/dev/virtualenvs/&#xD;&#xA;  [10]: http://datascience.stackexchange.com/a/775" />
  <row Id="2002" PostHistoryTypeId="2" PostId="776" RevisionGUID="ee936e7a-b365-4844-9baa-0ed5beb486b0" CreationDate="2014-07-18T15:24:38.007" UserId="2583" Text="I am far from an expert, but my understanding of the subject tells me that R (superb in statistics) and e.g. Python (superb in several of those things where R is lacking) complements each other quite well (as pointed out by previous posts). " />
  <row Id="2003" PostHistoryTypeId="6" PostId="739" RevisionGUID="00f60075-0136-4752-a7e8-560b95d85763" CreationDate="2014-07-18T19:29:05.073" UserId="553" Comment="adding a better tag for career-related questions" Text="&lt;education&gt;&lt;definitions&gt;&lt;career&gt;" />
  <row Id="2004" PostHistoryTypeId="24" PostId="739" RevisionGUID="00f60075-0136-4752-a7e8-560b95d85763" CreationDate="2014-07-18T19:29:05.073" Comment="Proposed by 553 approved by 434, 2489 edit id of 115" />
  <row Id="2006" PostHistoryTypeId="2" PostId="777" RevisionGUID="5bf76b73-30f6-4b4f-a8b8-32dc30cfc663" CreationDate="2014-07-18T22:29:05.017" UserId="2599" Text="New to the Data Science forum, and first poster here!&#xD;&#xA;&#xD;&#xA;This may be kind of a specific question (hopefully not too much so), but one I'd imagine others might be interested in.&#xD;&#xA;&#xD;&#xA;I'm looking for a way to basically query GitHub with something like this:&#xD;&#xA;&#xD;&#xA;    Give me a collection of all of the public repositories that have more than 10 stars, at&#xD;&#xA;    least two forks, and more than three committers.&#xD;&#xA;&#xD;&#xA;The result could take any viable form: a JSON data dump, a URL to the web page, etc. It more than likely will consist of information from 10,000 repos or something large.&#xD;&#xA;&#xD;&#xA;Is this sort of thing possible using the API or some other pre-built way, or am I going to have to build out my own custom solution where I try to scrape every page? If so, how feasible is this and how might I approach it?" />
  <row Id="2007" PostHistoryTypeId="1" PostId="777" RevisionGUID="5bf76b73-30f6-4b4f-a8b8-32dc30cfc663" CreationDate="2014-07-18T22:29:05.017" UserId="2599" Text="Getting GitHub repository information by different criteria" />
  <row Id="2008" PostHistoryTypeId="3" PostId="777" RevisionGUID="5bf76b73-30f6-4b4f-a8b8-32dc30cfc663" CreationDate="2014-07-18T22:29:05.017" UserId="2599" Text="&lt;bigdata&gt;&lt;data-mining&gt;&lt;python&gt;&lt;dataset&gt;" />
  <row Id="2009" PostHistoryTypeId="2" PostId="778" RevisionGUID="f8a40563-efcb-4bbd-a51e-8cb7d4c4b289" CreationDate="2014-07-18T22:34:48.080" UserId="890" Text="I read in this post https://datascience.stackexchange.com/questions/41/is-the-r-language-suitable-for-big-data that big data constitutes `5TB`, and while it does a good job of providing information about the feasibility of working with this type of data in `R` it provides very little information about `Python`. I was wondering if `Python` can work with this much data as well. " />
  <row Id="2010" PostHistoryTypeId="1" PostId="778" RevisionGUID="f8a40563-efcb-4bbd-a51e-8cb7d4c4b289" CreationDate="2014-07-18T22:34:48.080" UserId="890" Text="Is Python suitable for big data" />
  <row Id="2011" PostHistoryTypeId="3" PostId="778" RevisionGUID="f8a40563-efcb-4bbd-a51e-8cb7d4c4b289" CreationDate="2014-07-18T22:34:48.080" UserId="890" Text="&lt;bigdata&gt;&lt;python&gt;" />
  <row Id="2012" PostHistoryTypeId="2" PostId="779" RevisionGUID="c5c6e364-6605-419b-aa8c-d730e69e7907" CreationDate="2014-07-18T22:59:39.100" UserId="2600" Text="I believe the language itself has little to do with performance capabilities, when it comes to large data. What matters is:&#xD;&#xA;&#xD;&#xA; - How large is the data actually&#xD;&#xA; - What processing are you going to perform on it&#xD;&#xA; - What hardware are you going to use&#xD;&#xA; - Which are the specific libraries that you plan to use&#xD;&#xA;&#xD;&#xA;Anyway, Python is well adopted in data science communities." />
  <row Id="2013" PostHistoryTypeId="2" PostId="780" RevisionGUID="5460b8c4-59b8-42bf-bc47-d0bce4022d84" CreationDate="2014-07-19T02:19:46.530" UserId="2452" Text="Some good answers here. I would like to join the discussion by adding the following two **notes**:&#xD;&#xA;&#xD;&#xA;1) The question's emphasis on the **volume of data** while referring to *Big Data* is certainly understandable and valid, especially considering the **problem** of data volume growth **outpacing** technological capacities' exponential growth per *Moore's Law* (http://en.wikipedia.org/wiki/Moore%27s_law).&#xD;&#xA;&#xD;&#xA;2) Having said that, it is important to remember about other aspects of big data concept, based on *Gartner*'s definition (emphasis mine - AB): &quot;*Big data* is high **volume**, high **velocity**, and/or high **variety** information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization.&quot; (usually referred to as the &quot;**3Vs model**&quot;). I mention this, because it forces data scientists and other analysts to look for and use R packages that focus on **other than volume** aspects of big data (enabled by the **richness** of enormous *R ecosystem*).&#xD;&#xA;&#xD;&#xA;2) While existing answers mention some R packages, related to big data, for a more **comprehensive coverage**, I'd recommend to refer to *CRAN Task View* **&quot;High-Performance and Parallel Computing with R&quot;** (http://cran.r-project.org/web/views/HighPerformanceComputing.html), in particular, sections **&quot;Parallel computing: Hadoop&quot;** and **&quot;Large memory and out-of-memory data&quot;**." />
  <row Id="2014" PostHistoryTypeId="2" PostId="781" RevisionGUID="45c1894e-8578-4eec-807e-6e7f699941c5" CreationDate="2014-07-19T03:29:02.647" UserId="548" Text="To clarify, I feel like the original question references by OP probably isn't be best for a SO-type format, but I will certainly represent `python` in this particular case.&#xD;&#xA;&#xD;&#xA;Let me just start by saying that regardless of your data size, `python` shouldn't be your limiting factor. In fact, there are just a couple main issues that you're going to run into dealing with large datasets:&#xD;&#xA;&#xD;&#xA; - **Reading data into memory** - This is by far the most common issue faced in the world of big data. Basically, you can't read in more data than you have memory (RAM) for. The best way to fix this is by making atomic operations on your data instead of trying to read everything in at once.&#xD;&#xA; - **Storing data** - This is actually just another form of the earlier issue, by the time to get up to about `1TB`, you start having to look elsewhere for storage. AWS S3 is the most common resource, and `python` has the fantastic `boto` library to facilitate leading with large pieces of data.&#xD;&#xA; - **Network latency** - Moving data around between different services is going to be your bottleneck. There's not a huge amount you can do to fix this, other than trying to pick co-located resources and plugging into the wall." />
  <row Id="2015" PostHistoryTypeId="2" PostId="782" RevisionGUID="f2d1fadf-da9e-4e97-af6c-eba59342b605" CreationDate="2014-07-19T03:42:34.433" UserId="2452" Text="My limited understanding, based on brief browsing *GitHub API* documentation, is that currently there is NO **single API request** that supports **all** your listed criteria **at once**. However, I think that you could use the following sequence in order to achieve the goal from your example (at least, I would use this approach):&#xD;&#xA;&#xD;&#xA;1) **Request** information on all public repositories (API returns *summary representations* only): https://developer.github.com/v3/repos/#list-all-public-repositories;&#xD;&#xA;&#xD;&#xA;2) **Loop** through the list of all public repositories retrieved in step 1, requesting individual resources, and save it as new (detailed) list (this returns *detailed representations*, in other words, all attributes): https://developer.github.com/v3/repos/#get;&#xD;&#xA;&#xD;&#xA;3) **Loop** through the detailed list of all repositories, filtering corresponding fields by your criteria. For your example request, you'd be interested in the following attributes of the **parent** object: *stargazers_count*, *forks_count*. In order to filter the repositories by number of committers, you could use a separate API: https://developer.github.com/v3/repos/#list-contributors.&#xD;&#xA;&#xD;&#xA;Updates or comments from people more familiar with GitHub API are welcome!" />
  <row Id="2016" PostHistoryTypeId="2" PostId="783" RevisionGUID="c8ff1782-7ec8-46ba-bcce-3b7a57142e12" CreationDate="2014-07-19T05:27:22.773" UserId="2604" Text="I want to plot the bytes from a disk image in order to understand a pattern in them. This is mainly an academic task, since I'm almost sure this pattern was created by a disk testing program, but I'd like to reverse-engineer it anyway.&#xD;&#xA;&#xD;&#xA;I already know that the pattern is aligned, with a periodicity of 256 characters.&#xD;&#xA;&#xD;&#xA;I can envision two ways of visualizing this information: either a 16x16 plane viewed through time (3 dimensions), where each pixel's color is the ASCII code for the character, or a 256 pixel line for each period (2 dimensions).&#xD;&#xA;&#xD;&#xA;This is a snapshot of the pattern (you can see more than one), seen through `xxd` (32x16):&#xD;&#xA;&#xD;&#xA;![Pattern to analyze][1]&#xD;&#xA;&#xD;&#xA;Either way, I am trying to find a way of visualizing this information. This probably isn't hard for anyone into signal analysis, but I can't seem to find a way using open-source software.&#xD;&#xA;&#xD;&#xA;I'd like to avoid Matlab or Mathematica and I'd prefer an answer in R, since I have been learning it recently, but nonetheless, any language is welcome.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/zOFSK.gif" />
  <row Id="2017" PostHistoryTypeId="1" PostId="783" RevisionGUID="c8ff1782-7ec8-46ba-bcce-3b7a57142e12" CreationDate="2014-07-19T05:27:22.773" UserId="2604" Text="Data visualization for pattern analysis (language-independent, but R preferred)" />
  <row Id="2018" PostHistoryTypeId="3" PostId="783" RevisionGUID="c8ff1782-7ec8-46ba-bcce-3b7a57142e12" CreationDate="2014-07-19T05:27:22.773" UserId="2604" Text="&lt;r&gt;&lt;visualization&gt;" />
  <row Id="2019" PostHistoryTypeId="2" PostId="784" RevisionGUID="ad29df50-b321-463c-8345-722cd8de5ffd" CreationDate="2014-07-19T18:31:36.573" UserId="989" Text="I have a timeseries with hourly gas consumption. I want to use ARMA/ARIMA to forecast the consumption on the next hour, basing on the previous. Why should I analyze/find the seasonality (with STL?)?&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/hYyH8.png" />
  <row Id="2020" PostHistoryTypeId="1" PostId="784" RevisionGUID="ad29df50-b321-463c-8345-722cd8de5ffd" CreationDate="2014-07-19T18:31:36.573" UserId="989" Text="Why should I care about seasonal data when I forecast?" />
  <row Id="2021" PostHistoryTypeId="3" PostId="784" RevisionGUID="ad29df50-b321-463c-8345-722cd8de5ffd" CreationDate="2014-07-19T18:31:36.573" UserId="989" Text="&lt;machine-learning&gt;" />
  <row Id="2022" PostHistoryTypeId="2" PostId="785" RevisionGUID="ea8ddfa8-24f5-4b05-bb56-66d7892f03df" CreationDate="2014-07-19T18:47:38.787" UserId="941" Text="I know almost nothing about signal analysis, but 2-dimensional visualization could be easily done using R. Particularly you will need `reshape2` and `ggplot2` packages. Assuming your data is **wide** (e.g. [n X 256] size), first you need to transform it to [**long**][1] format using `melt()` function from `reshape2` package. Then use [`geom_tile`][2] geometry from `ggplot2`. Here is a nice [recipe][3] with [gist][4].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/&#xD;&#xA;  [2]: http://docs.ggplot2.org/current/geom_tile.html&#xD;&#xA;  [3]: http://www.r-bloggers.com/simplest-possible-heatmap-with-ggplot2/&#xD;&#xA;  [4]: https://gist.github.com/dsparks/3710171" />
  <row Id="2023" PostHistoryTypeId="2" PostId="786" RevisionGUID="e62e1112-9944-4e97-be28-29507c9f2c92" CreationDate="2014-07-19T20:46:52.740" UserId="2567" Text="I am trying to find stock data to practice with, is there a good resource for this? I found this: ftp://emi.nasdaq.com/ITCH/ but it only has the current year.&#xD;&#xA;&#xD;&#xA;I already have a way of parsing the protocol, but would like to have some more data to compare with. It doesn't have to be in the same format, as long as it has price, trades, and date statistics." />
  <row Id="2024" PostHistoryTypeId="1" PostId="786" RevisionGUID="e62e1112-9944-4e97-be28-29507c9f2c92" CreationDate="2014-07-19T20:46:52.740" UserId="2567" Text="NASDAQ Trade Data" />
  <row Id="2025" PostHistoryTypeId="3" PostId="786" RevisionGUID="e62e1112-9944-4e97-be28-29507c9f2c92" CreationDate="2014-07-19T20:46:52.740" UserId="2567" Text="&lt;data-mining&gt;&lt;dataset&gt;" />
  <row Id="2026" PostHistoryTypeId="2" PostId="787" RevisionGUID="eb091561-9142-42a6-915a-94d1c83bdf8d" CreationDate="2014-07-19T20:51:58.527" UserId="2614" Text="I am a relatively new user to Hadoop (using version 2.4.1). I installed hadoop on my first node without a hitch, but I can't seem to get the Resource Manager to start on my second node. &#xD;&#xA;&#xD;&#xA;I cleared up some &quot;shared library&quot; problems by adding this to yarn-env.sh and hadoop-env.sh:&#xD;&#xA;&#xD;&#xA;&gt;  export HADOOP_HOME=&quot;/usr/local/hadoop&quot;&#xD;&#xA;&gt; &#xD;&#xA;export HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib&quot;&#xD;&#xA;&#xD;&#xA;I also added this to hadoop-env.sh:&#xD;&#xA;&#xD;&#xA;&gt; export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native&#xD;&#xA;&#xD;&#xA;based on the advice of this post at horton works http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/ &#xD;&#xA;&#xD;&#xA;That cleared up all of my error messages; when I run /sbin/start-yarn.sh I get this:&#xD;&#xA;&gt; &#xD;&#xA;starting yarn daemons&#xD;&#xA;&gt; &#xD;&#xA;starting resourcemanager, &#xD;&#xA;&gt; logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-HdNode.out&#xD;&#xA;&gt; &#xD;&#xA;localhost: starting nodemanager, &#xD;&#xA;&gt; logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-HdNode.out&#xD;&#xA;&#xD;&#xA;The only problem is, JPS says that the Resource Manager isn't running. &#xD;&#xA;&#xD;&#xA;What's going on here?" />
  <row Id="2027" PostHistoryTypeId="1" PostId="787" RevisionGUID="eb091561-9142-42a6-915a-94d1c83bdf8d" CreationDate="2014-07-19T20:51:58.527" UserId="2614" Text="Hadoop Resource Manager Won't Start" />
  <row Id="2028" PostHistoryTypeId="3" PostId="787" RevisionGUID="eb091561-9142-42a6-915a-94d1c83bdf8d" CreationDate="2014-07-19T20:51:58.527" UserId="2614" Text="&lt;hadoop&gt;" />
  <row Id="2032" PostHistoryTypeId="5" PostId="784" RevisionGUID="7d2c939c-151f-4c1a-a1fb-9a5361070727" CreationDate="2014-07-20T19:34:08.117" UserId="84" Comment="Improving question." Text="I have a timeseries with hourly gas consumption. I want to use [ARMA](http://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model)/[ARIMA](http://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) to forecast the consumption on the next hour, basing on the previous. Why should I analyze/find the seasonality (with [Seasonal and Trend decomposition using Loess](https://www.otexts.org/fpp/6/5) (STL)?)?&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/hYyH8.png" />
  <row Id="2033" PostHistoryTypeId="6" PostId="784" RevisionGUID="7d2c939c-151f-4c1a-a1fb-9a5361070727" CreationDate="2014-07-20T19:34:08.117" UserId="84" Comment="Improving question." Text="&lt;machine-learning&gt;&lt;time-series&gt;" />
  <row Id="2034" PostHistoryTypeId="2" PostId="788" RevisionGUID="1f1f84d6-de5d-4cf9-8534-60aa2ad860fa" CreationDate="2014-07-20T21:37:27.857" UserId="434" Text="Check your version of JPS and make sure it's the same as the version of java that you are running.  Sometimes you start out with an out of the box jdk install, upgrade, set alternatives for the java bin, but still have the original jps binary being referenced.&#xD;&#xA;&#xD;&#xA;Run `ps -ef |grep java` and look for the resource manager threads.  Maybe it's actually running.  If it is, try `update-alternatives --config jps` to see what binary jps is pointing at.&#xD;&#xA;&#xD;&#xA;If the resource manager is not running, it's time to do some basic linux troubleshooting.  Check log files and barring that check actual command output.&#xD;&#xA;&#xD;&#xA;On the system I'm looking at now, the log files for resource manager are placed in the `hadoop-install/logs` directory in `yarn-username-resourcemanager-hostname.log` and `yarn-user-resourcemanager-hostname.out`.  Your configuration may place them in /var/logs or what have you.  Also, have a look at the syslog.&#xD;&#xA;&#xD;&#xA;If the logs don't yield any good information, which can happen, my process is to generally try to figure out the command line from the startup script (usually by prefixing the command line with `echo`), and then trying to run the command directly to see the output as it comes out.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;I have actually run into this problem before, but I can't remember the specific issue.  I'm sure the same result can manifest itself from a variety of problems.  Considering that you are as far as you are in the process of getting set up, I believe it's likely to be a minor configuration issue." />
  <row Id="2035" PostHistoryTypeId="5" PostId="788" RevisionGUID="a315ce39-e5c5-4b0d-b8dc-a09791e46432" CreationDate="2014-07-20T21:48:25.257" UserId="434" Comment="added 55 characters in body" Text="Check your version of JPS and make sure it's the same as the version of java that you are running.  Sometimes you start out with an out of the box jdk install, upgrade, set alternatives for the java bin, but still have the original jps binary being referenced.&#xD;&#xA;&#xD;&#xA;Run `ps -ef |grep java` and look for the resource manager threads.  Maybe it's actually running.  If it is, try `update-alternatives --config jps` to see what binary jps is pointing at and compare it with the java binary that you are using.&#xD;&#xA;&#xD;&#xA;If the resource manager is not running, it's time to do some basic linux troubleshooting.  Check log files and barring that check actual command output.&#xD;&#xA;&#xD;&#xA;On the system I'm looking at now, the log files for resource manager are placed in the `hadoop-install/logs` directory in `yarn-username-resourcemanager-hostname.log` and `yarn-user-resourcemanager-hostname.out`.  Your configuration may place them in /var/logs or what have you.  Also, have a look at the syslog.&#xD;&#xA;&#xD;&#xA;If the logs don't yield any good information, which can happen, my process is to generally try to figure out the command line from the startup script (usually by prefixing the command line with `echo`), and then trying to run the command directly to see the output as it comes out.&#xD;&#xA;&#xD;&#xA;---&#xD;&#xA;I have actually run into this problem before, but I can't remember the specific issue.  I'm sure the same result can manifest itself from a variety of problems.  Considering that you are as far as you are in the process of getting set up, I believe it's likely to be a minor configuration issue." />
  <row Id="2036" PostHistoryTypeId="2" PostId="789" RevisionGUID="81a40b9c-4aca-440c-80c6-e588e1cae8fd" CreationDate="2014-07-20T23:54:09.070" UserId="802" Text="You can pull stock data very easyly in python and R (probably other languages as well) with the following packages:&#xD;&#xA;&#xD;&#xA;In python with: https://pypi.python.org/pypi/ystockquote&#xD;&#xA;&#xD;&#xA;This is also a really nice tutorial in iPython which shows you how to pull the stock data and play with it: http://nbviewer.ipython.org/github/twiecki/financial-analysis-python-tutorial/blob/master/1.%20Pandas%20Basics.ipynb&#xD;&#xA;&#xD;&#xA;In R with: http://www.quantmod.com/&#xD;&#xA;&#xD;&#xA;HTH. &#xD;&#xA;" />
  <row Id="2037" PostHistoryTypeId="2" PostId="790" RevisionGUID="58f77885-f005-42cb-a2dd-7cd3116a4ce3" CreationDate="2014-07-21T00:35:13.440" UserId="381" Text="Absolutely. When you're working with data at that scale it's common to use a big data framework, in which case python or whatever language you're using is merely an interface. See for example [Spark's Python Programming Guide](http://spark.apache.org/docs/0.9.1/python-programming-guide.html). What kind of data do you have and what do you want to do with it?" />
  <row Id="2038" PostHistoryTypeId="2" PostId="791" RevisionGUID="5b30ca0a-3cee-46c6-b406-798b43bf91eb" CreationDate="2014-07-21T09:00:04.917" UserId="921" Text="I'm trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words &#xD;&#xA;&#xD;&#xA;&gt; senior, primary, lead, head, vp, director, stuff, principal, chief, &#xD;&#xA;&#xD;&#xA;or the words &#xD;&#xA;&#xD;&#xA;&gt; analyst, expert, modeler, researcher, scientist.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA; 1. So, how to get all such possible words with their distance ?&#xD;&#xA;&#xD;&#xA; 2. Can you post this type of words with their distance as an answer based on your own experience ?" />
  <row Id="2039" PostHistoryTypeId="1" PostId="791" RevisionGUID="5b30ca0a-3cee-46c6-b406-798b43bf91eb" CreationDate="2014-07-21T09:00:04.917" UserId="921" Text="Job title similarity" />
  <row Id="2040" PostHistoryTypeId="3" PostId="791" RevisionGUID="5b30ca0a-3cee-46c6-b406-798b43bf91eb" CreationDate="2014-07-21T09:00:04.917" UserId="921" Text="&lt;machine-learning&gt;&lt;dataset&gt;" />
  <row Id="2041" PostHistoryTypeId="2" PostId="792" RevisionGUID="abedd327-c5b1-4dbb-b856-52674b104d17" CreationDate="2014-07-21T11:59:49.203" UserId="743" Text="To handle such amount of data, programming language is not the main concern but the programming framework is. Frameworks such as MapReduce or Spark have bindings to many languages including Python. These frameworks certainly have many ready-to-use packages for data analysis tasks. But in the end it all comes to your requirement, i.e., what is your task? People have different definitions of data analysis tasks, some of them can be easily solved with relational databases. In that case, SQL is much better than all other alternatives." />
  <row Id="2042" PostHistoryTypeId="2" PostId="793" RevisionGUID="43d01a84-e725-48f2-ab11-f6cc00a2b857" CreationDate="2014-07-21T13:41:13.427" UserId="2643" Text="What are the major uses of NOSQL databases like mongodb in the field of data science?" />
  <row Id="2043" PostHistoryTypeId="1" PostId="793" RevisionGUID="43d01a84-e725-48f2-ab11-f6cc00a2b857" CreationDate="2014-07-21T13:41:13.427" UserId="2643" Text="Uses of NOSQL database in data science" />
  <row Id="2044" PostHistoryTypeId="3" PostId="793" RevisionGUID="43d01a84-e725-48f2-ab11-f6cc00a2b857" CreationDate="2014-07-21T13:41:13.427" UserId="2643" Text="&lt;bigdata&gt;&lt;nosql&gt;&lt;mongodb&gt;" />
  <row Id="2045" PostHistoryTypeId="2" PostId="794" RevisionGUID="78c1e705-e63f-4e0b-a230-18e1674b987b" CreationDate="2014-07-21T15:35:59.540" UserId="2648" Text="Pylearn2 seems to be the library of choice, however I find their YAML configuration files off-putting.&#xD;&#xA;&#xD;&#xA;Python itself was designed to be an easy language for prototyping, why would you **not** use it to define the network properties themselves? We have great editors with autocompletion that would make your life much easier and Python is not like C++ where you have to wait for long builds to finish before you can run your code.&#xD;&#xA;&#xD;&#xA;YAML files on the other hand you have to edit using a standard text editor with no assistance whatsoever and this makes the learning curve even steeper. &#xD;&#xA;&#xD;&#xA;I may be missing the big picture but I still don't understand what were they thinking, I don't think prototyping in code would be much slower. For that reason I'm considering Theanets or using Theano directly." />
  <row Id="2046" PostHistoryTypeId="2" PostId="795" RevisionGUID="00c3a861-89a9-486f-afec-de4a76389360" CreationDate="2014-07-21T16:15:07.010" UserId="375" Text="Not sure if this is exactly what you're looking for, but r-base has a function called &quot;adist&quot; which creates a distance matrix of approximate string distances (according to the Levenshtein distance). Type '?adist' for more.&#xD;&#xA;&#xD;&#xA;    words = c(&quot;senior&quot;, &quot;primary&quot;, &quot;lead&quot;, &quot;head&quot;, &quot;vp&quot;, &quot;director&quot;, &quot;stuff&quot;, &quot;principal&quot;, &quot;chief&quot;)&#xD;&#xA;    adist(words)&#xD;&#xA;&#xD;&#xA;          [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]&#xD;&#xA;     [1,]    0    6    5    5    6    5    5    7    5&#xD;&#xA;     [2,]    6    0    6    6    7    7    7    6    6&#xD;&#xA;     [3,]    5    6    0    1    4    7    5    8    5&#xD;&#xA;     [4,]    5    6    1    0    4    7    5    8    4&#xD;&#xA;     [5,]    6    7    4    4    0    8    5    8    5&#xD;&#xA;     [6,]    5    7    7    7    8    0    8    8    7&#xD;&#xA;     [7,]    5    7    5    5    5    8    0    9    4&#xD;&#xA;     [8,]    7    6    8    8    8    8    9    0    8&#xD;&#xA;     [9,]    5    6    5    4    5    7    4    8    0&#xD;&#xA;&#xD;&#xA;Also, if R isn't an option, the Levenshtein distance algorithm is implemented in many languages here:&#xD;&#xA;http://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance" />
  <row Id="2047" PostHistoryTypeId="2" PostId="796" RevisionGUID="e7d17d05-6fc1-4032-8b74-daa2ec065856" CreationDate="2014-07-21T16:58:22.660" UserId="2433" Text="There are couple off things you need to understand when dealing with Big data - &#xD;&#xA;&#xD;&#xA;###What is Big data?&#xD;&#xA;You might be aware of famous V's of Big data - Volume, Velocity, Variety... So, Python may not be suitable for all. And it goes with all data science tools available. You need to know which tool is good for what purpose.&#xD;&#xA;&#xD;&#xA;If dealing with large Volume of data:&#xD;&#xA;&#xD;&#xA; - Pig/Hive/Shark - Data cleaning and ETL work&#xD;&#xA; - Hadoop/Spark - Distributed parallel computing&#xD;&#xA; - Mahout/ML-Lib - Machine Learning&#xD;&#xA;&#xD;&#xA;Now, you can use R/Python in intermediate stages but you'll realize that they become bottleneck in your entire process.&#xD;&#xA;&#xD;&#xA;If dealing with Velocity of data:&#xD;&#xA;&#xD;&#xA; - Kafka/Storm - High throughput system&#xD;&#xA;&#xD;&#xA;People are trying to R/Python here but again it depends on kind of parallelism you want and your model complexity.&#xD;&#xA;&#xD;&#xA;###What sort of analysis you wish to do?  &#xD;&#xA;If your model demands the entire data to be first brought into memory then your model should not be complex because if the intermediate data is large then the code will break. And if you think of writing it into disk then you'll face additional delay because disk read/write is slow as compared to RAM.&#xD;&#xA;&#xD;&#xA;##Conclusion&#xD;&#xA;You can definitely use Python in Big data space (Definitely, since people are trying with R, why not Python) but know your data and business requirement first. There may be better tools available for same and always remember:&#xD;&#xA;&#xD;&#xA;&gt; Your tools shouldn’t determine how you answer questions. Your questions should determine what tools you use." />
  <row Id="2048" PostHistoryTypeId="2" PostId="797" RevisionGUID="81672812-80be-4d1f-864d-2357832d76b7" CreationDate="2014-07-21T19:06:43.223" UserId="548" Text="To be perfectly honest, most NOSQL databases are not very well suited to applications in big data. For the vast majority of all big data applications, the performance of `MongoDB` compared to a relational database like `MySQL` is [significantly][1] is poor enough to warrant staying away from something like `Mongo` entirely.&#xD;&#xA;&#xD;&#xA;With that said, there are a couple really useful properties of NOSQL databases that certainly work in your favor when you're working with large data sets, though the chance of those benefits outweighing the generally poor performance of `NOSQL` compared to `SQL` for read-intensive operations (most similar to typical big data use cases) is low.&#xD;&#xA;&#xD;&#xA; - **No Schema** - If you're working with a lot of unstructured data, it might be hard to actually decide on and rigidly apply a schema. `NOSQL` databases in general are very supporting of this, and will allow you to insert schema-less documents on the fly, which is certainly not something a `SQL` database will support.&#xD;&#xA; - **JSON** - If you happen to be working with `JSON`-style documents instead of with `csv`'s, then you'll see a lot of advantage in using something like `mongo` for a database-layer. Generally the workflow savings don't outweigh the increased query-times though.&#xD;&#xA; - **Ease of Use** - I'm not saying that `SQL` databases are always hard to use, or that `cassandra` is the easiest thing in the world to set up, but in general `NOSQL` databases are easier to set up and use than `SQL` databases. `Mongo` is a particularly strong example of this, known for being one of the easiest database layers to use (outside of `sqlite`). `SQL` also deals with a lot of normalization and there's a large legacy of `SQL` best practices that just generally bogs down the development process.&#xD;&#xA;&#xD;&#xA;Personally I might suggest you also check out [graph][2] databases such as [neo4j][3] that show really good performance for certain types of queries if you're looking into picking out a backend for your data science applications.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.moredevs.ro/mysql-vs-mongodb-performance-benchmark/&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Graph_database&#xD;&#xA;  [3]: http://www.neo4j.org/" />
  <row Id="2049" PostHistoryTypeId="2" PostId="798" RevisionGUID="e3d1f492-2524-4373-867f-50f9ddb37edf" CreationDate="2014-07-21T20:42:00.143" UserId="381" Text="If I understand your question, you can look at the co-occurrence matrix formed using the terms following the title; e.g., senior FOO, primary BAR, etc. Then you can compute the similarity between any pair of terms, such as &quot;senior&quot; and &quot;primary&quot;, using a suitable metric; e.g., the cosine similarity." />
  <row Id="2050" PostHistoryTypeId="2" PostId="799" RevisionGUID="33db8fc8-d3df-476c-8541-73cc1d154239" CreationDate="2014-07-21T21:17:07.303" UserId="381" Text="I would use a visual analysis. Since you know there is a repetition every 256 bytes, create an image 256 pixels wide by however many deep, and encode the data using brightness. In (i)python it would look like this:&#xD;&#xA;&#xD;&#xA;    import os, numpy, matplotlib.pyplot as plt&#xD;&#xA;    &#xD;&#xA;    %matplotlib inline&#xD;&#xA;    &#xD;&#xA;    def read_in_chunks(infile, chunk_size=256):&#xD;&#xA;        while True:&#xD;&#xA;            chunk = infile.read(chunk_size)&#xD;&#xA;            if chunk:&#xD;&#xA;                yield chunk&#xD;&#xA;            else:&#xD;&#xA;                # The chunk was empty, which means we're at the end&#xD;&#xA;                # of the file&#xD;&#xA;                return&#xD;&#xA;    &#xD;&#xA;    fname = 'enter something here'&#xD;&#xA;    srcfile = open(fname, 'rb')&#xD;&#xA;    height = 1 + os.path.getsize(fname)/256&#xD;&#xA;    data = numpy.zeros((height, 256), dtype=numpy.uint8)&#xD;&#xA;    &#xD;&#xA;    data.shape&#xD;&#xA;    for i, line in enumerate(read_in_chunks(srcfile)):&#xD;&#xA;        vals = list(map(int, line))&#xD;&#xA;        data[i,:len(vals)] = vals&#xD;&#xA;    &#xD;&#xA;    plt.imshow(data, aspect=1e-2);&#xD;&#xA;&#xD;&#xA;This is what a PDF looks like:&#xD;&#xA;&#xD;&#xA;![A PDF file visualized][1]&#xD;&#xA;&#xD;&#xA;A 256 byte periodic pattern would have manifested as vertical lines. Except for the header and tail it looks pretty noisy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/bicgF.png" />
  <row Id="2051" PostHistoryTypeId="2" PostId="800" RevisionGUID="313ba9e4-182b-4f4d-bd85-495d74e4eb78" CreationDate="2014-07-21T21:29:26.270" UserId="381" Text="One benefit of the schema-free NoSQL approach is that you don't commit prematurely and you can apply the right schema at query time using an appropriate tool like [Apache Drill](http://incubator.apache.org/drill/). See [this presentation](http://wiki.apache.org/incubator/DrillProposal?action=AttachFile&amp;do=get&amp;target=Drill+slides.pdf) for details. MySQL wouldn't be my first choice in a big data setting." />
  <row Id="2052" PostHistoryTypeId="2" PostId="801" RevisionGUID="838e9e67-a8e7-4f1b-a60f-139d92d460f0" CreationDate="2014-07-21T21:32:12.963" UserId="2433" Text="That's an interesting problem, thanks for bring out here on stack.&#xD;&#xA;&#xD;&#xA;I think this problem is similar to when we apply [LSA(Latent Semantic Analysis)](http://en.wikipedia.org/wiki/Latent_semantic_analysis) in sentiment analysis to find list of positive and negative words with polarity with respect to some predefined positive and negative words.&#xD;&#xA;&#xD;&#xA;*Good reads:*&#xD;&#xA;&#xD;&#xA; - [Learning Word Vectors for Sentiment Analysis](http://cs.stanford.edu/people/ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)&#xD;&#xA; - [Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus](http://arxiv.org/pdf/cs/0212012.pdf)&#xD;&#xA;&#xD;&#xA;So, according to me LSA is your best approach to begin with in this situation as it learns the underlying relation between the words from the corpus and probably that's what you are looking for." />
  <row Id="2053" PostHistoryTypeId="2" PostId="802" RevisionGUID="b29b7353-8769-4069-8a77-4b6fd3cc1c33" CreationDate="2014-07-21T23:53:11.120" UserId="1163" Text="I'm working on an application which requires creating a very large database of n-grams that exist in a large text corpus.&#xD;&#xA;&#xD;&#xA;I need three efficient operation types: Lookup and insertion indexed by the n-gram itself, and querying for all n-grams that contain a sub-n-gram.&#xD;&#xA;&#xD;&#xA;This sounds to me like the database should be a gigantic document tree, and document databases, e.g. Mongo, should be able to do the job well, but I've never used those at scale.&#xD;&#xA;&#xD;&#xA;Knowing the Stack Exchange question format, I'd like to clarify that I'm not asking for suggestions on specific technologies, but rather a type of database that I should be looking for to implement something like this at scale." />
  <row Id="2054" PostHistoryTypeId="1" PostId="802" RevisionGUID="b29b7353-8769-4069-8a77-4b6fd3cc1c33" CreationDate="2014-07-21T23:53:11.120" UserId="1163" Text="Efficient database model for storing data indexed by n-grams" />
  <row Id="2055" PostHistoryTypeId="3" PostId="802" RevisionGUID="b29b7353-8769-4069-8a77-4b6fd3cc1c33" CreationDate="2014-07-21T23:53:11.120" UserId="1163" Text="&lt;nlp&gt;&lt;databases&gt;" />
  <row Id="2056" PostHistoryTypeId="2" PostId="803" RevisionGUID="c96e4980-7583-4759-ab5f-139ad514efc0" CreationDate="2014-07-22T00:06:10.500" UserId="381" Text="I haven't done this before but it sounds like a job for a graph database given the functionality you want. [Here's a demo in neo4j](http://www.rene-pickhardt.de/download-google-n-gram-data-set-and-neo4j-source-code-for-storing-it/)." />
  <row Id="2057" PostHistoryTypeId="5" PostId="799" RevisionGUID="2722106e-8bcd-41ba-a13e-c2433a80a99f" CreationDate="2014-07-22T00:13:21.200" UserId="381" Comment="deleted 9 characters in body" Text="I would use a visual analysis. Since you know there is a repetition every 256 bytes, create an image 256 pixels wide by however many deep, and encode the data using brightness. In (i)python it would look like this:&#xD;&#xA;&#xD;&#xA;    import os, numpy, matplotlib.pyplot as plt&#xD;&#xA;    &#xD;&#xA;    %matplotlib inline&#xD;&#xA;    &#xD;&#xA;    def read_in_chunks(infile, chunk_size=256):&#xD;&#xA;        while True:&#xD;&#xA;            chunk = infile.read(chunk_size)&#xD;&#xA;            if chunk:&#xD;&#xA;                yield chunk&#xD;&#xA;            else:&#xD;&#xA;                # The chunk was empty, which means we're at the end&#xD;&#xA;                # of the file&#xD;&#xA;                return&#xD;&#xA;    &#xD;&#xA;    fname = 'enter something here'&#xD;&#xA;    srcfile = open(fname, 'rb')&#xD;&#xA;    height = 1 + os.path.getsize(fname)/256&#xD;&#xA;    data = numpy.zeros((height, 256), dtype=numpy.uint8)    &#xD;&#xA;&#xD;&#xA;    for i, line in enumerate(read_in_chunks(srcfile)):&#xD;&#xA;        vals = list(map(int, line))&#xD;&#xA;        data[i,:len(vals)] = vals&#xD;&#xA;    &#xD;&#xA;    plt.imshow(data, aspect=1e-2);&#xD;&#xA;&#xD;&#xA;This is what a PDF looks like:&#xD;&#xA;&#xD;&#xA;![A PDF file visualized][1]&#xD;&#xA;&#xD;&#xA;A 256 byte periodic pattern would have manifested itself as vertical lines. Except for the header and tail it looks pretty noisy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/bicgF.png" />
  <row Id="2058" PostHistoryTypeId="2" PostId="804" RevisionGUID="28e336d2-d721-4e23-a7e7-f3c1b374912e" CreationDate="2014-07-22T01:12:03.860" UserId="1117" Text="Basically, both are software systems that are based on data and algorithms." />
  <row Id="2059" PostHistoryTypeId="1" PostId="804" RevisionGUID="28e336d2-d721-4e23-a7e7-f3c1b374912e" CreationDate="2014-07-22T01:12:03.860" UserId="1117" Text="What's the difference between data products and intelligent systems?" />
  <row Id="2060" PostHistoryTypeId="3" PostId="804" RevisionGUID="28e336d2-d721-4e23-a7e7-f3c1b374912e" CreationDate="2014-07-22T01:12:03.860" UserId="1117" Text="&lt;tools&gt;&lt;education&gt;&lt;definitions&gt;" />
  <row Id="2061" PostHistoryTypeId="2" PostId="805" RevisionGUID="256b22f7-53b8-440c-9ad6-efbdc3a1fb23" CreationDate="2014-07-22T02:27:43.710" UserId="2452" Text="This is very vague question. However, I will try to make sense of it. Considering your statement that both *entities* are &quot;software systems that are based on data and algorithms&quot; and *logic* rules, it appears that **data products** are *intelligent systems* and **intelligent systems** are, to some degree, *data products*. Therefore, the difference between the terms &quot;data products&quot; and &quot;intelligent systems&quot; is purely in the **focus** (*source* of information or *purpose* of system dimensions) of each type of systems (*data* **vs.** *intelligence/algorithms*)." />
  <row Id="2062" PostHistoryTypeId="2" PostId="806" RevisionGUID="8e075c7b-6722-4a70-a96d-04bc31c46994" CreationDate="2014-07-22T03:43:20.327" UserId="2653" Text="I was starting to look into area under curve(AUC) and am a little confused about its usefulness. When first explained to me, AUC seemed to be a great measure of performance but in my research I've found that some claim its advantage is mostly marginal in that it is best for catching 'lucky' models with high standard accuracy measurements and low AUC.&#xD;&#xA;&#xD;&#xA;So should I avoid relying on AUC for validating models or would a combination be best? Thanks for all your help." />
  <row Id="2063" PostHistoryTypeId="1" PostId="806" RevisionGUID="8e075c7b-6722-4a70-a96d-04bc31c46994" CreationDate="2014-07-22T03:43:20.327" UserId="2653" Text="Advantages of AUC vs standard accuracy" />
  <row Id="2064" PostHistoryTypeId="3" PostId="806" RevisionGUID="8e075c7b-6722-4a70-a96d-04bc31c46994" CreationDate="2014-07-22T03:43:20.327" UserId="2653" Text="&lt;machine-learning&gt;&lt;accuracy&gt;" />
  <row Id="2065" PostHistoryTypeId="2" PostId="807" RevisionGUID="4943da37-2105-4a82-924e-2f508821e682" CreationDate="2014-07-22T04:10:18.353" UserId="548" Text="Really great question, and one that I find that most people don't really understand on an intuitive level. `AUC` is in fact often predicted over accuracy for binary classification for a number of different reasons. First though, let's talk about exactly what `AUC` is. Honestly, for being one of the most widely used efficacy metrics, it's surprisingly obtuse to figure out exactly how `AUC` works.&#xD;&#xA;&#xD;&#xA;`AUC` stands for `Area Under the Curve`, which curve you ask? Well that would be the `ROC` curve. `ROC` stands for [Receiver Operating Characteristic][1], which is actually slightly non-intuitive. The implicit goal of `AUC` is to deal with situations where you have a very skewed sample distribution, and don't want to overfit to a single class.&#xD;&#xA;&#xD;&#xA;A great example is in spam detection. Generally spam data sets are STRONGLY biased towards ham, or not-spam. If your data set is 90% ham, you can get a pretty damn good accuracy by just saying that every single email is ham, which is obviously something that indicates a non-ideal classifier. Let's start with a couple of metrics that are a little more useful for us, specifically the true positive rate (`TPR`) and the false positive rate (`FPR`):&#xD;&#xA;&#xD;&#xA;![ROC axes][2]&#xD;&#xA;&#xD;&#xA;Now in this graph, `TPR` is specifically the ratio of true positive to all positives, and `FPR` is the ratio of false positives to all negatives. (Keep in mind, this is only for binary classification.) On a graph like this, it should be pretty straightforward to figure out that a prediction of all 0's or all 1's will result in the points of `(0,0)` and `(1,1)` respectively. If you draw a line through these lines you get something like this:&#xD;&#xA;&#xD;&#xA;![Kind of like a triangle][3]&#xD;&#xA;&#xD;&#xA;Which looks basically like a diagonal line (it is), and by some easy geometry, you can see that the `AUC` of such a model would be `0.5` (height and base are both 1). Similarly, if you predict a random assortment of 0's and 1's, let's say 90% 1's, you could get the point `(0.9, 0.9)`, which again falls along that diagonal line.&#xD;&#xA;&#xD;&#xA;Now comes the interesting part. What if we weren't only predicting 0's and 1's? What if instead we wanted to say that, theoretically we were going to set a cutoff, above which every result was a 1, and below which every result were a 0. This would mean that at the extremes you get the original situation where you have all 0's and all 1's (at a cutoff of 0 and 1 respectively), but also a series of intermediate states that fall within the `1x1` graph that contains your `ROC`. In practice you get something like this:&#xD;&#xA;![Courtesy of Wikipedia][4]&#xD;&#xA;&#xD;&#xA;So basically, what you're actually getting when you do an `AUC` over accuracy is something that will strongly discourage people going for models that are representative, but not discriminative, as this will only actually select for models that achieve false positive and true positive rates that are significantly above random chance, which is not guaranteed for accuracy.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Receiver_operating_characteristic&#xD;&#xA;  [2]: http://i.stack.imgur.com/hNxTl.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/B1WT1.png&#xD;&#xA;  [4]: http://i.stack.imgur.com/13McM.png" />
  <row Id="2067" PostHistoryTypeId="5" PostId="805" RevisionGUID="100eb78e-1651-4a52-8a43-a37abb004e59" CreationDate="2014-07-22T05:58:59.537" UserId="2452" Comment="Fixed typo." Text="This is a very vague question. However, I will try to make sense of it. Considering your statement that both *entities* are &quot;software systems that are based on data and algorithms&quot; and *logic* rules, it appears that **data products** are *intelligent systems* and **intelligent systems** are, to some degree, *data products*. Therefore, the difference between the terms &quot;data products&quot; and &quot;intelligent systems&quot; is purely in the **focus** (*source* of information or *purpose* of system dimensions) of each type of systems (*data* **vs.** *intelligence/algorithms*)." />
  <row Id="2068" PostHistoryTypeId="5" PostId="805" RevisionGUID="13d6ef44-f209-427a-93c4-a48c756618d1" CreationDate="2014-07-22T06:06:31.513" UserId="2452" Comment="Re-phrased sentences for more clarity." Text="This is a very vague question. However, I will try to make sense of it. Considering rules of *logic* as well as your statement that both *entities* are &quot;software systems that are based on data and algorithms&quot;, it appears that **data products** are *intelligent systems* and **intelligent systems** are, to some degree, *data products*. Therefore, it can be argued that the difference between the terms &quot;data products&quot; and &quot;intelligent systems&quot; is purely in the **focus** (*source* of information or *purpose* of system dimensions) of each type of systems (*data* **vs.** *intelligence/algorithms*)." />
  <row Id="2069" PostHistoryTypeId="2" PostId="808" RevisionGUID="71908968-28dc-42b5-9205-268e21856f20" CreationDate="2014-07-22T08:39:33.810" UserId="1117" Text="i want to become a **data scientist**. I studied applied **statistics** (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in **Computer Science** focus in Intelligent Systems.&#xD;&#xA;&#xD;&#xA;Here is my study plan:&#xD;&#xA;-Machine learning&#xD;&#xA;-Advanced machine learning&#xD;&#xA;-Data mining&#xD;&#xA;-Fuzzy logic&#xD;&#xA;-Recommendation Systems&#xD;&#xA;-Distributed Data Systems&#xD;&#xA;-Cloud Computing&#xD;&#xA;-Knowledge discovery&#xD;&#xA;-Business Intelligence&#xD;&#xA;-Information retrieval&#xD;&#xA;-Text mining&#xD;&#xA;&#xD;&#xA;At the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?&#xD;&#xA;&#xD;&#xA;Thanks for the answers." />
  <row Id="2070" PostHistoryTypeId="1" PostId="808" RevisionGUID="71908968-28dc-42b5-9205-268e21856f20" CreationDate="2014-07-22T08:39:33.810" UserId="1117" Text="Statistics + Computer Science = Data Science?" />
  <row Id="2071" PostHistoryTypeId="3" PostId="808" RevisionGUID="71908968-28dc-42b5-9205-268e21856f20" CreationDate="2014-07-22T08:39:33.810" UserId="1117" Text="&lt;machine-learning&gt;&lt;bigdata&gt;&lt;statistics&gt;" />
  <row Id="2072" PostHistoryTypeId="5" PostId="791" RevisionGUID="3a8b3de6-fda8-404f-bc8b-218cabd0418b" CreationDate="2014-07-22T09:00:27.183" UserId="809" Comment="Removing off-topic request to provide dataset (e.g., do his work for him) and slightly fixing grammar." Text="I'm trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words &#xD;&#xA;&#xD;&#xA;&gt; senior, primary, lead, head, vp, director, stuff, principal, chief, &#xD;&#xA;&#xD;&#xA;or the words &#xD;&#xA;&#xD;&#xA;&gt; analyst, expert, modeler, researcher, scientist.&#xD;&#xA;&#xD;&#xA;How can I get all such possible words with their distance ?&#xD;&#xA;" />
  <row Id="2073" PostHistoryTypeId="24" PostId="791" RevisionGUID="3a8b3de6-fda8-404f-bc8b-218cabd0418b" CreationDate="2014-07-22T09:00:27.183" Comment="Proposed by 809 approved by 434, -1 edit id of 118" />
  <row Id="2074" PostHistoryTypeId="5" PostId="791" RevisionGUID="445a1c41-4d21-445c-aa59-335052cb2ca1" CreationDate="2014-07-22T09:00:27.183" UserId="921" Comment="Removing off-topic request to provide dataset (e.g., do his work for him) and slightly fixing grammar." Text="I'm trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words &#xD;&#xA;&#xD;&#xA;&gt; senior, primary, lead, head, vp, director, stuff, principal, chief, &#xD;&#xA;&#xD;&#xA;or the words &#xD;&#xA;&#xD;&#xA;&gt; analyst, expert, modeler, researcher, scientist, developer, engineer, architect.&#xD;&#xA;&#xD;&#xA;How can I get all such possible words with their distance ?&#xD;&#xA;" />
  <row Id="2075" PostHistoryTypeId="2" PostId="809" RevisionGUID="5cde8495-d532-4ef1-889b-c08dc63244da" CreationDate="2014-07-22T09:03:49.270" UserId="2452" Text="I think that you're on the right track toward becoming an **expert** *data scientist*. Recently I have answered related question here on Data Science StackExchange: http://datascience.stackexchange.com/a/742/2452 (pay attention to the *definition* I mention there, as it essentially answers your question by itself). I hope that you will find all that useful. Good luck in your career!" />
  <row Id="2076" PostHistoryTypeId="5" PostId="809" RevisionGUID="cd27daf5-bfdd-490d-b1b7-9015b6bdc10c" CreationDate="2014-07-22T09:11:35.213" UserId="2452" Comment="Improved wording." Text="I think that you're on the right track toward becoming an **expert** *data scientist*. Recently I have answered related question here on Data Science StackExchange: http://datascience.stackexchange.com/a/742/2452 (pay attention to the *definition* I mention there, as it essentially answers your question by itself, as well as to aspects of **practicing** *software engineering* and **applying** knowledge to solving *real-world* problems). I hope that you will find all that useful. Good luck in your career!" />
  <row Id="2077" PostHistoryTypeId="2" PostId="810" RevisionGUID="961211f2-c75e-4250-baec-3c33a5106a58" CreationDate="2014-07-22T12:29:10.050" UserId="2661" Text="My 'machine learning' task is of separating benign Internet traffic from malicious traffic. In the real world scenario, most (say 90% or more) of Internet traffic is benign. Thus I felt that I should go with the similar kind of data for training my models as well. But then I did come across a research paper or two (in my area of work) which have used a balanced data to train models, implying equal number of instances of benign and malicious traffic?&#xD;&#xA;&#xD;&#xA;Can someone shed more light on the *pros* and *cons* of both the choices, and which one to go for?" />
  <row Id="2078" PostHistoryTypeId="1" PostId="810" RevisionGUID="961211f2-c75e-4250-baec-3c33a5106a58" CreationDate="2014-07-22T12:29:10.050" UserId="2661" Text="Should I go for a 'balanced' dataset or a 'representative' dataset?" />
  <row Id="2079" PostHistoryTypeId="3" PostId="810" RevisionGUID="961211f2-c75e-4250-baec-3c33a5106a58" CreationDate="2014-07-22T12:29:10.050" UserId="2661" Text="&lt;machine-learning&gt;&lt;dataset&gt;" />
  <row Id="2080" PostHistoryTypeId="5" PostId="810" RevisionGUID="cd4d92ee-eec2-41c8-b5bc-5f4b9f736ba2" CreationDate="2014-07-22T12:35:30.687" UserId="2661" Comment="added 307 characters in body" Text="My 'machine learning' task is of separating benign Internet traffic from malicious traffic. In the real world scenario, most (say 90% or more) of Internet traffic is benign. Thus I felt that I should go with the similar kind of data for training my models as well. But then I did come across a research paper or two (in my area of work) which have used a balanced data to train models, implying equal number of instances of benign and malicious traffic.&#xD;&#xA;&#xD;&#xA;In general, if I am building ML models, should I go for a dataset which is representative of the real world problem, or is a balanced dataset better suited for building the models (since certain classifiers do not behave well with class imbalance, or due to other reasons not known to me)?&#xD;&#xA;&#xD;&#xA;Can someone shed more light on the *pros* and *cons* of both the choices, and how to decide which one to go for?" />
  <row Id="2081" PostHistoryTypeId="2" PostId="811" RevisionGUID="55361c59-2143-419b-95bb-7a948c5654ab" CreationDate="2014-07-22T14:22:49.137" UserId="2513" Text="I would say the answer depends on your use case. Based on my experience:&#xD;&#xA;&#xD;&#xA; - If you're trying to build a representative model -- one that describes the data rather than necessarily predicts -- then I would suggest using a representative sample of your data.&#xD;&#xA; - If you want to build a predictive model, particularly one that performs well by measure of AUC or rank-order and plan to use a basic ML framework (i.e. Decision Tree, SVM, Naive Bayes, etc), then I would suggest you feed the framework a balanced dataset. Much of the literature on class imbalance finds that random undersampling (down sampling the majority class to the size of the minority class) can drive performance gains.&#xD;&#xA; - If you're building a predictive model, but are using a more advanced framework (i.e. something that determines sampling parameters via wrapper or a modification of a bagging framework that samples to class equivalence), then I would suggest again feeding the representative sample and letting the algorithm take care of balancing the data for training." />
  <row Id="2082" PostHistoryTypeId="2" PostId="812" RevisionGUID="4e1b4950-a1aa-4e64-a072-2706b36e1074" CreationDate="2014-07-22T15:17:04.810" UserId="192" Text="What is the best tool to use to visualize (draw the vertices and edges) a graph with 1000000 vertices? There are about 50000 edges in the graph. And I can compute the location of individual vertices and edges.&#xD;&#xA;&#xD;&#xA;I am thinking about writing a program to generate a svg. Any other suggestions?  " />
  <row Id="2083" PostHistoryTypeId="1" PostId="812" RevisionGUID="4e1b4950-a1aa-4e64-a072-2706b36e1074" CreationDate="2014-07-22T15:17:04.810" UserId="192" Text="Visualizating a graph with 1000000 vertices" />
  <row Id="2084" PostHistoryTypeId="3" PostId="812" RevisionGUID="4e1b4950-a1aa-4e64-a072-2706b36e1074" CreationDate="2014-07-22T15:17:04.810" UserId="192" Text="&lt;visualization&gt;&lt;graphs&gt;" />
  <row Id="2085" PostHistoryTypeId="2" PostId="813" RevisionGUID="855143c2-0e32-41df-9cc0-68b826ab7dde" CreationDate="2014-07-22T15:44:04.160" UserId="471" Text="https://gephi.github.io/ says it can handle a million edges. If your graph has 1000000 vertices and only 50000 edges then most of your vertices won't have any edges anyway.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;In fact the Gephi spec is the dual of your example: &quot;Networks up to 50,000 nodes and 1,000,000 edges&quot;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2086" PostHistoryTypeId="2" PostId="814" RevisionGUID="81582efc-e8a8-4b49-a8f6-7345c8edc1c5" CreationDate="2014-07-22T16:40:17.733" UserId="2452" Text="I also suggest `Gephi` software (https://gephi.github.io), which seems to be quite powerful. Some additional information on using `Gephi` with **large networks** can be found [here][1] and, more generally, [here][2]. `Cytoscape` (http://www.cytoscape.org) is an alternative to `Gephi`, being an another popular platform for complex network analysis and visualization.&#xD;&#xA;&#xD;&#xA;If you'd like to work with networks **programmatically** (including visualization) in R, Python or C/C++, you can check `igraph` collection of libraries. Speaking of R, you may find interesting the following blog posts: on **using R with Cytoscape** (http://www.vesnam.com/Rblog/viznets1) and on **using R with Gephi** (http://www.vesnam.com/Rblog/viznets2).&#xD;&#xA;&#xD;&#xA;For **extensive lists** of *network analysis and visualization software*, including some comparison and reviews, you might want to check the following pages: 1) http://wiki.cytoscape.org/Network_analysis_links; 2) http://www.kdnuggets.com/software/social-network-analysis.html; 3) http://www.activatenetworks.net/social-network-analysis-sna-software-review.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://forum.gephi.org/viewtopic.php?t=1554&#xD;&#xA;  [2]: https://forum.gephi.org/viewforum.php?f=25" />
  <row Id="2087" PostHistoryTypeId="5" PostId="793" RevisionGUID="6bd11059-6c21-4739-abbd-0a3bc2a08204" CreationDate="2014-07-22T17:02:29.310" UserId="2643" Comment="added 58 characters in body" Text="How can NOSQL databases like  mongodb be used for data analysis? What are the features in them that can make data analysis faster and powerful?" />
  <row Id="2091" PostHistoryTypeId="4" PostId="812" RevisionGUID="7debc3d3-eded-4298-97b0-d6e3fe8059fa" CreationDate="2014-07-22T18:34:44.740" UserId="471" Comment="Made the title correct and saved ppl from counting zeroes." Text="Visualizing a graph with a million vertices" />
  <row Id="2092" PostHistoryTypeId="24" PostId="812" RevisionGUID="7debc3d3-eded-4298-97b0-d6e3fe8059fa" CreationDate="2014-07-22T18:34:44.740" Comment="Proposed by 471 approved by 434, 192 edit id of 119" />
  <row Id="2093" PostHistoryTypeId="2" PostId="815" RevisionGUID="4113cf3c-63b9-42b0-bc21-030955ed9356" CreationDate="2014-07-22T19:20:47.580" UserId="906" Text="I'm currently facing a project that I could solve with a relational database in a relatively painful way. Having heard so much about NOSQL, I'm wondering if there is not a more appropriate way of tackling it:&#xD;&#xA;&#xD;&#xA;Suppose we are tracking a group of animals in a forest (n ~ 500) and would like to keep a record of a set of observations (this is a fictional scenario).&#xD;&#xA;&#xD;&#xA;We would like to store the following information in a database:&#xD;&#xA;&#xD;&#xA;* a unique identifier for each animal&#xD;&#xA;* a description of the animal with structured fields: Species, Genus, Family, ...&#xD;&#xA;* a free text field with additional information&#xD;&#xA;* each time-point at which it was detected close to a reference point&#xD;&#xA;* a picture of the animal&#xD;&#xA;* an indication whether two given animals are siblings&#xD;&#xA;&#xD;&#xA;And:&#xD;&#xA;&#xD;&#xA;* there might be additional features appearing later as more data comes in&#xD;&#xA;&#xD;&#xA;We would like to be able to execute the following types of queries:&#xD;&#xA;&#xD;&#xA;* return all the animals spotted between in a given time interval&#xD;&#xA;* return all the animals of a given Species or Family&#xD;&#xA;* perform a text search on the free text field&#xD;&#xA;&#xD;&#xA;Which particular database system would you recommend ? Is there any tutorial / examples that I could use as a starting point ?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2094" PostHistoryTypeId="1" PostId="815" RevisionGUID="4113cf3c-63b9-42b0-bc21-030955ed9356" CreationDate="2014-07-22T19:20:47.580" UserId="906" Text="is this a good case for NOSQL?" />
  <row Id="2095" PostHistoryTypeId="3" PostId="815" RevisionGUID="4113cf3c-63b9-42b0-bc21-030955ed9356" CreationDate="2014-07-22T19:20:47.580" UserId="906" Text="&lt;nosql&gt;&lt;databases&gt;" />
  <row Id="2096" PostHistoryTypeId="2" PostId="816" RevisionGUID="1222ac47-e855-4c3a-9649-e046655f1674" CreationDate="2014-07-22T21:12:08.333" UserId="2668" Text="Well it depends on what kind of &quot;Data Science&quot; you wish to get in to.  For basic analytics and reporting statistics will certainly help, but for Machine Learning and Artificial Intelligence then you'll want a few more skills&#xD;&#xA;&#xD;&#xA; - **Probability theory** - you must have a solid background in pure probability so that you can decompose any problem, whether seen before or not, into probabilistic principles.  Statistics helps a lot for already solved problems, but new and unsolved problems require a deep understanding of probability so that you can design appropriate techniques.&#xD;&#xA;&#xD;&#xA; - **Information Theory** - this (relative to statistics) is quite a new field (though still decades old), the most important work was by Shannon, but even more important and often neglected note in literature is work by Hobson that proved that Kullback-Leibler Divergence is the only mathematical definition that truly captures the notion of a *&quot;measure of information&quot;*. Now fundamental to artificial intellgence is being able to quantify information.  Suggest reading &quot;Concepts in Statistical Mechanics&quot; - Arthur Hobson (very expensive book, only available in academic libraries).&#xD;&#xA;&#xD;&#xA; - **Complexity Theory** - A big problem many Data Scientists face that do not have a solid complexity theory background is that their algorithms do not scale, or just take an extremely long time to run on large data.  Take PCA for example, many peoples favourite answer to the interview question &quot;how do you reduce the number of features in our dataset&quot;, but even if you tell the candidate &quot;the data set is really really really large&quot; they still propose various forms of PCA that are O(n^3).  If you want to stand out, you want to be able to solve each problem on it's own, NOT throw some text book solution at it designed a long time ago before Big Data was such a hip thing.  For that you need to understand how long things take to run, not only theoretically, but practically - so how to use a cluster of computers to distribute an algorithm, or which data structures take up less memory.&#xD;&#xA;&#xD;&#xA; - **Communication Skills** - A huge part of Data Science is understanding business.  Whether it's inventing a product driven by data science, or giving business insight driven by data science, being able to communicate well with both the Project and Product Managers, the tech teams, and your fellow data scientists is very important.  You can have an amazing idea, say an awesome AI solution, but if you cannot effectively (a) communicate WHY that will make the business money, (b) convince your collegues it will work and (c) explain to tech people how you need their help to build it, then it wont get done." />
  <row Id="2097" PostHistoryTypeId="2" PostId="817" RevisionGUID="9dc14af4-af10-403e-a731-3ca8ee4a8e51" CreationDate="2014-07-22T21:24:33.820" UserId="2556" Text="Data scientist (to me) a big umbrella term. I would see a data scientist as a person who can proficiently use techniques from the fields of data mining, machine learning, pattern classification, and statistics. &#xD;&#xA;&#xD;&#xA;However, those terms are intertwined to: machine learning is tied together with pattern classification, and also data mining overlaps when it comes finding patterns in data. And all techniques have their underlying statistical principles. I always picture this as a Venn diagram with a huge intersection. &#xD;&#xA;&#xD;&#xA;Computer sciences is related to all those fields too. I would say that you need &quot;data science&quot; techniques to do computer-scientific research, but computer science knowledge is not necessarily implied in &quot;data science&quot;. However, programming skills - I see programming and computer science as different professions, where programming is more the tool in order solve problems - are also important to work with the data and to conduct data analysis.&#xD;&#xA;&#xD;&#xA;You have a really nice study plan, and it all makes sense. But I am not sure if you &quot;want&quot; to call yourself just &quot;data scientist&quot;, I have the impression that &quot;data scientist&quot; is such a ambiguous term that can mean everything or nothing. What I want to convey is that you will end up being something more - more &quot;specialized&quot; - than &quot;just&quot; a data scientist." />
  <row Id="2098" PostHistoryTypeId="2" PostId="818" RevisionGUID="9db346b5-5da6-4cc9-a16e-fe39c5f2385b" CreationDate="2014-07-23T05:29:06.903" UserId="941" Text="I think, that `Gephi` could face with lack-of-memory issues, you will need at least 8Gb of RAM. Though number of edges is not extremely huge. &#xD;&#xA;&#xD;&#xA;Possibly, more appropriate tool in this case would be [`GraphViz`][1]. It's a command line tool for network visualizations, and presumably would be more tolerant to graph size. Moreover, as I remember, in `GraphViz` it is possible to use precomputed coordinates to facilitate computations.&#xD;&#xA;&#xD;&#xA;I've tried to find a real-world examples of using `GraphViz` with huge graphs, but didn't succeed. Though I found similar discussion on [**Computational Science**][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.graphviz.org/&#xD;&#xA;  [2]: http://scicomp.stackexchange.com/questions/3315/visualizing-very-large-link-graphs" />
  <row Id="2099" PostHistoryTypeId="2" PostId="819" RevisionGUID="25af8766-1911-484f-9f4b-00359925302d" CreationDate="2014-07-23T07:01:01.140" UserId="471" Text="Three tables: **animal**, **observation**, and **sibling**. The observation has an **animal_id** column which links to the animal table, and the sibling table has **animal_1_id** and **animal_2_id** columns that indicates two animals are siblings for each row.&#xD;&#xA;&#xD;&#xA;Even with 5000 animals and 100000 observations I don't think query time will be a problem for something like PostgreSQL for most reasonable queries (obviously you can construct unreasonable queries but you can do that in any system). &#xD;&#xA;&#xD;&#xA;So I don't see how this is &quot;relatively painful&quot;. Relative to what? The only complexity is the sibling table. In NOSQL you might store the full list of siblings in the record for each animal, but then when you add a sibling relationship you have to add it to both sibling's animal records. With the relational table approach I've outlined, it only exists once, but at the expense of having to test against both columns to find an animal's siblings. &#xD;&#xA;&#xD;&#xA;I'd use PostgreSQL, and that gives you the option of using PostGIS if you have location data - this is a geospatial extension to PostgreSQL that lets you do spatial queries (point in polygon, points near a point etc) which might be something for you. &#xD;&#xA;&#xD;&#xA;I really don't think the properties of NOSQL databases are a problem here for you - you aren't changing your schema every ten minutes, you probably **do** care that your database is ACID-compliant, and you don't need something web-scale.&#xD;&#xA;&#xD;&#xA;http://www.mongodb-is-web-scale.com/ [warning: strong language]&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2100" PostHistoryTypeId="2" PostId="820" RevisionGUID="1fb62cce-3c05-4ccf-aa89-e350a29915e4" CreationDate="2014-07-23T08:06:21.417" UserId="95" Text="Apologies if this is very broad question, what I would like to know is how effective is A/B testing (or other methods) of effectively measuring the effects of a design decision.&#xD;&#xA;&#xD;&#xA;For instance we can analyse user interactions or click results, purchase/ browse decisions and then modify/tailor the results presented to the user.&#xD;&#xA;&#xD;&#xA;We could then test the effectiveness of this design change by subjecting 10% of users to the alternative model randomly but then how objective is this?&#xD;&#xA;&#xD;&#xA;How do we avoid influencing the user by the model change, for instance we could decided that search queries for 'David Beckham' are probably about football so search results become biased towards this but we could equally say that his lifestyle is just as relevant but this never makes it into the top 10 results that are returned.&#xD;&#xA;&#xD;&#xA;I am curious how this is dealt with and how to measure this effectively.&#xD;&#xA;&#xD;&#xA;My thoughts are that you could be in danger of pushing a model that you think is correct and the user obliges and this becomes a self-fulfilling prophecy.&#xD;&#xA;&#xD;&#xA;I've read an article on this: http://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/ and also the book: http://shop.oreilly.com/product/0636920028529.do which discussed this so it piqued my interest." />
  <row Id="2101" PostHistoryTypeId="1" PostId="820" RevisionGUID="1fb62cce-3c05-4ccf-aa89-e350a29915e4" CreationDate="2014-07-23T08:06:21.417" UserId="95" Text="How can we effectively measure the impact of our data decisions" />
  <row Id="2102" PostHistoryTypeId="3" PostId="820" RevisionGUID="1fb62cce-3c05-4ccf-aa89-e350a29915e4" CreationDate="2014-07-23T08:06:21.417" UserId="95" Text="&lt;search&gt;" />
  <row Id="2103" PostHistoryTypeId="5" PostId="767" RevisionGUID="a8e30a5c-1260-43c9-9913-6390f7642fe2" CreationDate="2014-07-23T12:00:44.207" UserId="2544" Comment="added Vagrant as an opensource alternative to Docker" Text="A great reproducibility tool for Python with a low learning curve is of course [**IPython Notebook**][1] (don't forget the [%logon and %logstart][2] magics and/or [Git][3] to keep a history of all your experiments).&#xD;&#xA;&#xD;&#xA;Another more general tool working with any language (with a Python API on [pypi][4]) is [**Sumatra**][5], which is specifically designed to produce **replicable** research (which aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium). Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a &quot;save game state&quot; often found in videogames. More precisely, it will will save:&#xD;&#xA;&#xD;&#xA;- all the parameters you provided;&#xD;&#xA;- the exact sourcecode state of your whole experimental application and config files;&#xD;&#xA;- the output/plots/results and also any file produced by your experimental application.&#xD;&#xA;&#xD;&#xA;It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don't want to save everything everytime).&#xD;&#xA;&#xD;&#xA;/EDIT: [dsign][6] touched a very important point here: the replicability of your setup is as important as the replicability of your application. In other words, you should at least provide a **full list of the libraries and compilers** you used along with their exact **versions** and the details of your **platform**.&#xD;&#xA;&#xD;&#xA;Personally, in scientific computing with Python, I have found that packaging an application along with the libraries is just too painful, thus I now just use an all-in-one scientific python package such as [Anaconda][7] (with the great package manager [conda][8]), and just advise users to use the same package. Another solution could be to provide a script to automatically generate a [virtualenv][9], or to package everything using the commercial [Docker application as cited by dsign][10] or the opensource [Vagrant][11] (with for example [pylearn2-in-a-box][12] which use Vagrant to produce an easily redistributable virtual environment package).&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://ipython.org/&#xD;&#xA;  [2]: https://damontallen.github.io/IPython-quick-ref-sheets/&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/Git_(software)&#xD;&#xA;  [4]: https://pypi.python.org/pypi/Sumatra&#xD;&#xA;  [5]: http://neuralensemble.org/sumatra/&#xD;&#xA;  [6]: http://datascience.stackexchange.com/a/775&#xD;&#xA;  [7]: https://store.continuum.io/cshop/anaconda/&#xD;&#xA;  [8]: http://www.continuum.io/blog/conda&#xD;&#xA;  [9]: http://docs.python-guide.org/en/latest/dev/virtualenvs/&#xD;&#xA;  [10]: http://datascience.stackexchange.com/a/775&#xD;&#xA;  [11]: http://www.vagrantup.com/&#xD;&#xA;  [12]: http://deeplearning.net/software/pylearn2/#other-methods" />
  <row Id="2104" PostHistoryTypeId="2" PostId="821" RevisionGUID="5f4eee5d-793b-4126-b6d1-80031a66094a" CreationDate="2014-07-23T12:57:37.073" UserId="471" Text="&quot;Because its there&quot;.&#xD;&#xA;&#xD;&#xA;The data has a seasonal pattern. So you model it. The data has a trend. So you model it. Maybe the data is correlated with the number of sunspots. So you model that. Eventually you hope to get nothing left to model than uncorrelated random noise.&#xD;&#xA;&#xD;&#xA;But I think you've screwed up your STL computation here. Your residuals are clearly not serially uncorrelated. I rather suspect you've not told the function that your &quot;seasonality&quot; is a 24-hour cycle rather than an annual one. But hey you haven't given us any code or data so we don't really have a clue what you've done, do we? What do you think &quot;seasonality&quot; even means here? Do you have any idea?&#xD;&#xA;&#xD;&#xA;Your data seems the have three peaks every 24 hours. Really? Is this 'gas'='gasoline'='petrol' or gas in some heating/electric generating system? Either way if you know a priori there's an 8 hour cycle, or an 8 hour cycle on top of a 24 hour cycle on top of what looks like a very high frequency one or two hour cycle you **put that in your model**.&#xD;&#xA;&#xD;&#xA;Actually you don't even say what your x-axis is so maybe its days and then I'd fit a daily cycle, a weekly cycle, and then an annual cycle. But given how it all changes at time=85 or so I'd not expect a model to do well on both sides of that.&#xD;&#xA;&#xD;&#xA;With statistics (which is what this is, sorry to disappoint you but you're not a data scientist yet) you don't just robotically go &quot;And.. Now.. I.. Fit.. An... S TL model....&quot;. You look at your data, try and get some understanding, then propose a model, fit it, test it, and use the parameters it make inferences about the data. Fitting cyclic seasonal patterns is part of that.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2105" PostHistoryTypeId="2" PostId="822" RevisionGUID="cb1e7149-e45d-4b66-9ef6-16ac2b891812" CreationDate="2014-07-23T13:49:01.423" UserId="178" Text="In A/B testing, bias is handled very well by ensuring visitors are randomly assigned to either version A or version B of the site.  This creates independent samples drawn from the same population.  Because the groups are independent and, on average, only differ in the version of the site seen, the test measures the effect of the design decision.&#xD;&#xA;&#xD;&#xA;*Slight aside*: Now you might argue that the A group or B group may differ in some demographic.  That commonly happens by random chance.  To a certain degree this can be taken care of by covariate adjusted randomization.  It can also be taken care of by adding covariates to the model that tests the effect of the design decision.  It should be noted that there is still some discussion about the proper way to do this within the statistics community.  Essentially A/B testing is an application of a [Randomized Control Trial](http://en.wikipedia.org/wiki/Randomized_controlled_trial) to website design.  Some people disagree with adding covariates to the test.  Others, such as Frank Harrel (see [Regression Modeling Strategies](http://www.amazon.com/exec/obidos/ASIN/0387952322/)) argue for the use of covariates in such models.&#xD;&#xA;&#xD;&#xA;I would offer the following suggestions:&#xD;&#xA;&#xD;&#xA;- Design the study in advance so as to take care of as much sources of bias and variation as possible.   &#xD;&#xA;- Let the data speak for itself.  As you get more data (like about searches for David Beckham), let it dominate your assumptions about how the data should be (as how the posterior dominates the prior in Bayesian analysis when the sample size becomes large).    &#xD;&#xA;- Make sure your data matches the assumptions of the model." />
  <row Id="2106" PostHistoryTypeId="2" PostId="823" RevisionGUID="4436c95f-b0ca-4815-9197-d12c91557aa3" CreationDate="2014-07-23T14:04:31.057" UserId="95" Text="There was a recent furore with [facebook experimenting on their users to see if they could alter user's emotions](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840).&#xD;&#xA;&#xD;&#xA;Whilst I am not a professional data scientist I read about [data science ethics](http://columbiadatascience.com/2013/11/25/data-science-ethics/) from [Cathy O'Neill's book 'Doing Data Science'](http://shop.oreilly.com/product/0636920028529.do) and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science *accidentally*." />
  <row Id="2107" PostHistoryTypeId="1" PostId="823" RevisionGUID="4436c95f-b0ca-4815-9197-d12c91557aa3" CreationDate="2014-07-23T14:04:31.057" UserId="95" Text="How should moral ethics be applied in data science" />
  <row Id="2108" PostHistoryTypeId="3" PostId="823" RevisionGUID="4436c95f-b0ca-4815-9197-d12c91557aa3" CreationDate="2014-07-23T14:04:31.057" UserId="95" Text="&lt;social-network-analysis&gt;" />
  <row Id="2109" PostHistoryTypeId="5" PostId="823" RevisionGUID="8f4598af-4f9d-441e-80d2-0ca6b4bc903b" CreationDate="2014-07-23T14:13:03.410" UserId="95" Comment="added 179 characters in body" Text="There was a recent furore with [facebook experimenting on their users to see if they could alter user's emotions](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840).&#xD;&#xA;&#xD;&#xA;Whilst I am not a professional data scientist I read about [data science ethics](http://columbiadatascience.com/2013/11/25/data-science-ethics/) from [Cathy O'Neill's book 'Doing Data Science'](http://shop.oreilly.com/product/0636920028529.do) and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science *accidentally*.&#xD;&#xA;&#xD;&#xA;By the way I am not making any judgements here or saying that all data scientists behave like this, I'm interested in what is taught academically and practised professionally." />
  <row Id="2110" PostHistoryTypeId="6" PostId="823" RevisionGUID="5c008180-a822-4d9d-819f-706a0bebb6bb" CreationDate="2014-07-23T14:34:58.483" UserId="178" Comment="Add ethics tag" Text="&lt;social-network-analysis&gt;&lt;ethics&gt;" />
  <row Id="2111" PostHistoryTypeId="24" PostId="823" RevisionGUID="5c008180-a822-4d9d-819f-706a0bebb6bb" CreationDate="2014-07-23T14:34:58.483" Comment="Proposed by 178 approved by 95 edit id of 120" />
  <row Id="2112" PostHistoryTypeId="2" PostId="824" RevisionGUID="d7612507-cdbc-42e6-9399-807751af606f" CreationDate="2014-07-23T15:46:48.977" UserId="1279" Text="(too long for a comment)&#xD;&#xA;&#xD;&#xA;Basically, @Emre's answer is correct: simple correlation matrix and cosine distance should work well*. There's one subtlety, though - job titles are too short to carry important context. Let me explain this. &#xD;&#xA;&#xD;&#xA;Imagine LinkedIn profiles (which is pretty good source for data). Normally, they contain 4-10 sentences describing person's skills and qualifications. It's pretty likely that you find phrases like &quot;lead data scientist&quot; and &quot;professional knowledge of Matlab and R&quot; in a same profile, but it's very unlikely to also see &quot;junior Java developer&quot; in it. So we may say that &quot;lead&quot; and &quot;professional&quot; (as well as &quot;data scientist&quot; and &quot;Matlab&quot; and &quot;R&quot;) often occur in same contexts, but they are rarely found together with &quot;junior&quot; and &quot;Java&quot;. &#xD;&#xA;&#xD;&#xA;Co-occurrence matrix shows exactly this. The more 2 words occur in same context, the more similar their vectors in the matrix will look like. And cosine distance is just a good way to measure this similarity. &#xD;&#xA;&#xD;&#xA;But what about job titles? Normally they are much shorter and don't actually create enough context to catch similarities. Luckily, you don't need source data to be titles themselves - you need to find similarities between skills in general, not specifically in titles. So you can simply build co-occurrence matrix from (long) profiles and then use it to measure similarity of titles. &#xD;&#xA;&#xD;&#xA;*** - in fact, it's already worked for me on a similar project. " />
  <row Id="2113" PostHistoryTypeId="2" PostId="825" RevisionGUID="70ac2f67-50c9-468e-88c8-b43878fdf582" CreationDate="2014-07-24T00:01:40.760" UserId="1192" Text="Most vehicle license/number plate extractors I've found involve reading a plate from an image (OCR) but I'm interested in something that could tag instances of license plates in a body of text. Are there any such annotators out there?" />
  <row Id="2114" PostHistoryTypeId="1" PostId="825" RevisionGUID="70ac2f67-50c9-468e-88c8-b43878fdf582" CreationDate="2014-07-24T00:01:40.760" UserId="1192" Text="Are there any annotators or Named Entity Recognition for license plate numbers?" />
  <row Id="2115" PostHistoryTypeId="3" PostId="825" RevisionGUID="70ac2f67-50c9-468e-88c8-b43878fdf582" CreationDate="2014-07-24T00:01:40.760" UserId="1192" Text="&lt;text-mining&gt;" />
  <row Id="2116" PostHistoryTypeId="2" PostId="826" RevisionGUID="0757c84d-ab18-419f-bb5f-c180b7df4673" CreationDate="2014-07-24T00:34:41.130" UserId="325" Text="This can be done using `regular expressions`. &#xD;&#xA;&#xD;&#xA;2 letters followed by a number (\d denotes digits) would be&#xD;&#xA;&#xD;&#xA;    [A-Z]{2} \d*&#xD;&#xA;&#xD;&#xA;2 or 3 letters followed by a number is&#xD;&#xA;    &#xD;&#xA;    [A-Z]{2,3} \d*" />
  <row Id="2117" PostHistoryTypeId="2" PostId="827" RevisionGUID="94a34d6c-8887-48b9-a501-e20000a6057b" CreationDate="2014-07-24T02:23:23.703" UserId="548" Text="There are a lot of pretty decent tools out there for text annotation in general, and given the broad nature of the task you're approaching (license plates are about as general as words), the annotation tools you are looking at should probably come from the more classical tools for annotation.&#xD;&#xA;&#xD;&#xA;There was actually a pretty good discussion about annotation tools on [this][1] question, which should actually apply to this problem. The most relied-upon thing in annotation right now is probably `brat`. You can learn more about `brat` [here][2].&#xD;&#xA;&#xD;&#xA;Hope that helps! Let me know if you've got any more questions.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/223/how-to-annotate-text-documents-with-meta-data/404#404&#xD;&#xA;  [2]: http://brat.nlplab.org/" />
  <row Id="2118" PostHistoryTypeId="2" PostId="828" RevisionGUID="7fae1fe0-4572-4e9f-b1b4-7cffb4bb80b9" CreationDate="2014-07-24T03:31:32.087" UserId="192" Text="Reporting back: I ended up coding graphml and using yEd for visualization (just because I am familiar with this combination. I bet gephi or graphviz would work fine and might even be better). Since I computed the location of all nodes, memory was not such big of an issue. Coding graphml is a little easier comparing to coding svg, since I don't have to explicitly specify the placement of edges." />
  <row Id="2119" PostHistoryTypeId="2" PostId="829" RevisionGUID="e5f6c137-7351-44ab-86d2-7587a2441992" CreationDate="2014-07-24T06:26:07.290" UserId="1314" Text="While running the below pig script I am getting error in line4: &#xD;&#xA;If it is GROUP then I am getting error.&#xD;&#xA;If I change from 'GROUP' TO 'group' in line4, then the script is running.&#xD;&#xA;What is the difference between group and GROUP.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );&#xD;&#xA;&#xD;&#xA;WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;&#xD;&#xA;&#xD;&#xA;WORDSGROUPED = GROUP WORDS BY ZIPS;&#xD;&#xA;&#xD;&#xA;WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);&#xD;&#xA;&#xD;&#xA;WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;&#xD;&#xA;&#xD;&#xA;DUMP WORDSSORT;" />
  <row Id="2120" PostHistoryTypeId="1" PostId="829" RevisionGUID="e5f6c137-7351-44ab-86d2-7587a2441992" CreationDate="2014-07-24T06:26:07.290" UserId="1314" Text="Pig script code error?" />
  <row Id="2121" PostHistoryTypeId="3" PostId="829" RevisionGUID="e5f6c137-7351-44ab-86d2-7587a2441992" CreationDate="2014-07-24T06:26:07.290" UserId="1314" Text="&lt;bigdata&gt;&lt;hadoop&gt;" />
  <row Id="2122" PostHistoryTypeId="2" PostId="830" RevisionGUID="5c1d608a-93c8-42cc-bdd1-00b62609ea35" CreationDate="2014-07-24T07:00:13.187" UserId="471" Text="'group' in strictly lower case in the FOREACH is the thing you are looping/grouping over.&#xD;&#xA;&#xD;&#xA;http://squarecog.wordpress.com/2010/05/11/group-operator-in-apache-pig/ says:&#xD;&#xA;&#xD;&#xA;&gt; When you group a relation, the result is a new relation with two&#xD;&#xA;&gt; columns: “group” and the name of the original relation.&#xD;&#xA;&#xD;&#xA;Column names are case sensitive, so you have to use lower-case 'group' in your FOREACH.&#xD;&#xA;&#xD;&#xA;'GROUP' in upper case is the grouping operator. You can't mix them. So don't do that.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2124" PostHistoryTypeId="2" PostId="831" RevisionGUID="b739a6e5-29fd-4fee-8594-f21b7a5ec60b" CreationDate="2014-07-24T07:38:07.163" UserId="366" Text="Have you heard of the &quot;Data Science Association&quot;? Do you expect it to become a professional body like the Actuaries Institute?&#xD;&#xA;If yes, then why?&#xD;&#xA;If no, then why not and do you see anyone else becoming the professional body?&#xD;&#xA;&#xD;&#xA;Lastly, is this question &quot;on-topic&quot; ?" />
  <row Id="2125" PostHistoryTypeId="1" PostId="831" RevisionGUID="b739a6e5-29fd-4fee-8594-f21b7a5ec60b" CreationDate="2014-07-24T07:38:07.163" UserId="366" Text="Data Science Association?" />
  <row Id="2126" PostHistoryTypeId="3" PostId="831" RevisionGUID="b739a6e5-29fd-4fee-8594-f21b7a5ec60b" CreationDate="2014-07-24T07:38:07.163" UserId="366" Text="&lt;knowledge-base&gt;" />
  <row Id="2127" PostHistoryTypeId="5" PostId="831" RevisionGUID="4e954e01-3262-4ffe-8850-6acfba11569c" CreationDate="2014-07-24T09:32:20.100" UserId="366" Comment="Added URL" Text="Have you heard of the &quot;Data Science Association&quot;? &#xD;&#xA;&lt;br&gt;URL: http://www.datascienceassn.org/&#xD;&#xA;&lt;br&gt;Do you expect it to become a professional body like the Actuaries Institute?&#xD;&#xA;&lt;br&gt;If yes, then why?&#xD;&#xA;&lt;br&gt;If no, then why not and do you see anyone else becoming the professional body?&#xD;&#xA;&#xD;&#xA;Lastly, is this question &quot;on-topic&quot; ?" />
  <row Id="2129" PostHistoryTypeId="34" PostId="224" RevisionGUID="0e804b2f-058f-4559-b7d8-7414ce7803b6" CreationDate="2014-07-24T12:43:43.173" UserId="-1" Comment="5" />
  <row Id="2130" PostHistoryTypeId="2" PostId="832" RevisionGUID="5241b6e8-ba5c-4262-9f3c-4787ca42cc5b" CreationDate="2014-07-24T14:04:09.533" UserId="2702" Text="My data contains a set of start times and duration for an action. I would like to plot this so that for a given time slice I can see how many actions are active. I'm currently thinking of this as a histogram with time on the x axis and number of active actions on the y axis.&#xD;&#xA;&#xD;&#xA;My question is, how should I adjust the data so that this is able to be plotted?&#xD;&#xA;&#xD;&#xA;The times for an action can be between 2 seconds and a minute. Ideally a single plot would be able to show hours of data. The accuracy of the data is in milliseconds.&#xD;&#xA;&#xD;&#xA;In the past the way that I have done this is to count for each second how many actions started , ended, or were active. This gave me a count of active actions for each second. The issue I found with this technique was that it made it difficult to adjust the time slice that I was looking at. Looking at a time slice of a minute was difficult to compute and looking at time slices of less than a second was impossible.&#xD;&#xA;&#xD;&#xA;I'm open to any advice on how to think about this issue.&#xD;&#xA;&#xD;&#xA;Thanks in advance!" />
  <row Id="2131" PostHistoryTypeId="1" PostId="832" RevisionGUID="5241b6e8-ba5c-4262-9f3c-4787ca42cc5b" CreationDate="2014-07-24T14:04:09.533" UserId="2702" Text="How do you plot overlapping durations?" />
  <row Id="2132" PostHistoryTypeId="3" PostId="832" RevisionGUID="5241b6e8-ba5c-4262-9f3c-4787ca42cc5b" CreationDate="2014-07-24T14:04:09.533" UserId="2702" Text="&lt;visualization&gt;" />
  <row Id="2133" PostHistoryTypeId="2" PostId="833" RevisionGUID="0821a23b-00b8-4a4e-bc30-9c7620167b6a" CreationDate="2014-07-24T21:14:45.793" UserId="325" Text="This can be done in `R` using `ggplot`. Based on [this][1] question, it could be done with this code where `limits` is the date range of the plot.&#xD;&#xA;&#xD;&#xA;    tasks &lt;- c(&quot;Task1&quot;, &quot;Task2&quot;)&#xD;&#xA;    dfr &lt;- data.frame(&#xD;&#xA;    name        = factor(tasks, levels = tasks),&#xD;&#xA;    start.date  = c(&quot;2014-08-07 09:03:25.815&quot;, &quot;2014-08-07 09:03:25.956&quot;),&#xD;&#xA;    end.date    = c(&quot;2014-08-07 09:03:28.300&quot;, &quot;2014-08-07 09:03:30.409&quot;)&#xD;&#xA;    )&#xD;&#xA;&#xD;&#xA;    mdfr &lt;- melt(dfr, measure.vars = c(&quot;start.date&quot;, &quot;end.date&quot;))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    mdfr$time&lt;-as.POSIXct(mdfr$value)&#xD;&#xA;&#xD;&#xA;    ggplot(mdfr, aes(time,name)) + &#xD;&#xA;    geom_line(size = 6) +&#xD;&#xA;    xlab(&quot;&quot;) + ylab(&quot;&quot;) +&#xD;&#xA;    theme_bw()+&#xD;&#xA;    scale_x_datetime(breaks=date_breaks(&quot;2 sec&quot;),&#xD;&#xA;    limits = as.POSIXct(c('2014-08-07 09:03:24','2014-08-07 09:03:29')))&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stackoverflow.com/questions/18102224/drawing-gantt-charts-with-r-to-sub-second-accuracy&#xD;&#xA;  [2]: http://i.stack.imgur.com/Bwwod.png" />
  <row Id="2134" PostHistoryTypeId="5" PostId="224" RevisionGUID="89284752-2d65-435f-a9ea-32bde043d010" CreationDate="2014-07-24T22:08:39.407" UserId="322" Comment="correct misspelled tag, remove tag from title, grammar" Text="The output of my word alignment file looks as such:&#xD;&#xA;&#xD;&#xA;    I wish to say with regard to the initiative of the Portuguese Presidency that we support the spirit and the political intention behind it .	In bezug auf die Initiative der portugiesischen Präsidentschaft möchte ich zum Ausdruck bringen , daß wir den Geist und die politische Absicht , die dahinter stehen , unterstützen .	0-0 5-1 5-2 2-3 8-4 7-5 11-6 12-7 1-8 0-9 9-10 3-11 10-12 13-13 13-14 14-15 16-16 17-17 18-18 16-19 20-20 21-21 19-22 19-23 22-24 22-25 23-26 15-27 24-28&#xD;&#xA;    It may not be an ideal initiative in terms of its structure but we accept Mr President-in-Office , that it is rooted in idealism and for that reason we are inclined to support it .	Von der Struktur her ist es vielleicht keine ideale Initiative , aber , Herr amtierender Ratspräsident , wir akzeptieren , daß sie auf Idealismus fußt , und sind deshalb geneigt , sie mitzutragen .	0-0 11-2 8-3 0-4 3-5 1-6 2-7 5-8 6-9 12-11 17-12 15-13 16-14 16-15 17-16 13-17 14-18 17-19 18-20 19-21 21-22 23-23 21-24 26-25 24-26 29-27 27-28 30-29 31-30 33-31 32-32 34-33&#xD;&#xA;&#xD;&#xA;How can I produce the phrase tables that are used by MOSES from this output?" />
  <row Id="2135" PostHistoryTypeId="4" PostId="224" RevisionGUID="89284752-2d65-435f-a9ea-32bde043d010" CreationDate="2014-07-24T22:08:39.407" UserId="322" Comment="correct misspelled tag, remove tag from title, grammar" Text="How to get phrase tables from word alignments?" />
  <row Id="2136" PostHistoryTypeId="6" PostId="224" RevisionGUID="89284752-2d65-435f-a9ea-32bde043d010" CreationDate="2014-07-24T22:08:39.407" UserId="322" Comment="correct misspelled tag, remove tag from title, grammar" Text="&lt;moses&gt;&lt;alignment&gt;&lt;machine-translation&gt;" />
  <row Id="2137" PostHistoryTypeId="24" PostId="224" RevisionGUID="89284752-2d65-435f-a9ea-32bde043d010" CreationDate="2014-07-24T22:08:39.407" Comment="Proposed by 322 approved by 122 edit id of 122" />
  <row Id="2138" PostHistoryTypeId="2" PostId="834" RevisionGUID="8c0c8a4a-1f62-4f97-95d2-badf93fa8cb9" CreationDate="2014-07-25T00:58:12.253" UserId="1131" Text="I am trying to build a recommendation engine using collaborative filtering. I have the usual [user, movie, rating] information. I would like to incorporate an additional feature like 'language' or 'duration of movie'. I am not sure what techniques I could use for such a problem. Please suggest references or packages in python/R. " />
  <row Id="2139" PostHistoryTypeId="1" PostId="834" RevisionGUID="8c0c8a4a-1f62-4f97-95d2-badf93fa8cb9" CreationDate="2014-07-25T00:58:12.253" UserId="1131" Text="Recommending movies with additional features using collaborative filtering" />
  <row Id="2140" PostHistoryTypeId="3" PostId="834" RevisionGUID="8c0c8a4a-1f62-4f97-95d2-badf93fa8cb9" CreationDate="2014-07-25T00:58:12.253" UserId="1131" Text="&lt;python&gt;&lt;r&gt;&lt;recommendation&gt;" />
  <row Id="2141" PostHistoryTypeId="2" PostId="835" RevisionGUID="ec66ce2c-4838-47f1-bff5-67f74472cadf" CreationDate="2014-07-25T03:05:38.107" UserId="2711" Text="Consider, try, and perhaps even use multiple databases. It's not just a &quot;performance&quot; issue at play here. It's really going to come down to your requirements. How much data are you talking about? what kind of data? how fast do you need it? Are you more read heavy or write heavy?&#xD;&#xA;&#xD;&#xA;Here's one thing you can't do in a SQL database: Calculate sentiment. http://www.slideshare.net/shift8/mongodb-machine-learning&#xD;&#xA;&#xD;&#xA;Of course the speed in that case may not be fast enough for your needs, but it is something that's possible. With some caching of specific aggregate values, it was quite acceptable even. Why would you do this? Convenience.&#xD;&#xA;&#xD;&#xA;Convenience really is something that you're going to be persuaded by. That's exactly why (in my opinion) NoSQL databases were created. Performance too of course, but I'm trying to discount benchmarks and focus more on other concerns.&#xD;&#xA;&#xD;&#xA;MongoDB (and some other NoSQL) databases have some very powerful features such as built-in map/reduce. This could result in a savings both in cost and time over using something like Hadoop. Or it could provide a prototype or MVP to launch a larger business.&#xD;&#xA;&#xD;&#xA;What about graph databases? They're &quot;NoSQL&quot; too. Look at databases like OrientDB. If you want to argue performance ...I don't think you're gonna show me a SQL database that's faster there =) ...and graph databases have some really amazing application based on what you need to do.&#xD;&#xA;&#xD;&#xA;Rule of technology (and the internet) don't get too comfortable with one thing. You're gonna be limited and set yourself up for failure." />
  <row Id="2142" PostHistoryTypeId="5" PostId="783" RevisionGUID="e96bc5f4-7efb-4b26-b067-406fdb91bbf7" CreationDate="2014-07-25T03:26:52.427" UserId="2604" Comment="Update 2014-07-25" Text="I want to plot the bytes from a disk image in order to understand a pattern in them. This is mainly an academic task, since I'm almost sure this pattern was created by a disk testing program, but I'd like to reverse-engineer it anyway.&#xD;&#xA;&#xD;&#xA;I already know that the pattern is aligned, with a periodicity of 256 characters.&#xD;&#xA;&#xD;&#xA;I can envision two ways of visualizing this information: either a 16x16 plane viewed through time (3 dimensions), where each pixel's color is the ASCII code for the character, or a 256 pixel line for each period (2 dimensions).&#xD;&#xA;&#xD;&#xA;This is a snapshot of the pattern (you can see more than one), seen through `xxd` (32x16):&#xD;&#xA;&#xD;&#xA;![Pattern to analyze][1]&#xD;&#xA;&#xD;&#xA;Either way, I am trying to find a way of visualizing this information. This probably isn't hard for anyone into signal analysis, but I can't seem to find a way using open-source software.&#xD;&#xA;&#xD;&#xA;I'd like to avoid Matlab or Mathematica and I'd prefer an answer in R, since I have been learning it recently, but nonetheless, any language is welcome.&#xD;&#xA;&#xD;&#xA;&lt;hr&gt;&#xD;&#xA;&#xD;&#xA;Update, 2014-07-25: given Emre's answer below, this is what the pattern looks like, given the first 30MB of the pattern, aligned at 512 instead of 256 (this alignment looks better):&#xD;&#xA;&#xD;&#xA;![Graphical pattern][2]&#xD;&#xA;&#xD;&#xA;Any further ideas are welcome!&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/zOFSK.gif&#xD;&#xA;  [2]: http://i.stack.imgur.com/4tDIA.png" />
  <row Id="2143" PostHistoryTypeId="2" PostId="836" RevisionGUID="6345363e-ae43-42ea-acfa-48bf8bc8dae3" CreationDate="2014-07-25T04:54:07.787" UserId="2452" Text="Here some resources that might be helpful:&#xD;&#xA;&#xD;&#xA;- **Recommenderlab** - a framework and open source software for developing and testing recommendation algorithms: http://lyle.smu.edu/IDA/recommenderlab. Corresponding `R` package `recommenderlab`: http://cran.r-project.org/package=recommenderlab.&#xD;&#xA;&#xD;&#xA;- The following blog post illustrates the use of `recommenderlab` package (which IMHO can be generalized for any open source recommendation engine) for building **movie recommendation** application, based on *collaborative filtering*: http://scn.sap.com/community/developer-center/hana/blog/2013/11/06/movie-recommendation-by-leveraging-r.&#xD;&#xA;&#xD;&#xA;- **Research on recommender systems** - a nice webpage with resources on the topic, maintained by Recommenderlab's lead developer Michael Hahsler: http://michael.hahsler.net/research/recommender.&#xD;&#xA;&#xD;&#xA;- **Mortar Recommendation Engine** - an open source customizable recommendation engine for `Hadoop` and `Pig`, written in `Python` and `Java`: https://github.com/mortardata/mortar-recsys. Company, sponsoring the development of this project, **Mortar Data**, offers general commercial *cloud platform* for development and hosting *data science* software projects, including ones based on the `Mortar Recommendation Engine` (development and hosting of public projects are free): http://www.mortardata.com. Mortar Data provides help in form of **public Q&amp;A forum** (https://answers.mortardata.com) as well as a **comprehensive tutorial** on building recommendation engine using open technologies (http://help.mortardata.com/data_apps/recommendation_engine).&#xD;&#xA;&#xD;&#xA;- **&quot;Introduction to Recommender Systems&quot;** - a relevant Coursera course (MOOC), which content and description provide additional resources on the topic: https://www.coursera.org/course/recsys.&#xD;&#xA;&#xD;&#xA;- **PredictionIO** - an open source machine learning server software, which allows building *data science applications*, including *recommendation systems*: http://prediction.io (source code is available on GitHub: https://github.com/PredictionIO). `PredictionIO` includes a built-in *recommendation engine* (http://docs.prediction.io/current/engines/itemrec/index.html) and supports a wide range of programming languages and frameworks via `RESTful` APIs as well as SDKs/plug-ins. PredictionIO maintains an `Amazon Machine Image` on **AWS Marketplace** for deploying applications on the AWS infrastructure: https://aws.amazon.com/marketplace/pp/B00ECGJYGE.&#xD;&#xA;&#xD;&#xA;- **Additional open source software projects**, relevant to the topic (discovered via `MLOSS` website on *machine learning* open source software: http://www.mloss.org):&#xD;&#xA;  + **Jubatus**: http://jubat.us/en&#xD;&#xA;  + **MyMediaLite**: http://mymedialite.net&#xD;&#xA;  + **TBEEF**: https://github.com/ChrisRackauckas/TBEEF&#xD;&#xA;  + **PREA**: http://prea.gatech.edu&#xD;&#xA;  + **CofiRank**: http://www.cofirank.org&#xD;&#xA;&#xD;&#xA;- The following relevant `R` **blog posts** are also interesting:&#xD;&#xA;  + &quot;Simple tools for building a recommendation engine&quot; (http://blog.revolutionanalytics.com/2012/04/simple-tools-for-building-a-recommendation-engine.html)&#xD;&#xA;  + &quot;Recommendation System in R&quot; (http://blog.yhathq.com/posts/recommender-system-in-r.html)" />
  <row Id="2144" PostHistoryTypeId="2" PostId="837" RevisionGUID="28b6fd61-3879-4923-abb2-e9630719940d" CreationDate="2014-07-25T12:59:00.850" UserId="471" Text="I would look at the `raster` package for this, which can read in raw binary data and present it as NxM grids. It can even extract subsets of large binary grids without having to read in the whole file (the R raster object itself is just a proxy to the data, not the data itself).&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2146" PostHistoryTypeId="36" PostId="838" RevisionGUID="9c875ded-47b3-4260-ae64-75b304d9e419" CreationDate="2014-07-25T15:02:21.217" UserId="-1" Comment="from http://stackoverflow.com/questions/24926631/pig-latin-code-error" />
  <row Id="2147" PostHistoryTypeId="36" PostId="839" RevisionGUID="453ebd8b-128d-40db-a6bb-64356fc16543" CreationDate="2014-07-25T15:02:21.217" UserId="-1" Comment="from http://stackoverflow.com/questions/24926631/pig-latin-code-error/24951968#24951968" />
  <row Id="2148" PostHistoryTypeId="2" PostId="839" RevisionGUID="291345e3-ddd8-4147-8ee8-6967e90ed122" CreationDate="2014-07-25T09:05:09.493" UserDisplayName="abhishek bafna" Text="When we do grouping of the data, Pig creates a new key named &quot;group&quot; and puts all the tuple matching that key into a bag and associates the bag with the key. So after the group operation schema of the grouped data will be something like &#xD;&#xA;&#xD;&#xA;    raw = load '$input' using PigStorage('\u0001') as (id1:int, name:chararray);&#xD;&#xA;    groupdata1 = group raw by (id1,name);  &#xD;&#xA;    describe groupdata1;&#xD;&#xA;    {group: (id1: int,name: chararray),raw: {(id1: int,name: chararray)}}&#xD;&#xA;&#xD;&#xA;The 'GROUP' in line 4 you are trying to access is one of the attribute of the schema from the last statement. These attribute name are case sensitive. It will produce the error saying it doesn't exist in the schema. So you need to use 'group' only to access it." />
  <row Id="2149" PostHistoryTypeId="2" PostId="838" RevisionGUID="cf2fa9f9-d89e-4edd-87a2-81485b9e48d0" CreationDate="2014-07-24T06:34:50.083" UserId="1314" Text="&#xD;&#xA;While running the below pig script I am getting error in line4: If it is `GROUP` then I am getting error. If I change from `GROUP` TO `group` in line4, then the script is running. What is the difference between group and GROUP.&#xD;&#xA;&#xD;&#xA;    LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );&#xD;&#xA;    &#xD;&#xA;    WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;&#xD;&#xA;    &#xD;&#xA;    WORDSGROUPED = GROUP WORDS BY ZIPS;&#xD;&#xA;    &#xD;&#xA;    WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);&#xD;&#xA;    &#xD;&#xA;    WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;&#xD;&#xA;    &#xD;&#xA;    DUMP WORDSSORT;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2150" PostHistoryTypeId="1" PostId="838" RevisionGUID="cf2fa9f9-d89e-4edd-87a2-81485b9e48d0" CreationDate="2014-07-24T06:34:50.083" UserId="1314" Text="Pig latin code error" />
  <row Id="2151" PostHistoryTypeId="3" PostId="838" RevisionGUID="cf2fa9f9-d89e-4edd-87a2-81485b9e48d0" CreationDate="2014-07-24T06:34:50.083" UserId="1314" Text="&lt;hadoop&gt;&lt;bigdata&gt;" />
  <row Id="2152" PostHistoryTypeId="10" PostId="838" RevisionGUID="002807ec-c52e-473c-8a79-dc01ecac858f" CreationDate="2014-07-25T16:26:00.903" UserId="21" Comment="101" Text="{&quot;OriginalQuestionIds&quot;:[829],&quot;Voters&quot;:[{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2153" PostHistoryTypeId="2" PostId="840" RevisionGUID="3b7901f6-ac11-4af8-8ba0-e607cde30c42" CreationDate="2014-07-25T17:18:21.393" UserId="974" Text="I've been trying to create a similarity matrix in Pandas from with a matrix multiplication operation on a document-term count matrix with 2264 rows and 20475 columns.&#xD;&#xA;&#xD;&#xA;The calculation completes in IPython but inspection shows the results all come back as NaN.&#xD;&#xA;&#xD;&#xA;I've also tried doing the same job in numpy, tried converting the original matrix to_sparse and even re-casting the values as integers, but still no joy.&#xD;&#xA;&#xD;&#xA;Can anyone suggest the best approach to tackle the problem?" />
  <row Id="2154" PostHistoryTypeId="1" PostId="840" RevisionGUID="3b7901f6-ac11-4af8-8ba0-e607cde30c42" CreationDate="2014-07-25T17:18:21.393" UserId="974" Text="How to fix similarity matrix in Pandas returning all NaNs?" />
  <row Id="2155" PostHistoryTypeId="3" PostId="840" RevisionGUID="3b7901f6-ac11-4af8-8ba0-e607cde30c42" CreationDate="2014-07-25T17:18:21.393" UserId="974" Text="&lt;pandas&gt;&lt;similarity&gt;" />
  <row Id="2156" PostHistoryTypeId="5" PostId="840" RevisionGUID="043ebd49-0390-4919-9620-f50045b27268" CreationDate="2014-07-25T17:56:59.177" UserId="974" Comment="Asked for code" Text="I've been trying to create a similarity matrix in Pandas from with a matrix multiplication operation on a document-term count matrix with 2264 rows and 20475 columns.&#xD;&#xA;&#xD;&#xA;The calculation completes in IPython but inspection shows the results all come back as NaN.&#xD;&#xA;&#xD;&#xA;I've also tried doing the same job in numpy, tried converting the original matrix to_sparse and even re-casting the values as integers, but still no joy.&#xD;&#xA;&#xD;&#xA;Can anyone suggest the best approach to tackle the problem?&#xD;&#xA;&#xD;&#xA;EDIT: Here's my code thus far:&#xD;&#xA;&#xD;&#xA;    path = &quot;../../reuters.db&quot;&#xD;&#xA;    %pylab inline&#xD;&#xA;    import pandas as pd&#xD;&#xA;    import numpy as np&#xD;&#xA;    import pandas.io.sql as psql&#xD;&#xA;    import sqlite3 as lite&#xD;&#xA;    con = lite.connect(path)&#xD;&#xA;    with con:&#xD;&#xA;        sql = &quot;SELECT * FROM Frequency&quot;&#xD;&#xA;        df = psql.frame_query(sql, con)&#xD;&#xA;        print df.shape&#xD;&#xA;    df = df.rename(columns={&quot;term&quot;:&quot;term_id&quot;, &quot;count&quot;:&quot;count_id&quot;})&#xD;&#xA;    pivoted = df.pivot('docid', 'term_id', 'count_id')&#xD;&#xA;    pivoted.to_sparse()&#xD;&#xA;    similarity_matrix = pivoted.dot(pivoted.T)" />
  <row Id="2157" PostHistoryTypeId="2" PostId="841" RevisionGUID="baa5ae44-d8a1-4123-a1dd-00af219f5a4f" CreationDate="2014-07-25T18:12:49.847" UserId="381" Text="Instead of collaborative filtering I would use the matrix factorization approach, wherein users and movies alike a represented by vectors of latent features whose dot products yield the ratings. Normally one merely selects the rank (number of features) without regard to what the features represent, and the algorithm does the rest. Like PCA, the result is not immediately interpretable but it yields good results. What you want to do is extend the movie matrix to include the additional features you mentioned and make sure that they stay fixed as the algorithm estimates the two matrices using regularizastion. The corresponding entries in the user matrix will be initialized randomly, then estimated by the matrix factorization algorithm. It's a versatile and performant approach but it takes some understanding of machine learning, or linear algebra at least.&#xD;&#xA;&#xD;&#xA;I saw a nice ipython notebook a while back but I can't find it right now, so I'll refer you to [another one](http://nbviewer.ipython.org/github/diktat/CPSC540machinelearning/blob/master/1.4%20Collaborative%20Filtering%20for%20Movie%20Recommendation.ipynb) which, while not as nice, still clarifies some of the maths." />
  <row Id="2158" PostHistoryTypeId="5" PostId="832" RevisionGUID="c86effde-d559-410b-a2d3-88cfccdf9787" CreationDate="2014-07-25T18:30:26.490" UserId="2702" Comment="added 87 characters in body" Text="My data contains a set of start times and duration for an action. I would like to plot this so that for a given time slice I can see how many actions are active. I'm currently thinking of this as a histogram with time on the x axis and number of active actions on the y axis.&#xD;&#xA;&#xD;&#xA;My question is, how should I adjust the data so that this is able to be plotted?&#xD;&#xA;&#xD;&#xA;The times for an action can be between 2 seconds and a minute. And, at any given time I would estimate there could be about 100 actions taking place. Ideally a single plot would be able to show hours of data. The accuracy of the data is in milliseconds.&#xD;&#xA;&#xD;&#xA;In the past the way that I have done this is to count for each second how many actions started , ended, or were active. This gave me a count of active actions for each second. The issue I found with this technique was that it made it difficult to adjust the time slice that I was looking at. Looking at a time slice of a minute was difficult to compute and looking at time slices of less than a second was impossible.&#xD;&#xA;&#xD;&#xA;I'm open to any advice on how to think about this issue.&#xD;&#xA;&#xD;&#xA;Thanks in advance!" />
  <row Id="2159" PostHistoryTypeId="2" PostId="842" RevisionGUID="9fd8d0a3-ae7f-428e-8f1f-3c796e93733e" CreationDate="2014-07-25T18:36:31.340" UserId="2725" Text="I don't know if this is a right place to ask this question, but a community dedicated to Data Science should be the most apt place in my opinion.&#xD;&#xA;&#xD;&#xA;I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.&#xD;&#xA;&#xD;&#xA;A mix of Data Science and Machine learning would be great.&#xD;&#xA;&#xD;&#xA;A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial." />
  <row Id="2160" PostHistoryTypeId="1" PostId="842" RevisionGUID="9fd8d0a3-ae7f-428e-8f1f-3c796e93733e" CreationDate="2014-07-25T18:36:31.340" UserId="2725" Text="Data Science Project Ideas" />
  <row Id="2161" PostHistoryTypeId="3" PostId="842" RevisionGUID="9fd8d0a3-ae7f-428e-8f1f-3c796e93733e" CreationDate="2014-07-25T18:36:31.340" UserId="2725" Text="&lt;machine-learning&gt;&lt;bigdata&gt;&lt;dataset&gt;" />
  <row Id="2162" PostHistoryTypeId="2" PostId="843" RevisionGUID="01e4aa54-eeb0-4a83-a50e-5a462a1fe7f4" CreationDate="2014-07-25T20:50:14.540" UserId="2452" Text="I would try to analyze and solve one or more of the problems published on **Kaggle Competitions** (https://www.kaggle.com/competitions). Note that the competitions are grouped by their expected *complexity*, from `101` (bottom of the list) to `Research` and `Featured` (top of the list). A color-coded vertical band is a *visual guideline* for grouping. You can **assess time** you could spend on a project by **adjusting** the expected *length* of corresponding competition, based on your *skills* and *experience*.&#xD;&#xA;&#xD;&#xA;A number of **data science project ideas** can be found by browsing the following `Coursolve` webpage: https://www.coursolve.org/browse-needs?query=Data%20Science.&#xD;&#xA;&#xD;&#xA;If you have skills and desire to work on a **real data science project**, focused on **social impacts**, visit `DataKind` projects page: http://www.datakind.org/projects. More projects with social impacts focus can be found at `Data Science for Social Good` fellowship webpage: http://dssg.io/projects.&#xD;&#xA;&#xD;&#xA;**Science Project Ideas** page at `My NASA Data` site looks like another place to visit for inspiration: http://mynasadata.larc.nasa.gov/804-2.&#xD;&#xA;&#xD;&#xA;If you would like to use **open data**, this long list of applications on `Data.gov` can provide you with some interesting *data science* project ideas: http://www.data.gov/applications." />
  <row Id="2163" PostHistoryTypeId="2" PostId="844" RevisionGUID="5137cf9c-796e-49f8-b06e-6661ab5c7039" CreationDate="2014-07-25T21:03:44.663" UserId="2726" Text="so I'm using Spark to do sentiment analysis, and I keep getting errors with the serializers it uses (I think) to pass python objects around.&#xD;&#xA;&#xD;&#xA;    PySpark worker failed with exception:&#xD;&#xA;    Traceback (most recent call last):&#xD;&#xA;      File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/worker.py&quot;, line 77, in main&#xD;&#xA;        serializer.dump_stream(func(split_index, iterator), outfile)&#xD;&#xA;      File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 191, in dump_stream&#xD;&#xA;        self.serializer.dump_stream(self._batched(iterator), stream)&#xD;&#xA;      File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 123, in dump_stream&#xD;&#xA;        for obj in iterator:&#xD;&#xA;      File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 180, in _batched&#xD;&#xA;        for item in iterator:&#xD;&#xA;    TypeError: __init__() takes exactly 3 arguments (2 given)&#xD;&#xA;&#xD;&#xA;and the code for serializers is available [here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://spark.apache.org/docs/latest/api/python/pyspark.serializers-pysrc.html#PickleSerializer.dumps" />
  <row Id="2164" PostHistoryTypeId="1" PostId="844" RevisionGUID="5137cf9c-796e-49f8-b06e-6661ab5c7039" CreationDate="2014-07-25T21:03:44.663" UserId="2726" Text="Using Apache Spark to do ML. Keep getting serializing errors" />
  <row Id="2165" PostHistoryTypeId="3" PostId="844" RevisionGUID="5137cf9c-796e-49f8-b06e-6661ab5c7039" CreationDate="2014-07-25T21:03:44.663" UserId="2726" Text="&lt;bigdata&gt;&lt;hadoop&gt;&lt;distributed&gt;&lt;scalability&gt;&lt;map-reduce&gt;" />
  <row Id="2166" PostHistoryTypeId="5" PostId="844" RevisionGUID="bb7414b8-3b72-47fb-bdc3-ad29d9037c65" CreationDate="2014-07-25T21:11:03.587" UserId="2726" Comment="added 120 characters in body" Text="so I'm using Spark to do sentiment analysis, and I keep getting errors with the serializers it uses (I think) to pass python objects around.&#xD;&#xA;&#xD;&#xA;    PySpark worker failed with exception:&#xD;&#xA;    Traceback (most recent call last):&#xD;&#xA;      File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/worker.py&quot;, line 77, in main&#xD;&#xA;        serializer.dump_stream(func(split_index, iterator), outfile)&#xD;&#xA;      File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 191, in dump_stream&#xD;&#xA;        self.serializer.dump_stream(self._batched(iterator), stream)&#xD;&#xA;      File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 123, in dump_stream&#xD;&#xA;        for obj in iterator:&#xD;&#xA;      File &quot;/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py&quot;, line 180, in _batched&#xD;&#xA;        for item in iterator:&#xD;&#xA;    TypeError: __init__() takes exactly 3 arguments (2 given)&#xD;&#xA;&#xD;&#xA;and the code for serializers is available [here][1]&#xD;&#xA;&#xD;&#xA;and my code is [here][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://spark.apache.org/docs/latest/api/python/pyspark.serializers-pysrc.html#PickleSerializer.dumps&#xD;&#xA;  [2]: https://github.com/seashark97/Scalable-Sentiment-Analysis/blob/master/spark_test.py" />
  <row Id="2167" PostHistoryTypeId="2" PostId="845" RevisionGUID="c989dcd9-2f94-41e9-9e9d-22c2cb2a23f5" CreationDate="2014-07-26T00:11:03.637" UserId="1279" Text="Most often serialization error in (Py)Spark means that some part of your distributed code (e.g. functions passed to `map`) has **dependencies** on **non-serializable data**. Consider following example: &#xD;&#xA;&#xD;&#xA;    rdd = sc.parallelize(range(5))&#xD;&#xA;    rdd = rdd.map(lambda x: x + 1)&#xD;&#xA;    rdd.collect()&#xD;&#xA;&#xD;&#xA;Here you have distributed collection and lambda function to send to all workers. Lambda is completely self-containing, so it's easy to copy its binary representation to other nodes without any worries. &#xD;&#xA;&#xD;&#xA;Now let's make things a bit more interesting: &#xD;&#xA;&#xD;&#xA;    f = open(&quot;/etc/hosts&quot;)&#xD;&#xA;    rdd = sc.parallelize(range(100))&#xD;&#xA;    rdd = rdd.map(lambda x: f.read())&#xD;&#xA;    rdd.collect()&#xD;&#xA;    f.close()&#xD;&#xA;&#xD;&#xA;Boom! Strange error in serialization module! What just happened is that we had attempted to pass `f`, which is a file object, to workers. Obviously, file object is a handle to _local_ data and thus cannot be sent to other machines. &#xD;&#xA;&#xD;&#xA;----&#xD;&#xA;&#xD;&#xA;So what's happening in your specific code? Without actual data and knowing record format, I cannot debug it completely, but I guess that problem goes from this line: &#xD;&#xA;&#xD;&#xA;    def vectorizer(text, vocab=vocab_dict):&#xD;&#xA;&#xD;&#xA;In Python, keyword arguments are initialized when function is called for the first time. When you call `sc.parallelize(...).map(vectorizer)` just after its definition, `vocab_dict` is available _locally_, but _remote_ workers know absolutely nothing about it. Thus function is called with fewer parameters than it expects which results in `__init__() takes exactly 3 arguments (2 given)` error. &#xD;&#xA;&#xD;&#xA;Also note, that you follow very bad pattern of  `sc.parallelize(...)...collect()` calls. First you spread your collection to entire cluster, do some computations, and then pull the result. But sending data back and forth is pretty pointless here. Instead, you can just do these computations locally, and run Spark's parallel processes only when you work with really big datasets (like you main `amazon_dataset`, I guess)." />
  <row Id="2168" PostHistoryTypeId="2" PostId="846" RevisionGUID="07f30f72-4e58-45a6-be0f-29e94fa2a687" CreationDate="2014-07-26T01:12:08.167" UserId="1279" Text="Take something from your everyday life. Create predictor of traffic jams in your region, craft personalised music recommender, analyse car market, etc. Choose **real problem** that you **want to solve** - this will not only keep you motivated, but also make you go through the whole development circle from data collection to hypothesis testing. " />
  <row Id="2170" PostHistoryTypeId="2" PostId="847" RevisionGUID="d47390b6-4187-4cee-af47-7b7bc21cc08c" CreationDate="2014-07-26T08:37:52.823" UserId="816" Text="[Introduction to Data Science][1] course that is being run on Coursera now includes real-world project assignment where companies post their problems and students are encouraged to solve them. This is done via [coursolve.com][2] (already mentioned here).&#xD;&#xA;&#xD;&#xA;More information [here][3] (you have to be enrolled in the course to see that link)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.coursera.org/course/datasci&#xD;&#xA;  [2]: https://www.coursolve.org/&#xD;&#xA;  [3]: https://class.coursera.org/datasci-002/wiki/OptionalRealWorldProject" />
  <row Id="2172" PostHistoryTypeId="10" PostId="842" RevisionGUID="808152a4-d0eb-4487-83aa-814da2369d65" CreationDate="2014-07-26T15:09:56.510" UserId="62" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="2173" PostHistoryTypeId="6" PostId="59" RevisionGUID="7190494a-16dd-46c3-a3bc-e485e1e28ec7" CreationDate="2014-07-26T15:10:51.000" UserId="62" Comment="Tag not relevant." Text="&lt;hadoop&gt;&lt;r&gt;" />
  <row Id="2174" PostHistoryTypeId="10" PostId="831" RevisionGUID="8e5926ce-a596-4081-874c-78ff75e3c1c1" CreationDate="2014-07-26T15:11:29.480" UserId="62" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="2175" PostHistoryTypeId="2" PostId="848" RevisionGUID="a8313402-ac0a-4a4d-9819-19ae6f808b7b" CreationDate="2014-07-26T16:04:25.363" UserId="924" Text="K-means is not the most appropriate algorithm here.&#xD;&#xA;&#xD;&#xA;The reason is that k-means is designed to **minimize variance**. This is, of course, appearling from a statistical and signal procssing point of view, but your data is not &quot;linear&quot;.&#xD;&#xA;&#xD;&#xA;Since your data is in latitude, longitude format, you should use an algorithm that can handle *arbitrary* distance functions, in particular geodetic distance functions. Hierarchical clustering, PAM, CLARA, and DBSCAN are popular examples of this.&#xD;&#xA;&#xD;&#xA;The problems of k-means are easy to see when you consider points close to the +-180 degrees wrap-around. Even if you hacked k-means to use Haversine distance, in the update step when it recomputes the *mean* the result will be badly screwed. **Worst case is, k-means will never converge!**" />
  <row Id="2176" PostHistoryTypeId="2" PostId="849" RevisionGUID="29028da1-a74f-42f6-9220-64a790fb9fd3" CreationDate="2014-07-26T21:03:13.470" UserId="2575" Text="Since you want to show so much data, I think that your best choice is going interactive. Check out this [demo][1], it is close to what you want but not quite. &#xD;&#xA;&#xD;&#xA;It is very difficult to show a lot of data in a single diagram, together with the finest details and the bird-eyes view. But you can let the user interact and look for the details. To show counts, one option is to use color-coding. Take a look at this image (code [here][2]): ![image][3].&#xD;&#xA;&#xD;&#xA;Here rgb channels have been used to encode (the logarithm of) the number of active events (red), events starting (green) and events ending (blue) for windows of different size. The X axis is time, and the Y axis represents window size, that is, duration. Thus, a point with coordinates (10, 4) represents the interval of time that goes from 10 to 14. &#xD;&#xA;&#xD;&#xA;To make a lot of data more detailed, it could be a good idea to make the diagram zoomable (like in the demo before), and to give the user the possibility of visualizing just one channel/magnitude.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://demo.zunzun.se/intervals/index.html&#xD;&#xA;  [2]: https://gist.github.com/dsign/4c598cfbfc81f6d491d5&#xD;&#xA;  [3]: http://i.stack.imgur.com/xmgHi.png" />
  <row Id="2182" PostHistoryTypeId="5" PostId="797" RevisionGUID="c995dfa4-af19-48d6-a7bf-e9276c2311fb" CreationDate="2014-07-27T03:34:44.953" UserId="2523" Comment="Copy edited (e.g. ref. &lt;http://en.wikipedia.org/wiki/NoSQL&gt;). Added some context. Removed unnecessary formatting. " Text="To be perfectly honest, most NoSQL databases are not very well suited to applications in big data. For the vast majority of all big data applications, the performance of MongoDB compared to a relational database like [MySQL][1] is [significantly][2] is poor enough to warrant staying away from something like MongoDB entirely.&#xD;&#xA;&#xD;&#xA;With that said, there are a couple of really useful properties of NoSQL databases that certainly work in your favor when you're working with large data sets, though the chance of those benefits outweighing the generally poor performance of NoSQL compared to [SQL][3] for read-intensive operations (most similar to typical big data use cases) is low.&#xD;&#xA;&#xD;&#xA; - **No Schema** - If you're working with a lot of unstructured data, it might be hard to actually decide on and rigidly apply a schema. NoSQL databases in general are very supporting of this, and will allow you to insert schema-less documents on the fly, which is certainly not something an SQL database will support.&#xD;&#xA; - **[JSON][4]** - If you happen to be working with JSON-style documents instead of with [CSV][5] files, then you'll see a lot of advantage in using something like MongoDB for a database-layer. Generally the workflow savings don't outweigh the increased query-times though.&#xD;&#xA; - **Ease of Use** - I'm not saying that SQL databases are always hard to use, or that [Cassandra][6] is the easiest thing in the world to set up, but in general NoSQL databases are easier to set up and use than SQL databases. MongoDB is a particularly strong example of this, known for being one of the easiest database layers to use (outside of [SQLite][7]). SQL also deals with a lot of normalization and there's a large legacy of SQL best practices that just generally bogs down the development process.&#xD;&#xA;&#xD;&#xA;Personally I might suggest you also check out [graph databases][8] such as [Neo4j][9] that show really good performance for certain types of queries if you're looking into picking out a backend for your data science applications.&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/MySQL&#xD;&#xA;  [2]: http://www.moredevs.ro/mysql-vs-mongodb-performance-benchmark/&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/SQL&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/JSON&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/Comma-separated_values&#xD;&#xA;  [6]: http://en.wikipedia.org/wiki/Apache_Cassandra&#xD;&#xA;  [7]: http://en.wikipedia.org/wiki/SQLite&#xD;&#xA;  [8]: http://en.wikipedia.org/wiki/Graph_database&#xD;&#xA;  [9]: http://en.wikipedia.org/wiki/Neo4j&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2183" PostHistoryTypeId="24" PostId="797" RevisionGUID="c995dfa4-af19-48d6-a7bf-e9276c2311fb" CreationDate="2014-07-27T03:34:44.953" Comment="Proposed by 2523 approved by 548 edit id of 125" />
  <row Id="2184" PostHistoryTypeId="6" PostId="808" RevisionGUID="1cbf3b5c-17e1-4824-bc37-86b23836c446" CreationDate="2014-07-27T03:35:00.173" UserId="553" Comment="adding a career tag" Text="&lt;machine-learning&gt;&lt;bigdata&gt;&lt;statistics&gt;&lt;career&gt;" />
  <row Id="2185" PostHistoryTypeId="24" PostId="808" RevisionGUID="1cbf3b5c-17e1-4824-bc37-86b23836c446" CreationDate="2014-07-27T03:35:00.173" Comment="Proposed by 553 approved by 434, 548 edit id of 121" />
  <row Id="2186" PostHistoryTypeId="5" PostId="842" RevisionGUID="2551aa1b-7f01-4ba1-8177-fbc868339a91" CreationDate="2014-07-27T03:35:06.853" UserId="1352" Comment="apt = appropriate" Text="I don't know if this is a right place to ask this question, but a community dedicated to Data Science should be the most appropriate place in my opinion.&#xD;&#xA;&#xD;&#xA;I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.&#xD;&#xA;&#xD;&#xA;A mix of Data Science and Machine learning would be great.&#xD;&#xA;&#xD;&#xA;A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial." />
  <row Id="2187" PostHistoryTypeId="24" PostId="842" RevisionGUID="2551aa1b-7f01-4ba1-8177-fbc868339a91" CreationDate="2014-07-27T03:35:06.853" Comment="Proposed by 1352 approved by 434, 548 edit id of 123" />
  <row Id="2188" PostHistoryTypeId="2" PostId="850" RevisionGUID="bd749343-3210-4942-b69f-342d5f3a6c6e" CreationDate="2014-07-27T03:58:22.907" UserId="548" Text="If you've got prior information then you should certainly not use simple mean in a split test. I assume you're trying to just predict which group will produce the greatest amount of revenue overall, by trying to emulate the underlying distribution.&#xD;&#xA;&#xD;&#xA;Firstly, it's worth noting that any metrics you choose will actually reduce to mean in a pretty trivial way. Eventually mean will necessarily work out, though using a standard bayesian method to estimate the mean is probably your best bet.&#xD;&#xA;&#xD;&#xA;If you've got a prior then using a standard bayesian approach to update the prior on your mean revenue is probably the best way to do it. Basically, just take the individual results you get and update a multinomial distribution representing your prior in each case.&#xD;&#xA;&#xD;&#xA;If you want some more full background on multinomial distributions as bayesian priors are pretty well, [this][1] Microsoft paper does a pretty good job of outlining it. In general, I wouldn't care so much about the fact that your distribution is technically discrete, as a multinomial distribution will effectively interpolate across your solution space, giving you a continuous distribution that is a very good approximation of your discrete space.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://research.microsoft.com/en-us/um/people/minka/papers/minka-multinomial.pdf" />
  <row Id="2190" PostHistoryTypeId="5" PostId="793" RevisionGUID="195c2898-f876-426d-b774-a63ac2a2a244" CreationDate="2014-07-27T07:36:51.510" UserId="2523" Comment="Copy edited. Added some context." Text="How can [NoSQL][1] databases like [MongoDB][2] be used for data analysis? What are the features in them that can make data analysis faster and powerful?&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/NoSQL&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/MongoDB&#xD;&#xA;" />
  <row Id="2191" PostHistoryTypeId="4" PostId="793" RevisionGUID="195c2898-f876-426d-b774-a63ac2a2a244" CreationDate="2014-07-27T07:36:51.510" UserId="2523" Comment="Copy edited. Added some context." Text="Uses of NoSQL database in data science" />
  <row Id="2192" PostHistoryTypeId="24" PostId="793" RevisionGUID="195c2898-f876-426d-b774-a63ac2a2a244" CreationDate="2014-07-27T07:36:51.510" Comment="Proposed by 2523 approved by 548, 434 edit id of 124" />
  <row Id="2193" PostHistoryTypeId="2" PostId="851" RevisionGUID="8220e1a6-9dc9-47da-af5c-c537c14edab1" CreationDate="2014-07-27T21:54:05.003" UserId="2744" Text="## Image Similarity based on Color Palette Distribution ##&#xD;&#xA;&#xD;&#xA;I am trying to compute similarity between two images based on their color palette distribution, let's say I have two sets of key value pairs as follows,&#xD;&#xA;&#xD;&#xA;Img1: `{'Brown': 14, 'White': 13, 'Black': 40, 'Gray': 31}`&#xD;&#xA;&#xD;&#xA;Img2: `{'Pink': 82, 'Brown': 8, 'White': 7}`&#xD;&#xA;&#xD;&#xA;Where the numbers denote the % of that color present in the image. What would be the best way to compute similarity on a scale of 0-100 between the two images?" />
  <row Id="2194" PostHistoryTypeId="1" PostId="851" RevisionGUID="8220e1a6-9dc9-47da-af5c-c537c14edab1" CreationDate="2014-07-27T21:54:05.003" UserId="2744" Text="Computing Image Similarity based on Color Distribution" />
  <row Id="2195" PostHistoryTypeId="3" PostId="851" RevisionGUID="8220e1a6-9dc9-47da-af5c-c537c14edab1" CreationDate="2014-07-27T21:54:05.003" UserId="2744" Text="&lt;data-mining&gt;&lt;clustering&gt;&lt;similarity&gt;" />
  <row Id="2199" PostHistoryTypeId="2" PostId="853" RevisionGUID="cb48a3ee-b41f-493e-915c-002979b88f41" CreationDate="2014-07-28T07:19:49.877" UserId="2750" Text="I have implemented NER system with the use of CRF algorithm with my handcrafted features that gave quite good results. The thing is that I used lots of different features including POS tags and lemmas.&#xD;&#xA;&#xD;&#xA;Now I want to make the same NER for different language. The problem here is that I can't use POS tags and lemmas. I started reading articles about deep learning and unsupervised feature learning.&#xD;&#xA;&#xD;&#xA;My question is, if it's possible to use methods for unsupervised feature learning with CRF algorithm. Did anyone try this and got any good result? Is there any article or tutorial about this matter.&#xD;&#xA;&#xD;&#xA;I still don't completely understand this way of feature creation so I don't want to spend to much time for something that won't work. So any information would be really helpful. To create whole NER system based on deep learning is a bit to much for now.&#xD;&#xA;&#xD;&#xA;Thank you in advance." />
  <row Id="2200" PostHistoryTypeId="1" PostId="853" RevisionGUID="cb48a3ee-b41f-493e-915c-002979b88f41" CreationDate="2014-07-28T07:19:49.877" UserId="2750" Text="Unsupervised Feature Learning for NER" />
  <row Id="2201" PostHistoryTypeId="3" PostId="853" RevisionGUID="cb48a3ee-b41f-493e-915c-002979b88f41" CreationDate="2014-07-28T07:19:49.877" UserId="2750" Text="&lt;nlp&gt;&lt;text-mining&gt;&lt;feature-extraction&gt;" />
  <row Id="2202" PostHistoryTypeId="2" PostId="854" RevisionGUID="92f89317-dc69-485d-abbb-0bf218018a90" CreationDate="2014-07-28T08:36:53.633" UserId="791" Text="I like @Kallestad answer very much, but I would like to add a meta-step: Make sure that you understand how the data where collected, and what types of constraints there are. &#xD;&#xA;I think it is very common to think that there where no non-obvious steps when the data where collected, but this is not the case: Most of the time, some process or indivudal did somethink with the data, and these steps can and will influence the shape of the data.&#xD;&#xA;&#xD;&#xA;Two examples:&#xD;&#xA;I had a study recently where the data where collected by various con&#xD;&#xA;tractors worldwide. I was not at the briefing, so that was opaque to me. Unfortunately, the measurements where off for some parts of france: People all liked ice cram, but we expected a random distribution. There was no obvious reason for this uniformity, so I began to hunt the errors. When I queried the contractors, one had misunderstood the briefing and selected only ice-cream lovers from his database.&#xD;&#xA;&#xD;&#xA;The second error was more challenging: When doing some geographic analysis, I found that a lot of people had extremely large movement patterns, which suggested that a lot of them traveled from Munich to Hamburg in minutes. When I spoke with ppeople upstream, they found a subtle bug in their data aggregation software, which was unnoticed before.&#xD;&#xA;&#xD;&#xA;Conclusions:&#xD;&#xA;&#xD;&#xA;- Do not assume that your data was collected by perfect processes /humans.&#xD;&#xA;- Do try to understand the limits of your data providers.&#xD;&#xA;- Look at individual patterns / values and try to determine if they are logical (easy for movement / geographic data)" />
  <row Id="2203" PostHistoryTypeId="2" PostId="855" RevisionGUID="bea11309-544c-4bf1-b126-f2ec5b84e238" CreationDate="2014-07-28T12:07:25.573" UserId="2452" Text="Below you can find a copy of my answer to a related (however, focused on data cleaning aspect) question here on *Data Science StackExchange* (http://datascience.stackexchange.com/a/722/2452), provided in its entirety for readers' convenience. I believe that it partially answers your question as well and hope it is helpful. While the answer is focused on `R` ecosystem, similar packages and/or libraries can be found for other *data analysis environments*. Moreover, while the two **cited papers** on data preparation also contain examples in R, these papers present **general** **workflow (framework)** and **best practices** that are applicable to **any** data analysis environment.&#xD;&#xA;&#xD;&#xA;R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**, **reshape2**, and **plyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.&#xD;&#xA;&#xD;&#xA;Additionally, R offers some packages specifically *focused* on data cleaning and transformation:&#xD;&#xA;&#xD;&#xA;- **editrules** (http://cran.r-project.org/web/packages/editrules/index.html)&#xD;&#xA;- **deducorrect** (http://cran.r-project.org/web/packages/deducorrect/index.html)&#xD;&#xA;- **StatMatch** (http://cran.r-project.org/web/packages/StatMatch/index.html)&#xD;&#xA;- **MatchIt** (http://cran.r-project.org/web/packages/MatchIt/index.html)&#xD;&#xA;- **DataCombine** (http://cran.r-project.org/web/packages/DataCombine)&#xD;&#xA;&#xD;&#xA;A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf." />
  <row Id="2209" PostHistoryTypeId="2" PostId="858" RevisionGUID="44bbf08a-3135-46c3-b8c2-296e9e23a527" CreationDate="2014-07-28T14:48:37.743" UserId="684" Text="Yes, it is entirely possible to combine unsupervised learning with the CRF model.  In particular, I would recommend that you explore the possibility of using [word2vec](https://code.google.com/p/word2vec/) features as inputs to your CRF.&#xD;&#xA;&#xD;&#xA;Word2vec trains a  to distinguish between words that are appropriate for a given context and words that are randomly selected.  Select weights of the model can then be interpreted as a dense vector representation of a given word.  &#xD;&#xA;&#xD;&#xA;These dense vectors have the appealing property that words that are semantically or syntactically similar have similar vector representations.  Basic vector arithmetic even reveals some interesting learned relationships between words.  &#xD;&#xA;For example, vector(&quot;Paris&quot;) - vector(&quot;France&quot;) + vector(&quot;Italy&quot;) yields a vector that is quite similar to vector(&quot;Rome&quot;).  &#xD;&#xA;&#xD;&#xA;At a high level, you can think of word2vec representations as being similar to LDA or LSA representations, in the sense that you can convert a sparse input vector into a dense output vector that contains word similarity information. &#xD;&#xA;&#xD;&#xA;For English text Google distributes word2vec models pretrained on a huge 100 billion word Google News dataset, but for other languages you'll have to train your own model.&#xD;&#xA;" />
  <row Id="2210" PostHistoryTypeId="2" PostId="859" RevisionGUID="bad074a4-6698-47f9-b811-13fe2f515d9d" CreationDate="2014-07-28T16:17:49.823" UserId="2643" Text="What are the different classes of data science problems that can be solved using mapreduce found in hadoop?" />
  <row Id="2211" PostHistoryTypeId="1" PostId="859" RevisionGUID="bad074a4-6698-47f9-b811-13fe2f515d9d" CreationDate="2014-07-28T16:17:49.823" UserId="2643" Text="Data science and Hadoop Map Reduce" />
  <row Id="2212" PostHistoryTypeId="3" PostId="859" RevisionGUID="bad074a4-6698-47f9-b811-13fe2f515d9d" CreationDate="2014-07-28T16:17:49.823" UserId="2643" Text="&lt;hadoop&gt;&lt;map-reduce&gt;" />
  <row Id="2213" PostHistoryTypeId="2" PostId="860" RevisionGUID="5c3baf92-ce10-48ca-a65a-16eeb9c23cb1" CreationDate="2014-07-28T16:23:39.600" UserId="1241" Text="I'll add one thing- if possible, do a reasonableness check by comparing you data against some other source. It seems that whenever I fail to do this, I get burnt:(" />
  <row Id="2214" PostHistoryTypeId="2" PostId="861" RevisionGUID="5dfab5ec-1b22-4427-aefc-629721a6e520" CreationDate="2014-07-28T16:39:45.703" UserDisplayName="user1361" Text="Data Science has many different sub-areas as described in [my post][1]). Nearly for each area, scientists and developer has significant contributions. To learn more about what can be done, please look at following websites:&#xD;&#xA;&#xD;&#xA; * Data Mining Algorithms &amp; Machine Learning -&gt; [Apache Mahout][3]&#xD;&#xA; * Statistics -&gt; [RHadoop][4]&#xD;&#xA; * Data Warehousing &amp; Database Querying -&gt; [SQL-MapReduce][5]&#xD;&#xA; * Social Network Analysis -&gt; [Article][6]&#xD;&#xA; * Bio-informatics -&gt; [Article - 1 ][7], [Article - 2][8]&#xD;&#xA;&#xD;&#xA;Also, there are some work on MapReduce + Excel + Cloud combination but I have not found the link.&#xD;&#xA;&#xD;&#xA;Do not forget that knowing what MapReduce can do is not enough for Data Science. You should also aware of [What MapReduce can't do][2], too.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[1]: http://www.datasciencecentral.com/profiles/blogs/ingredients-of-data-science-1&#xD;&#xA;[2]: http://www.analyticbridge.com/profiles/blogs/what-mapreduce-can-t-do&#xD;&#xA;[3]: https://mahout.apache.org/&#xD;&#xA;[4]: https://github.com/RevolutionAnalytics/RHadoop/wiki&#xD;&#xA;[5]: http://www.teradata.com.tr/Teradata-Aster-SQL-MapReduce/?LangType=1055&amp;LangSelect=true&#xD;&#xA;[6]: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber=5636636&#xD;&#xA;[7]: http://abhishek-tiwari.com/post/mapreduce-and-hadoop-algorithms-in-bioinformatics-papers?ModPagespeed=noscript&#xD;&#xA;[8]: http://www.biomedcentral.com/1471-2105/11/S12/S1&#xD;&#xA;" />
  <row Id="2215" PostHistoryTypeId="5" PostId="861" RevisionGUID="12d7b3ac-9675-411d-a173-a14a94a831b0" CreationDate="2014-07-28T16:49:55.573" UserDisplayName="user1361" Comment="added 397 characters in body" Text="Data Science has many different sub-areas as described in [my post][1]). Nearly for each area, scientists and developer has significant contributions. To learn more about what can be done, please look at following websites:&#xD;&#xA;&#xD;&#xA; * Data Mining Algorithms &amp; Machine Learning -&gt; [Apache Mahout][3]&#xD;&#xA; * Statistics -&gt; [RHadoop][4]&#xD;&#xA; * Data Warehousing &amp; Database Querying -&gt; [SQL-MapReduce][5]&#xD;&#xA; * Social Network Analysis -&gt; [Article][6]&#xD;&#xA; * Bio-informatics -&gt; [Article - 1 ][7], [Article - 2][8]&#xD;&#xA;&#xD;&#xA;Also, there are some work on MapReduce + Excel + Cloud combination but I have not found the link.&#xD;&#xA;&#xD;&#xA;&gt; What are the different classes of Data Science problems ...&#xD;&#xA;&#xD;&#xA;Each &quot;classes&quot; is not purely homogeneous problem domain, i.e. some problem cannot be solved via map and reduce approach due to its communication cost, or algorithm behavior. What I mean by behavior is that some problem wants to have control on all data sets instead of chunks. Thus, I refuse to list type of problem &quot;classes&quot;.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Do not forget that knowing what MapReduce can do is not enough for Data Science. You should also aware of [What MapReduce can't do][2], too.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[1]: http://www.datasciencecentral.com/profiles/blogs/ingredients-of-data-science-1&#xD;&#xA;[2]: http://www.analyticbridge.com/profiles/blogs/what-mapreduce-can-t-do&#xD;&#xA;[3]: https://mahout.apache.org/&#xD;&#xA;[4]: https://github.com/RevolutionAnalytics/RHadoop/wiki&#xD;&#xA;[5]: http://www.teradata.com.tr/Teradata-Aster-SQL-MapReduce/?LangType=1055&amp;LangSelect=true&#xD;&#xA;[6]: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber=5636636&#xD;&#xA;[7]: http://abhishek-tiwari.com/post/mapreduce-and-hadoop-algorithms-in-bioinformatics-papers?ModPagespeed=noscript&#xD;&#xA;[8]: http://www.biomedcentral.com/1471-2105/11/S12/S1&#xD;&#xA;" />
  <row Id="2216" PostHistoryTypeId="2" PostId="862" RevisionGUID="03316c70-70aa-4649-8b7b-9e335b7cdd65" CreationDate="2014-07-28T17:33:43.280" UserId="82" Text="I think that there is no universal technique for &quot;cleaning&quot; data before doing actual research. On the other hand, I'm aiming for doing as much [reproducible research][1] as possible. By doing reproducible research, if you used cleaning techniques with bugs or with poor parameters/assumptions it could be spot by others.&#xD;&#xA;&#xD;&#xA;There is nice R package [knitr][2] which helps a lot in reproducible research.&#xD;&#xA;&#xD;&#xA;Of course, not all research could be fully reproduced (for example live Twitter data) , but at least you can document cleaning, formating and preprocessing steps easily.&#xD;&#xA;&#xD;&#xA;You can check my [assessment][3] prepared for [Reproducible Research course at Coursera][4]. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Reproducibility#Reproducible_research&#xD;&#xA;  [2]: http://yihui.name/knitr/&#xD;&#xA;  [3]: http://rpubs.com/QuatnumDamage/storm&#xD;&#xA;  [4]: https://www.coursera.org/course/repdata" />
  <row Id="2218" PostHistoryTypeId="5" PostId="823" RevisionGUID="cd8c123b-9bb0-43f8-b78c-474ef536f882" CreationDate="2014-07-28T20:53:13.387" UserId="95" Comment="added 1811 characters in body" Text="There was a recent furore with [facebook experimenting on their users to see if they could alter user's emotions](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840).&#xD;&#xA;&#xD;&#xA;Whilst I am not a professional data scientist I read about [data science ethics](http://columbiadatascience.com/2013/11/25/data-science-ethics/) from [Cathy O'Neill's book 'Doing Data Science'](http://shop.oreilly.com/product/0636920028529.do) and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science *accidentally*.&#xD;&#xA;&#xD;&#xA;Whilst the linked article touched on data integrity, the book also discussed the moral ethics behind understanding the impact of the data models that are created and the impact of those models which can have adverse effects when used inappropriately (sometimes unwittingly) or when the models are inaccurate, again producing adverse results.&#xD;&#xA;&#xD;&#xA;The article discusses a code of practice and mentions the [Data Science Association's Code of conduct](http://www.datascienceassn.org/code-of-conduct.html), is this something that is in use? Rule 7 is of particular interest (quoted from their website):&#xD;&#xA;&#xD;&#xA;&gt; (a) A person who consults with a data scientist about the possibility&#xD;&#xA;&gt; of forming a client-data scientist relationship with respect to a&#xD;&#xA;&gt; matter is a prospective client.&#xD;&#xA;&gt; &#xD;&#xA;&gt; (b) Even when no client-data scientist relationship ensues, a data&#xD;&#xA;&gt; scientist who has learned information from a prospective client shall&#xD;&#xA;&gt; not use or reveal that information.&#xD;&#xA;&gt; &#xD;&#xA;&gt; (c) A data scientist subject to paragraph (b) shall not provide&#xD;&#xA;&gt; professional data science services for a client with interests&#xD;&#xA;&gt; materially adverse to those of a prospective client in the same or a&#xD;&#xA;&gt; substantially related industry if the data scientist received&#xD;&#xA;&gt; information from the prospective client that could be significantly&#xD;&#xA;&gt; harmful to that person in the matter&#xD;&#xA;&#xD;&#xA;Is this something that is practiced professionally? Many users blindly accept that we get some free service (mail, social network, image hosting, blog platform etc..) and agree to an EULA in order to have ads pushed at us. &#xD;&#xA;&#xD;&#xA;Finally how is this regulated, I often read about users being up in arms when the terms of a service change but it seems that it requires some liberty organisation or class action to react to such things before something happens.&#xD;&#xA;&#xD;&#xA;By the way I am not making any judgements here or saying that all data scientists behave like this, I'm interested in what is taught academically and practiced professionally." />
  <row Id="2224" PostHistoryTypeId="5" PostId="858" RevisionGUID="6bfe1731-e473-4fa7-b845-3e5574bead11" CreationDate="2014-07-29T03:29:42.940" UserId="684" Comment="added 193 characters in body" Text="Yes, it is entirely possible to combine unsupervised learning with the CRF model.  In particular, I would recommend that you explore the possibility of using [word2vec](https://code.google.com/p/word2vec/) features as inputs to your CRF.&#xD;&#xA;&#xD;&#xA;Word2vec trains a  to distinguish between words that are appropriate for a given context and words that are randomly selected.  Select weights of the model can then be interpreted as a dense vector representation of a given word.  &#xD;&#xA;&#xD;&#xA;These dense vectors have the appealing property that words that are semantically or syntactically similar have similar vector representations.  Basic vector arithmetic even reveals some interesting learned relationships between words.  &#xD;&#xA;For example, vector(&quot;Paris&quot;) - vector(&quot;France&quot;) + vector(&quot;Italy&quot;) yields a vector that is quite similar to vector(&quot;Rome&quot;).  &#xD;&#xA;&#xD;&#xA;At a high level, you can think of word2vec representations as being similar to LDA or LSA representations, in the sense that you can convert a sparse input vector into a dense output vector that contains word similarity information. &#xD;&#xA;&#xD;&#xA;For that matter, LDA and LSA are also valid options for unsupervised feature learning -- both attempt to represent words as combinations of &quot;topics&quot; and output dense word representations.  &#xD;&#xA;&#xD;&#xA;For English text Google distributes word2vec models pretrained on a huge 100 billion word Google News dataset, but for other languages you'll have to train your own model.&#xD;&#xA;" />
  <row Id="2227" PostHistoryTypeId="4" PostId="823" RevisionGUID="9430f2fa-53b9-458d-a562-85e4bca53caf" CreationDate="2014-07-29T12:39:59.630" UserId="95" Comment="edited title" Text="How should ethics be applied in data science" />
  <row Id="2228" PostHistoryTypeId="5" PostId="823" RevisionGUID="9430f2fa-53b9-458d-a562-85e4bca53caf" CreationDate="2014-07-29T12:39:59.630" UserId="95" Comment="edited title" Text="There was a recent furore with [facebook experimenting on their users to see if they could alter user's emotions](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840) and now [okcupid](http://www.bbc.co.uk/news/technology-28542642).&#xD;&#xA;&#xD;&#xA;Whilst I am not a professional data scientist I read about [data science ethics](http://columbiadatascience.com/2013/11/25/data-science-ethics/) from [Cathy O'Neill's book 'Doing Data Science'](http://shop.oreilly.com/product/0636920028529.do) and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science *accidentally*.&#xD;&#xA;&#xD;&#xA;Whilst the linked article touched on data integrity, the book also discussed the moral ethics behind understanding the impact of the data models that are created and the impact of those models which can have adverse effects when used inappropriately (sometimes unwittingly) or when the models are inaccurate, again producing adverse results.&#xD;&#xA;&#xD;&#xA;The article discusses a code of practice and mentions the [Data Science Association's Code of conduct](http://www.datascienceassn.org/code-of-conduct.html), is this something that is in use? Rule 7 is of particular interest (quoted from their website):&#xD;&#xA;&#xD;&#xA;&gt; (a) A person who consults with a data scientist about the possibility&#xD;&#xA;&gt; of forming a client-data scientist relationship with respect to a&#xD;&#xA;&gt; matter is a prospective client.&#xD;&#xA;&gt; &#xD;&#xA;&gt; (b) Even when no client-data scientist relationship ensues, a data&#xD;&#xA;&gt; scientist who has learned information from a prospective client shall&#xD;&#xA;&gt; not use or reveal that information.&#xD;&#xA;&gt; &#xD;&#xA;&gt; (c) A data scientist subject to paragraph (b) shall not provide&#xD;&#xA;&gt; professional data science services for a client with interests&#xD;&#xA;&gt; materially adverse to those of a prospective client in the same or a&#xD;&#xA;&gt; substantially related industry if the data scientist received&#xD;&#xA;&gt; information from the prospective client that could be significantly&#xD;&#xA;&gt; harmful to that person in the matter&#xD;&#xA;&#xD;&#xA;Is this something that is practiced professionally? Many users blindly accept that we get some free service (mail, social network, image hosting, blog platform etc..) and agree to an EULA in order to have ads pushed at us. &#xD;&#xA;&#xD;&#xA;Finally how is this regulated, I often read about users being up in arms when the terms of a service change but it seems that it requires some liberty organisation, class action or a [senator](http://www.cnet.com/news/senator-asks-ftc-to-investigate-facebooks-mood-study/) to react to such things before something happens.&#xD;&#xA;&#xD;&#xA;By the way I am not making any judgements here or saying that all data scientists behave like this, I'm interested in what is taught academically and practiced professionally." />
  <row Id="2229" PostHistoryTypeId="2" PostId="863" RevisionGUID="61ec9a4d-c296-426a-84bb-576149312d70" CreationDate="2014-07-29T13:43:39.060" UserId="1279" Text="Let's first split it into parts. &#xD;&#xA;&#xD;&#xA;**Data Science** is about making knowledge from raw data. It uses machine learning, statistics and other fields to simplify (or even automate) decision making. Data science techniques may work with any data size, but more data means better predictions and thus more precise decisions. &#xD;&#xA;&#xD;&#xA;**Hadoop** is a common name for a set of tools intended to work with large amounts of data. Two most important components in Hadoop are HDFS and MapReduce.&#xD;&#xA;&#xD;&#xA;**HDFS**, or Hadoop Distributed File System, is a special distributed storage capable of holding really large data amounts. Large files on HDFS are split into blocks, and for each block HDFS API exposes its _location_. &#xD;&#xA;&#xD;&#xA;**MapReduce** is framework for running computations on nodes with data. MapReduce heavily uses _data locality_ exposed by HDFS: when possible, data is not transferred between nodes, but instead code is copied to the nodes with data. &#xD;&#xA;&#xD;&#xA;So basically any problem (including data science tasks) that doesn't break data locality principle may be efficiently implemented using MapReduce (and a number of other problems may be solved not that efficiently, but still simply enough).&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;Let's take some examples. Very often analyst only needs some simple statistics over his tabular data. In this case [**Hive**](https://hive.apache.org/), which is basically SQL engine over MapReduce, works pretty well (there are also Impala, Shark and others, but they don't use Hadoop's MapReduce, so more on them later). &#xD;&#xA;&#xD;&#xA;In other cases analyst (or developer) may want to work with previously unstructured data. Pure MapReduce is pretty good for **transforming** and **standardizing** data.&#xD;&#xA;&#xD;&#xA;Some people are used to exploratory statistics and visualization using tools like R. It's possible to apply this approach to big data amounts using [**RHadoop**](https://github.com/RevolutionAnalytics/RHadoop/wiki) package. &#xD;&#xA;&#xD;&#xA;And when it comes to MapReduce-based machine learning [**Apache Mahout**](https://mahout.apache.org/) is the first to mention. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;There's, however, one type of algorithms that work pretty slowly on Hadoop even in presence of data locality, namely, iterative algorithms. Iterative algorithms tend to have multiple Map and Reduce stages. Hadoop's MR framework **reads** and **writes** data to disk on each stage (and sometimes in between), which makes iterative (as well as any multi-stage) tasks terribly slow. &#xD;&#xA;&#xD;&#xA;Fortunately, there are alternative frameworks that can both - use data locality and keep data in memory between stages. Probably, the most notable of them is [**Apache Spark**](https://spark.apache.org/). Spark is complete replacement for Hadoop's MapReduce that uses its own runtime and exposes pretty rich API for manipulating your distributed dataset. Spark has several sub-projects, closely related to data science: &#xD;&#xA;&#xD;&#xA; * [**Shark**](http://shark.cs.berkeley.edu/) and [**Spark SQL**](https://spark.apache.org/sql/)  provide alternative SQL-like interfaces to data stored on HDFS&#xD;&#xA; * [**Spark Streaming**](https://spark.apache.org/streaming/) makes it easy to work with continuous data streams (e.g. Twitter feed)&#xD;&#xA; * [**MLlib**](https://spark.apache.org/mllib/) implements a number of machine learning algorithms with a pretty simple and flexible API&#xD;&#xA; * [**GraphX**](https://spark.apache.org/graphx/) enables large-scale graph processing&#xD;&#xA;&#xD;&#xA;So there's pretty large set of data science problems that you can solve with Hadoop and related projects. " />
  <row Id="2230" PostHistoryTypeId="5" PostId="859" RevisionGUID="b52a07cb-3963-4217-a63d-b0bbc00788d5" CreationDate="2014-07-30T03:13:17.827" UserDisplayName="user1361" Comment="edit title and question body" Text="What are the different classes of data science problems that can be solved using mapreduce programming model?" />
  <row Id="2231" PostHistoryTypeId="4" PostId="859" RevisionGUID="b52a07cb-3963-4217-a63d-b0bbc00788d5" CreationDate="2014-07-30T03:13:17.827" UserDisplayName="user1361" Comment="edit title and question body" Text="Data science and MapReduce programming model of Hadoop" />
  <row Id="2232" PostHistoryTypeId="24" PostId="859" RevisionGUID="b52a07cb-3963-4217-a63d-b0bbc00788d5" CreationDate="2014-07-30T03:13:17.827" Comment="Proposed by 1361 approved by 434, 84 edit id of 126" />
  <row Id="2234" PostHistoryTypeId="2" PostId="864" RevisionGUID="5c01c4c2-a20e-47fe-833c-95946cc66b13" CreationDate="2014-07-30T07:49:14.467" UserId="381" Text="map/reduce is most appropriate for parallelizable offline computations. To be more precise, it works best when the result can be found from the result of some function of a partition of the input. Averaging is a trivial example; you can do this with map/reduce by summing each partition, returning the sum and the number of elements in the partition, then computing the overall mean using these intermediate results. It is less appropriate when the intermediate steps depend on the state of the other partitions." />
  <row Id="2237" PostHistoryTypeId="2" PostId="865" RevisionGUID="8ecbf926-0439-4cc8-84ce-c04db89c25f3" CreationDate="2014-07-30T10:24:54.180" UserId="988" Text="I need to build parse tree for some source code (on Python or any program language that describe by CFG).&#xD;&#xA;&#xD;&#xA;So, I have source code on some programming language and BNF this language.&#xD;&#xA;&#xD;&#xA;Can anybody give some advice how can I build parse tree in this case?&#xD;&#xA;Preferably, with tools for Python." />
  <row Id="2238" PostHistoryTypeId="1" PostId="865" RevisionGUID="8ecbf926-0439-4cc8-84ce-c04db89c25f3" CreationDate="2014-07-30T10:24:54.180" UserId="988" Text="How to build parse tree with BNF" />
  <row Id="2239" PostHistoryTypeId="3" PostId="865" RevisionGUID="8ecbf926-0439-4cc8-84ce-c04db89c25f3" CreationDate="2014-07-30T10:24:54.180" UserId="988" Text="&lt;python&gt;&lt;parsing&gt;" />
  <row Id="2240" PostHistoryTypeId="2" PostId="866" RevisionGUID="37113054-0551-40f3-a23e-0a567b54ba25" CreationDate="2014-07-30T11:45:08.313" UserId="2781" Text="I am, admittedly, very new to data science. I have spent the last 8 months or so learning as much as I can about the field and its methods.  I am having issues choosing which methods to apply.&#xD;&#xA;&#xD;&#xA;I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).&#xD;&#xA;&#xD;&#xA;My goals are to:&#xD;&#xA;&#xD;&#xA; 1. Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;&#xD;&#xA; 2. Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;&#xD;&#xA; 3. Do the same as 1 and 2, but with procedures and/or diagnoses.&#xD;&#xA; 4. Preferably, the results would be interpretable by a doctor&#xD;&#xA;&#xD;&#xA;I have looked at things like the [Heritage Health Prize Milestone papers][1] and have learned a lot from them, but they are focused on predicting hospitalizations.&#xD;&#xA;&#xD;&#xA;I have thrown a number of algorithms at the problem (random forests, logistic regression, CART, Cox regressions) and it's been an amazing learning experience.  I have not been able to decide on what &quot;works&quot; or &quot;doesn't work,&quot; if you know what I mean.  I have enough knowledge and skills to be misled by my own excitement and naivete; what I need is to be able to get excited about something real.&#xD;&#xA;&#xD;&#xA;So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.heritagehealthprize.com/c/hhp/details/milestone-winners" />
  <row Id="2241" PostHistoryTypeId="1" PostId="866" RevisionGUID="37113054-0551-40f3-a23e-0a567b54ba25" CreationDate="2014-07-30T11:45:08.313" UserId="2781" Text="Predicting next medical condition from past conditions in claims data" />
  <row Id="2242" PostHistoryTypeId="3" PostId="866" RevisionGUID="37113054-0551-40f3-a23e-0a567b54ba25" CreationDate="2014-07-30T11:45:08.313" UserId="2781" Text="&lt;machine-learning&gt;&lt;beginner&gt;" />
  <row Id="2243" PostHistoryTypeId="2" PostId="867" RevisionGUID="9f3fcd05-b307-4421-a81f-cfe684743e0c" CreationDate="2014-07-30T13:55:36.187" UserId="2785" Text="does anyone know what (from your experience) is the best open source natural language generators out there? What are the relative merits of each? I'm looking to do sophisticated text summarization and would like to use theme extraction/semantic modeling in conjunction with NLG tools to create accurate, context-aware, and natural-sounding text summaries." />
  <row Id="2244" PostHistoryTypeId="1" PostId="867" RevisionGUID="9f3fcd05-b307-4421-a81f-cfe684743e0c" CreationDate="2014-07-30T13:55:36.187" UserId="2785" Text="Relative merits of different open source natural language generators" />
  <row Id="2245" PostHistoryTypeId="3" PostId="867" RevisionGUID="9f3fcd05-b307-4421-a81f-cfe684743e0c" CreationDate="2014-07-30T13:55:36.187" UserId="2785" Text="&lt;nlp&gt;&lt;text-mining&gt;" />
  <row Id="2246" PostHistoryTypeId="5" PostId="866" RevisionGUID="1c185cb4-792d-467f-b1dd-529465e1d973" CreationDate="2014-07-30T13:58:31.410" UserId="2781" Comment="Added an example data frame" Text="I am, admittedly, very new to data science. I have spent the last 8 months or so learning as much as I can about the field and its methods.  I am having issues choosing which methods to apply.&#xD;&#xA;&#xD;&#xA;I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).&#xD;&#xA;&#xD;&#xA;My goals are to:&#xD;&#xA;&#xD;&#xA; 1. Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;&#xD;&#xA; 2. Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;&#xD;&#xA; 3. Do the same as 1 and 2, but with procedures and/or diagnoses.&#xD;&#xA; 4. Preferably, the results would be interpretable by a doctor&#xD;&#xA;&#xD;&#xA;I have looked at things like the [Heritage Health Prize Milestone papers][1] and have learned a lot from them, but they are focused on predicting hospitalizations.&#xD;&#xA;&#xD;&#xA;I have thrown a number of algorithms at the problem (random forests, logistic regression, CART, Cox regressions) and it's been an amazing learning experience.  I have not been able to decide on what &quot;works&quot; or &quot;doesn't work,&quot; if you know what I mean.  I have enough knowledge and skills to be misled by my own excitement and naivete; what I need is to be able to get excited about something real.&#xD;&#xA;&#xD;&#xA;So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?&#xD;&#xA;&#xD;&#xA;EDIT to add sample data frame:&#xD;&#xA;&#xD;&#xA;    structure(list(gender = structure(c(1L, 2L, 2L, 2L, 1L, 2L), .Label = c(&quot;Male&quot;,         &quot;Female&quot;), class = &quot;factor&quot;), patient_age = c(31, 29, 31, 53, &#xD;&#xA;    47, 48), anx.any = c(1, 1, 0, 1, 0, 0), art.any = c(0, 0, 1, &#xD;&#xA;    1, 1, 1), ast.any = c(1, 1, 1, 1, 0, 1), bpa.any = c(1, 1, 1, &#xD;&#xA;    1, 0, 1), can.any = c(0, 0, 0, 1, 0, 1), cer.any = c(0, 0, 0, &#xD;&#xA;    0, 0, 0), chf.any = c(0, 0, 0, 0, 0, 0), ckd.any = c(0, 0, 0, &#xD;&#xA;    0, 0, 0), dep.any = c(1, 1, 0, 1, 0, 0), dia.any = c(0, 0, 1, &#xD;&#xA;    0, 0, 0), end.any = c(1, 1, 0, 1, 0, 1), flu.any = c(1, 0, 0, &#xD;&#xA;    0, 0, 0), hrt.any = c(1, 0, 0, 1, 0, 1), hyp.any = c(1, 0, 0, &#xD;&#xA;    0, 1, 0), inf.any = c(0, 0, 0, 1, 0, 1), men.any = c(1, 0, 1, &#xD;&#xA;    0, 1, 0), ren.any = c(0, 0, 0, 0, 0, 0), sdp.any = c(0, 0, 0, &#xD;&#xA;    0, 0, 0), skn.any = c(1, 0, 0, 1, 0, 1), tra.any = c(1, 1, 0, &#xD;&#xA;    0, 1, 0), anx.isbefore.ckd = c(0, 0, 0, 0, 0, 0), art.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), ast.isbefore.ckd = c(0, 0, 0, 0, 0, 0), bpa.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), can.isbefore.ckd = c(0, 0, 0, 0, 0, 0), cer.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), chf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), ckd.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), dep.isbefore.ckd = c(0, 0, 0, 0, 0, 0), dia.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), end.isbefore.ckd = c(0, 0, 0, 0, 0, 0), flu.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), hrt.isbefore.ckd = c(0, 0, 0, 0, 0, 0), hyp.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), inf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), men.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), ren.isbefore.ckd = c(0, 0, 0, 0, 0, 0), sdp.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), skn.isbefore.ckd = c(0, 0, 0, 0, 0, 0), tra.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0)), .Names = c(&quot;gender&quot;, &quot;patient_age&quot;, &quot;anx.any&quot;, &#xD;&#xA;    &quot;art.any&quot;, &quot;ast.any&quot;, &quot;bpa.any&quot;, &quot;can.any&quot;, &quot;cer.any&quot;, &quot;chf.any&quot;, &#xD;&#xA;    &quot;ckd.any&quot;, &quot;dep.any&quot;, &quot;dia.any&quot;, &quot;end.any&quot;, &quot;flu.any&quot;, &quot;hrt.any&quot;, &#xD;&#xA;    &quot;hyp.any&quot;, &quot;inf.any&quot;, &quot;men.any&quot;, &quot;ren.any&quot;, &quot;sdp.any&quot;, &quot;skn.any&quot;, &#xD;&#xA;    &quot;tra.any&quot;, &quot;anx.isbefore.ckd&quot;, &quot;art.isbefore.ckd&quot;, &quot;ast.isbefore.ckd&quot;, &#xD;&#xA;    &quot;bpa.isbefore.ckd&quot;, &quot;can.isbefore.ckd&quot;, &quot;cer.isbefore.ckd&quot;, &quot;chf.isbefore.ckd&quot;, &#xD;&#xA;    &quot;ckd.isbefore.ckd&quot;, &quot;dep.isbefore.ckd&quot;, &quot;dia.isbefore.ckd&quot;, &quot;end.isbefore.ckd&quot;, &#xD;&#xA;    &quot;flu.isbefore.ckd&quot;, &quot;hrt.isbefore.ckd&quot;, &quot;hyp.isbefore.ckd&quot;, &quot;inf.isbefore.ckd&quot;, &#xD;&#xA;    &quot;men.isbefore.ckd&quot;, &quot;ren.isbefore.ckd&quot;, &quot;sdp.isbefore.ckd&quot;, &quot;skn.isbefore.ckd&quot;, &#xD;&#xA;    &quot;tra.isbefore.ckd&quot;), row.names = c(NA, 6L), class = &quot;data.frame&quot;)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.heritagehealthprize.com/c/hhp/details/milestone-winners" />
  <row Id="2247" PostHistoryTypeId="2" PostId="868" RevisionGUID="d9a3b14d-5711-4ba5-af39-51dfaf5094fe" CreationDate="2014-07-30T15:32:44.557" UserId="2787" Text="There is a paper you should look into:&#xD;&#xA;&#xD;&#xA;[MapReduce: Distributed Computing for Machine Learning](http://cs.smith.edu/dftwiki/images/6/68/MapReduceDistributedComputingMachineLearning.pdf)&#xD;&#xA;&#xD;&#xA;They distinguish 3 classes of machine-learning problems that are reasonable to address with MapReduce:&#xD;&#xA;&#xD;&#xA;1. Single pass algorithms&#xD;&#xA;2. Iterative algorithms&#xD;&#xA;3. Query based algorithms&#xD;&#xA;&#xD;&#xA;They also give examples for each class.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2248" PostHistoryTypeId="6" PostId="866" RevisionGUID="9283bf84-f6a4-4935-b3af-61fdab0519c7" CreationDate="2014-07-30T16:26:27.757" UserId="84" Comment="Adding programming language." Text="&lt;machine-learning&gt;&lt;r&gt;&lt;beginner&gt;" />
  <row Id="2249" PostHistoryTypeId="2" PostId="869" RevisionGUID="d5053bbf-bce7-43e0-b111-128ffd833263" CreationDate="2014-07-30T16:27:45.177" UserId="2790" Text="So, I'm just starting to learn how a neural network can operate to recognize patterns and categorize inputs, and I've seen how an artificial neural network can parse image data and categorize the images ([demo with convnetjs](http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)), and the key there is to downsample the image and each pixel stimulates one input neuron into the network.&#xD;&#xA;&#xD;&#xA;However, I'm trying to wrap my head around if this is possible to be done with string inputs? The use-case I've got is a &quot;recommendation engine&quot; for movies a user has watched. Movies have lots of string data (title, plot, tags), and I could imagine &quot;downsampling&quot; the text down to a few key words that describe that movie, but even if I parse out the top five words that describe this movie, I think I'd need input neurons for every english word in order to compare a set of movies? I could limit the input neurons just to the words used in the set, but then could it grow/learn by adding new movies (user watches a new movie, with new words)? Most of the libraries I've seen don't allow adding new neurons after the system has been trained?&#xD;&#xA;&#xD;&#xA;Is there a standard way to map string/word/character data to inputs into a neural network? Or is a neural network really not the right tool for the job of parsing string data like this (what's a better tool for pattern-matching in string data)?" />
  <row Id="2250" PostHistoryTypeId="1" PostId="869" RevisionGUID="d5053bbf-bce7-43e0-b111-128ffd833263" CreationDate="2014-07-30T16:27:45.177" UserId="2790" Text="Neural Network parse string data?" />
  <row Id="2251" PostHistoryTypeId="3" PostId="869" RevisionGUID="d5053bbf-bce7-43e0-b111-128ffd833263" CreationDate="2014-07-30T16:27:45.177" UserId="2790" Text="&lt;neuralnetwork&gt;" />
  <row Id="2252" PostHistoryTypeId="2" PostId="870" RevisionGUID="875de795-b315-44eb-941c-ccaaacfbbcc4" CreationDate="2014-07-30T17:42:09.797" UserId="381" Text="This is not a problem about neural networks per se, but about representing textual data in machine learning. You can represent the movies, cast, and theme as categorical variables. The plot is more complicated; you'd probably want a [topic model](http://en.wikipedia.org/wiki/Topic_model) for that, but I'd leave that out until you get the hang of things. It does precisely that textual &quot;downsampling&quot; you mentioned.&#xD;&#xA;&#xD;&#xA;Take a look at [this](http://visualstudiomagazine.com/articles/2013/07/01/neural-network-data-normalization-and-encoding.aspx) tutorial to learn how to encode categorical variables for neural networks. And good luck!" />
  <row Id="2253" PostHistoryTypeId="2" PostId="871" RevisionGUID="4f4ee11e-6a94-455d-a6d0-b5c5133d4c99" CreationDate="2014-07-30T17:53:40.330" UserId="684" Text="Using a neural network for prediction on natural language data can be a tricky task, but there are tried and true methods for making it possible.  &#xD;&#xA;&#xD;&#xA;In the Natural Language Processing (NLP) field, text is often represented using the bag of words model.  In other words, you have a vector of length n, where n is the number of words in your vocabulary, and each word corresponds to an element in the vector.  In order to convert text to numeric data, you simply count the number of occurrences of each word and place that value at the index of the vector that corresponds to the word. [Wikipedia does an excellent job of describing this conversion process.](http://en.wikipedia.org/wiki/Bag-of-words_model.)  Because the length of the vector is fixed, its difficult to deal with new words that don't map to an index, but there are ways to help mitigate this problem (lookup [feature hashing](http://en.wikipedia.org/wiki/Feature_hashing)).  &#xD;&#xA; &#xD;&#xA;This method of representation has many disadvantages -- it does not preserve the relationship between adjacent words, and results in very sparse vectors.  Looking at [n-grams](http://en.wikipedia.org/wiki/N-gram) helps to fix the problem of preserving word relationships, but for now let's focus on the second problem: sparsity.  &#xD;&#xA;&#xD;&#xA;It's difficult to deal directly with these sparse vectors (many linear algebra libraries do a poor job of handling sparse inputs), so often the next step is dimensionality reduction. For that we can refer to the field of [topic modeling](http://en.wikipedia.org/wiki/Topic_model):  Techniques like [Latent Dirichlet Allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) and [Latent Semantic Analysis](http://en.wikipedia.org/wiki/Latent_semantic_analysis) (LSA) allow the compression of these sparse vectors into dense vectors by representing a document as a combination of topics.  You can fix the number of topics used, and in doing so fix the size of the output vector producted by LDA or LSA. This dimensionality reduction process drastically reduces the size of the input vector while attempting to lose a minimal amount of information.  &#xD;&#xA;&#xD;&#xA;Finally, after all of these conversions, you can feed the outputs of the topic modeling process into the inputs of your neural network.     " />
  <row Id="2254" PostHistoryTypeId="2" PostId="872" RevisionGUID="047e6332-4cc5-4e05-8192-e6a021e10d3c" CreationDate="2014-07-30T18:45:13.790" UserId="2792" Text="I am attempting to use the tm package to convert a vector of text strings to a corpus element.&#xD;&#xA;&#xD;&#xA;My code looks something like this&#xD;&#xA;&#xD;&#xA;Corpus(d1$Yes)&#xD;&#xA;&#xD;&#xA;where d1$Yes is a factor with 124 levels, each containing a text string.&#xD;&#xA;&#xD;&#xA;For example, d1$Yes[246] = &quot;So we can get the boat out!&quot;&#xD;&#xA;&#xD;&#xA;I'm receiving the following error:&#xD;&#xA;&quot;Error: inherits(x, &quot;Source&quot;) is not TRUE&quot;&#xD;&#xA;&#xD;&#xA;I'm not sure how to remedy this." />
  <row Id="2255" PostHistoryTypeId="1" PostId="872" RevisionGUID="047e6332-4cc5-4e05-8192-e6a021e10d3c" CreationDate="2014-07-30T18:45:13.790" UserId="2792" Text="R error using package tm (text-mining)" />
  <row Id="2256" PostHistoryTypeId="3" PostId="872" RevisionGUID="047e6332-4cc5-4e05-8192-e6a021e10d3c" CreationDate="2014-07-30T18:45:13.790" UserId="2792" Text="&lt;r&gt;&lt;text-mining&gt;" />
  <row Id="2257" PostHistoryTypeId="2" PostId="873" RevisionGUID="3c08a959-3e9d-4976-8fd1-40cf203b1d15" CreationDate="2014-07-30T19:15:09.383" UserId="375" Text="You have to tell Corpus what kind of source you are using.  Try:&#xD;&#xA;&#xD;&#xA;    Corpus(VectorSource(d1$Yes))" />
  <row Id="2258" PostHistoryTypeId="2" PostId="874" RevisionGUID="7399861b-2041-4572-9443-0e45b03a6d27" CreationDate="2014-07-30T21:53:13.790" UserDisplayName="user1361" Text="I have some thoughts about your question. I hope it may help you to solve your problem.&#xD;&#xA;&#xD;&#xA;&gt; I'm planning to run some experiments with very large data sets, and I'd like to distribute the computation.&#xD;&#xA;&#xD;&#xA;In [one of my posts][1], I have done research on topic of _evaluation methods of Data Science_. With __Learning Curve__, you can evaluate your experiments _learning ability_. To talk a bit more, you will __fix__ commodity configuration, and then will run same experiment on the same number of machines with different size of data set you have (from starting from small chunk in size, incrementally increase the size until you reach the whole data set). &#xD;&#xA;&#xD;&#xA;To point on, you should avoid having _power distribution_ for result of performance test being run with different size of data sets. To avoid, you should carefully choose _step size_ (step size = amount of increments).&#xD;&#xA;&#xD;&#xA;&gt; I have about ten machines available, each with 200 GB of free space on hard disk. However, I would like to perform experiments on a greater number of nodes, to measure scalability more precisely.&#xD;&#xA;&#xD;&#xA;For this type question, I have intuitively searched and read materials; afterwards, published as a [blog post][2]. At the end of the post, I have briefly talked about how to test your hypothesis on real complex system. If you let me, I want to briefly talk about;&#xD;&#xA;&#xD;&#xA;First of all, __base requirement__ should be formed in order to run data set as a whole. The minimum requirement will build your __baseline evaluation score__ which is calculated with one/combination of evaluation metrics you have chosen, or with one/combination of methods being used to calculate `Running Time = Computation complexity + Communication cost + Synchronization cost`. &#xD;&#xA;&#xD;&#xA;After those steps, with an _evaluation strategy_, add new elements, e.g. new node, to the system you have doing scalability test; meanwhile, for each addition, measure performance w.r.t new system configuration. &#xD;&#xA;&#xD;&#xA;Just to note, _evaluation strategy_ must be planned along with considerations of default behavior of parallel and distributed systems. For example, what I mean by behavior is that adding just more cores will, after some point, automatically drop performance of the system not due to your algorithm characteristics. It is because more cores need more RAMs, more hard driver, or etc. In other words, there is a __N-Way__ relationship between hardware components. As a second example, adding more nodes to the distributed system will punished you with more communication and synchronization costs.&#xD;&#xA;&#xD;&#xA;As a last step, you will sketch two different graphs with your evaluation results via data analysis program or language (As a recommendation, use GNU Plot or R programming language). Print out and put those results at your desktop, and start to examine them, carefully. According to your investigation, modify/erase + rebuild _evaluation strategy_ and re-do the performance test.&#xD;&#xA;&#xD;&#xA;&gt; Are there commodity services which would grant me that only my application would be running at a given time? Has anyone used such services yet?&#xD;&#xA;&#xD;&#xA;I have no much experiment on commodity services, but I can easily say whether it grants or not depends on your configuration of services. If you configured say Hadoop to your node as an only service, Hadoop will grant your code will be only running at any time.&#xD;&#xA;&#xD;&#xA;[1]: http://www.datasciencecentral.com/profiles/blogs/meeting-justice-of-data-science&#xD;&#xA;[2]: http://www.datasciencecentral.com/profiles/blogs/scale-your-vision" />
  <row Id="2259" PostHistoryTypeId="5" PostId="423" RevisionGUID="e2b9f16b-1454-417e-83fd-f16c8fb07bb4" CreationDate="2014-07-30T22:36:59.310" UserDisplayName="user1361" Comment="edit title, and body of question" Text="I'm planning to run experiments with large datasets on distributed system in order to evaluate efficiency gains in comparison with previous proposals.&#xD;&#xA;&#xD;&#xA;I have limited number of machines nearly ten machines having 200 GB of free space on hard disk on each. On the contrary, I wished to perform experiments on more than available nodes in order to measure scalability, __more precisely__. Since I don't have any, I thought about using a commodity cluster. However, I'm not sure about the policies of usage, and I need to reliably measure execution times. &#xD;&#xA;&#xD;&#xA;Are there commodity services which will grant me that only my application would be running at a given time?" />
  <row Id="2260" PostHistoryTypeId="4" PostId="423" RevisionGUID="e2b9f16b-1454-417e-83fd-f16c8fb07bb4" CreationDate="2014-07-30T22:36:59.310" UserDisplayName="user1361" Comment="edit title, and body of question" Text="How to measure execution time on distributed system" />
  <row Id="2261" PostHistoryTypeId="6" PostId="423" RevisionGUID="e2b9f16b-1454-417e-83fd-f16c8fb07bb4" CreationDate="2014-07-30T22:36:59.310" UserDisplayName="user1361" Comment="edit title, and body of question" Text="&lt;bigdata&gt;&lt;scalability&gt;&lt;distributed&gt;&lt;commodity&gt;" />
  <row Id="2262" PostHistoryTypeId="24" PostId="423" RevisionGUID="e2b9f16b-1454-417e-83fd-f16c8fb07bb4" CreationDate="2014-07-30T22:36:59.310" Comment="Proposed by 1361 approved by 84 edit id of 129" />
  <row Id="2263" PostHistoryTypeId="2" PostId="875" RevisionGUID="6f5d71f9-49dd-43c3-9845-4d4760249b8a" CreationDate="2014-07-30T23:40:54.700" UserId="609" Text="I suggest you use [ANTLR][1], which is a very powerful parser generator. It has a good GUI for entering your BNF. It has a [Python target][2] capability.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.antlr.org/&#xD;&#xA;  [2]: https://theantlrguy.atlassian.net/wiki/display/ANTLR4/Python+Target" />
  <row Id="2267" PostHistoryTypeId="5" PostId="866" RevisionGUID="f923efc7-d2f3-47fa-a465-09b117f60976" CreationDate="2014-07-31T12:17:27.037" UserId="2781" Comment="Added a plaintext version of the table and an explanation for the codes." Text="I am, admittedly, very new to data science. I have spent the last 8 months or so learning as much as I can about the field and its methods.  I am having issues choosing which methods to apply.&#xD;&#xA;&#xD;&#xA;I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).&#xD;&#xA;&#xD;&#xA;My goals are to:&#xD;&#xA;&#xD;&#xA; 1. Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;&#xD;&#xA; 2. Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;&#xD;&#xA; 3. Do the same as 1 and 2, but with procedures and/or diagnoses.&#xD;&#xA; 4. Preferably, the results would be interpretable by a doctor&#xD;&#xA;&#xD;&#xA;I have looked at things like the [Heritage Health Prize Milestone papers][1] and have learned a lot from them, but they are focused on predicting hospitalizations.&#xD;&#xA;&#xD;&#xA;I have thrown a number of algorithms at the problem (random forests, logistic regression, CART, Cox regressions) and it's been an amazing learning experience.  I have not been able to decide on what &quot;works&quot; or &quot;doesn't work,&quot; if you know what I mean.  I have enough knowledge and skills to be misled by my own excitement and naivete; what I need is to be able to get excited about something real.&#xD;&#xA;&#xD;&#xA;So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?&#xD;&#xA;&#xD;&#xA;EDIT #2 to add plaintext table:&#xD;&#xA;&#xD;&#xA;CKD is the target condition, &quot;chronic kidney disease&quot;, &quot;.any&quot; denotes that they have acquired that condition at any time, &quot;.isbefore.ckd&quot; means they had that condition before their frist diagnosis of CKD.  The other abbreviations correspond with other conditions identified by ICD-9CM code groupings.  This grouping occurs in SQL during the import process. Each variable, with the exception of patient_age, is binary.&#xD;&#xA;&#xD;&#xA;      gender patient_age anx.any art.any ast.any bpa.any can.any cer.any chf.any ckd.any dep.any dia.any end.any flu.any hrt.any hyp.any inf.any men.any ren.any sdp.any&#xD;&#xA;    1   Male          31       1       0       1       1       0       0       0       0       1       0       1       1       1       1       0       1       0       0&#xD;&#xA;    2 Female          29       1       0       1       1       0       0       0       0       1       0       1       0       0       0       0       0       0       0&#xD;&#xA;    3 Female          31       0       1       1       1       0       0       0       0       0       1       0       0       0       0       0       1       0       0&#xD;&#xA;    4 Female          53       1       1       1       1       1       0       0       0       1       0       1       0       1       0       1       0       0       0&#xD;&#xA;    5   Male          47       0       1       0       0       0       0       0       0       0       0       0       0       0       1       0       1       0       0&#xD;&#xA;    6 Female          48       0       1       1       1       1       0       0       0       0       0       1       0       1       0       1       0       0       0&#xD;&#xA;      skn.any tra.any anx.isbefore.ckd art.isbefore.ckd ast.isbefore.ckd bpa.isbefore.ckd can.isbefore.ckd cer.isbefore.ckd chf.isbefore.ckd ckd.isbefore.ckd&#xD;&#xA;    1       1       1                0                0                0                0                0                0                0                0&#xD;&#xA;    2       0       1                0                0                0                0                0                0                0                0&#xD;&#xA;    3       0       0                0                0                0                0                0                0                0                0&#xD;&#xA;    4       1       0                0                0                0                0                0                0                0                0&#xD;&#xA;    5       0       1                0                0                0                0                0                0                0                0&#xD;&#xA;    6       1       0                0                0                0                0                0                0                0                0&#xD;&#xA;      dep.isbefore.ckd dia.isbefore.ckd end.isbefore.ckd flu.isbefore.ckd hrt.isbefore.ckd hyp.isbefore.ckd inf.isbefore.ckd men.isbefore.ckd ren.isbefore.ckd&#xD;&#xA;    1                0                0                0                0                0                0                0                0                0&#xD;&#xA;    2                0                0                0                0                0                0                0                0                0&#xD;&#xA;    3                0                0                0                0                0                0                0                0                0&#xD;&#xA;    4                0                0                0                0                0                0                0                0                0&#xD;&#xA;    5                0                0                0                0                0                0                0                0                0&#xD;&#xA;    6                0                0                0                0                0                0                0                0                0&#xD;&#xA;      sdp.isbefore.ckd skn.isbefore.ckd tra.isbefore.ckd&#xD;&#xA;    1                0                0                0&#xD;&#xA;    2                0                0                0&#xD;&#xA;    3                0                0                0&#xD;&#xA;    4                0                0                0&#xD;&#xA;    5                0                0                0&#xD;&#xA;    6                0                0                0&#xD;&#xA;&#xD;&#xA;EDIT to add sample data frame:&#xD;&#xA;&#xD;&#xA;    structure(list(gender = structure(c(1L, 2L, 2L, 2L, 1L, 2L), .Label = c(&quot;Male&quot;,         &quot;Female&quot;), class = &quot;factor&quot;), patient_age = c(31, 29, 31, 53, &#xD;&#xA;    47, 48), anx.any = c(1, 1, 0, 1, 0, 0), art.any = c(0, 0, 1, &#xD;&#xA;    1, 1, 1), ast.any = c(1, 1, 1, 1, 0, 1), bpa.any = c(1, 1, 1, &#xD;&#xA;    1, 0, 1), can.any = c(0, 0, 0, 1, 0, 1), cer.any = c(0, 0, 0, &#xD;&#xA;    0, 0, 0), chf.any = c(0, 0, 0, 0, 0, 0), ckd.any = c(0, 0, 0, &#xD;&#xA;    0, 0, 0), dep.any = c(1, 1, 0, 1, 0, 0), dia.any = c(0, 0, 1, &#xD;&#xA;    0, 0, 0), end.any = c(1, 1, 0, 1, 0, 1), flu.any = c(1, 0, 0, &#xD;&#xA;    0, 0, 0), hrt.any = c(1, 0, 0, 1, 0, 1), hyp.any = c(1, 0, 0, &#xD;&#xA;    0, 1, 0), inf.any = c(0, 0, 0, 1, 0, 1), men.any = c(1, 0, 1, &#xD;&#xA;    0, 1, 0), ren.any = c(0, 0, 0, 0, 0, 0), sdp.any = c(0, 0, 0, &#xD;&#xA;    0, 0, 0), skn.any = c(1, 0, 0, 1, 0, 1), tra.any = c(1, 1, 0, &#xD;&#xA;    0, 1, 0), anx.isbefore.ckd = c(0, 0, 0, 0, 0, 0), art.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), ast.isbefore.ckd = c(0, 0, 0, 0, 0, 0), bpa.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), can.isbefore.ckd = c(0, 0, 0, 0, 0, 0), cer.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), chf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), ckd.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), dep.isbefore.ckd = c(0, 0, 0, 0, 0, 0), dia.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), end.isbefore.ckd = c(0, 0, 0, 0, 0, 0), flu.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), hrt.isbefore.ckd = c(0, 0, 0, 0, 0, 0), hyp.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), inf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), men.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), ren.isbefore.ckd = c(0, 0, 0, 0, 0, 0), sdp.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0), skn.isbefore.ckd = c(0, 0, 0, 0, 0, 0), tra.isbefore.ckd = c(0, &#xD;&#xA;    0, 0, 0, 0, 0)), .Names = c(&quot;gender&quot;, &quot;patient_age&quot;, &quot;anx.any&quot;, &#xD;&#xA;    &quot;art.any&quot;, &quot;ast.any&quot;, &quot;bpa.any&quot;, &quot;can.any&quot;, &quot;cer.any&quot;, &quot;chf.any&quot;, &#xD;&#xA;    &quot;ckd.any&quot;, &quot;dep.any&quot;, &quot;dia.any&quot;, &quot;end.any&quot;, &quot;flu.any&quot;, &quot;hrt.any&quot;, &#xD;&#xA;    &quot;hyp.any&quot;, &quot;inf.any&quot;, &quot;men.any&quot;, &quot;ren.any&quot;, &quot;sdp.any&quot;, &quot;skn.any&quot;, &#xD;&#xA;    &quot;tra.any&quot;, &quot;anx.isbefore.ckd&quot;, &quot;art.isbefore.ckd&quot;, &quot;ast.isbefore.ckd&quot;, &#xD;&#xA;    &quot;bpa.isbefore.ckd&quot;, &quot;can.isbefore.ckd&quot;, &quot;cer.isbefore.ckd&quot;, &quot;chf.isbefore.ckd&quot;, &#xD;&#xA;    &quot;ckd.isbefore.ckd&quot;, &quot;dep.isbefore.ckd&quot;, &quot;dia.isbefore.ckd&quot;, &quot;end.isbefore.ckd&quot;, &#xD;&#xA;    &quot;flu.isbefore.ckd&quot;, &quot;hrt.isbefore.ckd&quot;, &quot;hyp.isbefore.ckd&quot;, &quot;inf.isbefore.ckd&quot;, &#xD;&#xA;    &quot;men.isbefore.ckd&quot;, &quot;ren.isbefore.ckd&quot;, &quot;sdp.isbefore.ckd&quot;, &quot;skn.isbefore.ckd&quot;, &#xD;&#xA;    &quot;tra.isbefore.ckd&quot;), row.names = c(NA, 6L), class = &quot;data.frame&quot;)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.heritagehealthprize.com/c/hhp/details/milestone-winners" />
  <row Id="2268" PostHistoryTypeId="2" PostId="876" RevisionGUID="f77446ce-9026-40c3-8b46-20407da114e6" CreationDate="2014-07-31T16:39:37.127" UserId="1241" Text="&quot;Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease&quot;&#xD;&#xA;&#xD;&#xA;I'm not sure that it's possible to ID _the_ most influential conditions; I think it will depend on what model you're using. Just yesterday I fit a random forest and a boosted regression tree to the same data, and the order and relative importance each model gave for the variables were quite different." />
  <row Id="2269" PostHistoryTypeId="2" PostId="877" RevisionGUID="68f94e47-fd90-469e-b009-2ede54899dbf" CreationDate="2014-07-31T18:52:56.307" UserId="2798" Text="Well this looks like the most suited place for this question.	&#xD;&#xA;&#xD;&#xA;Every website collect data of the user, some just for usability and personalization, but the majority like social networks track every move on the web, some free apps on your phone scan text messages, call history and so on.&#xD;&#xA;&#xD;&#xA;All this data siphoning is just for selling your profile for advertisers?&#xD;&#xA;" />
  <row Id="2270" PostHistoryTypeId="1" PostId="877" RevisionGUID="68f94e47-fd90-469e-b009-2ede54899dbf" CreationDate="2014-07-31T18:52:56.307" UserId="2798" Text="What is the use of user data collection besides serving ads?" />
  <row Id="2271" PostHistoryTypeId="3" PostId="877" RevisionGUID="68f94e47-fd90-469e-b009-2ede54899dbf" CreationDate="2014-07-31T18:52:56.307" UserId="2798" Text="&lt;data-mining&gt;" />
  <row Id="2272" PostHistoryTypeId="2" PostId="878" RevisionGUID="8ecc6c3d-c305-4cba-9d1b-ee041e18bf52" CreationDate="2014-07-31T23:03:33.767" UserId="1279" Text="A couple of days ago developers from one product company asked me how they can understand why new users were leaving their website. My first question to them was what these users' profiles looked like and how they were different from those who stayed. &#xD;&#xA;&#xD;&#xA;Advertising is only top of an iceberg. User profiles (either filled by users themselves or computed from users' behaviour) hold information about: &#xD;&#xA;&#xD;&#xA; - **user categories**, i.e. what kind of people tend to use your website/product&#xD;&#xA; - **paying client portraits**, i.e. who is more likely to use your paid services&#xD;&#xA; - **UX component performance**, e.g. how long it takes people to find the button they need&#xD;&#xA; - **action performance comparison**, e.g. what was more efficient - lower price for a weekend or propose gifts with each buy, etc. &#xD;&#xA;&#xD;&#xA;So it's more about improving product and making better user experience rather than selling this data to advertisers. " />
  <row Id="2273" PostHistoryTypeId="2" PostId="879" RevisionGUID="c5ca8056-c8f4-4c19-bb9d-e3734de518c9" CreationDate="2014-08-01T06:03:47.700" UserId="434" Text="Most companies won't sell the data, not on any small scale anyways.  Most will use it internally.&#xD;&#xA;&#xD;&#xA;User tracking data is important for understanding a lot of things.  There's basic A/B testing where you provide different experiences to see which is more effective.  There is understanding how your UI is utilized.  Categorizing your end users in different ways for a variety of reasons.  Figuring out where your end user base is, and within that group where the end users that matter are.  Correlating user experiences with social network updates.  Figuring out what will draw people to your product and what drives them away.  The list of potential for data mining and analysis projects could go on for days.  &#xD;&#xA;&#xD;&#xA;Data storage is cheap.  If you track everything out of the gate, you can figure out what you want to do with that data later.  &#xD;&#xA;&#xD;&#xA;Scanning text messages is sketchy territory when there isn't a good reason for it.  Even when there is a good reason it's sketchy territory.  I'd love to say that nobody does it, but there have been instances where big companies have done it and there are a lot of cases where no-name apps at least require access to that kind of data for installation.  I generally frown on that kind of thing myself as a consumer, but the data analyst in me would love to see if I could build anything useful from a set of information like that." />
  <row Id="2274" PostHistoryTypeId="2" PostId="880" RevisionGUID="efc6a71f-ea1f-4bcb-92ef-46aa8a1b4aad" CreationDate="2014-08-01T12:27:41.580" UserId="172" Text="There always is the solution to try both approaches and keep the one that maximizes the expected performances. &#xD;&#xA;&#xD;&#xA;In your case, I would assume you prefer minimizing false negatives at the cost of some false positive, so you want to bias your classifier against the strong negative prior, and address the imbalance by reducing the number of negative examples in your training set.&#xD;&#xA;&#xD;&#xA;Then compute the precision/recall, or sensitivity/specificity, or whatever criterion suits you on the full, imbalanced, dataset to make sure you haven't ignored a significant pattern present in the real data while building the model on the reduced data." />
  <row Id="2275" PostHistoryTypeId="2" PostId="881" RevisionGUID="e628b95d-e967-4d3f-a67d-a217f7b5c9d6" CreationDate="2014-08-01T14:08:13.043" UserId="1279" Text="I've never worked with medical data, but from general reasoning I'd say that relations between variables in healthcare are pretty complicated. Different models, such as random forests, regression, etc. could capture only part of relations and ignore others. In such circumstances it makes sense to use general **statistical exploration** and **modelling**. &#xD;&#xA;&#xD;&#xA;For example, the very first thing I would do is finding out **correlations** between possible precursor conditions and diagnoses. E.g. in what percent of cases chronic kidney disease was preceded by long flu? If it is high, it [doesn't always mean causality](http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation), but gives pretty good food for thought and helps to better understand relations between different conditions. &#xD;&#xA;&#xD;&#xA;Another important step is data visualisation. Does CKD happens in males more often than in females? What about their place of residence? What is distribution of CKD cases by age? It's hard to grasp large dataset as a set of numbers, plotting them out makes it much easier. &#xD;&#xA;&#xD;&#xA;When you have an idea of what's going on, perform [**hypothesis testing**](http://en.wikipedia.org/wiki/Statistical_hypothesis_testing) to check your assumption. If you reject null hypothesis (basic assumption) in favour of alternative one, congratulations, you've made &quot;something real&quot;. &#xD;&#xA;&#xD;&#xA;Finally, when you have a good understanding of your data, try to create complete **model**. It may be something general like [PGM](https://www.coursera.org/course/pgm) (e.g. manually-crafted Bayesian network), or something more specific like linear regression or [SVM](http://en.wikipedia.org/wiki/Support_vector_machine), or anything. But in any way you will already know how this model corresponds to your data and how you can measure its efficiency. &#xD;&#xA;&#xD;&#xA;---- &#xD;&#xA;&#xD;&#xA;As a good starting resource for learning statistical approach I would recommend [Intro to Statistics](https://www.udacity.com/course/st101) course by Sebastian Thrun. While it's pretty basic and doesn't include advanced topics, it describes most important concepts and gives systematic understanding of probability theory and statistics. " />
  <row Id="2276" PostHistoryTypeId="2" PostId="882" RevisionGUID="adf0fffe-fa80-4517-b3be-6eb35c972ab0" CreationDate="2014-08-01T14:10:38.267" UserId="403" Text="Here's a practical example of using web data for something other than advertising. Distil Networks (disclaimer, I work there) uses network traffic to determine whether page accesses are from humans or bots - scrapers, click fraud, form spam, etc.&#xD;&#xA;&#xD;&#xA;Another example is some of the work that Webtrends is doing. They allow site users to build a model for each visitor to predict whether they'll leave, buy, add to cart, etc. Then based on the probability of each action you can change the users experience (e.g. if they're about to leave, give them a coupon). Here's the slides from a talk by them: http://www.oscon.com/oscon2014/public/schedule/detail/34809" />
  <row Id="2277" PostHistoryTypeId="2" PostId="883" RevisionGUID="ba66fbea-968d-49f9-bbd7-5fb798ce357b" CreationDate="2014-08-01T18:13:06.063" UserId="2614" Text="I am working on a data set that has multiple traffic speed measurements per day. My data is from the city of chicago, and it is taken every minute for about six months. I wanted to consolidate this data into days only, so this is what I did:&#xD;&#xA;&#xD;&#xA;    traffic &lt;- read.csv(&quot;path.csv&quot;,header=TRUE)&#xD;&#xA;    traffic2 &lt;- aggregate(SPEED~DATE, data=traffic, FUN=MEAN)&#xD;&#xA;&#xD;&#xA;this was perfect because it took all of my data and averaged it by date. For example, my original data looked something like this:&#xD;&#xA;&#xD;&#xA;    DATE        SPEED  &#xD;&#xA;    12/31/2012   22&#xD;&#xA;    12/31/2012   25&#xD;&#xA;    12/31/2012   23&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;and the final looked like this: &#xD;&#xA;&#xD;&#xA;    DATE        SPEED &#xD;&#xA;    10/1/2012    22&#xD;&#xA;    10/2/2012    23&#xD;&#xA;    10/3/2012    22&#xD;&#xA;    ...&#xD;&#xA;&#xD;&#xA;The only problem, is my data is supposed to start at 9/1/2012. I plotted this data, and it turns out the data goes from 10/1/2012-12/31/2012 and then 9/1/2012-9/30/2012.&#xD;&#xA;&#xD;&#xA;What in the world is going on here?" />
  <row Id="2278" PostHistoryTypeId="1" PostId="883" RevisionGUID="ba66fbea-968d-49f9-bbd7-5fb798ce357b" CreationDate="2014-08-01T18:13:06.063" UserId="2614" Text="R aggregate() with dates" />
  <row Id="2279" PostHistoryTypeId="3" PostId="883" RevisionGUID="ba66fbea-968d-49f9-bbd7-5fb798ce357b" CreationDate="2014-08-01T18:13:06.063" UserId="2614" Text="&lt;r&gt;&lt;dataset&gt;&lt;beginner&gt;" />
  <row Id="2280" PostHistoryTypeId="2" PostId="884" RevisionGUID="e706efd6-d705-4f21-a9b1-9b99ad70c4a3" CreationDate="2014-08-01T18:16:57.543" UserId="2614" Text="So I was never able to find the error by looking through my logs. I ended up reinstalling it with CDH5 (which was MUCH easier than installing &quot;poor&quot; Hadoop)&#xD;&#xA;Now everything runs fine! &#xD;&#xA;&#xD;&#xA;I'm still having trouble getting things to save to the hdfs, but thats a question for another day... " />
  <row Id="2284" PostHistoryTypeId="2" PostId="886" RevisionGUID="ae203d9d-752b-4657-af1d-ccce4946f464" CreationDate="2014-08-01T23:21:34.360" UserId="2452" Text="I am going to agree with @user1683454's comment. After importing, your DATE column is of either `character`, or `factor` class (depending on your settings for `stringsAsFactors`). Therefore, I think that you can solve this issue in at least several ways, as follows:&#xD;&#xA;&#xD;&#xA;1) **Convert data** to correct type **during import**. To do this, just use the following options of `read.csv()`: `stringsAsFactors` (or `as.is`) and `colClasses`. By default, you can specify conversion to `Date` or `POSIXct` classes. If you need a non-standard format, you have two options. First, if you have a single Date column, you can use `as.Date.character()` to pass the desired format to `colClasses`. Second, if you have multiple Date columns, you can write a function for that and pass it to `colClasses` via `setAs()`. Both options are discussed here: http://stackoverflow.com/questions/13022299/specify-date-format-for-colclasses-argument-in-read-table-read-csv.&#xD;&#xA;&#xD;&#xA;2) **Convert data** to correct format **after import**. Thus, after calling `read.csv()`, you would have to execute the following code: `dateColumn &lt;- as.Date(dateColumn, &quot;%m/%d/%Y&quot;)` or `dateColumn &lt;- strptime(dateColumn, &quot;%m/%d/%Y&quot;)` (adjust the format to whatever Date format you need)." />
  <row Id="2285" PostHistoryTypeId="2" PostId="887" RevisionGUID="9f9262aa-1eee-4610-add4-ea7d03d9d0b0" CreationDate="2014-08-02T00:07:09.267" UserId="776" Text="I am currently on a project that will build a model (train and test) on Client-side Web data, but evaluate this model on Sever-side Web data.  Unfortunately building the model on Server-side data is not an option, nor is it an option to evaluate this model on Client-side data.&#xD;&#xA;&#xD;&#xA;This model will be based on metrics collected on specific visitors.  This is a real time system that will be calculating a likelihood based on metrics collected while visitors browse the website.&#xD;&#xA;&#xD;&#xA;I am looking for approaches to ensure the highest possible accuracy on the model evaluation.&#xD;&#xA;&#xD;&#xA;So far I have the following ideas,&#xD;&#xA;&#xD;&#xA; 1. Clean the Server-side data by removing webpages that are never seen Client-side.&#xD;&#xA; 2. Collect additional data Server-side data to make the Server-side data more closely resemble Client-side data.&#xD;&#xA; 3. Collect data on the Client and send this data to the Server.  This is possible and may be the best solution, but is currently undesirable. &#xD;&#xA; 4. Build one or more models that estimate Client-side Visitor metrics from Server-side Visitor metrics and use these estimates in the Likelihood model.&#xD;&#xA;&#xD;&#xA;Any other thoughts on evaluating over one Population while training (and testing) on another Population?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2286" PostHistoryTypeId="1" PostId="887" RevisionGUID="9f9262aa-1eee-4610-add4-ea7d03d9d0b0" CreationDate="2014-08-02T00:07:09.267" UserId="776" Text="Modelling on one Population and Evaluating on another Population" />
  <row Id="2287" PostHistoryTypeId="3" PostId="887" RevisionGUID="9f9262aa-1eee-4610-add4-ea7d03d9d0b0" CreationDate="2014-08-02T00:07:09.267" UserId="776" Text="&lt;data-cleaning&gt;&lt;evaluation&gt;" />
  <row Id="2288" PostHistoryTypeId="2" PostId="888" RevisionGUID="392cede8-56f6-4316-99a8-cb8fe593449d" CreationDate="2014-08-02T04:21:49.927" UserId="2452" Text="I'm not an expert on this, so take my advice with a grain of salt. It's not clear for me what is the relationship between server-side and client-side data. Are they both **representative** of the same population? If Yes, I think it's OK to use different data sets for testing/training and evaluating your models. If No, I think it might be a good idea to use some **resampling** technique, such as *bootstrapping*, *jackknifing* or *cross-validation*." />
  <row Id="2289" PostHistoryTypeId="2" PostId="889" RevisionGUID="9e5e2816-3d1a-409e-83e9-b1ec42ec31b6" CreationDate="2014-08-02T04:54:14.757" UserId="2666" Text="If the users who you are getting client-side data from are from the same population of users who you would get server-side data from. If that is true, then you aren't really training on one population and applying to another. The main difference is that the client side data happened in the past (by necessity unless you are constantly refitting your model) and the server side data will come in the future.&#xD;&#xA;&#xD;&#xA;Let's reformulate the question in terms of models rather than web clients and servers.&#xD;&#xA;&#xD;&#xA;You are fitting a model on one dataset and applying it to another. That is the classic use of predictive modeling/machine learning. Models use features from the data to make estimates of some parameter or parameters. Once you have a fitted (and tested) model, all that you need is the same set of features to feed into the model to get your estimates.&#xD;&#xA;&#xD;&#xA;Just make sure to model on a set of features (aka variables) that are available on the client-side and server-side. If that isn't possible, ask that question separately." />
  <row Id="2290" PostHistoryTypeId="5" PostId="722" RevisionGUID="1c381e49-1437-4014-b392-6f671983956f" CreationDate="2014-08-02T08:16:08.640" UserId="2452" Comment="Added info on 'dplyr' and 'data.table' packages." Text="R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**/**reshape2**, and **plyr**/**dplyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.&#xD;&#xA;&#xD;&#xA;Additionally, R offers some packages specifically *focused* on data cleaning and transformation:&#xD;&#xA;&#xD;&#xA;- **editrules** (http://cran.r-project.org/web/packages/editrules/index.html)&#xD;&#xA;- **deducorrect** (http://cran.r-project.org/web/packages/deducorrect/index.html)&#xD;&#xA;- **StatMatch** (http://cran.r-project.org/web/packages/StatMatch/index.html)&#xD;&#xA;- **MatchIt** (http://cran.r-project.org/web/packages/MatchIt/index.html)&#xD;&#xA;- **DataCombine** (http://cran.r-project.org/web/packages/DataCombine)&#xD;&#xA;- **data.table** (http://cran.r-project.org/web/packages/data.table)&#xD;&#xA;&#xD;&#xA;A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf." />
  <row Id="2291" PostHistoryTypeId="2" PostId="890" RevisionGUID="230f608e-4499-492a-a0b7-f505d7f27e01" CreationDate="2014-08-02T14:55:37.347" UserId="2809" Text="If your work is parallelizable enough for a distributed network of cpus to make a difference, why not try to run it on the gpu instead? That will require rather less investment than a large network of cpus with individual software licenses and still provide parallel processing which you can do runtime tracking on yourself." />
  <row Id="2292" PostHistoryTypeId="2" PostId="891" RevisionGUID="8cb72728-8d5a-41bc-827a-88915ef25486" CreationDate="2014-08-02T15:02:42.437" UserId="2809" Text="As others have pointed out, these are not distance &quot;metrics&quot;, because they do not satisfy the metric criteria. Say instead &quot;distance measure&quot;.&#xA;&#xA;Anyway, what are you measuring and why? That information will help us give a more useful answer for your situation." />
  <row Id="2293" PostHistoryTypeId="10" PostId="739" RevisionGUID="31347440-6d70-4283-983a-b4380b1e1ab0" CreationDate="2014-08-03T06:18:00.257" UserId="62" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="2294" PostHistoryTypeId="10" PostId="808" RevisionGUID="3d0c2814-6676-41fd-97ee-2af85dd8ddef" CreationDate="2014-08-03T06:18:34.900" UserId="62" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="2298" PostHistoryTypeId="2" PostId="893" RevisionGUID="b95f0f54-b303-4fe8-93fc-1d2fe7a1d34c" CreationDate="2014-08-03T13:07:24.143" UserId="1151" Text="I am building a regression model and I need to calculate the below to check for correlations&#xD;&#xA;&#xD;&#xA; 1. Correlation between 2 Multi level categorical variables&#xD;&#xA; 2. Correlation between a Multi level categorical variable and&#xD;&#xA;    continuous variable &#xD;&#xA; 3. VIF(variance inflation factor) for a Multi&#xD;&#xA;    level categorical variables&#xD;&#xA;&#xD;&#xA;I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. &#xD;&#xA;&#xD;&#xA;Please answer the below questions&#xD;&#xA;&#xD;&#xA; 1. Which correlation coefficient works best for the above cases ? &#xD;&#xA; 2. VIF calculation only works for continuous data so what is the&#xD;&#xA;    alternative? &#xD;&#xA; 3. What are the assumptions I need to check before I use the correlation coefficient you suggest? &#xD;&#xA; 4. How to implement them in SAS &amp; R?" />
  <row Id="2299" PostHistoryTypeId="1" PostId="893" RevisionGUID="b95f0f54-b303-4fe8-93fc-1d2fe7a1d34c" CreationDate="2014-08-03T13:07:24.143" UserId="1151" Text="How to get correlation between two categorical variable and a categorical variable and continuous variable?" />
  <row Id="2300" PostHistoryTypeId="3" PostId="893" RevisionGUID="b95f0f54-b303-4fe8-93fc-1d2fe7a1d34c" CreationDate="2014-08-03T13:07:24.143" UserId="1151" Text="&lt;r&gt;&lt;statistics&gt;" />
  <row Id="2302" PostHistoryTypeId="2" PostId="895" RevisionGUID="adad3fec-871f-4d39-a048-fe3f2d88751e" CreationDate="2014-08-03T16:09:35.750" UserId="2756" Text="I assume that each person on Facebook is represented as a node (of a Graph) in Facebook, and relationship/friendship between each person(node) is represented as an edge between the involved nodes.&#xD;&#xA;&#xD;&#xA;Given that there are millions of people on Facebook, how is the Graph stored?" />
  <row Id="2303" PostHistoryTypeId="1" PostId="895" RevisionGUID="adad3fec-871f-4d39-a048-fe3f2d88751e" CreationDate="2014-08-03T16:09:35.750" UserId="2756" Text="Facebook's Huge Database" />
  <row Id="2304" PostHistoryTypeId="3" PostId="895" RevisionGUID="adad3fec-871f-4d39-a048-fe3f2d88751e" CreationDate="2014-08-03T16:09:35.750" UserId="2756" Text="&lt;graphs&gt;" />
  <row Id="2305" PostHistoryTypeId="2" PostId="896" RevisionGUID="cc403f7e-d9fe-4f2d-aac9-7b1a49c2c8e4" CreationDate="2014-08-03T16:38:38.853" UserId="2817" Text="I am trying to implement the Brown Clustering Algorithm [(link to paper)](http://delivery.acm.org/10.1145/180000/176316/p467-brown.pdf?ip=197.78.151.216&amp;id=176316&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=522814165&amp;CFTOKEN=99738065&amp;__acm__=1407081874_5a9649adaf4a9d43b8faa5c1a2da47f8)&#xD;&#xA;&#xD;&#xA;The algorithm is supposed to in O(|V|k^2) where |V| is the size of the vocabulary and k is the number of clusters. I am unable to implement it this efficiently. In fact, the best I can manage is O(|V|k^3) which is too slow. My current implementation for the main part of the algorithm is as follows:&#xD;&#xA;&#xD;&#xA;    for w = number of clusters + 1 to |V|&#xD;&#xA;    {&#xD;&#xA;       word = next most frequent word in the corpus&#xD;&#xA;&#xD;&#xA;       assign word to a new cluster &#xD;&#xA;&#xD;&#xA;       initialize MaxQuality to 0&#xD;&#xA;&#xD;&#xA;       initialize ArgMax vector to (0,0)&#xD;&#xA;&#xD;&#xA;       for i = 0 to number of clusters - 1 &#xD;&#xA;       {&#xD;&#xA;          for j = i to number of clusters&#xD;&#xA;          {&#xD;&#xA;             Quality = Mutual Information if we merge cluster i and cluster j&#xD;&#xA;&#xD;&#xA;             if Quality &gt; MaxQuality&#xD;&#xA;             {&#xD;&#xA;                MaxQuality = Quality &#xD;&#xA;                &#xD;&#xA;                ArgMax = (i,j) &#xD;&#xA;             }&#xD;&#xA;          }&#xD;&#xA;       }&#xD;&#xA;    } &#xD;&#xA;&#xD;&#xA;I compute quality as follows:&#xD;&#xA;&#xD;&#xA;    1. Before entering the second loop compute the pre-merge quality i.e. quality before doing any merges.&#xD;&#xA;    2. Every time a cluster-pair merge step is considered:&#xD;&#xA;        i. assign quality := pre-merge quality&#xD;&#xA;       ii. quality = quality - any terms in the mutual information equation that contain cluster i or cluster j (pre-merge)&#xD;&#xA;      iii. quality = quality + any terms in the mutual information equation that contain (cluster i U cluster j)  (post-merge)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;In my implementation, the first loop has approx |V| iterations, the second and third loop approx k iterations each. To compute quality at each step requires approx a further k iterations. In total it runs in (|V|k^3) time.&#xD;&#xA;&#xD;&#xA;How do you get it to run in (|V|k^2)?&#xD;&#xA;" />
  <row Id="2306" PostHistoryTypeId="1" PostId="896" RevisionGUID="cc403f7e-d9fe-4f2d-aac9-7b1a49c2c8e4" CreationDate="2014-08-03T16:38:38.853" UserId="2817" Text="How to implement Brown Clustering Algorithm in O(|V|k^2)" />
  <row Id="2307" PostHistoryTypeId="3" PostId="896" RevisionGUID="cc403f7e-d9fe-4f2d-aac9-7b1a49c2c8e4" CreationDate="2014-08-03T16:38:38.853" UserId="2817" Text="&lt;nlp&gt;&lt;efficiency&gt;&lt;clustering&gt;" />
  <row Id="2308" PostHistoryTypeId="2" PostId="897" RevisionGUID="518edbd3-ab0e-49da-80d7-1a518244aced" CreationDate="2014-08-03T20:21:08.150" UserId="587" Text="Having worked with Facebook data a bit (harvested from Facebook users) we stored it just as a pair of values: USER_ID, FRIEND_USER_ID.&#xD;&#xA;&#xD;&#xA;But I guess your questions is a bit deeper? You can store it in different ways, depending on your research question. One interesting option is triads for example - http://mypersonality.org/wiki/doku.php?id=list_of_variables_available#triads" />
  <row Id="2311" PostHistoryTypeId="2" PostId="898" RevisionGUID="c3428f00-3669-4d12-ba83-f3b90b04d8e0" CreationDate="2014-08-04T09:42:03.590" UserId="816" Text="## Two Categorical Variables&#xD;&#xA;&#xD;&#xA;Checking if two categorical variables are independent can be done with Chi-Squared test of independence. &#xD;&#xA;&#xD;&#xA;This is a typical [Chi-Square test][1]: if we assume that two variables are independent, then the values of the contingency table for these variables should be distributed uniformly. And then we check how far away from uniform the actual values are&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### Example &#xD;&#xA;&#xD;&#xA;Suppose we have two variables&#xD;&#xA; &#xD;&#xA; - gender: male and female&#xD;&#xA; - city: Blois and Tours&#xD;&#xA;&#xD;&#xA;We observed the following data:&#xD;&#xA;&#xD;&#xA;![observed values][2] &#xD;&#xA;&#xD;&#xA;Are gender and city independent? Let's perform a Chi-Squred test. Null hypothesis: they are independent, Alternative hypothesis is that they are correlated in some way. &#xD;&#xA;&#xD;&#xA;Under the Null hypothesis, we assume uniform distribution. So our expected values are the following&#xD;&#xA;&#xD;&#xA;![expected value][3]&#xD;&#xA;&#xD;&#xA;So we run the chi-squared test and the resulting p-value here can be seen as a measure of correlation between these two variables.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### R &#xD;&#xA;&#xD;&#xA;    tbl = matrix(data=c(55, 45, 20, 30), nrow=2, ncol=2, byrow=T)&#xD;&#xA;    dimnames(tbl) = list(City=c('B', 'T'), Gender=c('M', 'F'))&#xD;&#xA;&#xD;&#xA;    chi2 = chisq.test(tbl, correct=F)&#xD;&#xA;    c(chi2$statistic, chi2$p.value)&#xD;&#xA;&#xD;&#xA;Here the p value is 0.08 - quite small, but still not enough to reject the hypothesis of independence. So we can say that the &quot;correlation&quot; here is 0.08&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Consider another dataset &#xD;&#xA;&#xD;&#xA;        Gender&#xD;&#xA;    City  M  F&#xD;&#xA;       B 51 49&#xD;&#xA;       T 24 26&#xD;&#xA;&#xD;&#xA;For this, it would give the following&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    tbl = matrix(data=c(51, 49, 24, 26), nrow=2, ncol=2, byrow=T)&#xD;&#xA;    dimnames(tbl) = list(City=c('B', 'T'), Gender=c('M', 'F'))&#xD;&#xA;&#xD;&#xA;    chi2 = chisq.test(tbl, correct=F)&#xD;&#xA;    c(chi2$statistic, chi2$p.value)&#xD;&#xA;&#xD;&#xA;The p-value is 0.72 which is far closer to 1.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;## Categorical vs Numerical Variables&#xD;&#xA;&#xD;&#xA;For this type we typically perform [One-way ANOVA test][4]: we calculate in-group variance and intra-group variance and then compare them.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### Example&#xD;&#xA;&#xD;&#xA;We want to study the relationship between absorbed fat from donuts vs the type of fat used to produce donuts (example is taken from [here][5])&#xD;&#xA;&#xD;&#xA;![donuts][6]&#xD;&#xA;&#xD;&#xA;Is there any dependence between the variables?&#xD;&#xA;For that we conduct ANOVA test and see that the p-value is just 0.007 - there's no correlation between these variables. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### R &#xD;&#xA;&#xD;&#xA;    t1 = c(164, 172, 168, 177, 156, 195)&#xD;&#xA;    t2 = c(178, 191, 197, 182, 185, 177)&#xD;&#xA;    t3 = c(175, 193, 178, 171, 163, 176)&#xD;&#xA;    t4 = c(155, 166, 149, 164, 170, 168)&#xD;&#xA;&#xD;&#xA;    val = c(t1, t2, t3, t4)&#xD;&#xA;    fac = gl(n=4, k=6, labels=c('type1', 'type2', 'type3', 'type4'))&#xD;&#xA;&#xD;&#xA;    aov1 = aov(val ~ fac)&#xD;&#xA;    summary(aov1)&#xD;&#xA;&#xD;&#xA;Output is &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;                Df Sum Sq Mean Sq F value  Pr(&gt;F)   &#xD;&#xA;    fac          3   1636   545.5   5.406 0.00688 **&#xD;&#xA;    Residuals   20   2018   100.9                   &#xD;&#xA;    ---&#xD;&#xA;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So we can take the p-value as the measure of correlation here as well.&#xD;&#xA;&#xD;&#xA;## References&#xD;&#xA;&#xD;&#xA; - https://en.wikipedia.org/wiki/Chi-square_test&#xD;&#xA; - http://0agr.ru/wiki/index.php/Chi-square_Test_of_Independence&#xD;&#xA; - http://courses.statistics.com/software/R/R1way.htm&#xD;&#xA; - http://0agr.ru/wiki/index.php/One-Way_ANOVA_F-Test&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Chi-square_test&#xD;&#xA;  [2]: http://i.stack.imgur.com/zcCfV.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/H8bKJ.png&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example&#xD;&#xA;  [5]: http://courses.statistics.com/software/R/R1way.htm&#xD;&#xA;  [6]: http://i.stack.imgur.com/LMOS3.png" />
  <row Id="2312" PostHistoryTypeId="2" PostId="899" RevisionGUID="82e6d756-4ab7-467c-b4b9-cfced49bc363" CreationDate="2014-08-04T09:48:09.660" UserId="816" Text="When I worked with social network data, we stoted the &quot;friendship&quot; relation in a database in the table `Friends(friend_a, friend_b, ...)` with a B-Tree index on `(friend_a, friend_b)` plus also some partitioning.&#xD;&#xA;&#xD;&#xA;In our case it was a little bit different since the graph was directed, so it wasn't really &quot;friendship&quot;, but rather &quot;following/follower&quot; relationship. But for friendship I would just store two edges: both `(friend_a, friend_b)` and `(friend_b, friend_a)`&#xD;&#xA;&#xD;&#xA;We used MySQL to store the data, if it matters, but I guess it shouldn't." />
  <row Id="2313" PostHistoryTypeId="2" PostId="900" RevisionGUID="19ba47d3-c8d6-447d-a426-758ec0d0f44a" CreationDate="2014-08-04T15:48:29.757" UserId="2574" Text="I think hierarchical clustering would be more time efficient in your case (with a single dimension).&#xD;&#xA;Depending on your task, you may implement something like this:&#xD;&#xA;&#xD;&#xA;Having N datapoints d&lt;sub&gt;i&lt;/sub&gt; with their 1-dimension value x&lt;sub&gt;i&lt;/sub&gt;:&#xD;&#xA;&#xD;&#xA;1. Sort datapoints based on their x&lt;sub&gt;i&lt;/sub&gt; value. &#xD;&#xA;2. Calculate distances between adjacent datapoints (N-1 distances). Each distance must be assigned a pair of original datapoints (d&lt;sub&gt;i&lt;/sub&gt;, d&lt;sub&gt;j&lt;/sub&gt;).&#xD;&#xA;3. Sort distances in descending order to generate list of datapoint pairs (d&lt;sub&gt;i&lt;/sub&gt;, d&lt;sub&gt;j&lt;/sub&gt;), starting from the closest one.&#xD;&#xA;4. Iteratively unite datapoints (d&lt;sub&gt;i&lt;/sub&gt;, d&lt;sub&gt;j&lt;/sub&gt;) into clusters, starting from beginning of the list (the closest pair). (Depending on current state of d&lt;sub&gt;i&lt;/sub&gt; and d&lt;sub&gt;j&lt;/sub&gt;, uniting them means: (a) creating new cluster for two unclustered datapoints, (b) adding a datapoint to existing cluster and (c) uniting two clusters.)&#xD;&#xA;5. Stop uniting, if the distance is over some threshold.&#xD;&#xA;6. Create singleton clusters for datapoints which did not get into clusters.&#xD;&#xA;&#xD;&#xA;This algorithm implements [single linkage][1] clustering. It can be tuned easily to implement average linkage. [Complete linkage][2] will be less efficient, but maybe easier ones will give good results depending on your data and task.&#xD;&#xA;&#xD;&#xA;I believe for 200K datapoints it must take under second, if you use proper data structures for above operations.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Single-linkage_clustering&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Complete_linkage_clustering&#xD;&#xA;" />
  <row Id="2314" PostHistoryTypeId="2" PostId="901" RevisionGUID="cedadb73-43bc-48ac-9d76-d5f8392a8e4b" CreationDate="2014-08-04T15:59:12.573" UserId="548" Text="Strange as it sounds, graphs and graph databases are typically implemented as [linked lists][1]. As alluded to [here][2], even the most popular/performant graph database out there (neo4j), is secretly using something akin to a doubly-linked list.&#xD;&#xA;&#xD;&#xA;Representing a graph this way has a number of significant benefits, but also a few drawbacks. Firstly, representing a graph this way means that you can do edge-based insertions in near-constant time. Secondly, this means that traversing the graph can happen extremely quickly, if we're only looking to either step up or down a linked list.&#xD;&#xA;&#xD;&#xA;The biggest drawback of this though comes from something sometimes called The Justin Bieber Effect, where nodes with a large number of connections tend to be extremely slow to evaluate. Imagine having to traverse a million semi-redundant links every time someone linked to Justin Bieber.&#xD;&#xA;&#xD;&#xA;I know that the awesome folks over at Neo4j are working on the second problem, but I'm not sure how they're going about it, or how much success they've had.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://docs.oracle.com/javase/7/docs/api/java/util/LinkedList.html&#xD;&#xA;  [2]: http://docs.neo4j.org/chunked/stable/cypher-cookbook-newsfeed.html" />
  <row Id="2315" PostHistoryTypeId="2" PostId="902" RevisionGUID="ffacf40c-7d21-4420-b077-7df86474b0d1" CreationDate="2014-08-04T19:10:57.187" UserId="1097" Text="Are there any general rules that one can use to infer what can be learned/generalized from a particular data set?  Suppose the dataset was taken from a sample of people.  Can these rules be stated as functions of the sample or total population?&#xD;&#xA;&#xD;&#xA;I understand the above may be vague, so a case scenario: Users participate in a search task, where the data are their queries, clicked results, and the HTML content (text only) of those results.  Each of these are tagged with their user and timestamp.  A user may generate a few pages - for a simple fact-finding task - or hundreds of pages - for a longer-term search task, like for class report.&#xD;&#xA;&#xD;&#xA;Theory and paper references are a plus!" />
  <row Id="2316" PostHistoryTypeId="1" PostId="902" RevisionGUID="ffacf40c-7d21-4420-b077-7df86474b0d1" CreationDate="2014-08-04T19:10:57.187" UserId="1097" Text="When is there enough data for generalization?" />
  <row Id="2317" PostHistoryTypeId="3" PostId="902" RevisionGUID="ffacf40c-7d21-4420-b077-7df86474b0d1" CreationDate="2014-08-04T19:10:57.187" UserId="1097" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;statistics&gt;&lt;search&gt;" />
  <row Id="2318" PostHistoryTypeId="5" PostId="902" RevisionGUID="0486f797-1576-4cd6-a558-e28b766789c5" CreationDate="2014-08-04T19:23:09.483" UserId="1097" Comment="added 170 characters in body" Text="Are there any general rules that one can use to infer what can be learned/generalized from a particular data set?  Suppose the dataset was taken from a sample of people.  Can these rules be stated as functions of the sample or total population?&#xD;&#xA;&#xD;&#xA;I understand the above may be vague, so a case scenario: Users participate in a search task, where the data are their queries, clicked results, and the HTML content (text only) of those results.  Each of these are tagged with their user and timestamp.  A user may generate a few pages - for a simple fact-finding task - or hundreds of pages - for a longer-term search task, like for class report.&#xD;&#xA;&#xD;&#xA;Edit:  In addition to generalizing about a population, given a sample, I'm interested in generalizing about an individual's overall search behavior, given a time slice.  Theory and paper references are a plus!" />
  <row Id="2319" PostHistoryTypeId="2" PostId="903" RevisionGUID="b622da73-8801-4229-a3a6-bebeba5686ef" CreationDate="2014-08-04T22:27:10.837" UserId="2830" Text="I have a set of results from an A/B test (one control group, one feature group) which do not fit a Normal Distribution. &#xD;&#xA;In fact the distribution resembles more closely the Landau Distribution.&#xD;&#xA;&#xD;&#xA;I believe the independent t-test requires that the samples be at least approximately normally distributed, which discourages me using the t-test as a valid method of significance testing.&#xD;&#xA;&#xD;&#xA;But my question is: &#xD;&#xA;**At what point can one say that the t-test is not a good method of significance testing?**&#xD;&#xA;&#xD;&#xA;Or put another way, how can one qualify how reliable the p-values of a t-test are, given only the data set?" />
  <row Id="2320" PostHistoryTypeId="1" PostId="903" RevisionGUID="b622da73-8801-4229-a3a6-bebeba5686ef" CreationDate="2014-08-04T22:27:10.837" UserId="2830" Text="Analyzing A/B test results which are not normally distributed, using independent t-test" />
  <row Id="2321" PostHistoryTypeId="3" PostId="903" RevisionGUID="b622da73-8801-4229-a3a6-bebeba5686ef" CreationDate="2014-08-04T22:27:10.837" UserId="2830" Text="&lt;statistics&gt;&lt;dataset&gt;" />
  <row Id="2322" PostHistoryTypeId="36" PostId="904" RevisionGUID="822a0735-9cac-41dd-93ac-f89e75b611b9" CreationDate="2014-08-04T22:59:23.877" UserId="-1" Comment="from http://stats.stackexchange.com/questions/110604/what-do-you-use-to-generate-a-dashboard-in-r" />
  <row Id="2323" PostHistoryTypeId="36" PostId="905" RevisionGUID="98e74a0d-be54-4c73-acd7-0079b9551a22" CreationDate="2014-08-04T22:59:23.877" UserId="-1" Comment="from http://stats.stackexchange.com/questions/110604/what-do-you-use-to-generate-a-dashboard-in-r/110605#110605" />
  <row Id="2324" PostHistoryTypeId="2" PostId="905" RevisionGUID="ad27813b-29c3-4bb6-99c9-cbac1687e299" CreationDate="2014-08-04T19:28:38.173" UserId="1156" Text="[Shiny][1] is a framework for generating HTML-based apps that execute R code dynamically. Shiny apps can stand alone or be built into Markdown documents with `knitr`, and Shiny development is fully integrated into RStudio. There's even a free service called [shinyapps.io][2] for hosting Shiny apps, the `shiny` package has functions for deploying Shiny apps directly from R, and RStudio has a GUI interface for calling those functions. There's plenty more info in the Tutorial section of the site.&#xD;&#xA;&#xD;&#xA;Since it essentially &quot;compiles&quot; the whole thing to JavaScript and HTML, you can use CSS to freely change the formatting and layout, although Shiny has decent wrapper functionality for this. But it just so happens that their default color scheme is similar to the one in the screenshot you posted.&#xD;&#xA;&#xD;&#xA;  [1]: http://shiny.rstudio.com&#xD;&#xA;  [2]: https://www.shinyapps.io&#xD;&#xA;&#xD;&#xA;edit: I just realized you don't need them to be dynamic. Shiny still makes very nice-looking webpages out of the box, with lots of options for rearranging elements. There's also functionality for downloading plots, so you can generate your dashboard every month by just updating your data files in the app, and then saving the resulting image to PDF." />
  <row Id="2325" PostHistoryTypeId="2" PostId="904" RevisionGUID="34bd5df0-ece1-4be5-9430-30735b425640" CreationDate="2014-08-04T19:21:45.067" UserDisplayName="aiolias" Text="I need to generate periodic (daily, monthly) web analytics dashboard reports. They will be static and don't require interaction, so imagine a PDF file as the target output. The reports will mix tables and charts (mainly sparkline and bullet graphs created with ggplot2). Think Stephen Few/Perceptual Edge style dashboards, such as: ![sample dashboard][1]&#xD;&#xA;&#xD;&#xA;but applied to web analytics. &#xD;&#xA;&#xD;&#xA;Any suggestions on what packages to use creating these dashboard reports? &#xD;&#xA;&#xD;&#xA;My first intuition is to use R markdown and knitr, but perhaps you've found a better solution. I can't seem to find rich examples of dashboards generated from R. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/Edh2e.png" />
  <row Id="2326" PostHistoryTypeId="1" PostId="904" RevisionGUID="34bd5df0-ece1-4be5-9430-30735b425640" CreationDate="2014-08-04T19:21:45.067" UserDisplayName="aiolias" Text="What do you use to generate a dashboard in R?" />
  <row Id="2327" PostHistoryTypeId="3" PostId="904" RevisionGUID="34bd5df0-ece1-4be5-9430-30735b425640" CreationDate="2014-08-04T19:21:45.067" UserDisplayName="aiolias" Text="&lt;untagged&gt;" />
  <row Id="2328" PostHistoryTypeId="2" PostId="906" RevisionGUID="33d304c9-2864-4b6a-83d7-74d668d9741e" CreationDate="2014-08-04T23:37:27.877" UserId="1156" Text="[PajekXXL][1] is designed to handle enormous networks. But Pajek is also kind of a bizarre program with an unintuitive interface.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://mrvar.fdv.uni-lj.si/pajek/PajekXXL.htm" />
  <row Id="2329" PostHistoryTypeId="2" PostId="907" RevisionGUID="c03a7433-246d-4fab-90eb-4b711d93105d" CreationDate="2014-08-05T07:17:34.750" UserId="2452" Text="I think that `Shiny` is an overkill in this situation and doesn't match your requirement of *dashboard reports* to be **static**. I guess, that your use of the term &quot;dashboard&quot; is a bit confusing, as some people might consider that it has more emphasis of **interactivity** (*real-time dashboards*), rather than **information layout**, as is my understanding (confirmed by the &quot;static&quot; requirement).&#xD;&#xA;&#xD;&#xA;My recommendation to you is to use **R Markdown** and **knitr**, especially since these packages have much lower learning curve than **Shiny**. Moreover, I have recently run across an R package, which, in my view, ideally suits your requirement of embedding small charts/plots in a report, as presented on your picture above. This package generates static or dynamic *graphical tables* and is called **sparkTable** (http://cran.r-project.org/web/packages/sparkTable). Its vignette is available here (there is no link to it on the package's home page): http://publik.tuwien.ac.at/files/PubDat_228663.pdf. Should you ever need some *interactivity*, `sparkTable` provides some via its simple interface to `Shiny`." />
  <row Id="2330" PostHistoryTypeId="2" PostId="908" RevisionGUID="fe657e42-9024-40b1-8796-364591f8d6c6" CreationDate="2014-08-05T08:09:17.240" UserId="2452" Text="It is my understanding that *random sampling* is a **mandatory condition** for making any *generalization* statements. IMHO, other parameters, such as sample size, just affect probability level (confidence) of generalization. Furthermore, clarifying the @ffriend's comment, I believe that you have to **calculate** needed *sample size*, based on desired values of *confidence interval*, *effect size*, *statistical power* and *number of predictors* (this is based on Cohen's work - see References section at the following link). For multiple regression, you can use the following calculator: http://www.danielsoper.com/statcalc3/calc.aspx?id=1.&#xD;&#xA;&#xD;&#xA;More information on **how to select, calculate and interpret effect sizes** can be found in the following nice and comprehensive paper, which is freely available: http://jpepsy.oxfordjournals.org/content/34/9/917.full.&#xD;&#xA;&#xD;&#xA;If you're using `R` (and even, if you don't), you may find the following Web page on **confidence intervals and R** interesting and useful: http://osc.centerforopenscience.org/static/CIs_in_r.html.&#xD;&#xA;&#xD;&#xA;Finally, the following **comprehensive guide** to survey **sampling** can be helpful, even if you're not using survey research designs. In my opinion, it contains a wealth of useful information on *sampling methods*, *sampling size determination* (including calculator) and much more: http://home.ubalt.edu/ntsbarsh/stat-data/Surveys.htm." />
  <row Id="2331" PostHistoryTypeId="2" PostId="909" RevisionGUID="8b61bcff-6432-4e9b-b50e-c7b3e5505b08" CreationDate="2014-08-05T08:12:15.647" UserId="816" Text="The distribution of your data doesn't need to be normal, it's the [Sampling Distribution][1] that has to be nearly normal. If your sample size is big enough, then the sampling distribution of means from Landau Distribution should to be nearly normal, due to the [Central Limit Theorem][2]. &#xD;&#xA;&#xD;&#xA;So it means you should be able to safely use t-test with your data.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### Example&#xD;&#xA;&#xD;&#xA;Let's consider this example: suppose we have a population with [Lognormal distribution][3] with mu=0 and sd=0.5 (it looks a bit similar to Landau)&#xD;&#xA;&#xD;&#xA;![lognormal density][4]&#xD;&#xA;&#xD;&#xA;So we sample 30 observations 5000 times from this distribution each time calculating the mean of the sample &#xD;&#xA;&#xD;&#xA;And this is what we get &#xD;&#xA;&#xD;&#xA;![sampling distribution][5]&#xD;&#xA;&#xD;&#xA;Looks quite normal, doesn't it? If we increase the sample size, it's even more apparent&#xD;&#xA;&#xD;&#xA;![sampling distribution][6]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### R code&#xD;&#xA;&#xD;&#xA;    x = seq(0, 4, 0.05)&#xD;&#xA;    y = dlnorm(x, mean=0, sd=0.5)&#xD;&#xA;    plot(x, y, type='l', bty='n')&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    n = 30&#xD;&#xA;    m = 1000&#xD;&#xA;    &#xD;&#xA;    set.seed(0)&#xD;&#xA;    samp = rep(NA, m)&#xD;&#xA;    &#xD;&#xA;    for (i in 1:m) {&#xD;&#xA;      samp[i] = mean(rlnorm(n, mean=0, sd=0.5))&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    hist(samp, col='orange', probability=T, breaks=25, main='sample size = 30')&#xD;&#xA;    x = seq(0.5, 1.5, 0.01)&#xD;&#xA;    lines(x, dnorm(x, mean=mean(samp), sd=sd(samp)))&#xD;&#xA;    &#xD;&#xA;    &#xD;&#xA;    n = 300&#xD;&#xA;    samp = rep(NA, m)&#xD;&#xA;    &#xD;&#xA;    for (i in 1:m) {&#xD;&#xA;      samp[i] = mean(rlnorm(n, mean=0, sd=0.5))&#xD;&#xA;    }&#xD;&#xA;    &#xD;&#xA;    hist(samp, col='orange', probability=T, breaks=25, main='sample size = 300')&#xD;&#xA;    x = seq(1, 1.25, 0.005)&#xD;&#xA;    lines(x, dnorm(x, mean=mean(samp), sd=sd(samp)))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Sampling_distribution&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Central_limit_theorem&#xD;&#xA;  [3]: https://en.wikipedia.org/wiki/Log-normal_distribution&#xD;&#xA;  [4]: http://i.stack.imgur.com/Hw5mM.png&#xD;&#xA;  [5]: http://i.stack.imgur.com/wjueS.png&#xD;&#xA;  [6]: http://i.stack.imgur.com/M0FQS.png" />
  <row Id="2332" PostHistoryTypeId="2" PostId="910" RevisionGUID="a2f27326-6eb6-4e54-a635-93e11e588e52" CreationDate="2014-08-05T09:07:42.393" UserId="2574" Text="Here are the basic Natural Language Processing capabilities (or annotators) that are usually necessary to extract language units from textual data for sake of search and other applications:&#xD;&#xA;&#xD;&#xA;[Sentence breaker][1] - to split text (usually, text paragraphs) to sentences. Even in English it can be hard for some cases like &quot;Mr. and Mrs. Brown stay in room no. 20.&quot;&#xD;&#xA;&#xD;&#xA;[Tokenizer][2] - to split text or sentences to words or word-level units. This task is not trivial for languages with no spaces and no stable understanding of word boundaries (e.g. Chinese, Japanese)&#xD;&#xA;&#xD;&#xA;[Part-of-speech Tagger][3] - to guess part of speech of each word in the context of sentence; usually each word is assigned a so-called POS-tag from a tagset developed in advance to serve your final task (for example, parsing).&#xD;&#xA;&#xD;&#xA;[Lemmatizer][4] - to convert a given word into its canonical form ([lemma][5]). Usually you need to know the word's POS-tag. For example, word &quot;heating&quot; as gerund must be converted to &quot;heat&quot;, but as noun it must be left unchanged.&#xD;&#xA;&#xD;&#xA;[Parser][6] - to perform syntactic analysis of the sentence and build a syntactic tree or graph. There're two main ways to represent syntactic structure of sentence: via [constituency or dependency][7].&#xD;&#xA;&#xD;&#xA;[Summarizer][8] - to generate a short summary of the text by selecting a set of top informative sentences of the document, representing its main idea. However can be done in more intelligent manner than just selecting the sentences from existing ones.&#xD;&#xA;&#xD;&#xA;[Named Entity Recognition][9] - to extract so-called named entities from the text. Named entities are the chunks of words from text, which refer to an entity of certain type. The types may include: geographic locations (countries, cities, rivers, ...), person names, organization names etc. Before going into NER task you must understand what do you want to get and, possible, predefine a taxonomy of named entity types to resolve.&#xD;&#xA;&#xD;&#xA;[Coreference Resolution][10] - to group named entities (or, depending on your task, any other text units) into clusters corresponding to a single real object/meaning. For example, &quot;B. Gates&quot;, &quot;William Gates&quot;, &quot;Founder of Microsoft&quot; etc. in one text may mean the same person, referenced by using different expressions.&#xD;&#xA;&#xD;&#xA;There're many other interesting NLP applications/annotators (see [NLP tasks category][11]), sentiment analysis, machine translation etc.). There're many books on this, the classical book: &quot;Speech and Language Processing&quot; by Daniel Jurafsky and James H. Martin., but it can be too detailed for you.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Sentence_boundary_disambiguation&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Tokenization&#xD;&#xA;  [3]: http://en.wikipedia.org/wiki/POS_tagger&#xD;&#xA;  [4]: http://en.wikipedia.org/wiki/Lemmatization&#xD;&#xA;  [5]: http://en.wikipedia.org/wiki/Lemma_(morphology)&#xD;&#xA;  [6]: http://en.wikipedia.org/wiki/Parser&#xD;&#xA;  [7]: http://en.wikipedia.org/wiki/Dependency_grammar#Dependency_vs._constituency&#xD;&#xA;  [8]: http://en.wikipedia.org/wiki/Automatic_summarization&#xD;&#xA;  [9]: http://en.wikipedia.org/wiki/Named-entity_recognition&#xD;&#xA;  [10]: http://en.wikipedia.org/wiki/Coreference_resolution&#xD;&#xA;  [11]: http://en.wikipedia.org/wiki/Category:Tasks_of_natural_language_processing" />
  <row Id="2333" PostHistoryTypeId="2" PostId="911" RevisionGUID="ee5323d5-1fa6-4be5-9a5f-388c2cae53ab" CreationDate="2014-08-05T10:15:33.713" UserId="108" Text="Basically an independent t-test or a 2 sample t-test is used to check if the averages of the two samples are significantly different. Or, to put in another words, if there is a significant difference between the means of the two samples. &#xD;&#xA;&#xD;&#xA;Now, the means of those 2 samples are two statistics, which according with CLT, have a normal distribution, if provided enough samples. Note that CLT works no matter of the distribution from which the mean statistic is built. &#xD;&#xA;&#xD;&#xA;Normally one can use a z-test, but if the variances are estimated from the sample (because it is unknown), some additional uncertainty is introduced, which is incorporated in t distribution. That's why 2-sample t-test applies here.  " />
  <row Id="2334" PostHistoryTypeId="2" PostId="912" RevisionGUID="07ebc09d-dbf5-43cd-88e0-33fbeb530311" CreationDate="2014-08-05T12:29:05.300" UserId="1156" Text="There is an excellent comparison of the common inner-product-based similarity metrics [here][1].&#xD;&#xA;&#xD;&#xA;In particular, Cosine Similarity is normalized to lie within [0,1], unlike the dot product which can be any real number, but, as everyone else is saying, that will require ignoring the magnitude of the vectors. Personally, I think that's a good thing. I think of magnitude as an internal (within-vector) structure, and angle between vectors as external (between vector) structure. They are different things and (in my opinion) are often best analyzed separately. I can't imagine a situation where I would rather compute inner products than compute cosine similarities and just compare the magnitudes afterward.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/" />
  <row Id="2335" PostHistoryTypeId="2" PostId="913" RevisionGUID="e56e5ad6-71c9-46ca-b417-22aad8b00816" CreationDate="2014-08-05T12:58:16.000" UserId="1156" Text="There are two rules for generalizability:&#xD;&#xA;&#xD;&#xA; 1. The sample must be **representative**. In expectation, at least, the distribution of features in your sample must match the distribution of features in the population. When you are fitting a model with a response variable, *this includes features that you do not observe, but that affect any response variables in your model*. Since it is, in many cases, impossible to know what you do not observe, **random sampling** is used.&#xD;&#xA;&#xD;&#xA; The idea with randomization is that a random sample, up to sampling error, *must* accurately reflect the distribution of all features in the population, observed and otherwise. This is why **randomization is the &quot;gold standard,&quot;** but if sample control is available by some other technique, or it is defensible to argue that there are no omitted features, then it isn't always necessary.&#xD;&#xA;&#xD;&#xA; 2. Your sample must be **large enough** that the effect of **sampling error** on the feature distribution is relatively small. This is, again, to ensure representativeness. But deciding who to sample is different from deciding how many people to sample.&#xD;&#xA;&#xD;&#xA;Since it sounds like you're fitting a model, there's the additional consideration that certain important combinations of features could be relatively rare in the population. This is not an issue for generalizability, but it bears heavily on your considerations for sample size. For instance, I'm working on a project now with (non-big) data that was originally collected to understand the experiences of minorities in college. As such, it was critically important to ensure that **statistical power** was high *specifically in the minority subpopulation*. For this reason, blacks and Latinos were deliberately **oversampled**. However, the proportion by which they were oversampled was also recorded. These are used to compute survey weights. These can be used to  re-weight the sample so as to reflect the estimated population proportions, in the event that a representative sample is required.&#xD;&#xA;&#xD;&#xA;An additional consideration arises if your model is hierarchical. A canonical use for a hierarchical model is one of children's behavior in schools. Children are &quot;grouped&quot; by school and share school-level traits. Therefore a representative sample of schools is required, and within each school a representative sample of children is required. This leads to **stratified sampling**. This and some other sampling designs are reviewed in surprising depth on [Wikipedia][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Sampling_(statistics)#Sampling_methods" />
  <row Id="2336" PostHistoryTypeId="5" PostId="898" RevisionGUID="d69cee38-e222-4870-9eaf-b36215621ada" CreationDate="2014-08-05T15:55:08.573" UserId="816" Comment="added crammer's v" Text="## Two Categorical Variables&#xD;&#xA;&#xD;&#xA;Checking if two categorical variables are independent can be done with Chi-Squared test of independence. &#xD;&#xA;&#xD;&#xA;This is a typical [Chi-Square test][1]: if we assume that two variables are independent, then the values of the contingency table for these variables should be distributed uniformly. And then we check how far away from uniform the actual values are.&#xD;&#xA;&#xD;&#xA;There also exists a [Crammer's V][2] that is a measure of correlation that follows from this test&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### Example &#xD;&#xA;&#xD;&#xA;Suppose we have two variables&#xD;&#xA; &#xD;&#xA; - gender: male and female&#xD;&#xA; - city: Blois and Tours&#xD;&#xA;&#xD;&#xA;We observed the following data:&#xD;&#xA;&#xD;&#xA;![observed values][3] &#xD;&#xA;&#xD;&#xA;Are gender and city independent? Let's perform a Chi-Squred test. Null hypothesis: they are independent, Alternative hypothesis is that they are correlated in some way. &#xD;&#xA;&#xD;&#xA;Under the Null hypothesis, we assume uniform distribution. So our expected values are the following&#xD;&#xA;&#xD;&#xA;![expected value][4]&#xD;&#xA;&#xD;&#xA;So we run the chi-squared test and the resulting p-value here can be seen as a measure of correlation between these two variables.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;To compute Crammer's V we first find the normalizing factor chi-squared-max which is typically the size of the sample, divide the chi-square by it and take a square root &#xD;&#xA;&#xD;&#xA;![crammers v][5]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### R &#xD;&#xA;&#xD;&#xA;    tbl = matrix(data=c(55, 45, 20, 30), nrow=2, ncol=2, byrow=T)&#xD;&#xA;    dimnames(tbl) = list(City=c('B', 'T'), Gender=c('M', 'F'))&#xD;&#xA;&#xD;&#xA;    chi2 = chisq.test(tbl, correct=F)&#xD;&#xA;    c(chi2$statistic, chi2$p.value)&#xD;&#xA;&#xD;&#xA;Here the p value is 0.08 - quite small, but still not enough to reject the hypothesis of independence. So we can say that the &quot;correlation&quot; here is 0.08&#xD;&#xA;&#xD;&#xA;We also compute V: &#xD;&#xA;&#xD;&#xA;    sqrt(chi2$statistic / sum(tbl))&#xD;&#xA;&#xD;&#xA;And get 0.14 (the smaller v, the lower the correlation) &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Consider another dataset &#xD;&#xA;&#xD;&#xA;        Gender&#xD;&#xA;    City  M  F&#xD;&#xA;       B 51 49&#xD;&#xA;       T 24 26&#xD;&#xA;&#xD;&#xA;For this, it would give the following&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    tbl = matrix(data=c(51, 49, 24, 26), nrow=2, ncol=2, byrow=T)&#xD;&#xA;    dimnames(tbl) = list(City=c('B', 'T'), Gender=c('M', 'F'))&#xD;&#xA;&#xD;&#xA;    chi2 = chisq.test(tbl, correct=F)&#xD;&#xA;    c(chi2$statistic, chi2$p.value)&#xD;&#xA;&#xD;&#xA;    sqrt(chi2$statistic / sum(tbl))&#xD;&#xA;&#xD;&#xA;The p-value is 0.72 which is far closer to 1, and v is 0.03 - very close to 0&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;## Categorical vs Numerical Variables&#xD;&#xA;&#xD;&#xA;For this type we typically perform [One-way ANOVA test][6]: we calculate in-group variance and intra-group variance and then compare them.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### Example&#xD;&#xA;&#xD;&#xA;We want to study the relationship between absorbed fat from donuts vs the type of fat used to produce donuts (example is taken from [here][7])&#xD;&#xA;&#xD;&#xA;![donuts][8]&#xD;&#xA;&#xD;&#xA;Is there any dependence between the variables?&#xD;&#xA;For that we conduct ANOVA test and see that the p-value is just 0.007 - there's no correlation between these variables. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;### R &#xD;&#xA;&#xD;&#xA;    t1 = c(164, 172, 168, 177, 156, 195)&#xD;&#xA;    t2 = c(178, 191, 197, 182, 185, 177)&#xD;&#xA;    t3 = c(175, 193, 178, 171, 163, 176)&#xD;&#xA;    t4 = c(155, 166, 149, 164, 170, 168)&#xD;&#xA;&#xD;&#xA;    val = c(t1, t2, t3, t4)&#xD;&#xA;    fac = gl(n=4, k=6, labels=c('type1', 'type2', 'type3', 'type4'))&#xD;&#xA;&#xD;&#xA;    aov1 = aov(val ~ fac)&#xD;&#xA;    summary(aov1)&#xD;&#xA;&#xD;&#xA;Output is &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;                Df Sum Sq Mean Sq F value  Pr(&gt;F)   &#xD;&#xA;    fac          3   1636   545.5   5.406 0.00688 **&#xD;&#xA;    Residuals   20   2018   100.9                   &#xD;&#xA;    ---&#xD;&#xA;    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So we can take the p-value as the measure of correlation here as well.&#xD;&#xA;&#xD;&#xA;## References&#xD;&#xA;&#xD;&#xA; - https://en.wikipedia.org/wiki/Chi-square_test&#xD;&#xA; - http://0agr.ru/wiki/index.php/Chi-square_Test_of_Independence&#xD;&#xA; - http://courses.statistics.com/software/R/R1way.htm&#xD;&#xA; - http://0agr.ru/wiki/index.php/One-Way_ANOVA_F-Test&#xD;&#xA; - http://0agr.ru/wiki/index.php/Crammer%27s_Coefficient&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Chi-square_test&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V&#xD;&#xA;  [3]: http://i.stack.imgur.com/zcCfV.png&#xD;&#xA;  [4]: http://i.stack.imgur.com/H8bKJ.png&#xD;&#xA;  [5]: http://i.stack.imgur.com/v7HY6.png&#xD;&#xA;  [6]: http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example&#xD;&#xA;  [7]: http://courses.statistics.com/software/R/R1way.htm&#xD;&#xA;  [8]: http://i.stack.imgur.com/LMOS3.png" />
  <row Id="2338" PostHistoryTypeId="2" PostId="915" RevisionGUID="c9beb7d8-58ec-419f-82f1-dbb57f29e166" CreationDate="2014-08-05T18:36:12.753" UserId="2841" Text="Is there a known general table of statistical techniques that explain how they scale with sample size and dimension? For example, a friend of mine told me the other day that the computation time of simply quick-sorting one dimensional data of size n goes as n*log(n).&#xD;&#xA;&#xD;&#xA;So, for example, if we regress y against X where X is a d-dimensional variable, does it go as O(n^2*d)? How does it scale if I want to find the solution via exact Gauss-Markov solution vs numerical least squares with Newton method? Or simply getting the solution vs using significance tests?&#xD;&#xA;&#xD;&#xA;I guess I more want a good source of answers (like a paper that summarizes the scaling of various statistical techniques) than a good answer here. Like, say, a list that includes the scaling of multiple regression, logistic regression, PCA, cox proportional hazard regression, K-means clustering, etc." />
  <row Id="2339" PostHistoryTypeId="1" PostId="915" RevisionGUID="c9beb7d8-58ec-419f-82f1-dbb57f29e166" CreationDate="2014-08-05T18:36:12.753" UserId="2841" Text="How do various statistical techniques (regression, PCA, etc) scale with $n$ and $d$?" />
  <row Id="2340" PostHistoryTypeId="3" PostId="915" RevisionGUID="c9beb7d8-58ec-419f-82f1-dbb57f29e166" CreationDate="2014-08-05T18:36:12.753" UserId="2841" Text="&lt;bigdata&gt;&lt;statistics&gt;&lt;efficiency&gt;&lt;scalability&gt;" />
  <row Id="2341" PostHistoryTypeId="4" PostId="915" RevisionGUID="46ce03b3-7ee1-4bda-95a6-6d485fa45419" CreationDate="2014-08-05T18:46:46.157" UserId="2841" Comment="edited title" Text="How do various statistical techniques (regression, PCA, etc) scale with sample size and dimension?" />
  <row Id="2342" PostHistoryTypeId="2" PostId="916" RevisionGUID="3e63146c-a12d-4b89-9566-57ff6b61512e" CreationDate="2014-08-05T20:24:09.200" UserId="172" Text="Most of the efficient (and non trivial) statistic algorithms are iterative in nature so that the worst case analysis `O()` is irrelevant as the worst case is 'it fails to converge'.&#xD;&#xA;&#xD;&#xA;Nevertheless, when you have a lot of data, even the linear algorithms (`O(n)`) can be slow and you then need to focus on the constant 'hidden' behind the notation. For instance, computing the variance of a single variate is naively done scanning the data twice (once for computing an estimate of the mean, and then once to estimate the variance). But it also can be done in [one pass][1].&#xD;&#xA;&#xD;&#xA;For iterative algorithms, what is more important is convergence rate and number of parameters as a function of the data dimensionality, an element that greatly influences convergence. Many models/algorithm grow a number of parameters that is exponential with the number of variables (e.g. splines) while some other grow linearly (e.g. support vector machines, random forests, ...) &#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm" />
  <row Id="2343" PostHistoryTypeId="2" PostId="917" RevisionGUID="10b9587e-aaa1-4585-88a3-881c1eac636e" CreationDate="2014-08-05T20:45:01.383" UserId="802" Text="&#xD;&#xA;I am attempting to solve a set of equations which has 40 independent variables (x1, ..., x40) and one dependent variable (y). The total number of equations (number of rows) is ~300, and I want to solve for the set of 40 coefficients that minimizes the total sum-of-square error between y and the predicted value. &#xD;&#xA;&#xD;&#xA;My problem is that the matrix is very sparse and I do not know the best way to solve the system of equations with sparse data. An example of the dataset is shown below:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;       y    x1  x2 x3 x4 x5 x6 ... x40&#xD;&#xA;    87169   14  0  1  0  0  2  ... 0 &#xD;&#xA;    46449   0   0  4  0  1  4  ... 12&#xD;&#xA;    846449  0   0  0  0  0  3  ... 0&#xD;&#xA;    ....&#xD;&#xA;&#xD;&#xA;I am currently using a Genetic Algorithm to solve this and the results are coming out &#xD;&#xA;with roughly a factor of two difference between observed and expected. &#xD;&#xA;&#xD;&#xA;Can anyone suggest different methods or techniques which are capable of solving a set of equations with sparse data.&#xD;&#xA;&#xD;&#xA;Thank you!" />
  <row Id="2344" PostHistoryTypeId="1" PostId="917" RevisionGUID="10b9587e-aaa1-4585-88a3-881c1eac636e" CreationDate="2014-08-05T20:45:01.383" UserId="802" Text="Solving a system of equations with spare data" />
  <row Id="2345" PostHistoryTypeId="3" PostId="917" RevisionGUID="10b9587e-aaa1-4585-88a3-881c1eac636e" CreationDate="2014-08-05T20:45:01.383" UserId="802" Text="&lt;machine-learning&gt;&lt;algorithms&gt;&lt;genetic&gt;" />
  <row Id="2346" PostHistoryTypeId="2" PostId="918" RevisionGUID="85bcc11d-a313-42df-9c48-1af29607e260" CreationDate="2014-08-05T22:34:04.550" UserId="2452" Text="If I understand you correctly, this is the case of **multiple linear regression with sparse data** (*sparse regression*). Assuming that, I hope you will find the following **resources** useful.&#xD;&#xA;&#xD;&#xA;1) NCSU **lecture slides on sparse regression** with overview of algorithms, notes, formulas, graphics and references to literature: http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect23sparse.pdf&#xD;&#xA;&#xD;&#xA;2) `R` ecosystem offers many **packages**, useful for sparse regression analysis, including:&#xD;&#xA;&#xD;&#xA;+ **Matrix** (http://cran.r-project.org/web/packages/Matrix)&#xD;&#xA;+ **SparseM** (http://cran.r-project.org/web/packages/SparseM)&#xD;&#xA;+ **MatrixModels** (http://cran.r-project.org/web/packages/MatrixModels)&#xD;&#xA;+ **glmnet** (http://cran.r-project.org/web/packages/glmnet)&#xD;&#xA;+ **flare** (http://cran.r-project.org/web/packages/flare)&#xD;&#xA;&#xD;&#xA;3) A blog post with an **example of sparse regression solution**, based on `SparseM`: http://aleph-nought.blogspot.com/2012/03/multiple-linear-regression-with-sparse.html&#xD;&#xA;&#xD;&#xA;4) A blog post on using **sparse matrices in R**, which includes a **primer** on using `glmnet`: http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r&#xD;&#xA;&#xD;&#xA;5) **More examples and some discussion** on the topic can be found on **StackOverflow**: http://stackoverflow.com/questions/3169371/large-scale-regression-in-r-with-a-sparse-feature-matrix" />
  <row Id="2347" PostHistoryTypeId="5" PostId="917" RevisionGUID="10cce655-ba0c-448e-b1dc-ea379ceec6cb" CreationDate="2014-08-06T03:05:32.037" UserId="84" Comment="deleted 16 characters in body; edited title" Text="I am attempting to solve a set of equations which has 40 independent variables (x1, ..., x40) and one dependent variable (y). The total number of equations (number of rows) is ~300, and I want to solve for the set of 40 coefficients that minimizes the total sum-of-square error between y and the predicted value. &#xD;&#xA;&#xD;&#xA;My problem is that the matrix is very sparse and I do not know the best way to solve the system of equations with sparse data. An example of the dataset is shown below:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;       y    x1  x2 x3 x4 x5 x6 ... x40&#xD;&#xA;    87169   14  0  1  0  0  2  ... 0 &#xD;&#xA;    46449   0   0  4  0  1  4  ... 12&#xD;&#xA;    846449  0   0  0  0  0  3  ... 0&#xD;&#xA;    ....&#xD;&#xA;&#xD;&#xA;I am currently using a Genetic Algorithm to solve this and the results are coming out &#xD;&#xA;with roughly a factor of two difference between observed and expected. &#xD;&#xA;&#xD;&#xA;Can anyone suggest different methods or techniques which are capable of solving a set of equations with sparse data." />
  <row Id="2348" PostHistoryTypeId="4" PostId="917" RevisionGUID="10cce655-ba0c-448e-b1dc-ea379ceec6cb" CreationDate="2014-08-06T03:05:32.037" UserId="84" Comment="deleted 16 characters in body; edited title" Text="Solving a system of equations with sparse data" />
  <row Id="2349" PostHistoryTypeId="2" PostId="919" RevisionGUID="bea47f69-9a2d-4c97-81c9-8307f5ca692f" CreationDate="2014-08-06T08:41:44.967" UserId="97" Text="Data set looks like:&#xD;&#xA;&#xD;&#xA;- 25000 observations&#xD;&#xA;- up to 15 predictors of different types: numeric, multi-class categorical, binary&#xD;&#xA;- target variable is binary&#xD;&#xA;&#xD;&#xA;Which cross validation method is typical for this type of problems?&#xD;&#xA;&#xD;&#xA;By default I'm using K-Fold. How many folds is enough in this case? (One of the models I use is random forest, which is time consuming...)" />
  <row Id="2350" PostHistoryTypeId="1" PostId="919" RevisionGUID="bea47f69-9a2d-4c97-81c9-8307f5ca692f" CreationDate="2014-08-06T08:41:44.967" UserId="97" Text="Which cross-validation type best suits to binary classification problem" />
  <row Id="2351" PostHistoryTypeId="3" PostId="919" RevisionGUID="bea47f69-9a2d-4c97-81c9-8307f5ca692f" CreationDate="2014-08-06T08:41:44.967" UserId="97" Text="&lt;classification&gt;&lt;cross-validation&gt;" />
  <row Id="2352" PostHistoryTypeId="2" PostId="920" RevisionGUID="11ad1c64-63a9-44e5-8516-428c71a073b0" CreationDate="2014-08-06T09:02:01.033" UserId="979" Text="There is a text summarization project called SUMMARIST. Apparently it is able to perform abstractive text summarization. I want to give it a try but unfortunately the demo links on the website do not work. Does anybody have any information regarding this? How can I test this tool?&#xD;&#xA;&#xD;&#xA;http://www.isi.edu/natural-language/projects/SUMMARIST.html&#xD;&#xA;&#xD;&#xA;Regards,&#xD;&#xA;PasMod" />
  <row Id="2353" PostHistoryTypeId="1" PostId="920" RevisionGUID="11ad1c64-63a9-44e5-8516-428c71a073b0" CreationDate="2014-08-06T09:02:01.033" UserId="979" Text="SUMMARIST: Automated Text Summarization" />
  <row Id="2354" PostHistoryTypeId="3" PostId="920" RevisionGUID="11ad1c64-63a9-44e5-8516-428c71a073b0" CreationDate="2014-08-06T09:02:01.033" UserId="979" Text="&lt;text-mining&gt;" />
  <row Id="2355" PostHistoryTypeId="2" PostId="921" RevisionGUID="4b9e1a07-d551-41cb-b68f-af5456936955" CreationDate="2014-08-06T09:03:20.857" UserId="97" Text="I am fitting a model in R.&#xD;&#xA;&#xD;&#xA;- use `createFolds` method to create several `k` folds from the data set&#xD;&#xA;- loop through the folds, repeating the following on each iteration:&#xD;&#xA;  - `train` the model on k-1 folds&#xD;&#xA;  - `predict` the outcomes for the i-th fold&#xD;&#xA;  - calculate prediction accuracy&#xD;&#xA;- average the accuracy&#xD;&#xA;&#xD;&#xA;Does R have a function that makes folds itself, repeats model tuning/predictions and gives the average accuracy back?" />
  <row Id="2356" PostHistoryTypeId="1" PostId="921" RevisionGUID="4b9e1a07-d551-41cb-b68f-af5456936955" CreationDate="2014-08-06T09:03:20.857" UserId="97" Text="Avoid iterations while calculating average model accuracy" />
  <row Id="2357" PostHistoryTypeId="3" PostId="921" RevisionGUID="4b9e1a07-d551-41cb-b68f-af5456936955" CreationDate="2014-08-06T09:03:20.857" UserId="97" Text="&lt;r&gt;&lt;accuracy&gt;&lt;cross-validation&gt;&lt;sampling&gt;&lt;beginner&gt;" />
  <row Id="2358" PostHistoryTypeId="2" PostId="922" RevisionGUID="e9d583d5-c990-46be-b42d-106a0f0f1628" CreationDate="2014-08-06T09:08:08.113" UserId="2850" Text="I have  set of documents and I want classify them to true and false &#xD;&#xA;&#xD;&#xA;My question is I have to take the whole words in the documents then I classify them depend on the similarity words in these documents or I can take only some words that I interested in then I compare it with the documents. Which one is more efficient in classify document and can work with SVM.       " />
  <row Id="2359" PostHistoryTypeId="1" PostId="922" RevisionGUID="e9d583d5-c990-46be-b42d-106a0f0f1628" CreationDate="2014-08-06T09:08:08.113" UserId="2850" Text="Can I classify set of documents using classifying method using limited number of concepts ?" />
  <row Id="2360" PostHistoryTypeId="3" PostId="922" RevisionGUID="e9d583d5-c990-46be-b42d-106a0f0f1628" CreationDate="2014-08-06T09:08:08.113" UserId="2850" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;text-mining&gt;" />
  <row Id="2361" PostHistoryTypeId="2" PostId="923" RevisionGUID="1bd4f458-d4f5-407b-9667-7c623fcc7ae7" CreationDate="2014-08-06T09:08:12.117" UserId="979" Text="I think in your case a 10-fold CV will be O.K. &#xD;&#xA;&#xD;&#xA;I think it is more important to randomize the cross validation process than selecting the ideal value for k.&#xD;&#xA;&#xD;&#xA;So repeat the CV process several times randomly and compute the variance of your classification result to determine if the results are realiable or not.&#xD;&#xA;" />
  <row Id="2362" PostHistoryTypeId="2" PostId="924" RevisionGUID="27e03d8d-a0fb-4220-8e2e-1cfdf3b8cd12" CreationDate="2014-08-06T12:16:22.850" UserId="802" Text="Yes, you can do all this using the Caret (http://caret.r-forge.r-project.org/training.html) package in R. For example,&#xD;&#xA;&#xD;&#xA;    fitControl &lt;- trainControl(## 10-fold CV&#xD;&#xA;                               method = &quot;repeatedcv&quot;,&#xD;&#xA;                               number = 10,&#xD;&#xA;                               ## repeated ten times&#xD;&#xA;                               repeats = 10)&#xD;&#xA;&#xD;&#xA;    gbmFit1 &lt;- train(Class ~ ., data = training,&#xD;&#xA;                     method = &quot;gbm&quot;,&#xD;&#xA;                     trControl = fitControl,&#xD;&#xA;                    ## This last option is actually one&#xD;&#xA;                    ## for gbm() that passes through&#xD;&#xA;                    verbose = FALSE)&#xD;&#xA;    gbmFit1&#xD;&#xA;&#xD;&#xA;which will give the output&#xD;&#xA;&#xD;&#xA;    Stochastic Gradient Boosting &#xD;&#xA;&#xD;&#xA;    157 samples&#xD;&#xA;     60 predictors&#xD;&#xA;      2 classes: 'M', 'R' &#xD;&#xA;&#xD;&#xA;    No pre-processing&#xD;&#xA;    Resampling: Cross-Validated (10 fold, repeated 10 times) &#xD;&#xA;&#xD;&#xA;    Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... &#xD;&#xA;&#xD;&#xA;    Resampling results across tuning parameters:&#xD;&#xA;&#xD;&#xA;      interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD&#xD;&#xA;      1                  50       0.8       0.5    0.1          0.2     &#xD;&#xA;      1                  100      0.8       0.6    0.1          0.2     &#xD;&#xA;      1                  200      0.8       0.6    0.09         0.2     &#xD;&#xA;      2                  50       0.8       0.6    0.1          0.2     &#xD;&#xA;      2                  100      0.8       0.6    0.09         0.2     &#xD;&#xA;      2                  200      0.8       0.6    0.1          0.2     &#xD;&#xA;      3                  50       0.8       0.6    0.09         0.2     &#xD;&#xA;      3                  100      0.8       0.6    0.09         0.2     &#xD;&#xA;      3                  200      0.8       0.6    0.08         0.2     &#xD;&#xA;&#xD;&#xA;    Tuning parameter 'shrinkage' was held constant at a value of 0.1&#xD;&#xA;    Accuracy was used to select the optimal model using  the largest value.&#xD;&#xA;    The final values used for the model were n.trees = 150, interaction.depth = 3     &#xD;&#xA;    and shrinkage = 0.1.&#xD;&#xA;&#xD;&#xA;Caret offers many other options as well so should be able to suit your needs. " />
  <row Id="2363" PostHistoryTypeId="2" PostId="925" RevisionGUID="92c51892-6a0b-47cc-b534-318148b6e6bc" CreationDate="2014-08-06T12:27:31.277" UserId="984" Text="Both methods work. However, if you retain all words in documents you would essentially be working with a higher dimensional vectors (each term representing one dimension). Consequently, a classifier, e.g. SVM, would take more time to converge.&#xD;&#xA;&#xD;&#xA;It is thus a standard practice to reduce the term-space dimensionality by pre-processing steps such as stop-word removal, stemming, Principal Component Analysis (PCA) etc.&#xD;&#xA;&#xD;&#xA;One approach could be to analyze the document corpora by a topic modelling technique such as LDA and then retaining only those words which are representative of the topics, i.e. those which have high membership values in a single topic class.&#xD;&#xA;&#xD;&#xA;Another approach (inspired by information retrieval) could be to retain the top K tf-idf terms from each document." />
  <row Id="2364" PostHistoryTypeId="5" PostId="925" RevisionGUID="8cea6fcd-ed1d-44e9-ba89-6faabc8057a0" CreationDate="2014-08-06T13:15:51.460" UserId="984" Comment="deleted 4 characters in body" Text="Both methods work. However, if you retain all words in documents you would essentially be working with high dimensional vectors (each term representing one dimension). Consequently, a classifier, e.g. SVM, would take more time to converge.&#xD;&#xA;&#xD;&#xA;It is thus a standard practice to reduce the term-space dimensionality by pre-processing steps such as stop-word removal, stemming, Principal Component Analysis (PCA) etc.&#xD;&#xA;&#xD;&#xA;One approach could be to analyze the document corpora by a topic modelling technique such as LDA and then retaining only those words which are representative of the topics, i.e. those which have high membership values in a single topic class.&#xD;&#xA;&#xD;&#xA;Another approach (inspired by information retrieval) could be to retain the top K tf-idf terms from each document." />
  <row Id="2365" PostHistoryTypeId="2" PostId="926" RevisionGUID="25f69cf3-0003-45d8-909a-91e383dd625a" CreationDate="2014-08-06T13:42:54.083" UserId="2853" Text="[Aleksandr's answer][1] is completely correct.&#xD;&#xA;&#xD;&#xA;However, the way the question is posed implies that this is a straightforward ordinary least squares regression question: minimizing the sum of squared residuals between a dependent variable and a linear combination of predictors.&#xD;&#xA;&#xD;&#xA;Now, while there may be many zeros in your design matrix, your system as such is not overly large: 300 observations on 40 predictors is no more than medium-sized. You can run such a regression using R without any special efforts for sparse data. Just use the `lm()` command (for &quot;linear model&quot;). Use `?lm` to see the help page. And note that `lm` will by default silently add a constant column of ones to your design matrix (the intercept) - include a `-1` on the right hand side of your formula to suppress this. Overall, assuming all your data (and nothing else) is in a `data.frame` called `foo`, you can do this:&#xD;&#xA;&#xD;&#xA;    model &lt;- lm(y~.-1,data=foo)&#xD;&#xA;&#xD;&#xA;And then you can look at parameter estimates etc. like this:&#xD;&#xA;&#xD;&#xA;    summary(model)&#xD;&#xA;    residuals(model)&#xD;&#xA;&#xD;&#xA;*If* your system is *much* larger, say on the order of 10,000 observations and hundreds of predictors, looking at specialized sparse solvers as per [Aleksandr's answer][1] may start to make sense.&#xD;&#xA;&#xD;&#xA;Finally, in your comment to [Aleksandr's answer][1], you mention constraints on your equation. If that is actually your key issue, there are ways to calculate constrained least squares in R. I personally like `pcls()` in the `mgcv` package. Perhaps you want to edit your question to include the type of constraints (box constraints, nonnegativity constraints, integrality constraints, linear constraints, ...) you face?&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/a/918/2853" />
  <row Id="2366" PostHistoryTypeId="2" PostId="927" RevisionGUID="3571f3fb-866d-4430-9a68-711b52096512" CreationDate="2014-08-06T15:07:07.457" UserId="2854" Text="&#xD;&#xA;&#xD;&#xA;I'm working on the dataset with lots of NA values with sklearn and pandas.DataFrame. I implemented different imputation strategies for different columns of the dataFrame based column names. For example NAs predictor 'var1' I impute with 0's and for 'var2' with mean.&#xD;&#xA;&#xD;&#xA;When I try to cross validate my model using train_test_split it returns me a nparray which does not have column names. How can I impute missing values in this nparray?&#xD;&#xA;&#xD;&#xA;P.S. I do not impute missing values in the original data set before splitting on purpose so I keep test and validation sets separately.&#xD;&#xA;" />
  <row Id="2367" PostHistoryTypeId="1" PostId="927" RevisionGUID="3571f3fb-866d-4430-9a68-711b52096512" CreationDate="2014-08-06T15:07:07.457" UserId="2854" Text="how to impute missing values on numpy array created by train_test_split from pandas.DataFrame?" />
  <row Id="2368" PostHistoryTypeId="3" PostId="927" RevisionGUID="3571f3fb-866d-4430-9a68-711b52096512" CreationDate="2014-08-06T15:07:07.457" UserId="2854" Text="&lt;pandas&gt;&lt;cross-validation&gt;&lt;sklearn&gt;" />
  <row Id="2369" PostHistoryTypeId="2" PostId="928" RevisionGUID="05c5240d-58f6-4572-8fb5-bacbc0b4c2b9" CreationDate="2014-08-06T15:10:59.600" UserId="2854" Text="K-Fold should do just fine for binary classification problem. Depending on the time it is taking to train your model and predict the outcome I would use 10-20 folds.&#xD;&#xA;&#xD;&#xA;However sometimes a single fold takes several minutes, in this case I use 3-5 folds but not less than 3. Hope it helps." />
  <row Id="2370" PostHistoryTypeId="2" PostId="929" RevisionGUID="cde06f7b-a6cc-4001-9a33-7b39254aa575" CreationDate="2014-08-06T17:07:17.520" UserId="802" Text="Can you just cast your nparray from train_test_split back into a pandas dataFrame so you can carry out your same strategy. This is very common to what I do when dealing with pandas and scikit. For example,&#xD;&#xA;&#xD;&#xA;     a = train_test_split&#xD;&#xA;     new_df = pd.DataFrame(a)" />
  <row Id="2371" PostHistoryTypeId="2" PostId="930" RevisionGUID="d07e0ae4-444c-445e-b8f0-f50dd3a03172" CreationDate="2014-08-06T18:17:29.700" UserId="2838" Text="From the link you mentioned in the comment, the train and test sets should be in the form of a  dataframe if you followed the first explanation.&#xD;&#xA;&#xD;&#xA;In that case, you could do something like this:&#xD;&#xA;&#xD;&#xA;    df[variable] = df[variable].fillna(df[variable].median())&#xD;&#xA;&#xD;&#xA;You have options on what to fill the N/A values with, check out the link.&#xD;&#xA;http://pandas.pydata.org/pandas-docs/stable/missing_data.html&#xD;&#xA;&#xD;&#xA;If you followed the second explanation, using sklearn's cross-validation, you could implement  mike1886's suggestion of transforming the arrays into dataframes and then use the fillna option.&#xD;&#xA;" />
  <row Id="2372" PostHistoryTypeId="5" PostId="918" RevisionGUID="00899b67-0133-4784-9f38-c90b0385489f" CreationDate="2014-08-06T21:32:39.427" UserId="2452" Comment="Updated the answer, based on comment by the question's author." Text="If I understand you correctly, this is the case of **multiple linear regression with sparse data** (*sparse regression*). Assuming that, I hope you will find the following **resources** useful.&#xD;&#xA;&#xD;&#xA;1) NCSU **lecture slides on sparse regression** with overview of algorithms, notes, formulas, graphics and references to literature: http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect23sparse.pdf&#xD;&#xA;&#xD;&#xA;2) `R` ecosystem offers many **packages**, useful for sparse regression analysis, including:&#xD;&#xA;&#xD;&#xA;+ **Matrix** (http://cran.r-project.org/web/packages/Matrix)&#xD;&#xA;+ **SparseM** (http://cran.r-project.org/web/packages/SparseM)&#xD;&#xA;+ **MatrixModels** (http://cran.r-project.org/web/packages/MatrixModels)&#xD;&#xA;+ **glmnet** (http://cran.r-project.org/web/packages/glmnet)&#xD;&#xA;+ **flare** (http://cran.r-project.org/web/packages/flare)&#xD;&#xA;&#xD;&#xA;3) A blog post with an **example of sparse regression solution**, based on `SparseM`: http://aleph-nought.blogspot.com/2012/03/multiple-linear-regression-with-sparse.html&#xD;&#xA;&#xD;&#xA;4) A blog post on using **sparse matrices in R**, which includes a **primer** on using `glmnet`: http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r&#xD;&#xA;&#xD;&#xA;5) **More examples and some discussion** on the topic can be found on **StackOverflow**: http://stackoverflow.com/questions/3169371/large-scale-regression-in-r-with-a-sparse-feature-matrix&#xD;&#xA;&#xD;&#xA;**UPDATE** (based on your comment):&#xD;&#xA;&#xD;&#xA;If you're trying to solve an LP problem with constraints, you may find this **theoretical paper** useful: http://web.stanford.edu/group/SOL/papers/gmsw84.pdf.&#xD;&#xA;&#xD;&#xA;Also, check R package **limSolve**: http://cran.r-project.org/web/packages/limSolve. And, in general, check packages in *CRAN Task View* **&quot;Optimization and Mathematical Programming&quot;**: http://cran.r-project.org/web/views/Optimization.html.&#xD;&#xA;&#xD;&#xA;Finally, check the book **&quot;Using R for Numerical Analysis in Science and Engineering&quot;** (by Victor A. Bloomfield). It has a section on solving systems of equations, represented by **sparse matrices** (section 5.7, pages 99-104), which includes examples, based on some of the above-mentioned packages: http://books.google.com/books?id=9ph_AwAAQBAJ&amp;pg=PA99&amp;lpg=PA99&amp;dq=r+limsolve+sparse+matrix&amp;source=bl&amp;ots=PHDE8nXljQ&amp;sig=sPi4n5Wk0M02ywkubq7R7KD_b04&amp;hl=en&amp;sa=X&amp;ei=FZjiU-ioIcjmsATGkYDAAg&amp;ved=0CDUQ6AEwAw#v=onepage&amp;q=r%20limsolve%20sparse%20matrix&amp;f=false. " />
  <row Id="2373" PostHistoryTypeId="2" PostId="931" RevisionGUID="a3802df1-92d2-4e1b-ab66-d6dfc00a6a17" CreationDate="2014-08-06T23:37:15.293" UserId="2724" Text="To answer a simpler, but related question, namely 'How well can my model generalize on the data that I have?' the method of learning curves might be applicable. [This][1] is a lecture given by Andrew Ng about them.&#xD;&#xA;&#xD;&#xA;The basic idea is to plot test set error and training set error vs. the complexity of the model you are using (this can be somewhat complicated). If the model is powerful enough to fully 'understand' your data, at some point the training set performance will be close to perfect. However, the variance of a complex model will likely cause the test set performance to increase at some point.&#xD;&#xA;&#xD;&#xA;This analysis tells you two main things, I think. The first is an upper limit on performance. It's pretty unlikely that you'll do better on data that you haven't seen than on your training data. The other thing it tells you is whether or not getting more data might help. If you can demonstrate that you fully understand your test data by driving training error to zero it might be possible, through the inclusion of more data, to drive your test error further down by getting a more complete sample and then training a powerful model on that. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.youtube.com/watch?v=g4XluwGYPaA" />
  <row Id="2374" PostHistoryTypeId="5" PostId="931" RevisionGUID="3996e8df-e4cc-4763-a10c-a97c1fb349ac" CreationDate="2014-08-06T23:52:44.867" UserId="2724" Comment="added 4 characters in body" Text="To answer a simpler, but related question, namely 'How well can my model generalize on the data that I have?' the method of learning curves might be applicable. [This][1] is a lecture given by Andrew Ng about them.&#xD;&#xA;&#xD;&#xA;The basic idea is to plot test set error and training set error vs. the complexity of the model you are using (this can be somewhat complicated). If the model is powerful enough to fully 'understand' your data, at some point the training set performance will be close to perfect. However, the variance of a complex model will likely cause the test set performance to increase at some point.&#xD;&#xA;&#xD;&#xA;This analysis tells you two main things, I think. The first is an upper limit on performance. It's pretty unlikely that you'll do better on data that you haven't seen than on your training data. The other thing it tells you is whether or not getting more data might help. If you can demonstrate that you fully understand your training data by driving training error to zero it might be possible, through the inclusion of more data, to drive your test error further down by getting a more complete sample and then training a powerful model on that. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.youtube.com/watch?v=g4XluwGYPaA" />
  <row Id="2375" PostHistoryTypeId="5" PostId="931" RevisionGUID="0c5dfb0c-cece-48e2-be57-387e49b22742" CreationDate="2014-08-07T00:20:05.817" UserId="2724" Comment="added 56 characters in body" Text="To answer a simpler, but related question, namely 'How well can my model generalize on the data that I have?' the method of learning curves might be applicable. [This][1] is a lecture given by Andrew Ng about them.&#xD;&#xA;&#xD;&#xA;The basic idea is to plot test set error and training set error vs. the complexity of the model you are using (this can be somewhat complicated). If the model is powerful enough to fully 'understand' your data, at some point the complexity of the model will be high enough that performance on the training set will be close to perfect. However, the variance of a complex model will likely cause the test set performance to increase at some point.&#xD;&#xA;&#xD;&#xA;This analysis tells you two main things, I think. The first is an upper limit on performance. It's pretty unlikely that you'll do better on data that you haven't seen than on your training data. The other thing it tells you is whether or not getting more data might help. If you can demonstrate that you fully understand your training data by driving training error to zero it might be possible, through the inclusion of more data, to drive your test error further down by getting a more complete sample and then training a powerful model on that. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://www.youtube.com/watch?v=g4XluwGYPaA" />
  <row Id="2376" PostHistoryTypeId="2" PostId="932" RevisionGUID="b39f047c-a792-4aa7-93c1-766cb6a226a8" CreationDate="2014-08-07T03:15:09.933" UserId="2817" Text="I have managed to resolve this. There is an excellent and thorough explanation of the optimization steps in the following thesis: [Semi-Supervised Learning for Natural Language by Percy Liang](http://cs.stanford.edu/~pliang/papers/meng-thesis.pdf).&#xD;&#xA;&#xD;&#xA;My mistake was trying to update the quality for all potential clusters pairs. Instead, you should initialize a table with the quality changes of doing each merge. Use this table to find the best merge, and the update the relevant terms that make up the table entries." />
  <row Id="2377" PostHistoryTypeId="2" PostId="933" RevisionGUID="9acbf627-78eb-4aff-9939-e0062ff76949" CreationDate="2014-08-07T09:33:50.817" UserId="2861" Text="It dates back to 1998, so most likely has been abandoned, or &quot;acquired&quot; by microsoft as the creator currently works there and has done since publishing that research.&#xD;&#xA;&#xD;&#xA;see http://research.microsoft.com/en-us/people/cyl/ists97.pdf&#xD;&#xA;&#xD;&#xA;and http://research.microsoft.com/en-us/people/cyl for the author. Maybe you could try to contact him." />
  <row Id="2378" PostHistoryTypeId="2" PostId="934" RevisionGUID="51d75735-5643-488f-9185-d48abe96e6c9" CreationDate="2014-08-07T10:45:38.557" UserId="2533" Text="I have train and test sets of chronological data consisting of 305000 instances and 70000,appropriately. There are 15 features in each instance and only 2 possible class values ( NEW,OLD). The problem is that there are only 725 OLD instances in the train set and 95 in the test. &#xD;&#xA;&#xD;&#xA;The only algorithm which succeeds for me to handle imbalance is NaiveBayes in Weka (0.02 precision for OLD class), others (trees) classify each instance as NEW.&#xD;&#xA;What is the best approach to handle the imbalance and the appropriate algorithm in such a case?&#xD;&#xA;&#xD;&#xA;Thank you in advance.&#xD;&#xA;" />
  <row Id="2379" PostHistoryTypeId="1" PostId="934" RevisionGUID="51d75735-5643-488f-9185-d48abe96e6c9" CreationDate="2014-08-07T10:45:38.557" UserId="2533" Text="Handling huge dataset imbalance (2 class values) and appropriate ML algorithm" />
  <row Id="2380" PostHistoryTypeId="3" PostId="934" RevisionGUID="51d75735-5643-488f-9185-d48abe96e6c9" CreationDate="2014-08-07T10:45:38.557" UserId="2533" Text="&lt;machine-learning&gt;&lt;dataset&gt;" />
  <row Id="2381" PostHistoryTypeId="2" PostId="935" RevisionGUID="fce33798-a605-4281-876d-a6dd7a95906d" CreationDate="2014-08-07T12:46:25.123" UserId="2863" Text="I'm not allowed to comment, but I have more a suggestion: you could try to implement some &quot;Over-sampling Techniques&quot; like SMOTE:&#xD;&#xA;http://scholar.google.com/scholar?q=oversampling+minority+classes" />
  <row Id="2382" PostHistoryTypeId="2" PostId="936" RevisionGUID="6c3f6d41-0d59-49e7-9b42-a8bd741b9967" CreationDate="2014-08-07T15:12:48.617" UserId="2792" Text="I am attempting to compile code using Knitr in R.&#xD;&#xA;&#xD;&#xA;My code below is returning the following error, and causes errors in the rest of the document.&#xD;&#xA;&#xD;&#xA;miss&lt;-sample$sensor_glucose[!is.na(sample$sensor_glucose)]&#xD;&#xA;Error: &quot;## Warning: is.na() applied to non-(list or vector) of type 'NULL'&quot;&#xD;&#xA;&#xD;&#xA;&gt; str(miss)&#xD;&#xA; int [1:103] 213 113 46 268 186 196 187 153 43 175 ...&#xD;&#xA;&#xD;&#xA;Does anyone know how to remedy this problem?&#xD;&#xA;&#xD;&#xA;Thanks in advance!" />
  <row Id="2383" PostHistoryTypeId="1" PostId="936" RevisionGUID="6c3f6d41-0d59-49e7-9b42-a8bd741b9967" CreationDate="2014-08-07T15:12:48.617" UserId="2792" Text="R error using Knitr" />
  <row Id="2384" PostHistoryTypeId="3" PostId="936" RevisionGUID="6c3f6d41-0d59-49e7-9b42-a8bd741b9967" CreationDate="2014-08-07T15:12:48.617" UserId="2792" Text="&lt;r&gt;" />
  <row Id="2385" PostHistoryTypeId="2" PostId="937" RevisionGUID="6a49a4b9-9622-4fc1-94e9-801ddaff2263" CreationDate="2014-08-07T15:33:43.793" UserId="2854" Text="I'm working on the problem with too many features and training my models takes way too long. I implemented forward selection algorithm to choose features.&#xD;&#xA;&#xD;&#xA;However, I was wondering does scikit-learn have forward selection/stepwise regression algorithm?" />
  <row Id="2386" PostHistoryTypeId="1" PostId="937" RevisionGUID="6a49a4b9-9622-4fc1-94e9-801ddaff2263" CreationDate="2014-08-07T15:33:43.793" UserId="2854" Text="Does scikit-learn have forward selection/stepwise regression algorithm?" />
  <row Id="2387" PostHistoryTypeId="3" PostId="937" RevisionGUID="6a49a4b9-9622-4fc1-94e9-801ddaff2263" CreationDate="2014-08-07T15:33:43.793" UserId="2854" Text="&lt;feature-selection&gt;&lt;sklearn&gt;&lt;scikit&gt;" />
  <row Id="2388" PostHistoryTypeId="2" PostId="938" RevisionGUID="8cbba5d4-2664-46b1-875c-6ba49a9ad5b6" CreationDate="2014-08-07T17:49:05.640" UserId="172" Text="You can apply a clustering algorithm to the instances in the majority class and train a classifier with the centroids/medoids offered by the cluster algorithm. This is subsampling the majority class, the converse of oversampling the minority class. " />
  <row Id="2389" PostHistoryTypeId="2" PostId="939" RevisionGUID="e9227a09-3052-4089-abdc-e38019356e24" CreationDate="2014-08-07T17:53:06.970" UserId="172" Text="You will have best results if you care to build the folds so that each variable (and most importantly the target variable) is approximately identically distributed in each fold. This is called, when applied to the target variable, stratified k-fold. One approach is to cluster the inputs and make sure each fold contains the same number of instances from each cluster proportional to their size." />
  <row Id="2390" PostHistoryTypeId="2" PostId="940" RevisionGUID="8b025320-54a1-4508-97e0-ba23b652abae" CreationDate="2014-08-07T22:30:52.913" UserId="2873" Text="We are storing the information about our users showing interest in our items. Based on this information, we would like to create a simple recommendation engine that will take the items I1, I2, I3 etc of the current user, search for all other users that had shown interest in those items, and then output the items I4, I5, I6 etc of the other users, sorted by their decreasing popularity. So, basically, the standard &quot;other buyer were also interested in...&quot; functionality.&#xD;&#xA;&#xD;&#xA;I'm asking myself what kind of a database is suitable for a realtime recommendation engine like this. My current idea is to build a trie of item IDs, then sort the item IDs of the current user (as the order of items is irrelevant) and to go down the trie; the children of the last trie node will build the needed output.&#xD;&#xA;&#xD;&#xA;The problem is that we have 2 million items so that according to our estimation the trie will have at least 1E12 nodes, so that we probably need a distributed sharded database to store it. Before we reinvent the wheel, are there any ready-to-use databases or generally, non-cloud solutions for recommendation engines out there?" />
  <row Id="2391" PostHistoryTypeId="1" PostId="940" RevisionGUID="8b025320-54a1-4508-97e0-ba23b652abae" CreationDate="2014-08-07T22:30:52.913" UserId="2873" Text="Database for a trie, or other appropriate structure for recommendation engine" />
  <row Id="2392" PostHistoryTypeId="3" PostId="940" RevisionGUID="8b025320-54a1-4508-97e0-ba23b652abae" CreationDate="2014-08-07T22:30:52.913" UserId="2873" Text="&lt;bigdata&gt;&lt;recommendation&gt;&lt;databases&gt;" />
  <row Id="2393" PostHistoryTypeId="5" PostId="808" RevisionGUID="a869d554-bba0-4f90-aa21-e3103d79c851" CreationDate="2014-08-07T23:37:20.750" UserId="156" Comment="Probably the intended indented (sic!) formatting " Text="i want to become a **data scientist**. I studied applied **statistics** (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in **Computer Science** focus in Intelligent Systems.&#xD;&#xA;&#xD;&#xA;Here is my study plan:&#xD;&#xA;&#xD;&#xA; - Machine learning&#xD;&#xA; - Advanced machine learning&#xD;&#xA; - Data mining&#xD;&#xA; - Fuzzy logic&#xD;&#xA; - Recommendation Systems&#xD;&#xA; - Distributed Data Systems&#xD;&#xA; - Cloud Computing&#xD;&#xA; - Knowledge discovery&#xD;&#xA; - Business Intelligence&#xD;&#xA;-Information retrieval&#xD;&#xA;-Text mining&#xD;&#xA;&#xD;&#xA;At the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?&#xD;&#xA;&#xD;&#xA;Thanks for the answers." />
  <row Id="2394" PostHistoryTypeId="24" PostId="808" RevisionGUID="a869d554-bba0-4f90-aa21-e3103d79c851" CreationDate="2014-08-07T23:37:20.750" Comment="Proposed by 156 approved by 434, -1 edit id of 128" />
  <row Id="2395" PostHistoryTypeId="5" PostId="808" RevisionGUID="a15450b7-4dc4-4f0d-9095-65ec45f70f7a" CreationDate="2014-08-07T23:37:20.750" UserId="21" Comment="Probably the intended indented (sic!) formatting " Text="i want to become a **data scientist**. I studied applied **statistics** (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in **Computer Science** focus in Intelligent Systems.&#xD;&#xA;&#xD;&#xA;Here is my study plan:&#xD;&#xA;&#xD;&#xA; - Machine learning&#xD;&#xA; - Advanced machine learning&#xD;&#xA; - Data mining&#xD;&#xA; - Fuzzy logic&#xD;&#xA; - Recommendation Systems&#xD;&#xA; - Distributed Data Systems&#xD;&#xA; - Cloud Computing&#xD;&#xA; - Knowledge discovery&#xD;&#xA; - Business Intelligence&#xD;&#xA; - Information retrieval&#xD;&#xA; - Text mining&#xD;&#xA;&#xD;&#xA;At the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?&#xD;&#xA;&#xD;&#xA;Thanks for the answers." />
  <row Id="2396" PostHistoryTypeId="6" PostId="904" RevisionGUID="16891448-a9ad-43fa-be19-1f2b20619833" CreationDate="2014-08-07T23:37:26.890" UserId="1156" Comment="added more relevant tags" Text="&lt;r&gt;&lt;visualization&gt;" />
  <row Id="2397" PostHistoryTypeId="24" PostId="904" RevisionGUID="16891448-a9ad-43fa-be19-1f2b20619833" CreationDate="2014-08-07T23:37:26.890" Comment="Proposed by 1156 approved by 434, 21 edit id of 130" />
  <row Id="2398" PostHistoryTypeId="5" PostId="936" RevisionGUID="af692b56-3c84-43e9-b58e-e25f7cc7c84f" CreationDate="2014-08-07T23:37:30.707" UserId="1333" Comment="Formatted the code properly." Text="I am attempting to compile code using Knitr in R.&#xD;&#xA;&#xD;&#xA;My code below is returning the following error, and causes errors in the rest of the document.&#xD;&#xA;&#xD;&#xA;    miss&lt;-sample$sensor_glucose[!is.na(sample$sensor_glucose)]&#xD;&#xA;    # Error: &quot;## Warning: is.na() applied to non-(list or vector) of type 'NULL'&quot;&#xD;&#xA;    &#xD;&#xA;    str(miss)&#xD;&#xA;    # int [1:103] 213 113 46 268 186 196 187 153 43 175 ...&#xD;&#xA;&#xD;&#xA;Does anyone know how to remedy this problem?&#xD;&#xA;&#xD;&#xA;Thanks in advance!&#xD;&#xA;" />
  <row Id="2399" PostHistoryTypeId="24" PostId="936" RevisionGUID="af692b56-3c84-43e9-b58e-e25f7cc7c84f" CreationDate="2014-08-07T23:37:30.707" Comment="Proposed by 1333 approved by 434, 21 edit id of 132" />
  <row Id="2400" PostHistoryTypeId="6" PostId="893" RevisionGUID="79d5bbc6-0da2-475c-aa93-8fedd8dc3612" CreationDate="2014-08-07T23:38:37.457" UserId="97" Comment="Add relevant tag" Text="&lt;r&gt;&lt;statistics&gt;&lt;correlation&gt;" />
  <row Id="2401" PostHistoryTypeId="24" PostId="893" RevisionGUID="79d5bbc6-0da2-475c-aa93-8fedd8dc3612" CreationDate="2014-08-07T23:38:37.457" Comment="Proposed by 97 approved by 434, 21 edit id of 131" />
  <row Id="2402" PostHistoryTypeId="10" PostId="266" RevisionGUID="aad6cb9a-f156-4b31-a14d-ec60c3cfaa14" CreationDate="2014-08-07T23:41:10.580" UserId="21" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2405" PostHistoryTypeId="2" PostId="941" RevisionGUID="04ecf21d-9850-4084-9c1f-63d3b534ec15" CreationDate="2014-08-08T00:15:05.527" UserId="2452" Text="I agree with @ssdecontrol that a minimal *reproducible example* would be the most helpful. However, looking at your code (pay attention to the sequence &quot;Error: ... Warning: ...&quot;), I think that the issue you're experiencing is due to an inappropriate setting of R's global `warn` **option**. Most likely your current setting is `2`, whereas, you, probably want treat warnings as such, without converting them to errors. If that is the case, you just need to set the option appropriately:&#xD;&#xA;&#xD;&#xA;    options(warn=1)  # print warnings as they occur&#xD;&#xA;    options(warn=2)  # treat warnings as errors&#xD;&#xA;&#xD;&#xA;**Note for moderators/administrators**: This question seems not to be a *data science* question, but purely an *R* question. Therefore, I think it should be moved to *StackOverflow*, where it belongs." />
  <row Id="2406" PostHistoryTypeId="2" PostId="942" RevisionGUID="310e96c2-5db1-415a-a661-0d245f9d7091" CreationDate="2014-08-08T00:25:04.210" UserId="1156" Text="You mentioned regression and PCA in the title, and there is a definite answer for each of those.&#xD;&#xA;&#xD;&#xA;The asymptotic complexity of linear regression reduces to O(P^2 * N) if N &gt; P, where P is the number of features and N is the number of observations. More detail in [Computational complexity of least square regression operation][1].&#xD;&#xA;&#xD;&#xA;Vanilla PCA is O(P^2 * N + P ^ 3), as in [Fastest PCA algorithm for high-dimensional data][2]. However fast algorithms exist for very large matrices, explained in that answer and [Best PCA Algorithm For Huge Number of Features?][3].&#xD;&#xA;&#xD;&#xA;However I don't think anyone's compiled a single lit review or reference or  book on the subject. Might not be a bad project for my free time...&#xD;&#xA;&#xD;&#xA;  [1]: http://math.stackexchange.com/a/84503/117452&#xD;&#xA;  [2]: http://scicomp.stackexchange.com/q/3220&#xD;&#xA;  [3]: http://stats.stackexchange.com/q/2806/36229" />
  <row Id="2407" PostHistoryTypeId="5" PostId="710" RevisionGUID="631c50c2-9531-4352-81d8-9c6e027d502e" CreationDate="2014-08-08T00:27:05.340" UserId="21" Comment="deleted 47 characters in body" Text="I'm looking for commercial text summarization tools (APIs, Libraries,...) which are able to perform any of the following tasks:&#xD;&#xA;&#xD;&#xA;1. Extractive Multi-Document Summarization (Generic or query-based)&#xD;&#xA;2. Extractive Single-Document Summarization (Generic or query-based)&#xD;&#xA;3. Generative Single-Document Summarization (Generic or query-based)&#xD;&#xA;4. Generative Multi-Document Summarization (Generic or query-based)" />
  <row Id="2409" PostHistoryTypeId="5" PostId="941" RevisionGUID="68276d65-434f-4481-9d5a-20c00a3d66be" CreationDate="2014-08-08T03:03:11.517" UserId="2452" Comment="Improved wording." Text="I agree with @ssdecontrol that a minimal *reproducible example* would be the most helpful. However, looking at your code (pay attention to the sequence `Error: ... Warning: ...`), I believe that the issue you are experiencing is due to an inappropriate setting of R's global `warn` **option**. It appears that your current setting is likely `2`, which refers to **converting warnings to errors**, whereas, you, most likely want the setting `1`, which is to treat warnings as such, without converting them to errors. If that is the case, you just need to set the option appropriately:&#xD;&#xA;&#xD;&#xA;    options(warn=1)  # print warnings as they occur&#xD;&#xA;    options(warn=2)  # treat warnings as errors&#xD;&#xA;&#xD;&#xA;**Note for moderators/administrators**: This question seems not to be a *data science* question, but purely an *R* question. Therefore, I think it should be moved to *StackOverflow*, where it belongs." />
  <row Id="2412" PostHistoryTypeId="2" PostId="944" RevisionGUID="3a43e641-b838-4d2c-ad0a-555fc04703d7" CreationDate="2014-08-08T07:12:52.680" UserId="816" Text="In addition to undersampling the majority class (i.e. taking only a few NEW), you may consider oversampling the minority class (in essence, duplicating your OLDs, but there are other smarter ways to do that)&#xD;&#xA;&#xD;&#xA;Note that oversampling may lead to overfitting, so pay special attention to testing your classifiers &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Check also this answer on CV: &#xD;&#xA;&#xD;&#xA; - http://stats.stackexchange.com/a/108325/49130" />
  <row Id="2415" PostHistoryTypeId="2" PostId="945" RevisionGUID="04debe7e-53f2-4363-aec1-c33ce6eb6552" CreationDate="2014-08-08T10:01:25.400" UserId="2878" Text="We have a classification algorithm to categorize Java exceptions in Production.&#xD;&#xA;This algorithm is based on hierarchical human defined rules so when a bunch of text forming an exception comes up, it determines what kind of exception is (development, availability, configuration, etc.) and the responsible component (the most inner component responsible of the exception). In Java an exception can have several causing exceptions, and the whole must be analyzed.&#xD;&#xA;&#xD;&#xA;For example, given the following example exception:&#xD;&#xA;&#xD;&#xA;    com.myapp.CustomException: Error printing ...&#xD;&#xA;    ... (stack)&#xD;&#xA;    Caused by: com.foo.webservice.RemoteException: Unable to communicate ...&#xD;&#xA;    ... (stack)&#xD;&#xA;    Caused by: com.acme.PrintException: PrintServer002: Timeout ....&#xD;&#xA;    ... (stack)&#xD;&#xA;&#xD;&#xA;First of all, our algorithm splits the whole stack in three isolated exceptions. Afterwards it starts analyzing these exceptions starting from the most inner one. In this case, it determines that this exception (the second caused by) is of type `Availability` and that the responsible component is a &quot;print server&quot;. This is because there is a rule that matches containing the word `Timeout` associated to the `Availability` type. There is also a rule that matches `com.acme.PrintException` and determines that the responsible component is a print server. As all the information needed is determined using only the most inner exception, the upper exceptions are ignored, but this is not always the case.&#xD;&#xA;&#xD;&#xA;As you can see this kind of approximation is very complex (and chaotic) as a human have to create new rules as new exceptions appear. Besides, the new rules have to be compatible with the current ones because a new rule for classifying a new exception must not change the classification of any of the already classified exceptions.&#xD;&#xA;&#xD;&#xA;We are thinking about using Machine Learning to automate this process. Obviously, I am not asking for a solution here as I know the complexity but I'd really appreciate some advice to achieve our goal." />
  <row Id="2416" PostHistoryTypeId="1" PostId="945" RevisionGUID="04debe7e-53f2-4363-aec1-c33ce6eb6552" CreationDate="2014-08-08T10:01:25.400" UserId="2878" Text="Classifying Java exceptions" />
  <row Id="2417" PostHistoryTypeId="3" PostId="945" RevisionGUID="04debe7e-53f2-4363-aec1-c33ce6eb6552" CreationDate="2014-08-08T10:01:25.400" UserId="2878" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;algorithms&gt;" />
  <row Id="2418" PostHistoryTypeId="10" PostId="865" RevisionGUID="cd4bd7f0-35d8-4340-9eaf-82b9b5e471ce" CreationDate="2014-08-08T12:23:02.223" UserId="21" Comment="103" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2419" PostHistoryTypeId="10" PostId="375" RevisionGUID="bf416bd4-bda0-4744-9107-f1c8a2a7ec9b" CreationDate="2014-08-08T12:24:27.027" UserId="21" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:108,&quot;DisplayName&quot;:&quot;rapaio&quot;},{&quot;Id&quot;:548,&quot;DisplayName&quot;:&quot;indico&quot;},{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2420" PostHistoryTypeId="10" PostId="218" RevisionGUID="b58e4bd6-a9a5-4eb4-bad3-7ace40ce50ca" CreationDate="2014-08-08T12:26:12.553" UserId="21" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2422" PostHistoryTypeId="6" PostId="936" RevisionGUID="4a8b34d3-6b7b-4ade-9f43-2624f6e0effa" CreationDate="2014-08-08T12:27:15.360" UserId="97" Comment="Adding relevant tags." Text="&lt;r&gt;&lt;knitr&gt;&lt;error-handling&gt;" />
  <row Id="2423" PostHistoryTypeId="24" PostId="936" RevisionGUID="4a8b34d3-6b7b-4ade-9f43-2624f6e0effa" CreationDate="2014-08-08T12:27:15.360" Comment="Proposed by 97 approved by 21 edit id of 133" />
  <row Id="2424" PostHistoryTypeId="2" PostId="946" RevisionGUID="af09d20e-ecda-445c-9464-b37302b9d6f7" CreationDate="2014-08-08T13:44:52.803" UserId="2883" Text="Cross posting this from Cross Validated:&#xD;&#xA;&#xD;&#xA;I've seen this question asked before, but I have yet to come across a definitive source answering the specific questions:&#xD;&#xA;&#xD;&#xA;* What's the most appropriate statistical test to apply to a small A/B test?&#xD;&#xA;* What's the R code and interpretation to analyze a small A/B test?&#xD;&#xA;&#xD;&#xA;I'm running a small test to figure out which ads perform better. I have the following results:&#xD;&#xA;&#xD;&#xA;**Position 1:**&#xD;&#xA;&#xD;&#xA;```variation,impressions,clicks&#xD;&#xA;row-1,753,26&#xD;&#xA;row-3,767 7&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;**Position 2:**&#xD;&#xA;&#xD;&#xA;```variation,impressions,clicks&#xD;&#xA;row-1,753,16&#xD;&#xA;row-3,767 13&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;**Position 3:**&#xD;&#xA;&#xD;&#xA;```variation,impressions,clicks&#xD;&#xA;row-1,753,2&#xD;&#xA;row-3,767 7&#xD;&#xA;```&#xD;&#xA;&#xD;&#xA;I think it's safe to say these numbers are small and likely to be not normally distributed. Also, it's click data so there's a binary outcome of clicked or not and the trials are independent.&#xD;&#xA;&#xD;&#xA;**Appropriate test**&#xD;&#xA;&#xD;&#xA;In analyzing each position for significance, I think comparison with a binomial or Poisson distribution makes the most sense. &#xD;&#xA;&#xD;&#xA;According to [the OpenIntro Stats](http://www.openintro.org/stat/textbook.php?stat_book=os) (and other sources) book, a variable follows a Poisson distribution &quot;... if the event being considered is rare, the population is large, and the events occur independently of each other.&quot;&#xD;&#xA;&#xD;&#xA;The same source classifies a binomial variable approximately the same way adding that the probability of success is the same and the number of trials is fixed.&#xD;&#xA;&#xD;&#xA;I appreciate this is not an either/or decision and analysis can be done using both distributions.&#xD;&#xA;&#xD;&#xA;Given A/B (split) testing is a science that has been practiced for several years, I imagine that there is a canonical test. However, looking around the internet, I mostly come across analysis that uses the standard normal distribution. That just seems wrong :)&#xD;&#xA;&#xD;&#xA;Is there a canonical test to use for A/B tests with small #'s of clicks?&#xD;&#xA;&#xD;&#xA;**Interpretation and R code**&#xD;&#xA;&#xD;&#xA;I've used the following R code to test significance for each position:&#xD;&#xA;&#xD;&#xA;Position 1:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    binom.test(7, 767, p=(26/753))&#xD;&#xA;    &#xD;&#xA;    Exact binomial test&#xD;&#xA;    &#xD;&#xA;    data:  7 and 767&#xD;&#xA;    number of successes = 7, number of trials = 767, p-value = 1.077e-05&#xD;&#xA;    alternative hypothesis: true probability of success is not equal to 0.03452855&#xD;&#xA;    95 percent confidence interval:&#xD;&#xA;     0.003676962 0.018713125&#xD;&#xA;    sample estimates:&#xD;&#xA;    probability of success &#xD;&#xA;               0.009126467&#xD;&#xA;&#xD;&#xA; &#xD;&#xA;I interpret this result to mean: The probability of success in the test group is indeed different than the control group with a 95% confidence interval that the success probability is between .368% and 1.87%&#xD;&#xA;&#xD;&#xA;    ppois(((26-1)/753), lambda=(7/767), lower.tail = F)&#xD;&#xA;    [1] 0.009084947&#xD;&#xA;&#xD;&#xA;I interpret this result to mean: Given a Poisson distribution with a click rate of 7 per 767 trials, there is a 0.9% chance of having a click rate of 26 or more per 753 trials in the same distribution. Contextualized in the ad example,&#xD;&#xA;there is a .1% chance that the control ad actually performs the same as the test ad.&#xD;&#xA;&#xD;&#xA;Is the above interpretation correct? Does the test and interpretation change with the different positions (i.e. are the results of the Poisson test more appropriate for Position 3 given the small numbers)?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2425" PostHistoryTypeId="1" PostId="946" RevisionGUID="af09d20e-ecda-445c-9464-b37302b9d6f7" CreationDate="2014-08-08T13:44:52.803" UserId="2883" Text="Analysis of Split (A/B) tests using Poisson and/or Binomial Distribution" />
  <row Id="2426" PostHistoryTypeId="3" PostId="946" RevisionGUID="af09d20e-ecda-445c-9464-b37302b9d6f7" CreationDate="2014-08-08T13:44:52.803" UserId="2883" Text="&lt;r&gt;" />
  <row Id="2427" PostHistoryTypeId="2" PostId="947" RevisionGUID="0895c081-a81f-4084-a6b5-e9970b1c789f" CreationDate="2014-08-08T13:53:36.847" UserId="1279" Text="First of all, some basics of classification (and in general any supervised ML tasks), just to make sure we have same set of concepts in mind. &#xD;&#xA;&#xD;&#xA;Any supervised ML algorithm consists of at least 2 components: &#xD;&#xA;&#xD;&#xA; 1. Dataset to train and test on.&#xD;&#xA; 2. Algorithm(s) to handle these data.&#xD;&#xA;&#xD;&#xA;Training dataset consists of a set of pairs `(x, y)`, where `x` is a **vector of features** and `y` is **predicted variable**. Predicted variable is just what you want to know, i.e. in your case it is exception type. Features are more tricky. You cannot just throw raw text into an algorithm, you need to extract meaningful parts of it and organize them as feature vectors first. You've already mentioned a couple of useful features - exception class name (e.g. `com.acme.PrintException`) and contained words (&quot;Timeout&quot;). All you need is to translate your row exceptions (and human-categorized exception types) into suitable dataset, e.g.: &#xD;&#xA;&#xD;&#xA;    ex_class                  contains_timeout  ...   | ex_type&#xD;&#xA;    -----------------------------------------------------------&#xD;&#xA;    [com.acme.PrintException, 1                , ...] | Availability&#xD;&#xA;    [java.lang.Exception    , 0                , ...] | Network&#xD;&#xA;     ...&#xD;&#xA;&#xD;&#xA;This representation is already much better for ML algorithms. But which one to take? &#xD;&#xA;&#xD;&#xA;Taking into account nature of the task and your current approach natural choice is to use **decision trees**. This class of algorithms will compute optimal decision criteria for all your exception types and print out resulting tree. This is especially useful, because you will have possibility to manually inspect how decision is made and see how much it corresponds to your manually-crafted rules. &#xD;&#xA;&#xD;&#xA;There's, however, possibility that some exceptions with exactly the same features will belong to different exception types. In this case probabilistic approach may work well. Despite its name, **Naive Bayes** classifier works pretty well in most cases. There's one issue with NB and our dataset representation, though: dataset contains *categorical* variables, and Naive Bayes can work with *numerical* attributes only*. Standard way to overcome this problem is to use [dummy variables](http://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29). In short, dummy variables are binary variables that simply indicate whether specific category presents or not. For example, single variable `ex_class` with values `{com.acme.PrintException, java.lang.Exception, ...}`, etc. may be split into several variables `ex_class_printexception`, `ex_class_exception`, etc. with values `{0, 1}`:&#xD;&#xA;&#xD;&#xA;    ex_class_printexception  ex_class_exception  contains_timeout | ex_type&#xD;&#xA;    -----------------------------------------------------------------------&#xD;&#xA;    [1,                    , 0                 , 1              ] | Availability&#xD;&#xA;    [0,                    , 1                 , 0              ] | Network&#xD;&#xA;&#xD;&#xA;One last algorithm to try is **Support Vector Machines (SVM)**. It neither provides helpful visualisation, nor is probabilistic, but often gives superior results. &#xD;&#xA;&#xD;&#xA;------&#xD;&#xA;&#xD;&#xA;*** - in fact, neither Bayes theorem, nor Naive Bayes itself state anything about variable type, but most software packages that come to mind rely on numerical features. " />
  <row Id="2428" PostHistoryTypeId="2" PostId="948" RevisionGUID="ce3382f9-9efb-43b3-9c93-37abedc0a91c" CreationDate="2014-08-08T16:49:13.850" UserId="1281" Text="I have found a number of libraries and tools for data science in Scala, I would like to know about which one has more adoption and which one is gaining adoption at a faster pace and to what extent this is the case. Basically, which one should I bet for (if any at this point).&#xD;&#xA;&#xD;&#xA;Some of the libraries I've found are (in no particular order):&#xD;&#xA;&#xD;&#xA;* Scalding&#xD;&#xA;* Breeze&#xD;&#xA;* Spark&#xD;&#xA;* Saddle&#xD;&#xA;* H2O&#xD;&#xA;* Spire&#xD;&#xA;* Mahout&#xD;&#xA;* Hadoop&#xD;&#xA;* MongoDB&#xD;&#xA;&#xD;&#xA;If I need to be more specific to make the question answerable: I'm not particularly interested in clusters and Big Data at this moment, but I'm interested in sizable data (up to 100 GB) and predictive analytics." />
  <row Id="2429" PostHistoryTypeId="1" PostId="948" RevisionGUID="ce3382f9-9efb-43b3-9c93-37abedc0a91c" CreationDate="2014-08-08T16:49:13.850" UserId="1281" Text="Any clear winner for Data Science in Scala?" />
  <row Id="2430" PostHistoryTypeId="3" PostId="948" RevisionGUID="ce3382f9-9efb-43b3-9c93-37abedc0a91c" CreationDate="2014-08-08T16:49:13.850" UserId="1281" Text="&lt;tools&gt;" />
  <row Id="2431" PostHistoryTypeId="5" PostId="948" RevisionGUID="10018936-7f45-44a5-90ec-aa771a13340c" CreationDate="2014-08-08T17:00:50.270" UserId="1281" Comment="added 24 characters in body" Text="I have found a number of libraries and tools for data science in Scala, I would like to know about which one has more adoption and which one is gaining adoption at a faster pace and to what extent this is the case. Basically, which one should I bet for (if any at this point).&#xD;&#xA;&#xD;&#xA;Some of the tools I've found are (in no particular order):&#xD;&#xA;&#xD;&#xA;* Scalding&#xD;&#xA;* Breeze&#xD;&#xA;* Spark&#xD;&#xA;* Saddle&#xD;&#xA;* H2O&#xD;&#xA;* Spire&#xD;&#xA;* Mahout&#xD;&#xA;* Hadoop&#xD;&#xA;* MongoDB&#xD;&#xA;&#xD;&#xA;If I need to be more specific to make the question answerable: I'm not particularly interested in clusters and Big Data at this moment, but I'm interested in sizable data (up to 100 GB) for information integration and predictive analytics." />
  <row Id="2432" PostHistoryTypeId="2" PostId="949" RevisionGUID="4a411501-3693-42d5-86ac-a8042eeffd72" CreationDate="2014-08-08T17:32:48.377" UserId="2433" Text="I am facing this bizarre issue while using `Apache Pig` **rank** utility. I am executing the following code:&#xD;&#xA;&#xD;&#xA;    email_id_ranked = rank email_id;&#xD;&#xA;    store email_id_ranked into '/tmp/';&#xD;&#xA;&#xD;&#xA;So, basically I am trying to get the following result&#xD;&#xA;&#xD;&#xA;    1,email1&#xD;&#xA;    2,email2&#xD;&#xA;    3,email3&#xD;&#xA;    ... &#xD;&#xA;&#xD;&#xA;Issue is sometime pig dumps the above result but sometimes it dumps only the emails without the rank. Also when I dump the data on screen using `dump` function pig returns both the columns. I don't know where the issue is. Kindly advice.&#xD;&#xA;&#xD;&#xA;Please let me know if you need any more information. Thanks in advance.&#xD;&#xA;&#xD;&#xA;**Pig version: Apache Pig version 0.11.0-cdh4.6.0**" />
  <row Id="2433" PostHistoryTypeId="1" PostId="949" RevisionGUID="4a411501-3693-42d5-86ac-a8042eeffd72" CreationDate="2014-08-08T17:32:48.377" UserId="2433" Text="Pig Rank function not generating rank in output" />
  <row Id="2434" PostHistoryTypeId="3" PostId="949" RevisionGUID="4a411501-3693-42d5-86ac-a8042eeffd72" CreationDate="2014-08-08T17:32:48.377" UserId="2433" Text="&lt;bigdata&gt;&lt;hadoop&gt;&lt;apache-pig&gt;&lt;pig&gt;" />
  <row Id="2435" PostHistoryTypeId="2" PostId="950" RevisionGUID="6d6879be-3505-41c8-bc0d-35a750a736c9" CreationDate="2014-08-08T19:23:33.637" UserId="2841" Text="The approximation Binomial(k,n,p) ~= Poisson(k,s) (where s = n*p) can be shown under the assumptions: &lt;br/&gt;&#xD;&#xA;1) n &gt;&gt; k (to say that n!/(n-k)! ~= n^k), &lt;br/&gt;&#xD;&#xA;2) p &lt;&lt;1 (to say that (1-p)^(n-k) ~= (1-p)^n). &lt;br/&gt;&#xD;&#xA;It's up to you whether those are sufficiently satisfied. If the exact calculation can be done quickly, in my opinion, it's nice to stay with that.&#xD;&#xA;&#xD;&#xA;Also since, if the probability of row 3 sample is different from the row 1 sample, it would almost certainly be on the lower side. It would probably best for you to use &lt;br/&gt;&#xD;&#xA;binom.test(7, 767, p=(26/753), alternative='less')&#xD;&#xA;the final option indicating that the alternative to your null hypothesis is that the probability is less than 26/753, not equal to. Of course, that's simply just the sum of Binomial probabilities from 0 to 7 (you can check yourself), the interpretation being that *this* is the probability of having gotten at most 7 rolls from random chance, if the probability truly was 26/753.&#xD;&#xA;&#xD;&#xA;Keep in mind the interpretation of that last sentence. These kinds of tests are generally used when we _know_ what the inherent probability is that we're comparing to (e.g. to see if the set of coin flips has a probability significantly different from 1/2 which is what we expect from a fair coin). In this case, we don't _know_ what the probability is that we're comparing to, we're just making the very crude guess that the 26/753 outcome of row 1 reflects the true probability. It's better than a regular Normal t-test in this case, but don't put _too_ much stock in it unless you have a much higher sample size for row 1." />
  <row Id="2436" PostHistoryTypeId="2" PostId="951" RevisionGUID="814fe49e-6599-44bb-8eb4-19028d3ffffc" CreationDate="2014-08-08T20:46:58.493" UserId="1279" Text="Not sure anybody have worked with _all_ these tools, so I'm going to share my experience with some of them and let others share their experience with the others. &#xD;&#xA;&#xD;&#xA;**MongoDB** addresses problems that involve heterogeneous and nested objects, while data mining mostly work with simple tabular data. MongoDB is neither fast with this type of data, nor provide any advanced tools for analysis (correct me if you know any). So I can think of a very few applications for Mongo in data mining. &#xD;&#xA;&#xD;&#xA;**Hadoop** is a large ecosystem, containing dozens of different tools. I will assume that you mean core Hadoop features - HDFS and MapReduce. HDFS provides flexible way to store large amounts of data, while MapReduce gives basis for processing them. It has its clear advantages for processing multi-terabyte datasets, but it also has significant drawbacks. In particular, because of intensive disk IO during MapReduce tasks (that slows down computations a lot) it is terrible for interactive development, iterative algorithms and working with not-that-big datasets. For more details see my [earlier answer](http://datascience.stackexchange.com/a/863/1279).&#xD;&#xA;&#xD;&#xA;Many algorithms in Hadoop require multiple MapReduce jobs with complicated data flow. This is where **Scalding** gets shiny. Scalding (and underlying Java's [Cascading](http://www.cascading.org/)) provides much simpler API, but at the moment uses same MapReduce as its runtime and thus holds all the same issues.  &#xD;&#xA;&#xD;&#xA;**Spark** addresses exactly these issues. It drops Hadoop's MapReduce and offers completely new computational framework based on distributed in-memory collections and delayed evaluations. Its API is somewhat similar to Scalding's with all MR complexity removed, so it's really easy to get started with it. Spark is also the first in this list that comes with data mining library - [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html). &#xD;&#xA;&#xD;&#xA;But Spark doesn't reinvent things like basic linear algebra. For this purpose it uses **Breeze**. To my opinion, Breeze is far in quality from scientific packages like SciPy, Octave or Julia, but it is still good enough for most practical use cases. &#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;**Mahout** relies on Hadoop's MapReduce and thus is terrible for iterative algorithms. **Spire** and **Saddle** look cute and probably have their niche, but seem to be much less well-known than Breeze. I couldn't find much information about **H2O**, so it doesn't look like a big player here (comments from people who used it are welcome).&#xD;&#xA;&#xD;&#xA;------- &#xD;&#xA;&#xD;&#xA;Some quick summary. Spark seems to be the most simple, flexible and fast-growing project for large-scale data processing. It facilitates a number of new projects (e.g. Shark or Spark SQL) and penetrates into existing (including [Cascading](http://www.concurrentinc.com/2014/05/cascading-now-supports-tez-spark-and-storm-up-next/) and [Mahout](https://mahout.apache.org/users/sparkbindings/home.html)). Spark knows how to utilize HDFS API, and thus scales to terabytes of data easily. And for data mining downshifters who don't want to bother with a cluster setup there's always pure Breeze. &#xD;&#xA;" />
  <row Id="2440" PostHistoryTypeId="10" PostId="948" RevisionGUID="b766d731-d62e-4120-8c15-378f9627c972" CreationDate="2014-08-09T01:26:26.703" UserId="62" Comment="104" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:62,&quot;DisplayName&quot;:&quot;AsheeshR&quot;}]}" />
  <row Id="2441" PostHistoryTypeId="5" PostId="951" RevisionGUID="46cfb9a8-b663-4976-8424-e28e39fe1e35" CreationDate="2014-08-09T21:25:58.250" UserId="1279" Comment="added 1 character in body" Text="Not sure anybody have worked with _all_ these tools, so I'm going to share my experience with some of them and let others share their experience with the others. &#xD;&#xA;&#xD;&#xA;**MongoDB** addresses problems that involve heterogeneous and nested objects, while data mining mostly works with simple tabular data. MongoDB is neither fast with this type of data, nor provide any advanced tools for analysis (correct me if you know any). So I can think of a very few applications for Mongo in data mining. &#xD;&#xA;&#xD;&#xA;**Hadoop** is a large ecosystem, containing dozens of different tools. I will assume that you mean core Hadoop features - HDFS and MapReduce. HDFS provides flexible way to store large amounts of data, while MapReduce gives basis for processing them. It has its clear advantages for processing multi-terabyte datasets, but it also has significant drawbacks. In particular, because of intensive disk IO during MapReduce tasks (that slows down computations a lot) it is terrible for interactive development, iterative algorithms and working with not-that-big datasets. For more details see my [earlier answer](http://datascience.stackexchange.com/a/863/1279).&#xD;&#xA;&#xD;&#xA;Many algorithms in Hadoop require multiple MapReduce jobs with complicated data flow. This is where **Scalding** gets shiny. Scalding (and underlying Java's [Cascading](http://www.cascading.org/)) provides much simpler API, but at the moment uses same MapReduce as its runtime and thus holds all the same issues.  &#xD;&#xA;&#xD;&#xA;**Spark** addresses exactly these issues. It drops Hadoop's MapReduce and offers completely new computational framework based on distributed in-memory collections and delayed evaluations. Its API is somewhat similar to Scalding's with all MR complexity removed, so it's really easy to get started with it. Spark is also the first in this list that comes with data mining library - [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html). &#xD;&#xA;&#xD;&#xA;But Spark doesn't reinvent things like basic linear algebra. For this purpose it uses **Breeze**. To my opinion, Breeze is far in quality from scientific packages like SciPy, Octave or Julia, but it is still good enough for most practical use cases. &#xD;&#xA;&#xD;&#xA;-----&#xD;&#xA;&#xD;&#xA;**Mahout** relies on Hadoop's MapReduce and thus is terrible for iterative algorithms. **Spire** and **Saddle** look cute and probably have their niche, but seem to be much less well-known than Breeze. I couldn't find much information about **H2O**, so it doesn't look like a big player here (comments from people who used it are welcome).&#xD;&#xA;&#xD;&#xA;------- &#xD;&#xA;&#xD;&#xA;Some quick summary. Spark seems to be the most simple, flexible and fast-growing project for large-scale data processing. It facilitates a number of new projects (e.g. Shark or Spark SQL) and penetrates into existing (including [Cascading](http://www.concurrentinc.com/2014/05/cascading-now-supports-tez-spark-and-storm-up-next/) and [Mahout](https://mahout.apache.org/users/sparkbindings/home.html)). Spark knows how to utilize HDFS API, and thus scales to terabytes of data easily. And for data mining downshifters who don't want to bother with a cluster setup there's always pure Breeze. &#xD;&#xA;" />
  <row Id="2451" PostHistoryTypeId="2" PostId="954" RevisionGUID="0318cf90-b57d-4b98-86d4-929c289d1f03" CreationDate="2014-08-11T07:59:22.780" UserId="979" Text="I need to do coreference resolution for German texts and I plan to use OpenNLP to perform this task.&#xD;&#xA;&#xD;&#xA;As far as I know OpenNLP coreference resolution does not support the German language.&#xD;&#xA;&#xD;&#xA;Which components/data do I need to adapt the code such that it is possible to perform coreference resolution for German texts?&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2452" PostHistoryTypeId="1" PostId="954" RevisionGUID="0318cf90-b57d-4b98-86d4-929c289d1f03" CreationDate="2014-08-11T07:59:22.780" UserId="979" Text="OpenNLP Coreference Resolution (German)" />
  <row Id="2453" PostHistoryTypeId="3" PostId="954" RevisionGUID="0318cf90-b57d-4b98-86d4-929c289d1f03" CreationDate="2014-08-11T07:59:22.780" UserId="979" Text="&lt;nlp&gt;&lt;text-mining&gt;" />
  <row Id="2454" PostHistoryTypeId="2" PostId="955" RevisionGUID="c482766b-cc60-471d-932b-fc876b3c87c2" CreationDate="2014-08-11T12:25:47.700" UserId="979" Text="does anybofy know a libarary for performing coreference resolution on German texts?" />
  <row Id="2455" PostHistoryTypeId="1" PostId="955" RevisionGUID="c482766b-cc60-471d-932b-fc876b3c87c2" CreationDate="2014-08-11T12:25:47.700" UserId="979" Text="Coreference Resolution for German Texts" />
  <row Id="2456" PostHistoryTypeId="3" PostId="955" RevisionGUID="c482766b-cc60-471d-932b-fc876b3c87c2" CreationDate="2014-08-11T12:25:47.700" UserId="979" Text="&lt;nlp&gt;" />
  <row Id="2457" PostHistoryTypeId="2" PostId="956" RevisionGUID="1629b4fd-5ee1-46d1-a583-a4ffb3a715c5" CreationDate="2014-08-11T12:48:52.320" UserId="2910" Text="I had to create a web based dashboard. My main charting tool was d3js. But I needed to use ggplot2 to generate few charts. Through d3js's ggplot2 extension, I could create the same. If your charts can be generated through existing extension, then web has better alternatives. Later you can export them to PDF for distribution. " />
  <row Id="2458" PostHistoryTypeId="2" PostId="957" RevisionGUID="62fac958-01d5-489c-8faf-16e47a941f69" CreationDate="2014-08-11T15:19:02.047" UserId="989" Text="I know that ARIMA can't detect multiple seasonality, but it is possible to [use fourier functions to add a second seasonality](http://robjhyndman.com/hyndsight/dailydata/).&#xD;&#xA;&#xD;&#xA;I need to forecast gas consumption composed by a daily, weekly (week days-weekend), yearly seasonality. Does it make sense to apply three times the [STL decomposition by LOESS](http://stat.ethz.ch/R-manual/R-devel/library/stats/html/stl.html)? The reason is that I applied the fourier method and I have bad results but I don't know if it is only because I applied it wrong.&#xD;&#xA;&#xD;&#xA;I'm interested in the theoretical explanation, but here you find also the code:&#xD;&#xA;&#xD;&#xA;ARIMA + 2 STL:&#xD;&#xA;&#xD;&#xA;    b &lt;- ts(drop(coredata(dat.ts)), deltat=1/12/30/24, start=1)&#xD;&#xA;    fit &lt;- stl(b, s.window=&quot;periodic&quot;)&#xD;&#xA;    b &lt;- seasadj(fit)&#xD;&#xA;    dat.ts &lt;- xts(b, index(dat.ts))&#xD;&#xA;&#xD;&#xA;    # The weekdays are extracted&#xD;&#xA;    dat.weekdays &lt;- dat.ts[.indexwday(dat.ts) %in% 1:5]&#xD;&#xA;    dat.weekdaysTS &lt;- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)&#xD;&#xA;    fit &lt;- stl(dat.weekdaysTS, s.window=&quot;periodic&quot;)&#xD;&#xA;    dat.weekdaysTS &lt;- seasadj(fit)&#xD;&#xA;&#xD;&#xA;    arima &lt;- Arima(dat.weekdaysTS, order=c(3,0,5))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;With fourier:&#xD;&#xA;&#xD;&#xA;    dat.weekdays &lt;- dat.ts[.indexwday(dat.ts) %in% 1:5]&#xD;&#xA;    dat.weekdaysTS &lt;- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)&#xD;&#xA;    z &lt;- fourier(ts(dat.weekdaysTS, frequency=365.25), K=5)&#xD;&#xA;    arima &lt;- Arima(dat.weekdaysTS, order=c(3,0,5),xreg=z)&#xD;&#xA;" />
  <row Id="2459" PostHistoryTypeId="1" PostId="957" RevisionGUID="62fac958-01d5-489c-8faf-16e47a941f69" CreationDate="2014-08-11T15:19:02.047" UserId="989" Text="Multiple seasonality with ARIMA?" />
  <row Id="2460" PostHistoryTypeId="3" PostId="957" RevisionGUID="62fac958-01d5-489c-8faf-16e47a941f69" CreationDate="2014-08-11T15:19:02.047" UserId="989" Text="&lt;time-series&gt;" />
  <row Id="2461" PostHistoryTypeId="2" PostId="958" RevisionGUID="291bca1c-3e0b-46f8-91e7-f047aaad63ed" CreationDate="2014-08-11T16:05:02.197" UserId="2912" Text="Have a look at [Apache Mahout][1]. Last version features also user-item-based recommenders.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://mahout.apache.org/" />
  <row Id="2463" PostHistoryTypeId="2" PostId="959" RevisionGUID="c9396a26-9548-4bc8-aca2-204c820de71e" CreationDate="2014-08-11T18:33:37.023" UserId="2913" Text="While I am not a data scientist, I am an epidemiologist working in a clinical setting. Your research question did not specify a time period (ie odds of developing CKD in 1 year, 10 years, lifetime?).&#xD;&#xA;&#xD;&#xA;Generally, I would go through a number of steps before even thinking about modeling (univariate analysis, bivariate analysis, colinearity checks, etc). However, the most commonly used method for trying to predict a binary event (using continuous OR binary variables) is logistic regression. If you wanted to look at CKD as a lab value (urine albumin, eGFR) you would use linear regression (continuous outcome).&#xD;&#xA;&#xD;&#xA;While the methods used should be informed by your data and questions, clinicians are used to seeing odds ratios and risk ratios as these the most commonly reported measures of association in medical journals such as NEJM and JAMA. &#xD;&#xA;&#xD;&#xA;If you are working on this problem from a human health perspective (as opposed to Business Intelligence) this Steyerberg's [Clinical Prediction Models][1] is an excellent resource. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.springer.com/medicine/internal/book/978-0-387-77243-1" />
  <row Id="2464" PostHistoryTypeId="2" PostId="960" RevisionGUID="65c1ebcd-3dde-4f08-87ad-40afed9c1cb1" CreationDate="2014-08-11T19:21:50.137" UserId="2556" Text="I have to agree that k-fold should do &quot;just&quot; fine. However, there is a nice article about the &quot;Bootstrap .632+&quot; method (basically a smoothened cross validation) that is supposed to be superior (however, they did the comparisons on not-binary data as far as I can tell)&#xD;&#xA;&#xD;&#xA;Maybe you want to check out this article here: http://www.jstor.org/stable/2965703" />
  <row Id="2465" PostHistoryTypeId="2" PostId="961" RevisionGUID="70ffe0e7-9f27-4f1d-bc67-5742491790fa" CreationDate="2014-08-11T21:13:36.230" UserId="1279" Text="**(question unfinished: pressed Enter too early, wait for a while, please)**&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same. &#xD;&#xA;&#xD;&#xA;I can overcome this by increasing number of weight vectors to 256 or more. In this case I get something like this: &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/lBxL5.png" />
  <row Id="2466" PostHistoryTypeId="1" PostId="961" RevisionGUID="70ffe0e7-9f27-4f1d-bc67-5742491790fa" CreationDate="2014-08-11T21:13:36.230" UserId="1279" Text="Why RBM tends to learn very similar weights?" />
  <row Id="2467" PostHistoryTypeId="3" PostId="961" RevisionGUID="70ffe0e7-9f27-4f1d-bc67-5742491790fa" CreationDate="2014-08-11T21:13:36.230" UserId="1279" Text="&lt;rbm&gt;&lt;deep-learning&gt;" />
  <row Id="2468" PostHistoryTypeId="5" PostId="961" RevisionGUID="61c1e27b-1bc0-4686-b5fb-619d4433b43a" CreationDate="2014-08-11T21:33:34.637" UserId="1279" Comment="added 512 characters in body; edited tags" Text="**(question unfinished: pressed Enter too early, wait for a while, please)**&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same. &#xD;&#xA;&#xD;&#xA;I can overcome this by increasing number of weight vectors to 256 or more. In this case I get something like this: &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (even pretty large), different hyper-parameters, etc. &#xD;&#xA;&#xD;&#xA;My question is: what is the **most likely reason** for weights to get **very similar values**? Do they all just get to some local minimum? Or is it a sign of overfitting? &#xD;&#xA;&#xD;&#xA;I currently use binary-Gaussian RBM, code may be found [here](https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl). &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/lBxL5.png" />
  <row Id="2469" PostHistoryTypeId="6" PostId="961" RevisionGUID="61c1e27b-1bc0-4686-b5fb-619d4433b43a" CreationDate="2014-08-11T21:33:34.637" UserId="1279" Comment="added 512 characters in body; edited tags" Text="&lt;rbm&gt;&lt;representation-learning&gt;" />
  <row Id="2470" PostHistoryTypeId="5" PostId="961" RevisionGUID="227d1d1b-0856-4b81-af90-b310c8cf5e2a" CreationDate="2014-08-11T22:03:26.183" UserId="1279" Comment="added 512 characters in body; edited tags" Text="![enter image description here][1]&#xD;&#xA;&#xD;&#xA;These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same. &#xD;&#xA;&#xD;&#xA;I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc. &#xD;&#xA;&#xD;&#xA;My question is: what is the **most likely reason** for weights to get **very similar values**? Do they all just get to some local minimum? Or is it a sign of overfitting? &#xD;&#xA;&#xD;&#xA;I currently use binary-Gaussian RBM, code may be found [here](https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl). &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/lBxL5.png" />
  <row Id="2471" PostHistoryTypeId="2" PostId="962" RevisionGUID="855053f7-1905-49c1-a652-e5982d196595" CreationDate="2014-08-12T03:50:52.303" UserId="2916" Text="I know the difference between cluster and classification in machine learning. &#xD;&#xA;But I don't know what is difference between text classification and topic models in documents&#xD;&#xA;Also can I use topic model for the documents to identify one topic later on can I use the classification to classify the text inside this documents ?  " />
  <row Id="2472" PostHistoryTypeId="1" PostId="962" RevisionGUID="855053f7-1905-49c1-a652-e5982d196595" CreationDate="2014-08-12T03:50:52.303" UserId="2916" Text="what is difference between text classification and topic models?" />
  <row Id="2473" PostHistoryTypeId="3" PostId="962" RevisionGUID="855053f7-1905-49c1-a652-e5982d196595" CreationDate="2014-08-12T03:50:52.303" UserId="2916" Text="&lt;classification&gt;&lt;text-mining&gt;&lt;topic-model&gt;" />
  <row Id="2474" PostHistoryTypeId="2" PostId="963" RevisionGUID="4e3682db-e672-4ad3-98cf-f4b86f55f6e4" CreationDate="2014-08-12T07:57:03.283" UserId="2920" Text="For experimenting we'd like to use the [Emoji][1] embedded in many Tweets as a ground truth/training data for simple quantitative senitment analysis. Tweets are usually too unstructured for NLP to work well.&#xD;&#xA;&#xD;&#xA;Anyway, there are 722 Emoji in Unicode 6.0, and probably another 250 will be added in Unicode 7.0.&#xD;&#xA;&#xD;&#xA;Is there a database (like e.g. SentiWordNet) that contains sentiment annotations for them?&#xD;&#xA;&#xD;&#xA;Also, if you have experience with using them for sentiment analysis, I'd be interested to hear.&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Emoji" />
  <row Id="2475" PostHistoryTypeId="1" PostId="963" RevisionGUID="4e3682db-e672-4ad3-98cf-f4b86f55f6e4" CreationDate="2014-08-12T07:57:03.283" UserId="2920" Text="Sentiment data for Emoji" />
  <row Id="2476" PostHistoryTypeId="3" PostId="963" RevisionGUID="4e3682db-e672-4ad3-98cf-f4b86f55f6e4" CreationDate="2014-08-12T07:57:03.283" UserId="2920" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;parsing&gt;" />
  <row Id="2477" PostHistoryTypeId="2" PostId="964" RevisionGUID="bdcc1aa7-dd81-4b1e-a8f6-8d07fad0993d" CreationDate="2014-08-12T07:59:13.060" UserId="2920" Text="Topic models are usually **unsupervised**. There are &quot;supervised topic models&quot;, too; but even then they try to model **topics within a classes**.&#xD;&#xA;&#xD;&#xA;E.g. you may have a class &quot;football&quot;, but there may be topics inside this class that relate to particular matches or teams.&#xD;&#xA;&#xD;&#xA;The challenge with topics is that they change over time; consider the matches example above. Such topics may emerge, and disappear again." />
  <row Id="2478" PostHistoryTypeId="2" PostId="965" RevisionGUID="35f6e90c-cab8-4ed6-9721-a98e7a5a8438" CreationDate="2014-08-12T08:54:34.903" UserId="2923" Text="I'm looking for a product that allows us to take in a collection of datastreams, and then after some event, will find any data that changes or correlates with that event. (For example, having a headache, and identifying that I drank too much beer last night and didn't drink enough water)" />
  <row Id="2479" PostHistoryTypeId="1" PostId="965" RevisionGUID="35f6e90c-cab8-4ed6-9721-a98e7a5a8438" CreationDate="2014-08-12T08:54:34.903" UserId="2923" Text="Tool for finding correlations between data after some event" />
  <row Id="2480" PostHistoryTypeId="3" PostId="965" RevisionGUID="35f6e90c-cab8-4ed6-9721-a98e7a5a8438" CreationDate="2014-08-12T08:54:34.903" UserId="2923" Text="&lt;correlation&gt;" />
  <row Id="2481" PostHistoryTypeId="2" PostId="966" RevisionGUID="3e1bff8e-2fef-4893-9d93-1e3930f316ae" CreationDate="2014-08-12T09:52:01.153" UserId="21" Text="**Text Classification**&#xD;&#xA;&#xD;&#xA;I give you a bunch of documents, each of which has a label attached. I ask you to learn why you think the contents of the documents have been given these labels based on their words. Then I give you new documents and ask what you think the label for each one should be. The labels have meaning to me, not to you necessarily.&#xD;&#xA;&#xD;&#xA;**Topic Modeling**&#xD;&#xA;&#xD;&#xA;I give you a bunch of documents, without labels. I ask you to explain why the documents have the words they do by identifying some topics that each is &quot;about&quot;. You tell me the topics, by telling me how much of each is in each document, and I decide what the topics &quot;mean&quot; if anything.&#xD;&#xA;&#xD;&#xA;You'd have to clarify what you me by &quot;identify one topic&quot; or &quot;classify the text&quot;." />
  <row Id="2482" PostHistoryTypeId="5" PostId="955" RevisionGUID="0237e5b0-4ba8-49cb-a8b8-e1e741e03034" CreationDate="2014-08-12T11:00:13.273" UserId="979" Comment="added 245 characters in body" Text="does anybody know a libarary for performing coreference resolution on German texts?&#xD;&#xA;&#xD;&#xA;As far as I know OpenNLP and Standord NLP are not able to perform coreference resolution for German Texts.&#xD;&#xA;&#xD;&#xA;The only tool that I know is [CorZu][1] which is a python library.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.cl.uzh.ch/static/news.php?om=view&amp;nid=163" />
  <row Id="2483" PostHistoryTypeId="2" PostId="967" RevisionGUID="4e6c08da-f134-48fe-b648-6f87fe811c0a" CreationDate="2014-08-12T12:40:44.907" UserId="1279" Text="Total of 972 emoji is not really that big not to be able to label them manually, but I doubt that they will work as a good ground truth. Sources like Twitter are full of irony, sarcasm and other tricky settings where emotional symbols (such as emoji or emoticon) mean something different from normal interpretation. For example, someone may write &quot;xxx cheated their clients, and now they are cheated themselves! ha ha ha! :D&quot;. This is definitely negative comment, but author is glad to see xxx company in trouble and thus adds positive emoticon. These cases are not that frequent, but definitely not suitable for ground truth. &#xD;&#xA;&#xD;&#xA;Much more common approach is to use emoticon as a **seed for collecting actual data set**. For example, in [this paper](http://www.saifmohammad.com/WebDocs/NRC-Sentiment-JAIR-2014.pdf) authors use emoticon and emotional hash tags to grab lexicon of words useful for further classification. " />
  <row Id="2484" PostHistoryTypeId="5" PostId="963" RevisionGUID="1f17df8c-2e75-4732-8f67-7c95c315769b" CreationDate="2014-08-12T16:06:24.710" UserId="2920" Comment="added 346 characters in body" Text="For experimenting we'd like to use the [Emoji][1] embedded in many Tweets as a ground truth/training data for simple quantitative senitment analysis. Tweets are usually too unstructured for NLP to work well.&#xD;&#xA;&#xD;&#xA;Anyway, there are 722 Emoji in Unicode 6.0, and probably another 250 will be added in Unicode 7.0.&#xD;&#xA;&#xD;&#xA;Is there a database (like e.g. SentiWordNet) that contains sentiment annotations for them?&#xD;&#xA;&#xD;&#xA;(Note that SentiWordNet does allow for *ambiguous* meanings, too. Consider e.g. [funny][2], which is not just positive: &quot;this tastes funny&quot; is probably not positive... same will hold for `;-)` for example. But I don't think this is harder for Emoji than it is for regular words...)&#xD;&#xA;&#xD;&#xA;Also, if you have experience with using them for sentiment analysis, I'd be interested to hear.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://en.wikipedia.org/wiki/Emoji&#xD;&#xA;  [2]: http://sentiwordnet.isti.cnr.it/search.php?q=funny" />
  <row Id="2490" PostHistoryTypeId="5" PostId="867" RevisionGUID="fc9ac128-cefe-4bd4-86be-fba2b0cb3a35" CreationDate="2014-08-12T16:52:12.833" UserId="84" Comment="Minor formatting changes." Text="Does anyone know what (from your experience) is the best open source natural language generators (NLG) out there? What are the relative merits of each?&#xD;&#xA;&#xD;&#xA;I'm looking to do sophisticated text summarization and would like to use theme extraction/semantic modeling in conjunction with NLG tools to create accurate, context-aware, and natural-sounding text summaries." />
  <row Id="2491" PostHistoryTypeId="2" PostId="968" RevisionGUID="276c1667-02a5-4db8-a07d-a399ff4c01d8" CreationDate="2014-08-12T17:11:45.447" UserId="2928" Text="I am using the `stepAIC` function in R to do a bi-directional (forward and backward) stepwise regression. I do not understand what each return value from the function means. &#xD;&#xA;&#xD;&#xA;The output is:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;              Df     Sum of Sq    RSS       AIC&#xD;&#xA;    &lt;none&gt;                        350.71   -5406.0&#xD;&#xA;    - aaa      1     0.283        350.99   -5405.9&#xD;&#xA;    - bbb      1     0.339        351.05   -5405.4&#xD;&#xA;    - ccc      1     0.982        351.69   -5400.5&#xD;&#xA;    - ddd      1     0.989        351.70   -5400.5&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Question 1) What do each of the return values `Df`, `Sum of Sq`, `RSS`, and `AIC` mean? What are their ranges? Do I want each value to be close to the min or max of its theoretical range?  &#xD;&#xA;&#xD;&#xA;Question 2) Are the values listed under `Df`, `Sum of Sq`, `RSS`, and `AIC` the values for a model where only one variable would be considered as the independent variable (i.e. y ~&#xD;&#xA;aaa, y ~ bbb, etc.)? &#xD;&#xA;&#xD;&#xA;Question 3) Why is AIC sometimes negative / positive? " />
  <row Id="2492" PostHistoryTypeId="1" PostId="968" RevisionGUID="276c1667-02a5-4db8-a07d-a399ff4c01d8" CreationDate="2014-08-12T17:11:45.447" UserId="2928" Text="Understanding output setAIC" />
  <row Id="2493" PostHistoryTypeId="3" PostId="968" RevisionGUID="276c1667-02a5-4db8-a07d-a399ff4c01d8" CreationDate="2014-08-12T17:11:45.447" UserId="2928" Text="&lt;data-mining&gt;&lt;r&gt;&lt;feature-selection&gt;" />
  <row Id="2494" PostHistoryTypeId="4" PostId="968" RevisionGUID="ba8fbd96-47c2-42de-a3f7-85d98b826438" CreationDate="2014-08-12T18:10:24.237" UserId="2928" Comment="edited title" Text="Understanding output stepAIC" />
  <row Id="2495" PostHistoryTypeId="5" PostId="968" RevisionGUID="72d0b292-30a7-43f4-abce-610538da18b0" CreationDate="2014-08-12T18:36:08.880" UserId="2928" Comment="deleted 248 characters in body" Text="I am using the `stepAIC` function in R to do a bi-directional (forward and backward) stepwise regression. I do not understand what each return value from the function means. &#xD;&#xA;&#xD;&#xA;The output is:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;              Df     Sum of Sq    RSS       AIC&#xD;&#xA;    &lt;none&gt;                        350.71   -5406.0&#xD;&#xA;    - aaa      1     0.283        350.99   -5405.9&#xD;&#xA;    - bbb      1     0.339        351.05   -5405.4&#xD;&#xA;    - ccc      1     0.982        351.69   -5400.5&#xD;&#xA;    - ddd      1     0.989        351.70   -5400.5&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Question Are the values listed under `Df`, `Sum of Sq`, `RSS`, and `AIC` the values for a model where only one variable would be considered as the independent variable (i.e. y ~&#xD;&#xA;aaa, y ~ bbb, etc.)? &#xD;&#xA;&#xD;&#xA;" />
  <row Id="2496" PostHistoryTypeId="5" PostId="731" RevisionGUID="912d42a6-d13a-40be-9ee7-c758105583ca" CreationDate="2014-08-13T08:36:04.350" UserId="322" Comment="polished grammar" Text="When I started with artificial neural networks (NN) I thought I'd have to fight overfitting as the main problem. But in practice I can't even get my NN to pass the 20% error rate barrier. I can't even beat my score on random forest!&#xD;&#xA;&#xD;&#xA;I'm seeking some very general or not so general advice on what should one do to make a NN start capturing trends in data.&#xD;&#xA;&#xD;&#xA;For implementing NN I use Theano Stacked Auto Encoder with [the code from tutorial][1] that works great (less than 5% error rate) for classifying the MNIST dataset. It is a multilayer perceptron, with softmax layer on top with each hidden later being pre-trained as autoencoder (fully described at [tutorial][2], chapter 8). There are ~50 input features and ~10 output classes. The NN has sigmoid neurons and all data are normalized to [0,1]. I tried lots of different configurations: number of hidden layers and neurons in them (100-&gt;100-&gt;100, 60-&gt;60-&gt;60, 60-&gt;30-&gt;15, etc.), different learning and pre-train rates, etc.&#xD;&#xA;&#xD;&#xA;And the best thing I can get is a 20% error rate on the validation set and a 40% error rate on the test set.&#xD;&#xA;&#xD;&#xA;On the other hand, when I try to use Random Forest (from scikit-learn) I easily get a 12% error rate on the validation set and 25%(!) on the test set.&#xD;&#xA;&#xD;&#xA;How can it be that my deep NN with pre-training behaves so badly? What should I try? &#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/SdA.py&#xD;&#xA;  [2]: http://deeplearning.net/tutorial/deeplearning.pdf" />
  <row Id="2497" PostHistoryTypeId="4" PostId="731" RevisionGUID="912d42a6-d13a-40be-9ee7-c758105583ca" CreationDate="2014-08-13T08:36:04.350" UserId="322" Comment="polished grammar" Text="How to fight underfitting in a deep neural net" />
  <row Id="2498" PostHistoryTypeId="24" PostId="731" RevisionGUID="912d42a6-d13a-40be-9ee7-c758105583ca" CreationDate="2014-08-13T08:36:04.350" Comment="Proposed by 322 approved by 2452, 2471 edit id of 134" />
  <row Id="2499" PostHistoryTypeId="2" PostId="969" RevisionGUID="4943012a-36a6-4a3b-aa19-a157183d2403" CreationDate="2014-08-13T14:18:58.917" UserId="2928" Text="After consulting with someone I found out that the &lt;none&gt; corresponds to a model that would include all the variables, in other words none of the variables were removed. So consider the line in the output for the variable aaa. The listed RSS and AIC are the values for a model that would include all variables but aaa and we see an increase in the RSS and AIC. The other listed results can be considered in the same fashion. The best model is then the one where none are removed since this has the smallest AIC. " />
  <row Id="2504" PostHistoryTypeId="2" PostId="972" RevisionGUID="c705b315-8623-48c3-9d3f-bd8c69bdcf9e" CreationDate="2014-08-13T20:30:18.143" UserId="2948" Text="While finding frequent subgraphs in single large graph, subgraph isomorphism (test) is not considered because its not anti-monotone. How and why subgraph isomorphism is not anti-monotone ?" />
  <row Id="2505" PostHistoryTypeId="1" PostId="972" RevisionGUID="c705b315-8623-48c3-9d3f-bd8c69bdcf9e" CreationDate="2014-08-13T20:30:18.143" UserId="2948" Text="Subgraph isomorphism and Anti-monotone property" />
  <row Id="2506" PostHistoryTypeId="3" PostId="972" RevisionGUID="c705b315-8623-48c3-9d3f-bd8c69bdcf9e" CreationDate="2014-08-13T20:30:18.143" UserId="2948" Text="&lt;bigdata&gt;&lt;data-mining&gt;&lt;dataset&gt;&lt;graphs&gt;&lt;similarity&gt;" />
  <row Id="2507" PostHistoryTypeId="2" PostId="973" RevisionGUID="3c6af354-bd75-4f76-b64e-b3ee7d47ffb5" CreationDate="2014-08-13T20:53:15.443" UserId="2949" Text="I like to find the feature weights in a [structured SVM][1] for ranking the features w.r.t. importance. I know that in a binary SVM the weight vector can be written as a [linear combination of examples][2]. But how do you compute the same for an SSVM?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Structured_support_vector_machine&#xD;&#xA;  [2]: http://pyml.sourceforge.net/doc/howto.pdf" />
  <row Id="2508" PostHistoryTypeId="1" PostId="973" RevisionGUID="3c6af354-bd75-4f76-b64e-b3ee7d47ffb5" CreationDate="2014-08-13T20:53:15.443" UserId="2949" Text="feature weights in structured support vector machine" />
  <row Id="2509" PostHistoryTypeId="3" PostId="973" RevisionGUID="3c6af354-bd75-4f76-b64e-b3ee7d47ffb5" CreationDate="2014-08-13T20:53:15.443" UserId="2949" Text="&lt;svm&gt;" />
  <row Id="2510" PostHistoryTypeId="2" PostId="974" RevisionGUID="6484f2e7-14f0-4235-8883-a701ab9e7fc9" CreationDate="2014-08-14T00:50:51.103" UserId="1242" Text="I have a big sparse matrix of users and items they like (in the order of 1M users and 100K items, with a very low level of sparsity). I'm exploring ways in which I could perform kNN search on it. Given the size of my dataset and some initial tests I performed, my assumption is that the method I will use will need to be either parallel or distributed. So I'm considering two classes of possible solutions: one that is either available (or implementable in a reasonably easy way) on a single multicore machine, the other on a Spark cluster, i.e. as a MapReduce program. Here are three broad ideas that I considered:&#xD;&#xA;&#xD;&#xA;* Assuming a cosine similarity metric, perform the full multiplication of the normalized matrix by its transpose (implemented as a sum of outer products)&#xD;&#xA;* Using locality-sensitive hashing (LSH)&#xD;&#xA;* Reducing first the dimensionality of the problem with a PCA&#xD;&#xA;&#xD;&#xA;I'd appreciate any thoughts or advices about possible other ways in which I could tackle this problem.&#xD;&#xA;" />
  <row Id="2511" PostHistoryTypeId="1" PostId="974" RevisionGUID="6484f2e7-14f0-4235-8883-a701ab9e7fc9" CreationDate="2014-08-14T00:50:51.103" UserId="1242" Text="Nearest neighbors search for very high dimensional data" />
  <row Id="2512" PostHistoryTypeId="3" PostId="974" RevisionGUID="6484f2e7-14f0-4235-8883-a701ab9e7fc9" CreationDate="2014-08-14T00:50:51.103" UserId="1242" Text="&lt;machine-learning&gt;&lt;distributed&gt;&lt;map-reduce&gt;&lt;dimensionality-reduction&gt;" />
  <row Id="2514" PostHistoryTypeId="2" PostId="975" RevisionGUID="93839bca-fc6f-483e-9773-d4a244cd53d0" CreationDate="2014-08-14T04:28:36.933" UserId="2452" Text="I'm not an expert on this topic - just interested. Therefore, I can't offer you a specific advice. However, I hope that you might get additional **ideas** toward solving your problem from the following **resources**:&#xD;&#xA;&#xD;&#xA;1) Research paper **&quot;Efficient K-Nearest Neighbor Join Algorithms for High Dimensional Sparse Data&quot;**: http://arxiv.org/abs/1011.2807&#xD;&#xA;&#xD;&#xA;2) Class project paper **&quot;Recommendation System Based on Collaborative Filtering&quot;** (Stanford University): http://cs229.stanford.edu/proj2008/Wen-RecommendationSystemBasedOnCollaborativeFiltering.pdf&#xD;&#xA;&#xD;&#xA;3) Project for the **Netflix Prize Competition (*k-NN*-based)**: http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/index.html&#xD;&#xA;&#xD;&#xA;4) Research paper **&quot;Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data&quot;** on the *curse of dimensionality* phenomenon and its relation to *machine learning*, in general, and *k-NN algorithm*, in particular: http://jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf&#xD;&#xA;&#xD;&#xA;5) **Software for sparse k-NN classification** (free, but appears not to be open source - might clarify with authors): http://www.autonlab.org/autonweb/10408.html&#xD;&#xA;&#xD;&#xA;6) Several **discussion threads** on *StackOverflow*:&#xD;&#xA;&#xD;&#xA;  + http://stackoverflow.com/questions/20333092/knn-with-big-sparse-matrices-in-python&#xD;&#xA;  + http://stackoverflow.com/questions/18164348/efficient-nearest-neighbour-search-for-sparse-matrices&#xD;&#xA;  + http://stackoverflow.com/questions/21085990/scipy-sparse-distance-matrix-scikit-or-scipy&#xD;&#xA;  + http://stackoverflow.com/questions/10472681/handling-incomplete-data-data-sparsity-in-knn&#xD;&#xA;  + http://stackoverflow.com/questions/5560218/computing-sparse-pairwise-distance-matrix-in-r (unlike all previous discussions, which refer to `Python`, this one refers to `R` ecosystem)&#xD;&#xA;&#xD;&#xA;7) Pay attention to *GraphLab*, an open source **parallel framework for machine learning** (http://select.cs.cmu.edu/code/graphlab), which supports *parallel clustering* via `MapReduce`-like features: http://select.cs.cmu.edu/code/graphlab/clustering.html&#xD;&#xA;&#xD;&#xA;You might also check my answer here on Data Science StackExchange on *sparse regression* for links to relevant `R` packages and `CRAN Task View` pages: http://datascience.stackexchange.com/a/918/2452." />
  <row Id="2515" PostHistoryTypeId="5" PostId="975" RevisionGUID="155dbaa4-dcb1-47f1-bef6-fb0f0c0e4e11" CreationDate="2014-08-14T04:47:02.543" UserId="2452" Comment="Improved wording." Text="I'm not an expert on this topic - just interested. Therefore, I can't offer you a specific advice. However, I hope that you might get additional **ideas** toward solving your problem from the following **resources**:&#xD;&#xA;&#xD;&#xA;1) Research paper **&quot;Efficient K-Nearest Neighbor Join Algorithms for High Dimensional Sparse Data&quot;**: http://arxiv.org/abs/1011.2807&#xD;&#xA;&#xD;&#xA;2) Class project paper **&quot;Recommendation System Based on Collaborative Filtering&quot;** (Stanford University): http://cs229.stanford.edu/proj2008/Wen-RecommendationSystemBasedOnCollaborativeFiltering.pdf&#xD;&#xA;&#xD;&#xA;3) Project for the **Netflix Prize Competition (*k-NN*-based)**: http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/index.html&#xD;&#xA;&#xD;&#xA;4) Research paper **&quot;Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data&quot;** on the *curse of dimensionality* phenomenon and its relation to *machine learning*, in general, and *k-NN algorithm*, in particular: http://jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf&#xD;&#xA;&#xD;&#xA;5) **Software for sparse k-NN classification** (free, but appears not to be open source - might clarify with authors): http://www.autonlab.org/autonweb/10408.html&#xD;&#xA;&#xD;&#xA;6) Several **discussion threads** on *StackOverflow*:&#xD;&#xA;&#xD;&#xA;  + http://stackoverflow.com/questions/20333092/knn-with-big-sparse-matrices-in-python&#xD;&#xA;  + http://stackoverflow.com/questions/18164348/efficient-nearest-neighbour-search-for-sparse-matrices&#xD;&#xA;  + http://stackoverflow.com/questions/21085990/scipy-sparse-distance-matrix-scikit-or-scipy&#xD;&#xA;  + http://stackoverflow.com/questions/10472681/handling-incomplete-data-data-sparsity-in-knn&#xD;&#xA;  + http://stackoverflow.com/questions/5560218/computing-sparse-pairwise-distance-matrix-in-r (unlike all previous discussions, which refer to `Python`, this one refers to `R` ecosystem)&#xD;&#xA;&#xD;&#xA;7) Pay attention to *GraphLab*, an open source **parallel framework for machine learning** (http://select.cs.cmu.edu/code/graphlab), which supports *parallel clustering* via `MapReduce` model: http://select.cs.cmu.edu/code/graphlab/clustering.html&#xD;&#xA;&#xD;&#xA;You might also check my answer here on Data Science StackExchange on *sparse regression* for links to relevant `R` packages and `CRAN Task View` pages: http://datascience.stackexchange.com/a/918/2452." />
  <row Id="2516" PostHistoryTypeId="5" PostId="975" RevisionGUID="802dc5fe-a47a-428f-a9ab-5c8419b0fb50" CreationDate="2014-08-14T11:52:35.513" UserId="2452" Comment="Improved wording." Text="I hope that the following **resources** might get you additional **ideas** toward solving the problem:&#xD;&#xA;&#xD;&#xA;1) Research paper **&quot;Efficient K-Nearest Neighbor Join Algorithms for High Dimensional Sparse Data&quot;**: http://arxiv.org/abs/1011.2807&#xD;&#xA;&#xD;&#xA;2) Class project paper **&quot;Recommendation System Based on Collaborative Filtering&quot;** (Stanford University): http://cs229.stanford.edu/proj2008/Wen-RecommendationSystemBasedOnCollaborativeFiltering.pdf&#xD;&#xA;&#xD;&#xA;3) Project for the **Netflix Prize Competition (*k-NN*-based)**: http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/index.html&#xD;&#xA;&#xD;&#xA;4) Research paper **&quot;Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data&quot;** on the *curse of dimensionality* phenomenon and its relation to *machine learning*, in general, and *k-NN algorithm*, in particular: http://jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf&#xD;&#xA;&#xD;&#xA;5) **Software for sparse k-NN classification** (free, but appears not to be open source - might clarify with authors): http://www.autonlab.org/autonweb/10408.html&#xD;&#xA;&#xD;&#xA;6) Several **discussion threads** on *StackOverflow*:&#xD;&#xA;&#xD;&#xA;  + http://stackoverflow.com/questions/20333092/knn-with-big-sparse-matrices-in-python&#xD;&#xA;  + http://stackoverflow.com/questions/18164348/efficient-nearest-neighbour-search-for-sparse-matrices&#xD;&#xA;  + http://stackoverflow.com/questions/21085990/scipy-sparse-distance-matrix-scikit-or-scipy&#xD;&#xA;  + http://stackoverflow.com/questions/10472681/handling-incomplete-data-data-sparsity-in-knn&#xD;&#xA;  + http://stackoverflow.com/questions/5560218/computing-sparse-pairwise-distance-matrix-in-r (unlike all previous discussions, which refer to `Python`, this one refers to `R` ecosystem)&#xD;&#xA;&#xD;&#xA;7) Pay attention to *GraphLab*, an open source **parallel framework for machine learning** (http://select.cs.cmu.edu/code/graphlab), which supports *parallel clustering* via `MapReduce` model: http://select.cs.cmu.edu/code/graphlab/clustering.html&#xD;&#xA;&#xD;&#xA;You might also check my answer here on Data Science StackExchange on *sparse regression* for links to relevant `R` packages and `CRAN Task View` pages: http://datascience.stackexchange.com/a/918/2452." />
  <row Id="2517" PostHistoryTypeId="5" PostId="973" RevisionGUID="05845407-9c1f-4146-a8e4-f4b1399a90dd" CreationDate="2014-08-14T14:42:08.143" UserId="2949" Comment="Improved clarity" Text="I like to find the weight vector for input-space features in a [structured SVM][1]. The idea is to identify the most important set of input-space features (based on the magnitude of their corresponding weights). I know that in a binary SVM the weight vector can be written as a [linear combination of examples][2], and the magnitude of those weights represents how much they were effective for the prediction problem at hand. But how do you compute the same for an SSVM?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Structured_support_vector_machine&#xD;&#xA;  [2]: http://pyml.sourceforge.net/doc/howto.pdf" />
  <row Id="2518" PostHistoryTypeId="2" PostId="976" RevisionGUID="9e546c45-9f1f-4060-95c6-897ea61c713b" CreationDate="2014-08-14T17:28:40.453" UserId="1237" Text="I gave a very limited partial answer for the confirmatory factor analysis package that I developed for Stata in this [Stata Journal article](http://www.stata-journal.com/article.html?article=st0169) based on timing the actual simulations. Confirmatory factor analysis was implemented as a maximum likelihood estimation technique, and I could see very easily how the computation time grew with each dimension (sample size `n`, number of variables `p`, number of factors `k`). As it is heavily dependent on how Stata thinks about the data (optimized to compute across columns/observations rather than rows), I found performance to be `O(n^{0.68} (k+p)^{2.4})` where 2.4 is the fastest matrix inversion asymptotics (and there's hell of a lot of that in confirmatory factor analysis iterative maximization). I did not give a reference for the latter, but I think I got this from [Wikipedia](http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra).&#xD;&#xA;&#xD;&#xA;Note that there is also a matrix inversion step in OLS. However, for reasons of numerical accuracy, no one would really brute-force inverse the `X'X` matrix, and would rather use sweep operators and identify the dangerously collinear variables to deal with precision issues. If you add up $10^8$ numbers that originally were in [double precision](http://en.wikipedia.org/wiki/Double-precision_floating-point_format), you will likely end up with a number that only has a single precision. Numerical computing issues may become a forgotten corner of big data calculations as you start optimizing for speed.&#xD;&#xA;" />
  <row Id="2520" PostHistoryTypeId="2" PostId="977" RevisionGUID="8ef2711d-b827-4eb0-b6f2-e50bf776859c" CreationDate="2014-08-14T19:09:29.523" UserId="725" Text="Let me show you an example of a hypothetical online clustering application:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;At time n points 1,2,3,4 are allocated to the blue cluster A and points b,5,6,7 are allocated to the red cluster B.&#xD;&#xA;&#xD;&#xA;At time n+1 a new point a is introduced which is assigned to the blue cluster A but also causes the point b to be assigned to the blue cluster A as well.&#xD;&#xA;&#xD;&#xA;In the end points 1,2,3,4,a,b belong to A and points 5,6,7 to B. To me this seems reasonable.&#xD;&#xA;&#xD;&#xA;What seems simple at first glance is actually a bit tricky - to maintain identifiers across time steps. Let me try to make this point clear with a more borderline example:&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;The green point will cause two blue and two red points to be merged into one cluster which I arbitrarily decided to color blue - mind this is already my human heuristical thinking at work!&#xD;&#xA;&#xD;&#xA;A computer to make this decision will have to use rules. For example when points are merged into a cluster then the identity of the cluster is determined by the majority. In this case we would face a draw - both blue and red might be valid choices for the new (here blue colored) cluster. &#xD;&#xA;&#xD;&#xA;Imagine a fifth red point close to the green one. Then the majority would be red (3 red vs 2 blue) so red would be a good choice for the new cluster - but this would contradict the even clearer choice of red for the rightmost cluster as those have been red and probably should stay that way.&#xD;&#xA;&#xD;&#xA;I find it fishy to think about this. At the end of the day I guess there are no perfect rules for this - rather heuristics optimizing some stability criterea.&#xD;&#xA;&#xD;&#xA;This finally leads to my questions:&#xD;&#xA;&#xD;&#xA;1. Does this &quot;problem&quot; have a name that it can be referred to?&#xD;&#xA;2. Are there &quot;standard&quot; solutions to this and ...&#xD;&#xA;3. ... is there maybe even an R package for that?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/ZVOQN.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/sQj4h.jpg" />
  <row Id="2521" PostHistoryTypeId="1" PostId="977" RevisionGUID="8ef2711d-b827-4eb0-b6f2-e50bf776859c" CreationDate="2014-08-14T19:09:29.523" UserId="725" Text="Solutions for Continuous Online Cluster Identification?" />
  <row Id="2522" PostHistoryTypeId="3" PostId="977" RevisionGUID="8ef2711d-b827-4eb0-b6f2-e50bf776859c" CreationDate="2014-08-14T19:09:29.523" UserId="725" Text="&lt;machine-learning&gt;&lt;clustering&gt;" />
  <row Id="2523" PostHistoryTypeId="2" PostId="978" RevisionGUID="a8f86e7c-0dfe-456d-a819-0e154bcca359" CreationDate="2014-08-14T21:38:57.630" UserId="819" Text="&gt;  But I don't know what is difference between text classification and topic models in documents&#xD;&#xA;&#xD;&#xA;Text classification is a form of supervised learning -- the set of possible classes are known/defined in advance and don't change.&#xD;&#xA;&#xD;&#xA;Topic modeling is a form of unsupervised learning (akin to clustering) -- the set of possible topics are unknown apriori. They're defined as part of generating the topic models. With a non-deterministic algorithm like LDA, you'll get different topics each time you run the algorithm.&#xD;&#xA;&#xD;&#xA;Text classification often involves mutually-exclusive classes -- think of these as buckets. But it doesn't have to -- given the right kind of labeled input data, you can set of a series of non-mutually-exclusive binary classifiers.&#xD;&#xA;&#xD;&#xA;Topic modeling is generally not mutually-exclusive -- the same document can have its probability distribution spread across many topics. In addition, there are also hierarchical topic modeling methods, etc.&#xD;&#xA;&#xD;&#xA;&gt; Also can I use topic model for the documents to identify one topic later on can I use the classification to classify the text inside this documents ? &#xD;&#xA;&#xD;&#xA;If you're asking whether you can take all of the documents assigned to one topic by a topic modeling algorithm and then apply a classifier to that collection, then yes, you certainly can do that. I'm not sure it makes much sense, though -- at a minimum, you'd need to pick a threshold for the topic probability distribution above which you'll include documents in your collection (typically 0.05-0.1). Can you elaborate on your use case?&#xD;&#xA;&#xD;&#xA;By the way, there's a great tutorial on topic modeling using the MALLET library for Java available here: [Getting Started with Topic Modeling and MALLET][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://programminghistorian.org/lessons/topic-modeling-and-mallet" />
  <row Id="2524" PostHistoryTypeId="2" PostId="979" RevisionGUID="4520d267-0d61-43fb-b9b2-af787e5d085b" CreationDate="2014-08-15T13:10:20.937" UserId="2958" Text="I have a problem of clustering huge amount of sentences into groups by their meanings. This is similar to a problem when you have lots of sentences and want to group them by their meanings.&#xD;&#xA;&#xD;&#xA;What algorithms are suggested to do this? I don't know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?&#xD;&#xA;&#xD;&#xA;I'm trying now the simplest features with just list of words and distance between sentences as next   ![enter image description here][1] A and B are corresponding sets of words in sentence A and B. Does it make sense at all? &#xD;&#xA;&#xD;&#xA;I'm trying to apply [Mean-Shift][2] algorithm from scikit library to this distance as it does not require number of clusters in advance.&#xD;&#xA;&#xD;&#xA;If anyone will advise better methods/approaches for the problem - it will be very much appreciated as I'm still new to the topic.&#xD;&#xA;&#xD;&#xA;Thanks.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/dHB9X.jpg&#xD;&#xA;  [2]: http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py" />
  <row Id="2525" PostHistoryTypeId="1" PostId="979" RevisionGUID="4520d267-0d61-43fb-b9b2-af787e5d085b" CreationDate="2014-08-15T13:10:20.937" UserId="2958" Text="Algorithms for text clustering" />
  <row Id="2526" PostHistoryTypeId="3" PostId="979" RevisionGUID="4520d267-0d61-43fb-b9b2-af787e5d085b" CreationDate="2014-08-15T13:10:20.937" UserId="2958" Text="&lt;text-mining&gt;&lt;clustering&gt;&lt;sklearn&gt;&lt;k-means&gt;" />
  <row Id="2527" PostHistoryTypeId="5" PostId="979" RevisionGUID="a01169c0-fab0-4b5b-bde8-71874b8e4d21" CreationDate="2014-08-15T13:35:43.170" UserId="84" Comment="deleted 11 characters in body" Text="I have a problem of clustering huge amount of sentences into groups by their meanings. This is similar to a problem when you have lots of sentences and want to group them by their meanings.&#xD;&#xA;&#xD;&#xA;What algorithms are suggested to do this? I don't know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?&#xD;&#xA;&#xD;&#xA;I'm trying now the simplest features with just list of words and distance between sentences as next   ![enter image description here][1] A and B are corresponding sets of words in sentence A and B. Does it make sense at all? &#xD;&#xA;&#xD;&#xA;I'm trying to apply [Mean-Shift][2] algorithm from scikit library to this distance as it does not require number of clusters in advance.&#xD;&#xA;&#xD;&#xA;If anyone will advise better methods/approaches for the problem - it will be very much appreciated as I'm still new to the topic.&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/dHB9X.jpg&#xD;&#xA;  [2]: http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py" />
  <row Id="2528" PostHistoryTypeId="2" PostId="980" RevisionGUID="dba9f70b-da14-4dd8-9c52-b00beea5b90e" CreationDate="2014-08-15T14:00:14.927" UserId="2452" Text="Check the **Stanford NLP Group**'s open source software (http://www-nlp.stanford.edu/software), in particular, **Stanford Classifier** (http://www-nlp.stanford.edu/software/classifier.shtml). The software is written in `Java`, which will likely delight you, but also has bindings for some other languages. Note, the *licensing* - if you plan to use their code in commercial products, you have to acquire commercial license.&#xD;&#xA;&#xD;&#xA;Another interesting set of open source libraries, IMHO suitable for this task and much more, is **parallel framework for machine learning GraphLab** (http://select.cs.cmu.edu/code/graphlab), which includes **clustering library**, implementing various clustering algorithms (http://select.cs.cmu.edu/code/graphlab/clustering.html). It is especially suitable for **very large volume of data** (like you have), as it implements `MapReduce` model and, thus, supports *multicore* and *multiprocessor* **parallel processing**." />
  <row Id="2529" PostHistoryTypeId="5" PostId="980" RevisionGUID="429e0a45-66ba-4bf0-92f2-3f194a59dbee" CreationDate="2014-08-15T14:13:35.977" UserId="2452" Comment="Added info on NLTK library." Text="Check the **Stanford NLP Group**'s open source software (http://www-nlp.stanford.edu/software), in particular, **Stanford Classifier** (http://www-nlp.stanford.edu/software/classifier.shtml). The software is written in `Java`, which will likely delight you, but also has bindings for some other languages. Note, the *licensing* - if you plan to use their code in commercial products, you have to acquire commercial license.&#xD;&#xA;&#xD;&#xA;Another interesting set of open source libraries, IMHO suitable for this task and much more, is **parallel framework for machine learning GraphLab** (http://select.cs.cmu.edu/code/graphlab), which includes **clustering library**, implementing various clustering algorithms (http://select.cs.cmu.edu/code/graphlab/clustering.html). It is especially suitable for **very large volume of data** (like you have), as it implements `MapReduce` model and, thus, supports *multicore* and *multiprocessor* **parallel processing**.&#xD;&#xA;&#xD;&#xA;You most likely are aware of the following, but I will mention it just in case. **Natural Language Toolkit (NLTK)** for `Python` (http://www.nltk.org) contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the `NLTK Book`: http://www.nltk.org/book/ch06.html." />
  <row Id="2530" PostHistoryTypeId="5" PostId="980" RevisionGUID="2cd55170-4595-455c-bc63-d7b7b6b6419a" CreationDate="2014-08-15T16:31:41.337" UserId="2452" Comment="Added UPDATE on algorithms." Text="Check the **Stanford NLP Group**'s open source software (http://www-nlp.stanford.edu/software), in particular, **Stanford Classifier** (http://www-nlp.stanford.edu/software/classifier.shtml). The software is written in `Java`, which will likely delight you, but also has bindings for some other languages. Note, the *licensing* - if you plan to use their code in commercial products, you have to acquire commercial license.&#xD;&#xA;&#xD;&#xA;Another interesting set of open source libraries, IMHO suitable for this task and much more, is **parallel framework for machine learning GraphLab** (http://select.cs.cmu.edu/code/graphlab), which includes **clustering library**, implementing various clustering algorithms (http://select.cs.cmu.edu/code/graphlab/clustering.html). It is especially suitable for **very large volume of data** (like you have), as it implements `MapReduce` model and, thus, supports *multicore* and *multiprocessor* **parallel processing**.&#xD;&#xA;&#xD;&#xA;You most likely are aware of the following, but I will mention it just in case. **Natural Language Toolkit (NLTK)** for `Python` (http://www.nltk.org) contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the `NLTK Book`: http://www.nltk.org/book/ch06.html.&#xD;&#xA;&#xD;&#xA;**UPDATE:**&#xD;&#xA;&#xD;&#xA;Speaking of **algorithms**, it seems that you've tried most of the ones from `scikit-learn`, such as illustrated in this topic extraction example: http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html. However, you may find useful other libraries, which implement a wide variety of *clustering algorithms*, including *Non-Negative Matrix Factorization (NMF)*. One of such libraries is **Python Matrix Factorization (PyMF)** with home page at https://code.google.com/p/pymf and source code at https://github.com/nils-werner/pymf. Another, even more interesting, library, also Python-based, is **NIMFA**, which implements various *NMF algorithms*: http://nimfa.biolab.si. Here's a research paper, describing `NIMFA`: http://jmlr.org/papers/volume13/zitnik12a/zitnik12a.pdf. Here's an example from its documentation, which presents the solution for very similar text processing problem of *topic clustering*: http://nimfa.biolab.si/nimfa.examples.documents.html." />
  <row Id="2531" PostHistoryTypeId="5" PostId="49" RevisionGUID="64bea266-f539-4d0f-8a6b-57b3234e5dcf" CreationDate="2014-08-15T16:38:27.880" UserId="2961" Comment="added tag summary" Text="R is a free, open-source programming language and software environment for statistical computing, bioinformatics, and graphics." />
  <row Id="2532" PostHistoryTypeId="24" PostId="49" RevisionGUID="64bea266-f539-4d0f-8a6b-57b3234e5dcf" CreationDate="2014-08-15T16:38:27.880" Comment="Proposed by 2961 approved by 84 edit id of 135" />
  <row Id="2533" PostHistoryTypeId="2" PostId="981" RevisionGUID="329ca4a9-213a-422d-9822-89202664ca5f" CreationDate="2014-08-15T20:22:11.750" UserId="2958" Text="Basically for this task you can efficiently use any SQL database with good support of B+tree based indexes (MySQL will suite you needs just perfect). &#xD;&#xA;&#xD;&#xA;Create 3 tables:&#xD;&#xA;&#xD;&#xA; 1. Documents table, columns: id/document&#xD;&#xA; 2. N-grams table: n_gram_id/n_gram&#xD;&#xA; 3. Mapping between n-grams and documents: document_id/n_gram_id&#xD;&#xA;&#xD;&#xA;Create indexes on N-gram table/n_gram string and Mapping table/n_gram_id, also primary keys will be indexed by default well.&#xD;&#xA;&#xD;&#xA;Your operations will be efficient:&#xD;&#xA;&#xD;&#xA; 1. Insertion of document: just extract all n-grams and insert into document table and N-grams table&#xD;&#xA; 2. Lookup for in_gram will be quick with support of index&#xD;&#xA; 3. Querying for all n-grams that contain a sub-n-gram: in 2 steps - just query based on index all n-grams which contain sub-n-gram from 2nd table. Then - retrieve all corresponding documents for each of these n-grams.&#xD;&#xA;&#xD;&#xA;You don't even need to use joins to achieve all these operations so indexes will help a lot. Also if data will not suite in one machine - you can implement sharding scheme, like storing n_grams started from a-n on one server and o-z on another or other suitable scheme. &#xD;&#xA;&#xD;&#xA;Also you can use MongoDB, but I'm not sure how exactly you need to implement indexing scheme. For MongoDB you will get sharding scheme for free as it is already built-in." />
  <row Id="2534" PostHistoryTypeId="2" PostId="982" RevisionGUID="39f1b9a3-67cc-467e-8a97-1eabb3b5f5f6" CreationDate="2014-08-16T10:25:19.687" UserId="924" Text="See [Lucene NGramTokenizer][1]&#xD;&#xA;&#xD;&#xA;Are you sure you can't just use lucene or similar indexing techniques?&#xD;&#xA;&#xD;&#xA;Inverted indexes will store the n-gram only once, then just the document ids that contain the ngram; they don't store this as highly redundant raw text.&#xD;&#xA;&#xD;&#xA;As for finding ngrams that contain your query sub-n-gram, I would build an index on the observed ngrams, e.g. using a second lucene index, or [any other substring index][2] such as a trie or suffix tree. If your data is dynamic, probably lucene is a reasonable choice, using phrase queries to find your n-grams.&#xD;&#xA;&#xD;&#xA;  [1]: https://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Substring_index" />
  <row Id="2535" PostHistoryTypeId="2" PostId="983" RevisionGUID="3dcbb699-8adc-454d-b7a3-465f426cf72d" CreationDate="2014-08-16T10:31:40.637" UserId="2971" Text="I have to get data from a remote database and store it in my local database. And mapping between the two database is something like shown in the below image.&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So, for this I'm looking for a **Python library**(or just any thing) which helps to automate **syncing**(insert or update) tables and it also have to be easy as just configuring it like below.&#xD;&#xA;&#xD;&#xA;    {&#xD;&#xA;        &quot;localdb.table1&quot;: {&#xD;&#xA;            &quot;id&quot;: &quot;remotedb.table1.id&quot;,&#xD;&#xA;            &quot;field2&quot;: &quot;remotedb.table1.field3&quot;,&#xD;&#xA;            &quot;field2&quot;: &quot;remotedb.table1.field5&quot;&#xD;&#xA;            &quot;field3&quot;: &quot;remotedb.table3.field2&quot; # get this value by ID&#xD;&#xA;        }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/vNhx5.jpg" />
  <row Id="2536" PostHistoryTypeId="1" PostId="983" RevisionGUID="3dcbb699-8adc-454d-b7a3-465f426cf72d" CreationDate="2014-08-16T10:31:40.637" UserId="2971" Text="Easy way to automate data mining" />
  <row Id="2537" PostHistoryTypeId="3" PostId="983" RevisionGUID="3dcbb699-8adc-454d-b7a3-465f426cf72d" CreationDate="2014-08-16T10:31:40.637" UserId="2971" Text="&lt;data-mining&gt;&lt;python&gt;" />
  <row Id="2538" PostHistoryTypeId="4" PostId="983" RevisionGUID="86afad93-4280-400c-ba23-a2fc0d3b5469" CreationDate="2014-08-16T10:36:10.867" UserId="924" Comment="This it not data mining. This is just collecting and filtering data." Text="Easy way to automate database synchronization" />
  <row Id="2539" PostHistoryTypeId="6" PostId="983" RevisionGUID="86afad93-4280-400c-ba23-a2fc0d3b5469" CreationDate="2014-08-16T10:36:10.867" UserId="924" Comment="This it not data mining. This is just collecting and filtering data." Text="&lt;python&gt;&lt;databases&gt;" />
  <row Id="2540" PostHistoryTypeId="24" PostId="983" RevisionGUID="86afad93-4280-400c-ba23-a2fc0d3b5469" CreationDate="2014-08-16T10:36:10.867" Comment="Proposed by 924 approved by 2971 edit id of 137" />
  <row Id="2541" PostHistoryTypeId="2" PostId="984" RevisionGUID="941f2b19-e8a4-4aed-baae-67a4b3600c9a" CreationDate="2014-08-16T10:42:58.337" UserId="2972" Text="I have no knowledge about the climate or soil. And I just want to find out more about these kind of dataset. I heard that Climate Corporation asked its candidates to perform statistical analysis on various climate dataset. &#xD;&#xA;&#xD;&#xA;This is why I am asking this question. Please do not get me wrong. I am not trying to get the dataset to prepare myself for an interview, as I know they give out different dataset to people from different background. &#xD;&#xA;&#xD;&#xA;I know that Climate Corporation only hires PHD, which I am not. I only want to play around with their dataset such that I can learn and implement **time series analysis**. That's it. &#xD;&#xA;&#xD;&#xA;So, if anyone does not mind sharing their dataset. Please post the link them below. Thank you very much. " />
  <row Id="2542" PostHistoryTypeId="1" PostId="984" RevisionGUID="941f2b19-e8a4-4aed-baae-67a4b3600c9a" CreationDate="2014-08-16T10:42:58.337" UserId="2972" Text="Can anyone provide the 24 hour challenge dataset from Climate Corporation?" />
  <row Id="2543" PostHistoryTypeId="3" PostId="984" RevisionGUID="941f2b19-e8a4-4aed-baae-67a4b3600c9a" CreationDate="2014-08-16T10:42:58.337" UserId="2972" Text="&lt;dataset&gt;" />
  <row Id="2544" PostHistoryTypeId="5" PostId="977" RevisionGUID="812eb5dc-33b8-4540-9d85-d7144ca4b3eb" CreationDate="2014-08-16T10:49:06.163" UserId="725" Comment="added 202 characters in body" Text="Let me show you an example of a hypothetical online clustering application:&#xD;&#xA;&#xD;&#xA;![enter image description here][1]&#xD;&#xA;&#xD;&#xA;At time n points 1,2,3,4 are allocated to the blue cluster A and points b,5,6,7 are allocated to the red cluster B.&#xD;&#xA;&#xD;&#xA;At time n+1 a new point a is introduced which is assigned to the blue cluster A but also causes the point b to be assigned to the blue cluster A as well.&#xD;&#xA;&#xD;&#xA;In the end points 1,2,3,4,a,b belong to A and points 5,6,7 to B. To me this seems reasonable.&#xD;&#xA;&#xD;&#xA;What seems simple at first glance is actually a bit tricky - to maintain identifiers across time steps. Let me try to make this point clear with a more borderline example:&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;The green point will cause two blue and two red points to be merged into one cluster which I arbitrarily decided to color blue - mind this is already my human heuristical thinking at work!&#xD;&#xA;&#xD;&#xA;A computer to make this decision will have to use rules. For example when points are merged into a cluster then the identity of the cluster is determined by the majority. In this case we would face a draw - both blue and red might be valid choices for the new (here blue colored) cluster. &#xD;&#xA;&#xD;&#xA;Imagine a fifth red point close to the green one. Then the majority would be red (3 red vs 2 blue) so red would be a good choice for the new cluster - but this would contradict the even clearer choice of red for the rightmost cluster as those have been red and probably should stay that way.&#xD;&#xA;&#xD;&#xA;I find it fishy to think about this. At the end of the day I guess there are no perfect rules for this - rather heuristics optimizing some stability criterea.&#xD;&#xA;&#xD;&#xA;This finally leads to my questions:&#xD;&#xA;&#xD;&#xA;1. Does this &quot;problem&quot; have a name that it can be referred to?&#xD;&#xA;2. Are there &quot;standard&quot; solutions to this and ...&#xD;&#xA;3. ... is there maybe even an R package for that?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;&#xD;&#xA;[Reasonable Inheritance of Cluster Identities in Repetitive Clustering][3]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/ZVOQN.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/sQj4h.jpg&#xD;&#xA;  [3]: http://www.joyofdata.de/blog/reasonable-inheritance-of-cluster-identities-in-repetitive-clustering/" />
  <row Id="2545" PostHistoryTypeId="2" PostId="985" RevisionGUID="48304a69-f551-4cfe-a1ed-537c7aa048d3" CreationDate="2014-08-16T11:05:43.353" UserId="2916" Text="I have a question about classifying documents using supervised learning and unsupervised learning.&#xD;&#xA; &#xD;&#xA;For example: - I have a bunch of documents talking about football.  &#xD;&#xA;As we know football has different meaning in UK, USA and Australia. Therefore, it is difficult to classify these documents to three different categorizations which are soccer, American football and Australian football.  &#xD;&#xA;My approach tries to use cosine similarity terms which is based on unsupervised. After we use the cluster learning, we are able to create a number of clusters based on cosine similarity which each cluster will contain similar documents terms. After we create the clusters, we can use a semantic feature to identify these clusters depend on supervised model like SVM to make accurate categorizations.  &#xD;&#xA;&#xD;&#xA;My goal is to create more accurate categorizations because a new document test can be related to these categorizations or not.  " />
  <row Id="2546" PostHistoryTypeId="1" PostId="985" RevisionGUID="48304a69-f551-4cfe-a1ed-537c7aa048d3" CreationDate="2014-08-16T11:05:43.353" UserId="2916" Text="Can I use unsupervised learning then I use supervised learning?" />
  <row Id="2547" PostHistoryTypeId="3" PostId="985" RevisionGUID="48304a69-f551-4cfe-a1ed-537c7aa048d3" CreationDate="2014-08-16T11:05:43.353" UserId="2916" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;clustering&gt;" />
  <row Id="2548" PostHistoryTypeId="5" PostId="985" RevisionGUID="4c03e4ad-5ec0-443b-b53b-6bf3672f70ef" CreationDate="2014-08-16T16:21:39.327" UserId="2916" Comment="added 42 characters in body" Text="I have a question about classifying documents using supervised learning and unsupervised learning.&#xD;&#xA; &#xD;&#xA;For example: - I have a bunch of documents talking about football.  &#xD;&#xA;As we know football has different meaning in UK, USA and Australia. Therefore, it is difficult to classify these documents to three different categorizations which are soccer, American football and Australian football.  &#xD;&#xA;My approach tries to use cosine similarity terms which is based on unsupervised. After we use the cluster learning, we are able to create a number of clusters based on cosine similarity which each cluster will contain similar documents terms. After we create the clusters, we can use a semantic feature to identify these clusters depend on supervised model like SVM to make accurate categorizations.  &#xD;&#xA;&#xD;&#xA;My goal is to create more accurate categorizations because if I want to test a new document I want know if this document can be related to these categorizations or not.  " />
  <row Id="2549" PostHistoryTypeId="14" PostId="983" RevisionGUID="0b81fefd-fe00-4306-8a16-c857d3d8fada" CreationDate="2014-08-16T17:29:13.093" UserId="-1" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:-1,&quot;DisplayName&quot;:&quot;Community&quot;}]}" />
  <row Id="2550" PostHistoryTypeId="35" PostId="983" RevisionGUID="88767713-d453-409d-ac06-04090a6e1225" CreationDate="2014-08-16T17:29:13.093" UserId="21" Comment="to http://dba.stackexchange.com/questions/74165/easy-way-to-automate-database-synchronization" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2551" PostHistoryTypeId="10" PostId="983" RevisionGUID="a6f2ccdb-68ee-4f78-90ff-a1751edbc7c3" CreationDate="2014-08-16T17:29:13.093" UserId="21" Comment="102" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2552" PostHistoryTypeId="5" PostId="48" RevisionGUID="820de325-f112-4cdb-8d6b-dfba5119b4cd" CreationDate="2014-08-16T17:29:43.517" UserId="2961" Comment="added text" Text="[R][1] is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&amp;T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.&#xD;&#xA;&#xD;&#xA;R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, ...) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.&#xD;&#xA;&#xD;&#xA;One of R's strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.&#xD;&#xA;&#xD;&#xA;R was created by [Ross Ihaka][2] and [Robert Gentleman][3] and is now developed by the [R Development Core Team][4]. The R environment is easily extended through a packaging system on [CRAN][5]. &#xD;&#xA;&#xD;&#xA;R is available as Free Software under the terms of the Free Software Foundation's GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and Mac OS.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.r-project.org&#xD;&#xA;  [2]: http://www.stat.auckland.ac.nz/~ihaka/&#xD;&#xA;  [3]: http://www.gene.com/scientists/our-scientists/robert-gentleman&#xD;&#xA;  [4]: http://www.r-project.org/contributors.html&#xD;&#xA;  [5]: http://cran.r-project.org &quot;CRAN The Comprehensive R Archive Network&quot;" />
  <row Id="2553" PostHistoryTypeId="24" PostId="48" RevisionGUID="820de325-f112-4cdb-8d6b-dfba5119b4cd" CreationDate="2014-08-16T17:29:43.517" Comment="Proposed by 2961 approved by 21 edit id of 136" />
  <row Id="2554" PostHistoryTypeId="2" PostId="986" RevisionGUID="d60a5c8e-4356-4b26-a55c-a4dd1727061c" CreationDate="2014-08-17T14:17:41.307" UserId="2978" Text="OK, so here's your data. &#xD;&#xA;&#xD;&#xA;    dd &lt;- data.frame(position=rep(1:3, each=2), &#xD;&#xA;                     variation=rep(c(1,3), 3), &#xD;&#xA;                     impressions=rep(c(753, 767), 3), &#xD;&#xA;                     clicks=c(26,7,16,13,2,7))&#xD;&#xA;&#xD;&#xA;which is&#xD;&#xA;&#xD;&#xA;      position variation impressions clicks&#xD;&#xA;    1        1         1         753     26&#xD;&#xA;    2        1         3         767      7&#xD;&#xA;    3        2         1         753     16&#xD;&#xA;    4        2         3         767     13&#xD;&#xA;    5        3         1         753      2&#xD;&#xA;    6        3         3         767      7&#xD;&#xA;&#xD;&#xA;The two model assumptions you're thinking about are Binomial&#xD;&#xA;&#xD;&#xA;    mod.bin &lt;- glm(cbind(clicks, impressions-clicks) ~ variation + position,&#xD;&#xA;                   family=binomial, data=dd)&#xD;&#xA;&#xD;&#xA;where the dependent variable is constructed to have the count of the event of interest in the first column, and the Poisson&#xD;&#xA;&#xD;&#xA;    md.pois &lt;- glm(clicks ~ variation + position + offset(log(impressions)), &#xD;&#xA;                   family=poisson, data=dd)&#xD;&#xA;&#xD;&#xA;where the `log(impressions)` offset is necessary whenever the number of trials differs across observations.  This means coefficients are interpretable in terms of change in rate not change in count, which is what you want.  &#xD;&#xA;&#xD;&#xA;The first model generalises the `binom.test` to a setting with covariates, which is what you have.  That gets you a more direct answer to your question, and better (if not perfect) measurement of the relevant uncertainty.&#xD;&#xA;&#xD;&#xA;**Notes**&#xD;&#xA;&#xD;&#xA;Both models assume no interaction between variation and position ('independent effects').  This may or may not be reasonable.  You'd want more replications to investigate that properly.  Swap the `+` for a `*` to do so.&#xD;&#xA;&#xD;&#xA;In this data `summary` confirms that the two models give rather similar results, so concerns about Poisson vs Binomial don't seem to matter much.&#xD;&#xA;&#xD;&#xA;In the wild, count data is usually overdispersed, that is: more variable than you'd expect from a Poisson with a constant rate or a Binomial with constant click probability, often due to unmodeled determinants of click rate / probability.  If that's the case then prediction intervals from these models will be too narrow.&#xD;&#xA;" />
  <row Id="2555" PostHistoryTypeId="5" PostId="986" RevisionGUID="4682c521-3eea-43dd-b5c9-625a6af3e00f" CreationDate="2014-08-17T15:04:13.880" UserId="2978" Comment="[Edit removed during grace period]" Text="OK, so here's your data. &#xD;&#xA;&#xD;&#xA;    dd &lt;- data.frame(position=rep(1:3, each=2), &#xD;&#xA;                     variation=rep(c(1,3), 3), &#xD;&#xA;                     impressions=rep(c(753, 767), 3), &#xD;&#xA;                     clicks=c(26,7,16,13,2,7))&#xD;&#xA;&#xD;&#xA;which is&#xD;&#xA;&#xD;&#xA;      position variation impressions clicks&#xD;&#xA;    1        1         1         753     26&#xD;&#xA;    2        1         3         767      7&#xD;&#xA;    3        2         1         753     16&#xD;&#xA;    4        2         3         767     13&#xD;&#xA;    5        3         1         753      2&#xD;&#xA;    6        3         3         767      7&#xD;&#xA;&#xD;&#xA;The two model assumptions you're thinking about are Binomial&#xD;&#xA;&#xD;&#xA;    mod.bin &lt;- glm(cbind(clicks, impressions-clicks) ~ variation + position,&#xD;&#xA;                   family=binomial, data=dd)&#xD;&#xA;&#xD;&#xA;where the dependent variable is constructed to have the count of the event of interest in the first column, and the Poisson&#xD;&#xA;&#xD;&#xA;    md.pois &lt;- glm(clicks ~ variation + position + offset(log(impressions)), &#xD;&#xA;                   family=poisson, data=dd)&#xD;&#xA;&#xD;&#xA;where the `log(impressions)` offset is necessary whenever the number of trials differs across observations.  This means coefficients are interpretable in terms of change in rate not change in count, which is what you want.  &#xD;&#xA;&#xD;&#xA;The first model generalises the `binom.test` to a setting with covariates, which is what you have.  That gets you a more direct answer to your question, and better (if not perfect) measurement of the relevant uncertainty.&#xD;&#xA;&#xD;&#xA;**Notes**&#xD;&#xA;&#xD;&#xA;Both models assume no interaction between variation and position ('independent effects').  This may or may not be reasonable.  You'd want more replications to investigate that properly.  Swap the `+` for a `*` to do so.&#xD;&#xA;&#xD;&#xA;In this data `summary` confirms that the two models give rather similar results, so concerns about Poisson vs Binomial don't seem to matter much.&#xD;&#xA;&#xD;&#xA;In the wild, count data is usually overdispersed, that is: more variable than you'd expect from a Poisson with a constant rate or a Binomial with constant click probability, often due to unmodeled determinants of click rate / probability.  If that's the case then prediction intervals from these models will be too narrow.&#xD;&#xA;" />
  <row Id="2556" PostHistoryTypeId="2" PostId="987" RevisionGUID="d5f0645f-9684-4d92-9d6a-ea99f04edc8d" CreationDate="2014-08-17T17:29:44.123" UserId="2979" Text="The problem I am tackling is categorizing short texts into multiple classes. My current approach is to use tf-idf weighted term frequencies and learn a simple linear classifier (logistic regression). This works reasonably well (around 90% macro F-1 on test set, nearly 100% on training set). A big problem are unseen words/n-grams. &#xD;&#xA;&#xD;&#xA;I am trying to improve the classifier by adding other features, e.g. a fixed sized vector computed using distributional similarities (as computed by word2vec) or other categorical features of the examples. My idea was to just add the features to the sparse input features from the bag of words. However, this results in worse performance on the test and training set. The additional features by themselves give about 80% F-1 on the test set, so they aren't garbage. Scaling the features didn't help as well. My current thinking is that these kind of features don't mix well with the (sparse) bag of words features.&#xD;&#xA;&#xD;&#xA;So the question is: assuming the additional features provide additional information, what is the best way to incorporate them? Could training separate classifiers and combining them in some kind of ensemble work (this would probably have the drawback that no interaction between the features of the different classifiers could be captured)? Are there other more complex models I should consider?" />
  <row Id="2557" PostHistoryTypeId="1" PostId="987" RevisionGUID="d5f0645f-9684-4d92-9d6a-ea99f04edc8d" CreationDate="2014-08-17T17:29:44.123" UserId="2979" Text="Text categorization: combining different kind of features" />
  <row Id="2558" PostHistoryTypeId="3" PostId="987" RevisionGUID="d5f0645f-9684-4d92-9d6a-ea99f04edc8d" CreationDate="2014-08-17T17:29:44.123" UserId="2979" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;feature-selection&gt;&lt;logistic-regression&gt;&lt;information-retrieval&gt;" />
  <row Id="2560" PostHistoryTypeId="5" PostId="961" RevisionGUID="fc05cec7-f50e-4f17-8b85-fc3df5eaa496" CreationDate="2014-08-17T19:47:22.887" UserId="1279" Comment="added 685 characters in body" Text="![enter image description here][1]&#xD;&#xA;&#xD;&#xA;These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same. &#xD;&#xA;&#xD;&#xA;I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc. &#xD;&#xA;&#xD;&#xA;My question is: what is the **most likely reason** for weights to get **very similar values**? Do they all just get to some local minimum? Or is it a sign of overfitting? &#xD;&#xA;&#xD;&#xA;I currently use binary-Gaussian RBM, code may be found [here](https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl). &#xD;&#xA;&#xD;&#xA;**UPD.** My dataset is based on [CK+](http://www.pitt.edu/~emotion/ck-spread.htm), which contains &gt; 10k images of 327 individuals. However I do pretty heavy preprocessing. First, I clip only pixels inside of outer contour of a face. Second, I transform each face (using piecewise affine wrapping) to the same grid (e.g. eyebrows, nose, lips etc. are in the same (x,y) position on all images). After preprocessing images look like this: &#xD;&#xA;&#xD;&#xA;![enter image description here][2] ![enter image description here][3]&#xD;&#xA;&#xD;&#xA;When training RBM, I take only non-zero pixels, so outer black region is ignored. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/lBxL5.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/FQMSp.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/Hjcaw.png" />
  <row Id="2561" PostHistoryTypeId="2" PostId="988" RevisionGUID="8310b575-14d8-4752-812a-11589f8439b7" CreationDate="2014-08-18T08:49:11.933" UserId="2985" Text="Here is a couple of tools that may be worth a look:&#xD;&#xA;&#xD;&#xA; - Bart, an open source tool that have been used for several languages, including German. [Available from the website][1]&#xD;&#xA; - Sucre is a tool developed at the University of Stuttgart. I don't know if it's available easily. [You can see this paper about it][2].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.bart-coref.org/&#xD;&#xA;  [2]: http://hnk.ffzg.hr/bibl/acl2011/CoNLL-ST/pdf/CoNLL-ST3.pdf" />
  <row Id="2562" PostHistoryTypeId="2" PostId="989" RevisionGUID="c7dd32e8-9dfc-41c0-8c01-3fc3ccd96871" CreationDate="2014-08-18T10:46:57.360" UserId="793" Text="I am trying to run SVR using scikit learn ( python ) on a training dataset having 595605 rows and 5 columns(features) and test dataset having 397070 rows. The data has been pre-processed and regularized.&#xD;&#xA;&#xD;&#xA;I am able to successfully run the test examples but on executing using my dataset and letting it run for over an hour, I could still not see any output or termination of program. I have tried executing using a different IDE and even from terminal but that doesn't seem to be the issue.&#xD;&#xA;I have also tried changing the 'C' parameter value from 1 to 1e3.&#xD;&#xA;&#xD;&#xA;I am facing similar issues with all svm implementations using scikit.&#xD;&#xA;&#xD;&#xA;Am I not waiting enough for it to complete ?&#xD;&#xA;How much time should this execution take ?&#xD;&#xA;&#xD;&#xA;From my experience it shouldn't require over a few minutes.&#xD;&#xA;&#xD;&#xA;Here is my system configuration:&#xD;&#xA;Ubuntu 14.04, 8GB RAM, lots of free memory, 4th gen i7 processor" />
  <row Id="2563" PostHistoryTypeId="1" PostId="989" RevisionGUID="c7dd32e8-9dfc-41c0-8c01-3fc3ccd96871" CreationDate="2014-08-18T10:46:57.360" UserId="793" Text="SVM using scikit learn runs endlessly and never completes execution" />
  <row Id="2564" PostHistoryTypeId="3" PostId="989" RevisionGUID="c7dd32e8-9dfc-41c0-8c01-3fc3ccd96871" CreationDate="2014-08-18T10:46:57.360" UserId="793" Text="&lt;python&gt;&lt;svm&gt;&lt;scikit&gt;" />
  <row Id="2565" PostHistoryTypeId="2" PostId="990" RevisionGUID="279ae776-6a8f-446d-969d-6115ecd5ec68" CreationDate="2014-08-18T11:33:32.617" UserId="2987" Text="This makes sense. IIUC, the speed of execution of support vector operations is bound by number of samples, not dimensionality. In other words, it is capped by CPU time and not RAM. I'm not sure exactly how much time this should take, but I'm running some benchmarks to find out." />
  <row Id="2566" PostHistoryTypeId="5" PostId="961" RevisionGUID="c0b07bd1-6715-4391-ad4d-1909ad6dadae" CreationDate="2014-08-18T11:59:06.607" UserId="1279" Comment="added 13 characters in body" Text="![enter image description here][1]&#xD;&#xA;&#xD;&#xA;These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same. &#xD;&#xA;&#xD;&#xA;I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc. &#xD;&#xA;&#xD;&#xA;My question is: what is the **most likely reason** for weights to get **very similar values**? Do they all just get to some local minimum? Or is it a sign of overfitting? &#xD;&#xA;&#xD;&#xA;I currently use a kind of Gaussian-Bernoulli RBM, code may be found [here](https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl). &#xD;&#xA;&#xD;&#xA;**UPD.** My dataset is based on [CK+](http://www.pitt.edu/~emotion/ck-spread.htm), which contains &gt; 10k images of 327 individuals. However I do pretty heavy preprocessing. First, I clip only pixels inside of outer contour of a face. Second, I transform each face (using piecewise affine wrapping) to the same grid (e.g. eyebrows, nose, lips etc. are in the same (x,y) position on all images). After preprocessing images look like this: &#xD;&#xA;&#xD;&#xA;![enter image description here][2] ![enter image description here][3]&#xD;&#xA;&#xD;&#xA;When training RBM, I take only non-zero pixels, so outer black region is ignored. &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/lBxL5.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/FQMSp.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/Hjcaw.png" />
  <row Id="2567" PostHistoryTypeId="2" PostId="991" RevisionGUID="67dd8bfc-d153-4485-a2ae-743470c719db" CreationDate="2014-08-18T13:35:28.000" UserId="1279" Text="Linear models simply add their features multiplied by corresponding weights. If, for example, you have 1000 sparse features only 3 or 4 of which are active in each instance (and the others are zeros) and 20 dense features that are all non-zeros, then it's pretty likely that dense features will make most of the impact while sparse features will add only a little value. You can check this by looking at feature weights for a few instances and how they influence resulting sum.&#xD;&#xA;&#xD;&#xA;One way to fix it is to go away from additive model. Here's a couple of candidate models. &#xD;&#xA;&#xD;&#xA;**SVM** is based on separating hyperplanes. Though hyperplane is linear model itself, SVM doesn't sum up its parameters, but instead tries to split feature space in an optimal way. Given the number of features, I'd say that linear SVM should work fine while more complicated kernels may tend to overfit the data. &#xD;&#xA;&#xD;&#xA;Despite its name, **Naive Bayes** is pretty powerful statistical model that showed good results for text classification. It's also flexible enough to capture imbalance in frequency of sparse and dense features, so you should definitely give it a try. &#xD;&#xA;&#xD;&#xA;Finally, **random forests** may work as a good ensemble method in this case. Randomization will ensure that different kinds of features (sparse/dense) will be used as primary decision nodes in different trees. RF/decision trees are also good for inspecting features themselves, so it's worth to note their structure anyway.&#xD;&#xA;&#xD;&#xA;Note that all of these methods have their drawbacks that may turn them into a garbage in your case. Combing sparse and dense features isn't really well-studied task, so let us know what of these approaches works best for your case. " />
  <row Id="2568" PostHistoryTypeId="2" PostId="992" RevisionGUID="b00586ea-8d49-42d0-9f14-a68bb7cb93e0" CreationDate="2014-08-18T14:56:13.800" UserId="1241" Text="I've been analyzing a data set of ~400k records and 9 variables. I've fitted a logistic regression, a regression tree, a random forest, and a gradient boosted tree. All of them give virtual identical goodness of fit numbers when I validate them on another data set.&#xD;&#xA;&#xD;&#xA;Why is this so? I'm guessing that it's because my observations to variable ratio is so high. If this is correct, at what observation to variable ratio will different models start to give different results? " />
  <row Id="2569" PostHistoryTypeId="1" PostId="992" RevisionGUID="b00586ea-8d49-42d0-9f14-a68bb7cb93e0" CreationDate="2014-08-18T14:56:13.800" UserId="1241" Text="Why might several types of models give almost identical results?" />
  <row Id="2570" PostHistoryTypeId="3" PostId="992" RevisionGUID="b00586ea-8d49-42d0-9f14-a68bb7cb93e0" CreationDate="2014-08-18T14:56:13.800" UserId="1241" Text="&lt;data-mining&gt;" />
  <row Id="2571" PostHistoryTypeId="2" PostId="993" RevisionGUID="21b988fa-aca6-46ef-8198-e8ed0a72d4c9" CreationDate="2014-08-18T16:28:02.147" UserId="2853" Text="&gt; I'm guessing that it's because my observations to variable ratio is so high.&#xD;&#xA;&#xD;&#xA;I think this explanation makes perfect sense.&#xD;&#xA;&#xD;&#xA;&gt; If this is correct, at what observation to variable ratio will different models start to give different results? &#xD;&#xA;&#xD;&#xA;This will probably depend very much on your specific data (for instance, even whether your nine variables are continuous, factors, ordinary or binary), as well as any tuning decisions you made while fitting your model.&#xD;&#xA;&#xD;&#xA;But you can play around with the observation-to-variable ratio - not by increasing the number of variables, but by decreasing the number of observations. Randomly draw 100 observations, fit models and see whether different models yield different results. (I guess they will.) Do this multiple times with different samples drawn from your total number of observations. Then look at subsamples of 1,000 observations... 10,000 observations... and so forth." />
  <row Id="2572" PostHistoryTypeId="2" PostId="994" RevisionGUID="241eb3d9-21a7-49ad-83e1-7f6bc6ac2228" CreationDate="2014-08-18T17:05:19.957" UserId="1256" Text="its worth also looking at the training errors. &#xD;&#xA;&#xD;&#xA;basically I disagree with your  analysis. if logistic regression etc are all giving the same results it would suggest that the 'best model' is a very simple one (that all models can fit equally well - eg basically linear).&#xD;&#xA; &#xD;&#xA;So then the question might be why is the best model a simple model?:&#xD;&#xA;It might suggest that your variables are not very predictive. Its of course hard to analyse without knowing the data. " />
  <row Id="2573" PostHistoryTypeId="2" PostId="995" RevisionGUID="82212744-0b00-4237-a706-c2d750f47b19" CreationDate="2014-08-18T17:25:27.007" UserId="964" Text="As @seanv507 suggested, the similar performance may simply be due to the data being best separated by a linear model. But in general, the statement that it is because the &quot;observations to variable ratio is so high&quot; is incorrect. Even as your ratio of sample size to number of variables goes to infinity, you should not expect different models to perform nearly identically, unless they all provide the same predictive bias." />
  <row Id="2574" PostHistoryTypeId="5" PostId="992" RevisionGUID="ee45f103-6ff1-4ab7-a6f5-47d0d36ac2ce" CreationDate="2014-08-18T17:31:27.327" UserId="1241" Comment="added 33 characters in body" Text="I've been analyzing a data set of ~400k records and 9 variables The dependent variable is binary. I've fitted a logistic regression, a regression tree, a random forest, and a gradient boosted tree. All of them give virtual identical goodness of fit numbers when I validate them on another data set.&#xD;&#xA;&#xD;&#xA;Why is this so? I'm guessing that it's because my observations to variable ratio is so high. If this is correct, at what observation to variable ratio will different models start to give different results? " />
  <row Id="2575" PostHistoryTypeId="2" PostId="996" RevisionGUID="f4df1883-d6d3-4a05-93ed-2abe3725002c" CreationDate="2014-08-19T00:56:40.890" UserId="2997" Text="Kernelized SVMs require the computation of a distance function between each point in the dataset, which is the dominating cost of O(n_features x n_observations^2). The storage of the distances is a burden on memory, so they're recomputed on the fly. Thankfully, only the points nearest the decision boundary are needed most of the time. Frequently computed distances are stored in a cache. If the cache is getting thrashed then the running time blows up to O(n_features x n_observations^3). (Seriously, no LaTeX?)&#xD;&#xA;&#xD;&#xA;You can increase this cache by invoking SVR as&#xD;&#xA;&#xD;&#xA;    model = SVR(cache_size=7000)&#xD;&#xA;&#xD;&#xA;In general, this is not going to work. But all is not lost. You can subsample the data and use the rest as a validation set, or you can pick a different model. Above the 200,000 observation range, it's wise to choose linear learners.&#xD;&#xA;&#xD;&#xA;Kernel SVM can be approximated, by approximating the kernel matrix and feeding it to a linear SVM. This allows you to trade off between accuracy and performance in linear time.&#xD;&#xA;&#xD;&#xA;A popular means of achieving this is to use 100 or so cluster centers found by kmeans/kmeans++ as the basis of your kernel function. The new derived features are then fed into a linear model. This works very well in practice. Tools like [sophia-ml][1] and [vowpal wabbit][2] are how Google, Yahoo and Microsoft do this. Input/output becomes the dominating cost for simple linear learners.&#xD;&#xA;&#xD;&#xA;In the abundance of data, nonparametric models perform roughly the same for most problems. The exceptions being structured inputs, like text, images, time series, audio.&#xD;&#xA;&#xD;&#xA;Further reading:&#xD;&#xA;&#xD;&#xA;[How to implement this.][3]&#xD;&#xA;&#xD;&#xA;[How to train an ngram neural network with dropout that scales linearly][4]&#xD;&#xA;&#xD;&#xA;[Kernel Approximations][5]&#xD;&#xA;&#xD;&#xA;[A formal paper on using kmeans to approximate kernel machines][6]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://code.google.com/p/sofia-ml/&#xD;&#xA;  [2]: https://github.com/JohnLangford/vowpal_wabbit/wiki&#xD;&#xA;  [3]: http://fastml.com/the-secret-of-the-big-guys/&#xD;&#xA;  [4]: http://fastml.com/go-non-linear-with-vowpal-wabbit/&#xD;&#xA;  [5]: http://peekaboo-vision.blogspot.co.uk/2012/12/kernel-approximations-for-efficient.html&#xD;&#xA;  [6]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.9009&amp;rep=rep1&amp;type=pdf" />
  <row Id="2576" PostHistoryTypeId="2" PostId="997" RevisionGUID="8be17476-d086-4339-8bf5-aed4dd7c98e9" CreationDate="2014-08-19T03:41:24.207" UserId="2972" Text="Where can I find free spatio-temporal dataset for download so that I can play with it in R ? &#xD;&#xA;&#xD;&#xA;Thanks" />
  <row Id="2577" PostHistoryTypeId="1" PostId="997" RevisionGUID="8be17476-d086-4339-8bf5-aed4dd7c98e9" CreationDate="2014-08-19T03:41:24.207" UserId="2972" Text="Where can I find free spatio-temporal dataset for download ?" />
  <row Id="2578" PostHistoryTypeId="3" PostId="997" RevisionGUID="8be17476-d086-4339-8bf5-aed4dd7c98e9" CreationDate="2014-08-19T03:41:24.207" UserId="2972" Text="&lt;dataset&gt;" />
  <row Id="2579" PostHistoryTypeId="2" PostId="998" RevisionGUID="c6c30270-1fc6-4803-b34a-b31aee75e6cb" CreationDate="2014-08-19T13:33:32.453" UserId="2969" Text="No, sklearn doesn't seem to have a forward selection algorithm. However, it does provide recursive feature elimination, which is a greedy feature elimination algorithm similar to sequential backward selection. See the documentation here:&#xD;&#xA;&#xD;&#xA;http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2580" PostHistoryTypeId="2" PostId="999" RevisionGUID="e168fe05-382a-4abf-b4e7-1c9cc749a097" CreationDate="2014-08-19T14:13:07.147" UserId="1237" Text="This results means that whatever method you use, you are able to get reasonably close to the optimal decision rule (aka [Bayes rule](http://en.wikipedia.org/wiki/Admissible_decision_rule#Bayes_rules_and_generalized_Bayes_rules)). The underlying reasons have been explained in Hastie, Tibshirani and Friedman's [&quot;Elements of Statistical Learning&quot;](http://statweb.stanford.edu/~tibs/ElemStatLearn/). They demonstrated how the different methods perform by comparing Figs. 2.1, 2.2, 2.3, 5.11 (in my first edition -- in section on multidimensional splines), 12.2, 12.3 (support vector machines), and probably some others. If you have not read that book, you need to drop everything **RIGHT NOW** and read it up. (I mean, it isn't worth losing your job, but it is worth missing a homework or two if you are a student.)&#xD;&#xA;&#xD;&#xA;I don't think that observations to variable ratio is the explanation. In light of my rationale offered above, it is the relatively simple form of the boundary separating your classes in the multidimensional space that all of the methods you tried have been able to identify." />
  <row Id="2581" PostHistoryTypeId="2" PostId="1000" RevisionGUID="74a14eb1-6b2d-4c2f-a14f-4998d6927ca8" CreationDate="2014-08-19T15:21:57.833" UserId="2969" Text="You can get some documented, publicly available EEG data from the HeadIT database at UCSD.&#xD;&#xA;http://headit-beta.ucsd.edu/studies&#xD;&#xA;&#xD;&#xA;The data itself appears to be in Biosemi Data Format (.bdf) files, described here: http://www.biosemi.com/faq/file_format.htm&#xD;&#xA;&#xD;&#xA;Biosemi provides links to several open-source methods to access and import .bdf files on their website, including several functions for importing into Matlab, as well as into Python (BioSig) and C/C++ libraries:&#xD;&#xA;http://www.biosemi.com/download.htm&#xD;&#xA;&#xD;&#xA;Just as a forewarning, EEG data can be a bit of a bear to work with, due to it's inherently low signal/noise ratio." />
  <row Id="2582" PostHistoryTypeId="2" PostId="1001" RevisionGUID="d458ff0d-ef85-45a0-a96d-1724495a0428" CreationDate="2014-08-19T15:47:29.413" UserId="525" Text="First thing that came to mind would be one's personal workout data from running or biking apps.&#xD;&#xA;&#xD;&#xA;Otherwise there is a dataset around NYC's taxi trip data. Quick Googling brought me this: http://www.andresmh.com/nyctaxitrips/. Variables include time and location for both pickups and dropoffs.&#xD;&#xA;&#xD;&#xA;Another dataset comes from Chicago's bikesharing service. It can be found here: https://www.divvybikes.com/datachallenge." />
  <row Id="2583" PostHistoryTypeId="2" PostId="1002" RevisionGUID="0debddc2-9400-4fa2-a9cd-2aaa88e8a011" CreationDate="2014-08-19T17:50:52.583" UserId="2861" Text="Caveat: I am a complete beginner when it comes to machine learning, but eager to learn.&#xD;&#xA;&#xD;&#xA;I have a large dataset and I'm trying to find pattern in it. There may / may not be correlation across the data, either with known variables, or variables that are contained in the data but which I haven't yet realised are actually variables / relevant.&#xD;&#xA;&#xD;&#xA;I'm guessing this would be a familiar problem in the world of data analysis, so I have a few questions:&#xD;&#xA;&#xD;&#xA;1. The 'silver bullet' would be to throw this all this data into a stats / data analysis program and for it to crunch the data looking for known / unknown patterns trying to find relations. Is SPSS suitable, or are there other applications which may be better suited.&#xD;&#xA;&#xD;&#xA;2. Should I learn a language like R, and figure out how to manually process the data. Wouldn't this comprimise finding relations as I would have to manually specify what and how to analyse the data?&#xD;&#xA;&#xD;&#xA;3. How would a professional data miner approach this problem and what steps would s/he take?" />
  <row Id="2584" PostHistoryTypeId="1" PostId="1002" RevisionGUID="0debddc2-9400-4fa2-a9cd-2aaa88e8a011" CreationDate="2014-08-19T17:50:52.583" UserId="2861" Text="Making sense of large data sets" />
  <row Id="2585" PostHistoryTypeId="3" PostId="1002" RevisionGUID="0debddc2-9400-4fa2-a9cd-2aaa88e8a011" CreationDate="2014-08-19T17:50:52.583" UserId="2861" Text="&lt;data-mining&gt;" />
  <row Id="2586" PostHistoryTypeId="2" PostId="1003" RevisionGUID="9bad51a6-9339-4ece-a86b-44783e8ef692" CreationDate="2014-08-19T18:59:03.013" UserId="1097" Text="I recently read [Similarity Measures for Short Segments of Text](http://research.microsoft.com/en-us/um/people/sdumais/ecir07-metzlerdumaismeek-final.pdf) (Metzler et al.).  It describes basic methods for measuring query similarity, and in the paper, the data consists of queries and their top results. Results are lists of page urls, page titles, and short page snippets.  In the paper, the authors collect 200 results per query.&#xD;&#xA;&#xD;&#xA;When using the public Google APIs to retrieve results, I was only able to collect 4-10 results per query.  There's a substantial difference between 10 and 200.  Hence, how much data is commonly used in practice to measure query similarity (e.g., how many results per query)?&#xD;&#xA;&#xD;&#xA;References are a plus!" />
  <row Id="2587" PostHistoryTypeId="1" PostId="1003" RevisionGUID="9bad51a6-9339-4ece-a86b-44783e8ef692" CreationDate="2014-08-19T18:59:03.013" UserId="1097" Text="Query similarity: how much data is used in practice?" />
  <row Id="2588" PostHistoryTypeId="3" PostId="1003" RevisionGUID="9bad51a6-9339-4ece-a86b-44783e8ef692" CreationDate="2014-08-19T18:59:03.013" UserId="1097" Text="&lt;machine-learning&gt;&lt;dataset&gt;&lt;search&gt;" />
  <row Id="2593" PostHistoryTypeId="2" PostId="1005" RevisionGUID="8d2f2492-595a-4f45-8bd5-210098995854" CreationDate="2014-08-20T03:06:01.753" UserId="2452" Text="Another idea is to combine OpenStreetMap project map data, for example, using corresponding nice R package (http://www.r-bloggers.com/the-openstreetmap-package-opens-up), with census data (population census data, such as the US data: http://www.census.gov/data/data-tools.html, as well as census data in other categories: http://national.census.okfn.org) to analyze temporal patterns of geosocial trends. HTH." />
  <row Id="2594" PostHistoryTypeId="2" PostId="1006" RevisionGUID="f24f0d86-665c-4ac8-a533-a4a37a9e714a" CreationDate="2014-08-20T05:43:08.610" UserId="2452" Text="I will try to answer your questions, but before I'd like to note that using term &quot;large dataset&quot; is misleading, as &quot;large&quot; is a *relative* concept. You have to provide more details. If you're dealing with **bid data**, then this fact will most likely affect selection of preferred *tools*, *approaches* and *algorithms* for your **data analysis**. I hope that the following thoughts of mine on data analysis address your sub-questions. Please note that the numbering of my points does not match the numbering of your sub-questions. However, I believe that it better reflects general **data analysis workflow**, at least, how I understand it.&#xD;&#xA;&#xD;&#xA;1) Firstly, I think that you need to have at least some kind of **conceptual model** in mind (or, better, on paper). This model should guide you in your *exploratory data analysis (EDA)*. A presence of a *dependent variable (DV)* in the model means that in your *machine learning (ML)* phase later in the analysis you will deal with so called supervised ML, as opposed to unsupervised ML in the absence of an identified DV.&#xD;&#xA;&#xD;&#xA;2) Secondly, **EDA** is a crucial part. IMHO, EDA should include **multiple iterations** of producing *descriptive statistics* and *data visualization*, as you refine your understanding about the data. Not only this phase will give you valuable insights about your datasets, but it will feed your next important phase - **data cleaning and transformation**. Just throwing your raw data into a statistical software package won't give much - for any **valid** statistical analysis, data should be *clean, correct and consistent*. This is often the most time- and effort-consuming, but absolutely necessary part. For more details on this topic, read these nice papers: http://vita.had.co.nz/papers/tidy-data.pdf (by Hadley Wickham) and http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf (by Edwin de Jonge and Mark van der Loo).&#xD;&#xA;&#xD;&#xA;3) Now, as you're hopefully done with *EDA* as well as data cleaning and transformation, your ready to start some more statistically-involved phases. One of such phases is *exploratory factor analysis (EFA)*, which will allow you to extract the underlying **structure** of your data. For datasets with large number of variables, the positive side effect of EFA is *dimensionality reduction*. And, while in that sense EFA is similar to *principal components analysis (PCA)* and other dimensionality reduction approaches, I think that EFA is more important as it allows to refine your conceptual model of the phenomena that your data &quot;describe&quot;, thus making sense of your datasets. Of course, in addition to EFA, you can/should perform **regression analysis** as well as apply **machine learning techniques**, based on your findings in previous phases.&#xD;&#xA;&#xD;&#xA;Finally, a note on **software tools**. In my opinion, current state of statistical software packages is at such point that practically any major software packages have comparable offerings feature-wise. If you study or work in an organization that have certain policies and preferences in term of software tools, then you are *constrained* by them. However, if that is not the case, I would heartily recommend **open source** statistical software, based on your comfort with its specific *programming language*, *learning curve* and your *career perspectives*. My current platform of choice is **R Project**, which offers mature, powerful, flexible, extensive and open statistical software, along with amazing ecosystem of packages, experts and enthusiasts. Other nice choices include *Python*, *Julia* and specific open source software for processing **big data**, such as *Hadoop*, *Spark*, *NoSQL* databases, *WEKA*. For more examples of open source software for **data mining**, which include general and specific statistical and ML software, see this section of a Wikipedia page: http://en.wikipedia.org/wiki/Data_mining#Free_open-source_data_mining_software_and_applications." />
  <row Id="2595" PostHistoryTypeId="5" PostId="1006" RevisionGUID="7c798888-6f30-4aa4-b617-0500a6515b9e" CreationDate="2014-08-20T06:28:06.767" UserId="2452" Comment="Added info on Rattle software." Text="I will try to answer your questions, but before I'd like to note that using term &quot;large dataset&quot; is misleading, as &quot;large&quot; is a *relative* concept. You have to provide more details. If you're dealing with **bid data**, then this fact will most likely affect selection of preferred *tools*, *approaches* and *algorithms* for your **data analysis**. I hope that the following thoughts of mine on data analysis address your sub-questions. Please note that the numbering of my points does not match the numbering of your sub-questions. However, I believe that it better reflects general **data analysis workflow**, at least, how I understand it.&#xD;&#xA;&#xD;&#xA;1) Firstly, I think that you need to have at least some kind of **conceptual model** in mind (or, better, on paper). This model should guide you in your *exploratory data analysis (EDA)*. A presence of a *dependent variable (DV)* in the model means that in your *machine learning (ML)* phase later in the analysis you will deal with so called supervised ML, as opposed to unsupervised ML in the absence of an identified DV.&#xD;&#xA;&#xD;&#xA;2) Secondly, **EDA** is a crucial part. IMHO, EDA should include **multiple iterations** of producing *descriptive statistics* and *data visualization*, as you refine your understanding about the data. Not only this phase will give you valuable insights about your datasets, but it will feed your next important phase - **data cleaning and transformation**. Just throwing your raw data into a statistical software package won't give much - for any **valid** statistical analysis, data should be *clean, correct and consistent*. This is often the most time- and effort-consuming, but absolutely necessary part. For more details on this topic, read these nice papers: http://vita.had.co.nz/papers/tidy-data.pdf (by Hadley Wickham) and http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf (by Edwin de Jonge and Mark van der Loo).&#xD;&#xA;&#xD;&#xA;3) Now, as you're hopefully done with *EDA* as well as data cleaning and transformation, your ready to start some more statistically-involved phases. One of such phases is *exploratory factor analysis (EFA)*, which will allow you to extract the underlying **structure** of your data. For datasets with large number of variables, the positive side effect of EFA is *dimensionality reduction*. And, while in that sense EFA is similar to *principal components analysis (PCA)* and other dimensionality reduction approaches, I think that EFA is more important as it allows to refine your conceptual model of the phenomena that your data &quot;describe&quot;, thus making sense of your datasets. Of course, in addition to EFA, you can/should perform **regression analysis** as well as apply **machine learning techniques**, based on your findings in previous phases.&#xD;&#xA;&#xD;&#xA;Finally, a note on **software tools**. In my opinion, current state of statistical software packages is at such point that practically any major software packages have comparable offerings feature-wise. If you study or work in an organization that have certain policies and preferences in term of software tools, then you are *constrained* by them. However, if that is not the case, I would heartily recommend **open source** statistical software, based on your comfort with its specific *programming language*, *learning curve* and your *career perspectives*. My current platform of choice is **R Project**, which offers mature, powerful, flexible, extensive and open statistical software, along with amazing ecosystem of packages, experts and enthusiasts. Other nice choices include *Python*, *Julia* and specific open source software for processing **big data**, such as *Hadoop*, *Spark*, *NoSQL* databases, *WEKA*. For more examples of open source software for **data mining**, which include general and specific statistical and ML software, see this section of a Wikipedia page: http://en.wikipedia.org/wiki/Data_mining#Free_open-source_data_mining_software_and_applications.&#xD;&#xA;&#xD;&#xA;UPDATE: Forgot to mention *Rattle* (http://rattle.togaware.com), which is also a very popular open source R-oriented GUI software for data mining." />
  <row Id="2596" PostHistoryTypeId="6" PostId="1002" RevisionGUID="c4111f36-1fd8-4aeb-bfb8-5c2b98f62cb4" CreationDate="2014-08-20T09:50:18.473" UserId="97" Comment="Added relevant tags." Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;tools&gt;&lt;beginner&gt;" />
  <row Id="2597" PostHistoryTypeId="24" PostId="1002" RevisionGUID="c4111f36-1fd8-4aeb-bfb8-5c2b98f62cb4" CreationDate="2014-08-20T09:50:18.473" Comment="Proposed by 97 approved by 2452, 21 edit id of 138" />
  <row Id="2598" PostHistoryTypeId="6" PostId="992" RevisionGUID="8bf693ce-e8ac-4969-95dc-d001ce903041" CreationDate="2014-08-20T11:06:54.447" UserId="97" Comment="More relevant tags." Text="&lt;data-mining&gt;&lt;classification&gt;&lt;binary&gt;" />
  <row Id="2599" PostHistoryTypeId="24" PostId="992" RevisionGUID="8bf693ce-e8ac-4969-95dc-d001ce903041" CreationDate="2014-08-20T11:06:54.447" Comment="Proposed by 97 approved by 1241 edit id of 140" />
  <row Id="2600" PostHistoryTypeId="6" PostId="1003" RevisionGUID="c8aaba30-05f0-420b-9bd8-dc228ee5428d" CreationDate="2014-08-20T12:14:01.593" UserId="97" Comment="Added more relevant tags." Text="&lt;machine-learning&gt;&lt;dataset&gt;&lt;text-mining&gt;&lt;search&gt;&lt;google-api&gt;" />
  <row Id="2601" PostHistoryTypeId="24" PostId="1003" RevisionGUID="c8aaba30-05f0-420b-9bd8-dc228ee5428d" CreationDate="2014-08-20T12:14:01.593" Comment="Proposed by 97 approved by 2452, 1097 edit id of 139" />
  <row Id="2602" PostHistoryTypeId="2" PostId="1007" RevisionGUID="17f2b70e-c8c1-4629-bec3-3be4f9b44879" CreationDate="2014-08-20T14:12:03.870" UserId="867" Text="I want to scrap some data from a website. &#xD;&#xA;I have used import.io but still not much satisfied.. can any of you suggest about it.. whats the best tool to get the unstructured data from web" />
  <row Id="2603" PostHistoryTypeId="1" PostId="1007" RevisionGUID="17f2b70e-c8c1-4629-bec3-3be4f9b44879" CreationDate="2014-08-20T14:12:03.870" UserId="867" Text="Looking for Web scrapping tool for unstructured data" />
  <row Id="2604" PostHistoryTypeId="3" PostId="1007" RevisionGUID="17f2b70e-c8c1-4629-bec3-3be4f9b44879" CreationDate="2014-08-20T14:12:03.870" UserId="867" Text="&lt;tools&gt;&lt;crawling&gt;" />
  <row Id="2605" PostHistoryTypeId="2" PostId="1008" RevisionGUID="6adcf555-c3d1-4515-89be-c3a929fcfc7f" CreationDate="2014-08-20T15:34:00.830" UserId="403" Text="Try BeautifulSoup - http://www.crummy.com/software/BeautifulSoup/&#xD;&#xA;&#xD;&#xA;From the website &quot;Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping.&quot;&#xD;&#xA;I have no personally used it, but it often comes up in regards to a nice library for scraping. Here's a blog post on using it to scrape Craigslist http://www.gregreda.com/2014/07/27/scraping-craigslist-for-tickets/" />
  <row Id="2606" PostHistoryTypeId="2" PostId="1009" RevisionGUID="69567b12-e9c7-4e97-8159-b9f4f94a45da" CreationDate="2014-08-20T15:53:29.403" UserId="471" Text="If you have R and the `spacetime` package then you are only `data(package=&quot;spacetime&quot;)` away from a list of space-time data sets bundled with the package:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    Data sets in package ‘spacetime’:&#xD;&#xA;    &#xD;&#xA;    DE_NUTS1 (air)          Air quality data, rural background PM10 in&#xD;&#xA;                            Germany, daily averages 1998-2009&#xD;&#xA;    fires                   Northern Los Angeles County Fires&#xD;&#xA;    rural (air)             Air quality data, rural background PM10 in&#xD;&#xA;                            Germany, daily averages 1998-2009&#xD;&#xA;then for example:&#xD;&#xA;&#xD;&#xA;    &gt; data(fires)&#xD;&#xA;    &gt; str(fires)&#xD;&#xA;    'data.frame':	313 obs. of  3 variables:&#xD;&#xA;     $ Time: int  5863 5870 6017 6018 6034 6060 6176 6364 6366 6372 ...&#xD;&#xA;     $ X   : num  63.9 64.3 64.1 64 64.4 ...&#xD;&#xA;     $ Y   : num  19.4 20.1 19.7 19.8 20.3 ...&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2607" PostHistoryTypeId="2" PostId="1010" RevisionGUID="b10522a6-83c5-4c81-8c92-76f83b05a779" CreationDate="2014-08-20T19:08:39.807" UserId="2861" Text="You don't mention what language you're programming in (please consider adding it as a tag), so general help would be to seek out a HTML parser and use that to pull the data. Some web sites can have simply awful HTML code and can be very difficult to scrape, and just when you think you have it...&#xD;&#xA;&#xD;&#xA;A HTML parser will parse all the html and allow you to access it in a structured sort of way, whether that's from an array, an object etc." />
  <row Id="2614" PostHistoryTypeId="2" PostId="1013" RevisionGUID="e72474ef-cfb1-418d-96d9-6dca6d4cea9b" CreationDate="2014-08-20T21:12:49.927" UserId="375" Text="I've just started reading about AB testing, as it pertains to optimizing website design.  I find it interesting that most of the methods assume that changes to the layout and appearance are independent of each other.  I understand that the most common method of optimization is the ['multi-armed bandit'][1] procedure.  While I grasp the concept of it, it seems to ignore the fact that changes (changes to the website in this case) are not independent to each other.&#xD;&#xA;&#xD;&#xA;For example, if company is testing the placement and color of the logo on the website, they find the optimal color first then the optimal placement.  Not that I'm some expert on human psychology, but shouldn't these be related? Can the multi-armed bandit method be efficiently used in this case or more complicated cases?&#xD;&#xA;&#xD;&#xA;My first instinct is to say no.  On that note, why haven't people used heuristic algorithms to optimize over complicated AB testing sample spaces?  For an example, I thought someone might have used a genetic algorithm to optimize a website layout, but I can find no examples of something like this out there. This leads me to believe that I'm missing something important in my understanding of AB testing as it applies to website optimization.&#xD;&#xA;&#xD;&#xA;Why isn't heuristic optimization used on more complicated websites?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Multi-armed_bandit" />
  <row Id="2615" PostHistoryTypeId="1" PostId="1013" RevisionGUID="e72474ef-cfb1-418d-96d9-6dca6d4cea9b" CreationDate="2014-08-20T21:12:49.927" UserId="375" Text="Using Heuristic Methods for AB Testing" />
  <row Id="2616" PostHistoryTypeId="3" PostId="1013" RevisionGUID="e72474ef-cfb1-418d-96d9-6dca6d4cea9b" CreationDate="2014-08-20T21:12:49.927" UserId="375" Text="&lt;optimization&gt;&lt;consumerweb&gt;&lt;ab-testing&gt;" />
  <row Id="2618" PostHistoryTypeId="2" PostId="1015" RevisionGUID="0a0454df-f6df-423a-be9c-133105f16d7d" CreationDate="2014-08-21T06:31:50.197" UserId="895" Text="I have installed [Drake][1] on Windows 7 64-bit.&#xD;&#xA;&#xD;&#xA;I am using JDK 1.7.0_51.&#xD;&#xA;&#xD;&#xA;I tried both using the pre-compiled jar file and&#xD;&#xA;compiling from the Clojure source using [leiningen][2].&#xD;&#xA;The resulting Drake version is 0.1.6, the current development version.&#xD;&#xA;&#xD;&#xA;When running Drake, I get the current version number.&#xD;&#xA;&#xD;&#xA;Next, I tried to go through [the tutorial][3]. The command:&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    java -jar drake.jar  -w .\workflow.d&#xD;&#xA;&#xD;&#xA;results in the following Exception:&#xD;&#xA;&#xD;&#xA;    java.lang.Exception: no input data found in locations: D:\tools\drake\in.c&#xD;&#xA;    sv&#xD;&#xA;&#xD;&#xA;Even though the file exists and has text inside it. &#xD;&#xA;The same scenario works in a similar installation on Ubuntu 12.04.&#xD;&#xA;Am I doing something wrong, or is this a Windows-specific bug?&#xD;&#xA;&#xD;&#xA;  [1]: https://github.com/Factual/drake&#xD;&#xA;  [2]: https://github.com/technomancy/leiningen&#xD;&#xA;  [3]: https://github.com/Factual/drake/wiki/Tutorial" />
  <row Id="2619" PostHistoryTypeId="1" PostId="1015" RevisionGUID="0a0454df-f6df-423a-be9c-133105f16d7d" CreationDate="2014-08-21T06:31:50.197" UserId="895" Text="Making Factual drake work on Windows 7 64-bit" />
  <row Id="2620" PostHistoryTypeId="3" PostId="1015" RevisionGUID="0a0454df-f6df-423a-be9c-133105f16d7d" CreationDate="2014-08-21T06:31:50.197" UserId="895" Text="&lt;tools&gt;" />
  <row Id="2624" PostHistoryTypeId="2" PostId="1017" RevisionGUID="c3d129a6-ba96-45cb-b8e2-df55ec1a1631" CreationDate="2014-08-21T10:13:54.130" UserId="3030" Text="I'm studying RL in order to implement a kind of time series pattern analyzer such as market.&#xD;&#xA;&#xD;&#xA;The most examples I have seen are based on the maze environment.&#xD;&#xA;&#xD;&#xA;But in real market environment, the signal changes endlessly as time passes and I can not guess how can I model environment and states.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Another question is about buy sell modeling.&#xD;&#xA;&#xD;&#xA;Let's assume that the agent randomly buy at time t and sell at time t + alpha.&#xD;&#xA;&#xD;&#xA;It's so simple to calculate reward.&#xD;&#xA;The problem is how can I model Q matrix and how can I model signals between buy ans sell actions.&#xD;&#xA;&#xD;&#xA;Can you share some source code or guidance for similar situation?&#xD;&#xA;&#xD;&#xA;Thanks in advance,&#xD;&#xA;" />
  <row Id="2625" PostHistoryTypeId="1" PostId="1017" RevisionGUID="c3d129a6-ba96-45cb-b8e2-df55ec1a1631" CreationDate="2014-08-21T10:13:54.130" UserId="3030" Text="How can I model open environment in reinforcement learning?" />
  <row Id="2626" PostHistoryTypeId="3" PostId="1017" RevisionGUID="c3d129a6-ba96-45cb-b8e2-df55ec1a1631" CreationDate="2014-08-21T10:13:54.130" UserId="3030" Text="&lt;machine-learning&gt;" />
  <row Id="2627" PostHistoryTypeId="4" PostId="997" RevisionGUID="54809862-f276-46ed-964e-fc708ea34cfe" CreationDate="2014-08-21T11:52:31.890" UserId="97" Comment="More relevant tags." Text="Where can I find free spatio-temporal dataset for download?" />
  <row Id="2628" PostHistoryTypeId="6" PostId="997" RevisionGUID="54809862-f276-46ed-964e-fc708ea34cfe" CreationDate="2014-08-21T11:52:31.890" UserId="97" Comment="More relevant tags." Text="&lt;dataset&gt;&lt;open-source&gt;&lt;freebase&gt;" />
  <row Id="2629" PostHistoryTypeId="24" PostId="997" RevisionGUID="54809862-f276-46ed-964e-fc708ea34cfe" CreationDate="2014-08-21T11:52:31.890" Comment="Proposed by 97 approved by 21 edit id of 141" />
  <row Id="2630" PostHistoryTypeId="5" PostId="1007" RevisionGUID="6b3a8a15-e730-4999-a589-6cc2ac949426" CreationDate="2014-08-21T11:52:35.660" UserId="471" Comment="scraping not scrapping." Text="I want to scrape some data from a website. &#xD;&#xA;I have used import.io but still not much satisfied.. can any of you suggest about it.. whats the best tool to get the unstructured data from web" />
  <row Id="2631" PostHistoryTypeId="4" PostId="1007" RevisionGUID="6b3a8a15-e730-4999-a589-6cc2ac949426" CreationDate="2014-08-21T11:52:35.660" UserId="471" Comment="scraping not scrapping." Text="Looking for Web scraping tool for unstructured data" />
  <row Id="2632" PostHistoryTypeId="24" PostId="1007" RevisionGUID="6b3a8a15-e730-4999-a589-6cc2ac949426" CreationDate="2014-08-21T11:52:35.660" Comment="Proposed by 471 approved by 2452, 21 edit id of 142" />
  <row Id="2634" PostHistoryTypeId="2" PostId="1019" RevisionGUID="9203de8c-e10b-40b5-bda7-5ec6f9fff9b2" CreationDate="2014-08-22T09:03:13.333" UserId="979" Text="I think it always depends on the scenario. Using a representative data set is not always the solution. Assume that your training set with 1000 negative examples and 20 positive examples. Without any modeification of the classifier, your algorithm will tend to classify all new examples as negative. In some scenarios this is O.K. But in many cases the costs of missing postive examples is high so you have to find a solution for it.&#xD;&#xA;&#xD;&#xA;In such cases you can use a cost sensitive machine learning algorithm. For example in the case of medical diagnosis data analysis.&#xD;&#xA;&#xD;&#xA;In summary: Classification erros do not have the same cost! " />
  <row Id="2644" PostHistoryTypeId="2" PostId="1020" RevisionGUID="56642b23-61e8-44b0-b4db-6c88c45e0e33" CreationDate="2014-08-22T14:01:23.640" UserId="3044" Text="I am hoping to model the characteristics of the users of a specific page on Facebook, which has roughly 2 million likes. I have been looking at the Facebook SDK/API, but I can't really see if what I would like to do is possible. It seems that the users share quite different amounts of data so I probably discard a lot of users and only use the ones with a quite open public profile. I would like to have the following data:&#xD;&#xA;&#xD;&#xA;1) See the individuals that have 'liked' the page.&#xD;&#xA;&#xD;&#xA;2) See the list of friends for each person that have 'liked' the page.&#xD;&#xA;&#xD;&#xA;3) See gender for each person (optional)&#xD;&#xA;&#xD;&#xA;4) See other pages that each person has liked (optional)&#xD;&#xA;&#xD;&#xA;Could anyone tell me if it is possible to get this data? As mentioned earlier it is okay if I discard data for users that don't like to share this data." />
  <row Id="2645" PostHistoryTypeId="1" PostId="1020" RevisionGUID="56642b23-61e8-44b0-b4db-6c88c45e0e33" CreationDate="2014-08-22T14:01:23.640" UserId="3044" Text="Available data about 'likers' as a page on Facebook" />
  <row Id="2646" PostHistoryTypeId="3" PostId="1020" RevisionGUID="56642b23-61e8-44b0-b4db-6c88c45e0e33" CreationDate="2014-08-22T14:01:23.640" UserId="3044" Text="&lt;social-network-analysis&gt;" />
  <row Id="2647" PostHistoryTypeId="2" PostId="1021" RevisionGUID="0a36001e-e244-4286-9611-e2f7be21d52d" CreationDate="2014-08-22T15:59:07.097" UserId="3047" Text="I have thousands of lists of strings, and each list has about 10 strings. Most strings in a given list are very similar, though some strings are (rarely) completely unrelated to the others and some strings contain irrelevant words. I am looking for an algorithm or a library that will convert each list into a &quot;most probable&quot;, &quot;most consensual&quot; string.&#xD;&#xA;&#xD;&#xA;Here is one such list.&#xD;&#xA;&#xD;&#xA;&lt;ul&gt;&#xD;&#xA;&lt;li&gt;Star Wars: Episode IV A New Hope | StarWars.com&lt;/li&gt;&#xD;&#xA;&lt;li&gt;Star Wars Episode IV - A New Hope (1977)&lt;/li&gt;&#xD;&#xA;&lt;li&gt;Star Wars: Episode IV - A New Hope - Rotten Tomatoes&lt;/li&gt;&#xD;&#xA;&lt;li&gt;Watch Star Wars: Episode IV - A New Hope Online Free&lt;/li&gt;&#xD;&#xA;&lt;li&gt;Star Wars (1977) - Greatest Films&lt;/li&gt;&#xD;&#xA;&lt;li&gt;[REC] 4 poster promises death by outboard motor - SciFiNow&lt;/li&gt;&#xD;&#xA;&lt;/ul&gt;&#xD;&#xA;&#xD;&#xA;For this list, any string matching the regular expression `^Star Wars:? Episode IV (- )?A New Hope$` would be acceptable.&#xD;&#xA;&#xD;&#xA;I have looked at Andrew Ng's course on Machine Learning on Coursera, but I was not able to find a similar problem." />
  <row Id="2648" PostHistoryTypeId="1" PostId="1021" RevisionGUID="0a36001e-e244-4286-9611-e2f7be21d52d" CreationDate="2014-08-22T15:59:07.097" UserId="3047" Text="Most representative string among list" />
  <row Id="2649" PostHistoryTypeId="3" PostId="1021" RevisionGUID="0a36001e-e244-4286-9611-e2f7be21d52d" CreationDate="2014-08-22T15:59:07.097" UserId="3047" Text="&lt;nlp&gt;&lt;similarity&gt;&lt;information-retrieval&gt;" />
  <row Id="2650" PostHistoryTypeId="2" PostId="1022" RevisionGUID="2522fe43-42d4-49fe-89fc-99736814abee" CreationDate="2014-08-22T18:27:04.377" UserId="941" Text="Want to wish you good luck. Some time ago faced with the same problem, but didn't find any satisfying solution. &#xA;First of all, there is no way to get list of users, who &quot;liked&quot; a particular page. Even, if you are an administrator of this page (I was). One only can get list of last 3 or 5 hundred users. &#xA;&#xA;Friendships data for most of the users is also inaccessible. Looks like gender is the only thing from your list, that you can get. &#xA;&#xA;Data about pages, that exact user &quot;likes&quot;, should be available (as it's written in docs), but in reality, through API you can collect something only for friends and FoF. Even though this data is available through web interface. So the only way is to try dirty trick with parsing and scraping (but remember, that I didn't advise it ;) ). " />
  <row Id="2651" PostHistoryTypeId="2" PostId="1023" RevisionGUID="e72189e8-ee4f-4c69-914d-229fcf6bfc38" CreationDate="2014-08-23T09:19:08.577" UserId="979" Text="As a naive solution I would suggest to first select the strings which contain the most frequent tokens inside the list. In this way you can get rid of irrelevant string.&#xD;&#xA;&#xD;&#xA;In the second phrase I would do a majority voting. Assuming the 3 sentences:&#xD;&#xA;&#xD;&#xA; - Star Wars: Episode IV A New Hope | StarWars.com&#xD;&#xA; - Star Wars Episode IV - A New Hope (1977)&#xD;&#xA; - Star Wars: Episode IV - A New Hope - Rotten Tomatoes&#xD;&#xA;&#xD;&#xA;I would go through the tokens one by one. We start by &quot;Star&quot;. It wins as all the string start with it. &quot;Wars&quot; will also win. The next one is &quot;:&quot;. It will also win.&#xD;&#xA;&#xD;&#xA;All the tokens will ein in majority voting till &quot;Hope&quot;. The next token after &quot;Hope&quot; will be either &quot;|&quot;, or &quot;(&quot; or &quot;-&quot;. None of the will win in majority voting so I will stop here!&#xD;&#xA;&#xD;&#xA;Another solution would be probably to use [Longest common subsequence][1].&#xD;&#xA;&#xD;&#xA;As I said I have not though about it much. So there might be much more better solutions to your problem :-)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Longest_common_subsequence_problem" />
  <row Id="2652" PostHistoryTypeId="6" PostId="1020" RevisionGUID="44d49a63-828d-4d5a-84d3-c048c65725cc" CreationDate="2014-08-23T09:22:19.073" UserId="21" Comment="edited tags" Text="&lt;social-network-analysis&gt;&lt;facebook&gt;" />
  <row Id="2653" PostHistoryTypeId="2" PostId="1024" RevisionGUID="7c0f8fbe-2297-4fae-bcfb-39d0b3e452e3" CreationDate="2014-08-23T13:47:01.907" UserId="1241" Text="I've fit a GLM (Poisson) to a data set where one of the variables is categorical for the year a customer bought a product from my company, ranging from 1999 to 2012. There's a linear trend of the coefficients for the values of the variable as the year of sale increases.&#xD;&#xA;&#xD;&#xA;Is there any problem with trying to improve predictions for 2013 and maybe 2014 by extrapolating to get the coefficients for those years?" />
  <row Id="2654" PostHistoryTypeId="1" PostId="1024" RevisionGUID="7c0f8fbe-2297-4fae-bcfb-39d0b3e452e3" CreationDate="2014-08-23T13:47:01.907" UserId="1241" Text="Extrapolating GLM coefficients for year a product was sold for future years?" />
  <row Id="2655" PostHistoryTypeId="3" PostId="1024" RevisionGUID="7c0f8fbe-2297-4fae-bcfb-39d0b3e452e3" CreationDate="2014-08-23T13:47:01.907" UserId="1241" Text="&lt;data-mining&gt;&lt;statistics&gt;" />
  <row Id="2656" PostHistoryTypeId="4" PostId="1024" RevisionGUID="ebafe88f-eca5-44c6-841f-f715739f6b50" CreationDate="2014-08-23T13:55:05.587" UserId="1241" Comment="edited title" Text="Extrapolating GLM coefficients for year a product was sold into future years?" />
  <row Id="2657" PostHistoryTypeId="2" PostId="1025" RevisionGUID="93bd42d1-7f77-451d-b83b-9450aedc5ad9" CreationDate="2014-08-23T13:56:43.813" UserId="3052" Text="I have been developing a chess program which makes use of alpha-beta pruning algorithm and an evaluation function that evaluates positions using the following features namely material, kingsafety, mobility, pawn-structure and trapped pieces etc..... My evaluation function is derived from the f(p) = w1 * material + w2 * kingsafety + w3 * mobility + w4 * pawn-structure + w5 * trapped pieces , where &quot;w&quot; is the weight assigned to each feature. At this point i want to tune the weights of my evaluation function using temporal difference, where the agent plays against itself and in the process gather training data from its environment (which is a form of reinforcement learning). i have read some books and articles in order to have an insight on how to implement this in java but they seems to be theoretical rather than practical. please i need a detailed explanation and pseudo codes on how to automatically tune the weights of my evaluation function based on previous games. thanks in advances." />
  <row Id="2658" PostHistoryTypeId="1" PostId="1025" RevisionGUID="93bd42d1-7f77-451d-b83b-9450aedc5ad9" CreationDate="2014-08-23T13:56:43.813" UserId="3052" Text="implementing temporal difference in chess" />
  <row Id="2659" PostHistoryTypeId="3" PostId="1025" RevisionGUID="93bd42d1-7f77-451d-b83b-9450aedc5ad9" CreationDate="2014-08-23T13:56:43.813" UserId="3052" Text="&lt;machine-learning&gt;&lt;algorithms&gt;" />
  <row Id="2660" PostHistoryTypeId="2" PostId="1026" RevisionGUID="ef60fcbf-9006-49d8-ba91-ebe8b2b66338" CreationDate="2014-08-23T14:15:58.660" UserId="1256" Text="I think there are two separate issues to consider: Training time, and prediction accuracy. &#xD;&#xA;&#xD;&#xA;Take a simple example : consider you have two classes, that have a multivariate normal distribution. Basically you need to estimate the respective class means and class covariances.  Now the first thing you care about is your estimate of the difference in the class means: but your performance is limited by the accuracy of the worst estimated mean: its no good estimating one mean to the 100th decimal place - if the other mean is only estimated to 1 decimal place.  So its a waste of computing resources to use all the data - you can instead undersample the  more common class AND reweight the classes appropriately. ( those computing resources can then be used exploring different input variables etc)&#xD;&#xA;&#xD;&#xA;Now the second issue is predictive accuracy: different algorithms use different error metrics, which may or may not agree with your own objectives. eg logistic regression will penalise overall probability error,  so if  most of your data is from one class, then it will tend to try to improve accurate probability estimates ( eg 90 vs 95% probability) of that one class rather than trying to identify the rare class. in that case you would definitely want to try to reweight to emphasize the rare class ( and subsequently adjust the estimate [by adjusting the bias term] to get the probability estimates realigned)&#xD;&#xA;" />
  <row Id="2661" PostHistoryTypeId="2" PostId="1027" RevisionGUID="aad5350d-8e69-41a8-96df-7391f77cbfc5" CreationDate="2014-08-23T15:25:44.903" UserId="3053" Text="A first remark, you should watch 'Wargames' to know what you're getting yourself into.&#xD;&#xA;&#xD;&#xA;What you want is f(p) such that f(p) is as close as possible to strength of position.&#xD;&#xA;&#xD;&#xA;A very simple solution using genetic algo would be to setup 10000 players with different weights and see which wins. Then keep the top 1000 winners' weight, copy them 10 times, alter them slightly to explore weight space, and run the simulation again. That's standard GA, given a functional form, what are the best coefficients for it.&#xD;&#xA;&#xD;&#xA;Another solution is to extract the positions, so you have a table '(material, kingsafety, mobility, pawn-structure, trappedpieces) -&gt; goodness of position' where goodness of position is some objective factor (outcome win/lose computed using simulations above or known matches, depth of available tree, number of moves under the tree where one of the 5 factors gets better. You can then try different functional forms for your f(p), regression, svm." />
  <row Id="2662" PostHistoryTypeId="2" PostId="1028" RevisionGUID="4b5a061b-2637-4a98-8069-0f7b8e8166ab" CreationDate="2014-08-23T16:54:06.380" UserId="3054" Text="I have been reading around about Random Forests but I cannot really find a definitive answer about the problem of overfitting. According to the original paper of Breiman, they should not overfit when increasing the number of trees in the forest, but it seems that there is not consensus about this. This is creating me quite some confusion about the issue.&#xD;&#xA;&#xD;&#xA;Maybe someone more expert than me can give me a more concrete answer or point me in the right direction to better understand the problem." />
  <row Id="2663" PostHistoryTypeId="1" PostId="1028" RevisionGUID="4b5a061b-2637-4a98-8069-0f7b8e8166ab" CreationDate="2014-08-23T16:54:06.380" UserId="3054" Text="Do Random Forest overfit?" />
  <row Id="2664" PostHistoryTypeId="3" PostId="1028" RevisionGUID="4b5a061b-2637-4a98-8069-0f7b8e8166ab" CreationDate="2014-08-23T16:54:06.380" UserId="3054" Text="&lt;machine-learning&gt;&lt;random-forest&gt;" />
  <row Id="2665" PostHistoryTypeId="2" PostId="1029" RevisionGUID="2303e9a9-4fee-4acf-ab87-44508d1a20fe" CreationDate="2014-08-23T19:34:09.417" UserId="3057" Text="Which one will be the dominating programming language for next 5 years for analytics , machine learning . R verses python verses SAS. Advantage and disadvantage." />
  <row Id="2666" PostHistoryTypeId="1" PostId="1029" RevisionGUID="2303e9a9-4fee-4acf-ab87-44508d1a20fe" CreationDate="2014-08-23T19:34:09.417" UserId="3057" Text="Which one will be the dominating programming language for next 5 years for analytics , machine learning . R or python or SAS" />
  <row Id="2667" PostHistoryTypeId="3" PostId="1029" RevisionGUID="2303e9a9-4fee-4acf-ab87-44508d1a20fe" CreationDate="2014-08-23T19:34:09.417" UserId="3057" Text="&lt;machine-learning&gt;&lt;r&gt;&lt;python&gt;" />
  <row Id="2668" PostHistoryTypeId="2" PostId="1030" RevisionGUID="6d85fbc5-f967-4df2-a848-ab1c3287afca" CreationDate="2014-08-23T20:05:38.700" UserId="2452" Text="I believe that this is a case for applying *time series analysis*, in particular *time series forecasting* (http://en.wikipedia.org/wiki/Time_series)." />
  <row Id="2669" PostHistoryTypeId="5" PostId="1030" RevisionGUID="c3feccfa-de96-4fd1-b1de-fc82d3cddae8" CreationDate="2014-08-23T20:32:06.300" UserId="2452" Comment="Added resources on the topic." Text="I believe that this is a case for applying *time series analysis*, in particular *time series forecasting* (http://en.wikipedia.org/wiki/Time_series). Consider the following resources:&#xD;&#xA;&#xD;&#xA; - http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471363553.html&#xD;&#xA; - http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf (especially section&#xD;&#xA;   4.6)&#xD;&#xA; - http://arxiv.org/abs/0802.0219 (Bayesian approach)" />
  <row Id="2670" PostHistoryTypeId="5" PostId="1030" RevisionGUID="b73da1df-c791-4de1-822d-5b169566c0ba" CreationDate="2014-08-23T20:37:43.057" UserId="2452" Comment="Added resources on the topic." Text="I believe that this is a case for applying *time series analysis*, in particular *time series forecasting* (http://en.wikipedia.org/wiki/Time_series). Consider the following resources on **time series regression**:&#xD;&#xA;&#xD;&#xA; - http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471363553.html&#xD;&#xA; - http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf (especially section&#xD;&#xA;   4.6)&#xD;&#xA; - http://arxiv.org/abs/0802.0219 (Bayesian approach)" />
  <row Id="2671" PostHistoryTypeId="2" PostId="1031" RevisionGUID="153192f3-74b1-4e48-bf85-b96a2b1f1650" CreationDate="2014-08-24T00:46:03.813" UserId="3051" Text="There is a great [survey](http://blog.revolutionanalytics.com/2014/01/in-data-scientist-survey-r-is-the-most-used-tool-other-than-databases.html) published by O'Reilly collected at Strata.&#xD;&#xA;&#xD;&#xA;You can see that SAS is not widely popular, and there is no reason why that should change at this point. One can rule that out.&#xD;&#xA;&#xD;&#xA;R is barely ahead of Python, 43% vs 41%. You can find many blogs expressing the rise of Python in data science. I would go with Python in the near future.&#xD;&#xA;&#xD;&#xA;But 5 years is a very long time. I think Golang will steal a lot of developers from Python in general. This might spill over to data science usage as well. Code can be written to execute in parallel very easily, which makes it a perfect vehicle for Big Data processing. [Julia's](http://julialang.org/) benchmarks for technical computing are even more impressive, and you can have iPython like stuff with iJulia. Hence Python is likely to lose some steam to both. But there are ways to call Julia functions from R and Python, so you can experiment using best sides of each." />
  <row Id="2672" PostHistoryTypeId="6" PostId="1025" RevisionGUID="e1dea512-eb36-46a7-8ac5-c7f4a57c1e1c" CreationDate="2014-08-24T03:31:06.623" UserId="97" Comment="Additional tags." Text="&lt;machine-learning&gt;&lt;algorithms&gt;&lt;chess&gt;" />
  <row Id="2673" PostHistoryTypeId="24" PostId="1025" RevisionGUID="e1dea512-eb36-46a7-8ac5-c7f4a57c1e1c" CreationDate="2014-08-24T03:31:06.623" Comment="Proposed by 97 approved by 84 edit id of 146" />
  <row Id="2674" PostHistoryTypeId="2" PostId="1032" RevisionGUID="788bd2ec-3973-4fb3-8ecc-36c48dfa1312" CreationDate="2014-08-24T08:22:23.497" UserId="816" Text="You may want to check [cross-validated][1] - a stachexchange website for many things, including machine learning. &#xD;&#xA;&#xD;&#xA;In particular, this question (with exactly same title) has already been answered multiple times. Check these links: http://stats.stackexchange.com/search?q=random+forest+overfit&#xD;&#xA;&#xD;&#xA;But I may give you the short answer to it: yes, it does overfit, and sometimes you need to control the complexity of the trees in your forest, or even prune when they grow too much - but this depends on the library you use for building the forest. E.g. in `randomForest` in R you can only control the complexity&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stats.stackexchange.com/" />
  <row Id="2675" PostHistoryTypeId="2" PostId="1033" RevisionGUID="97758086-a6f3-400f-8d83-f93497844b15" CreationDate="2014-08-24T11:03:08.773" UserId="2668" Text="Due to the very Big increase in **Big Data** (pun intended) and the desire for robust stable scalable applications I actually believe it to be **Scala**.  **Spark** will inevitably become the main Big Data Machine Learning tool, and it's main API is in Scala.  Furthermore you simply cannot build a product with scripting languages like Python and R, one can only experiment with these languages.  What Scala brings is a way to BOTH experiment and produce a product.  More reasons&#xD;&#xA;&#xD;&#xA;1. Think functionally - write faster code and more readable code&#xD;&#xA;2. Scala means the end of the two team development cycle. So better product ownership, more agile cross functional teams, and half as many employees required to make a product as we will no longer need both a &quot;research&quot; team and an engineering team, Data Scientists will be able to do both.  This is because Scala is;&#xD;&#xA;&#xD;&#xA; - A production quality language - static typing, but with the flexibility of dynamic typing due to implicits&#xD;&#xA; - Interoperable with rest of Java world (so Apache Commons Math, Databases, Cassandra, HBase, HDFS, Akka, Storm, many many databases, more spark components (e.g. graphx, SparkStreaming)&#xD;&#xA;&#xD;&#xA;3. Step into Spark code easily and understand it, also helps with debugging&#xD;&#xA;&#xD;&#xA;4. Scala is awesome:&#xD;&#xA; - Amazing IDE support due to static typing&#xD;&#xA; - Property based tests with ScalaCheck - insane unit testing&#xD;&#xA; - Very concise language&#xD;&#xA; - Suits mathematicians perfectly (especially Pure Mathematicians)&#xD;&#xA;&#xD;&#xA;5. A little more efficient as compiled not interpreted&#xD;&#xA;&#xD;&#xA;6. Python Spark API sits on Scala API and therefore will always be behind Scala API&#xD;&#xA;&#xD;&#xA;7. Much easier to do Mathematics in Scala as it's a Scalable Language where one can easily define DSLs and due to being so functional&#xD;&#xA;&#xD;&#xA;8. Akka - another way other than storm to do High Velocity&#xD;&#xA;&#xD;&#xA;9. Pimp my library pattern makes adding methods to Spark RDDs really easy" />
  <row Id="2676" PostHistoryTypeId="2" PostId="1034" RevisionGUID="02ea46a5-5b2c-48f2-ad93-aac5e1136f59" CreationDate="2014-08-24T17:09:40.510" UserId="3064" Text="I am exploring different types of parse tree structures. The two widely known parse tree structures are &#xD;&#xA;a) Constituency based parse tree and &#xD;&#xA;b) Dependency based parse tree structures. &#xD;&#xA;&#xD;&#xA;I am able to use generate both types of parse tree structures using Stanford NLP package. However, I am not sure how to use these tree structures for my classification task. &#xD;&#xA;&#xD;&#xA;For e.g If I want to do sentiment analysis and want to categorize text into positive and negative classes, what features can I derive from parse tree structures for my classification task?" />
  <row Id="2677" PostHistoryTypeId="1" PostId="1034" RevisionGUID="02ea46a5-5b2c-48f2-ad93-aac5e1136f59" CreationDate="2014-08-24T17:09:40.510" UserId="3064" Text="What features are generally used from Parse trees in classification process in NLP?" />
  <row Id="2678" PostHistoryTypeId="3" PostId="1034" RevisionGUID="02ea46a5-5b2c-48f2-ad93-aac5e1136f59" CreationDate="2014-08-24T17:09:40.510" UserId="3064" Text="&lt;machine-learning&gt;&lt;nlp&gt;&lt;feature-selection&gt;&lt;feature-extraction&gt;" />
  <row Id="2679" PostHistoryTypeId="2" PostId="1035" RevisionGUID="cfdda994-b00a-43a9-a69f-f45696beda13" CreationDate="2014-08-24T22:02:01.050" UserId="609" Text="First compute the edit distance between all pairs of strings.  See http://en.wikipedia.org/wiki/Edit_distance and http://web.stanford.edu/class/cs124/lec/med.pdf.  Then exclude any outliers strings based on some distance threshold.&#xD;&#xA;&#xD;&#xA;With remaining strings, you can use the distance matrix to identify the most central string.  Depending on the method you use, you might get ambiguous results for some data. No method is perfect for all possibilities.  For your purposes, all you need is some heuristic rules to resolve ambiguities -- i.e. pick two or more candidates.&#xD;&#xA;&#xD;&#xA;Maybe you don't want to pick &quot;most central&quot; from your list of strings, but instead want to generate a regular expression that captures the pattern common to all the non-outlier strings.  One way to do this is to synthesize a string that is equidistant from all the non-outlier strings.  You can work out the required edit distance from the matrix, and then you'd randomly generate regular using those distances as constraints.  Then you'd test candidate regular expressions and accept the first one that fits the constraints and also accepts all the strings in your non-outlier list.  (Start building regular expressions from longest common substring lists, because those are non-wildcard characters.)" />
  <row Id="2680" PostHistoryTypeId="2" PostId="1036" RevisionGUID="a88573cb-460e-4dc9-a3e0-05130f55018e" CreationDate="2014-08-25T00:28:09.003" UserId="3068" Text="I am exploring how to model a data set using normal distributions with both mean and variance defined as linear functions of independent variables.&#xD;&#xA;&#xD;&#xA;Something like N ~ (f(x), g(x)).&#xD;&#xA;&#xD;&#xA;I generate a random sample like this:&#xD;&#xA;&#xD;&#xA;    def draw(x):&#xD;&#xA;        return norm(5 * x + 2, 3 *x + 4).rvs(1)[0]&#xD;&#xA;&#xD;&#xA;So I want to retrieve 5, 2 and 4 as the parameters for my distribution.&#xD;&#xA;&#xD;&#xA;I generate my sample:&#xD;&#xA;&#xD;&#xA;smp = np.zeros((100,2))&#xD;&#xA;&#xD;&#xA;    for i in range(0, len(smp)):&#xD;&#xA;        smp[i][0] = i&#xD;&#xA;        smp[i][1] = draw(i)&#xD;&#xA;&#xD;&#xA;The likelihood function is:&#xD;&#xA;&#xD;&#xA;    def lh(p):&#xD;&#xA;        p_loc_b0 = p[0]&#xD;&#xA;        p_loc_b1 = p[1]&#xD;&#xA;        p_scl_b0 = p[2]&#xD;&#xA;        p_scl_b1 = p[3]&#xD;&#xA;    &#xD;&#xA;        l = 1&#xD;&#xA;        for i in range(0, len(smp)):&#xD;&#xA;            x = smp[i][0]&#xD;&#xA;            y = smp[i][1]&#xD;&#xA;            l = l * norm(p_loc_b0 + p_loc_b1 * x, p_scl_b0 + p_scl_b1 * x).pdf(y)&#xD;&#xA;    &#xD;&#xA;        return -l&#xD;&#xA;&#xD;&#xA;So the parameters for the linear functions used in the model are given in the p 4-variable vector.&#xD;&#xA;&#xD;&#xA;Using scipy.optimize, I can solve for the MLE parameters using an extremely low xtol, and already giving the solution as the starting point:&#xD;&#xA;&#xD;&#xA;    fmin(lh, x0=[2,5,3,4], xtol=1e-35)&#xD;&#xA;&#xD;&#xA;Which does not work to well:&#xD;&#xA;&#xD;&#xA;    Warning: Maximum number of function evaluations has been exceeded.&#xD;&#xA;    array([ 3.27491346,  4.69237042,  5.70317719,  3.30395462])&#xD;&#xA;&#xD;&#xA;Raising the xtol to higher values does no good.&#xD;&#xA;&#xD;&#xA;So i try using a starting solution far from the real solution:&#xD;&#xA;&#xD;&#xA;    &gt;&gt;&gt; fmin(lh, x0=[1,1,1,1], xtol=1e-8)&#xD;&#xA;    Optimization terminated successfully.&#xD;&#xA;             Current function value: -0.000000&#xD;&#xA;             Iterations: 24&#xD;&#xA;             Function evaluations: 143&#xD;&#xA;    array([ 1.,  1.,  1.,  1.])&#xD;&#xA;&#xD;&#xA;Which makes me think:&#xD;&#xA;&#xD;&#xA;PDF are largely clustered around the mean, and have very low gradients only a few standard deviations away from the mean, which must be not too good for numerical methods.&#xD;&#xA;&#xD;&#xA;So how does one go about doing these kind of numerical estimation in functions where gradient is very near to zero away from the solution?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2681" PostHistoryTypeId="1" PostId="1036" RevisionGUID="a88573cb-460e-4dc9-a3e0-05130f55018e" CreationDate="2014-08-25T00:28:09.003" UserId="3068" Text="How to numerically estimate MLE estimators in python when gradients are very small far from the optimal solution?" />
  <row Id="2682" PostHistoryTypeId="3" PostId="1036" RevisionGUID="a88573cb-460e-4dc9-a3e0-05130f55018e" CreationDate="2014-08-25T00:28:09.003" UserId="3068" Text="&lt;python&gt;&lt;statistics&gt;" />
  <row Id="2683" PostHistoryTypeId="2" PostId="1037" RevisionGUID="298def1a-1df8-4ed3-a87c-26875b7801f3" CreationDate="2014-08-25T03:01:09.837" UserId="3070" Text="If I understand you question correctly, there are two reasons why genetic algorithm might not a good idea for optimizing website features:&#xD;&#xA;&#xD;&#xA;1) Feedback data is coming it too slow, say once a day, genetic algorithm might take a while to converge.&#xD;&#xA;&#xD;&#xA;2) In the process of testing genetic algorithm will probably come up with combinations that are 'strange' and that might not be the risk the company wants to take.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2684" PostHistoryTypeId="5" PostId="1021" RevisionGUID="684ff727-ba8a-4186-93f2-bf67376b18a9" CreationDate="2014-08-25T08:11:49.307" UserId="3047" Comment="Introduced the word 'canonical' as suggested by Sean Owen" Text="I have thousands of lists of strings, and each list has about 10 strings. Most strings in a given list are very similar, though some strings are (rarely) completely unrelated to the others and some strings contain irrelevant words. They can be considered to be noisy variations of a canonical string. I am looking for an algorithm or a library that will convert each list into this canonical string.&#xD;&#xA;&#xD;&#xA;Here is one such list.&#xD;&#xA;&#xD;&#xA;&lt;ul&gt;&#xD;&#xA;&lt;li&gt;Star Wars: Episode IV A New Hope | StarWars.com&lt;/li&gt;&#xD;&#xA;&lt;li&gt;Star Wars Episode IV - A New Hope (1977)&lt;/li&gt;&#xD;&#xA;&lt;li&gt;Star Wars: Episode IV - A New Hope - Rotten Tomatoes&lt;/li&gt;&#xD;&#xA;&lt;li&gt;Watch Star Wars: Episode IV - A New Hope Online Free&lt;/li&gt;&#xD;&#xA;&lt;li&gt;Star Wars (1977) - Greatest Films&lt;/li&gt;&#xD;&#xA;&lt;li&gt;[REC] 4 poster promises death by outboard motor - SciFiNow&lt;/li&gt;&#xD;&#xA;&lt;/ul&gt;&#xD;&#xA;&#xD;&#xA;For this list, any string matching the regular expression `^Star Wars:? Episode IV (- )?A New Hope$` would be acceptable.&#xD;&#xA;&#xD;&#xA;I have looked at Andrew Ng's course on Machine Learning on Coursera, but I was not able to find a similar problem." />
  <row Id="2685" PostHistoryTypeId="4" PostId="1021" RevisionGUID="684ff727-ba8a-4186-93f2-bf67376b18a9" CreationDate="2014-08-25T08:11:49.307" UserId="3047" Comment="Introduced the word 'canonical' as suggested by Sean Owen" Text="Extract canonical string from a list of noisy strings" />
  <row Id="2686" PostHistoryTypeId="2" PostId="1038" RevisionGUID="fa73acad-d874-4896-b391-945f6ae09ebf" CreationDate="2014-08-25T09:53:13.823" UserId="1314" Text="I have created external table in Hive in the hdfs path 'hdfs://localhost.localdomain:8020/user/hive/training'. If I apply describe command I can find the table path as shown below. But when I browse through the namenode web page, the table name does not showing up in the path.&#xD;&#xA;&#xD;&#xA;    hive&gt; describe extended testtable4;&#xD;&#xA;    OK&#xD;&#xA;    firstname   string  &#xD;&#xA;    lastname    string  &#xD;&#xA;    address string  &#xD;&#xA;    city    string  &#xD;&#xA;    state   string  &#xD;&#xA;    country string  &#xD;&#xA;    &#xD;&#xA;        ***Detailed Table Information  Table(tableName:testtable4, dbName:default, owner:cloudera, createTime:1408765301, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:firstname, type:string, comment:null), FieldSchema(name:lastname, type:string, comment:null), FieldSchema(name:address, type:string, comment:null), FieldSchema(name:city, type:string, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], location:hdfs://localhost.localdomain:8020/user/hive/training, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,, line.delim=    &#xD;&#xA;        }), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1408765301}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)       &#xD;&#xA;        Time taken: 0.7 seconds***" />
  <row Id="2687" PostHistoryTypeId="1" PostId="1038" RevisionGUID="fa73acad-d874-4896-b391-945f6ae09ebf" CreationDate="2014-08-25T09:53:13.823" UserId="1314" Text="Hive External table does not showing in Namenode (Cloudera-QuickstartVm)" />
  <row Id="2688" PostHistoryTypeId="3" PostId="1038" RevisionGUID="fa73acad-d874-4896-b391-945f6ae09ebf" CreationDate="2014-08-25T09:53:13.823" UserId="1314" Text="&lt;bigdata&gt;" />
  <row Id="2689" PostHistoryTypeId="2" PostId="1039" RevisionGUID="e16007b0-f0e5-43dd-8c32-333b6f3c8c40" CreationDate="2014-08-25T11:29:35.047" UserId="979" Text="I think dependencies can be used to improve the accurary of your sentiment classifier. Consider the following examples:&#xD;&#xA;&#xD;&#xA;E1: Bill is not a scientist&#xD;&#xA;&#xD;&#xA;and assume that the token &quot;scientist&quot; has a positive sentiment in a specific domain.&#xD;&#xA;&#xD;&#xA;Knowing the dependency neg(scientist, not) we can see that the example above has a negative sentiment. Without knowing this dependency we would probably classify the sentence as positive.&#xD;&#xA;&#xD;&#xA;Another types of dependencies can be used probably in the same way to improve the accuracy of the classifiers." />
  <row Id="2693" PostHistoryTypeId="5" PostId="1037" RevisionGUID="e7ac6d32-18f9-4df4-929c-071827338e9b" CreationDate="2014-08-25T16:52:44.817" UserId="3070" Comment="edited body" Text="If I understand you question correctly, there are two reasons why genetic algorithm might not a good idea for optimizing website features:&#xD;&#xA;&#xD;&#xA;1) Feedback data is coming in too slow, say once a day, genetic algorithm might take a while to converge.&#xD;&#xA;&#xD;&#xA;2) In the process of testing genetic algorithm will probably come up with combinations that are 'strange' and that might not be the risk the company wants to take.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2695" PostHistoryTypeId="2" PostId="1041" RevisionGUID="03cd336b-ebaf-4b11-a980-fbdd38fa3ab8" CreationDate="2014-08-26T07:14:54.767" UserId="471" Text="If you suspect your response is linear with year, then put year in as a numeric term in your model rather than a categorical.&#xD;&#xA;&#xD;&#xA;Extrapolation is then perfectly valid based on the usual assumptions of the GLM family. Make sure you correctly get the errors on your extrapolated estimates." />
  <row Id="2696" PostHistoryTypeId="5" PostId="1041" RevisionGUID="076d707a-ebd6-45c4-aedb-02853cdc6d5e" CreationDate="2014-08-26T07:20:52.097" UserId="471" Comment="added 353 characters in body" Text="If you suspect your response is linear with year, then put year in as a numeric term in your model rather than a categorical.&#xD;&#xA;&#xD;&#xA;Extrapolation is then perfectly valid based on the usual assumptions of the GLM family. Make sure you correctly get the errors on your extrapolated estimates.&#xD;&#xA;&#xD;&#xA;Just extrapolating the parameters from a categorical variable is wrong for a number of reasons. The first one I can think of is that there may be more observations in some years than others, so any linear extrapolation needs to weight those year's estimates more. Just eyeballing a line - or even fitting a line to the coefficients - won't do this. " />
  <row Id="2697" PostHistoryTypeId="2" PostId="1042" RevisionGUID="deff398a-46c6-48b5-bce4-e64ab2bfd40d" CreationDate="2014-08-26T07:33:40.080" UserId="3058" Text="I am seeking a basic list of key data analysis methods used for studying social media platforms online. Are there such key methods, or does this process generally vary according to topic? And is there a standard order in which these methods are applied?(The particular context I'm interested in is how the news is impacting on social media)" />
  <row Id="2698" PostHistoryTypeId="1" PostId="1042" RevisionGUID="deff398a-46c6-48b5-bce4-e64ab2bfd40d" CreationDate="2014-08-26T07:33:40.080" UserId="3058" Text="Key data analysis methods used for studying social media platforms online?" />
  <row Id="2699" PostHistoryTypeId="3" PostId="1042" RevisionGUID="deff398a-46c6-48b5-bce4-e64ab2bfd40d" CreationDate="2014-08-26T07:33:40.080" UserId="3058" Text="&lt;social-network-analysis&gt;" />
  <row Id="2700" PostHistoryTypeId="6" PostId="1024" RevisionGUID="857b9c13-b8c2-45df-91f0-535a19dc6d46" CreationDate="2014-08-26T11:36:01.420" UserId="471" Comment="note data mining" Text="&lt;statistics&gt;" />
  <row Id="2701" PostHistoryTypeId="24" PostId="1024" RevisionGUID="857b9c13-b8c2-45df-91f0-535a19dc6d46" CreationDate="2014-08-26T11:36:01.420" Comment="Proposed by 471 approved by -1 edit id of 147" />
  <row Id="2702" PostHistoryTypeId="6" PostId="1024" RevisionGUID="86fc47e3-adbd-4f2d-8602-277fc0e67dca" CreationDate="2014-08-26T11:36:01.420" UserId="21" Comment="note data mining" Text="&lt;statistics&gt;&lt;glm&gt;&lt;regression&gt;" />
  <row Id="2703" PostHistoryTypeId="10" PostId="1029" RevisionGUID="2e3f126f-c1c5-436f-8e9c-4fcefc62e9e9" CreationDate="2014-08-26T11:36:17.830" UserId="21" Comment="105" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2704" PostHistoryTypeId="2" PostId="1043" RevisionGUID="3b6f0f32-d5f2-4cff-bf5e-6dc1d76ae4dc" CreationDate="2014-08-26T12:44:35.247" UserId="2511" Text="I am trying to find out if there have been any A/B tests done by any of the various services with messaging to see if showing when the other user is typing helps your KPIs. I know there are a lot of forum questions about how to disable this feature so the question arises if it is worth the extra effort to add." />
  <row Id="2705" PostHistoryTypeId="1" PostId="1043" RevisionGUID="3b6f0f32-d5f2-4cff-bf5e-6dc1d76ae4dc" CreationDate="2014-08-26T12:44:35.247" UserId="2511" Text="&quot;Is Typing&quot; A/B test" />
  <row Id="2706" PostHistoryTypeId="3" PostId="1043" RevisionGUID="3b6f0f32-d5f2-4cff-bf5e-6dc1d76ae4dc" CreationDate="2014-08-26T12:44:35.247" UserId="2511" Text="&lt;ab-testing&gt;" />
  <row Id="2707" PostHistoryTypeId="2" PostId="1044" RevisionGUID="d6b915dd-3de0-4f46-927e-cbb9bcfb9f2d" CreationDate="2014-08-26T14:53:40.647" UserId="3097" Text="As what I described in the title, we are especially interested in those for dealing with big data----ts efficiency and stability, and used in industry not in experiment or university. Thanks!" />
  <row Id="2708" PostHistoryTypeId="1" PostId="1044" RevisionGUID="d6b915dd-3de0-4f46-927e-cbb9bcfb9f2d" CreationDate="2014-08-26T14:53:40.647" UserId="3097" Text="which programming language has a large library that can do machine learning algorithm, R, matlab or python" />
  <row Id="2709" PostHistoryTypeId="3" PostId="1044" RevisionGUID="d6b915dd-3de0-4f46-927e-cbb9bcfb9f2d" CreationDate="2014-08-26T14:53:40.647" UserId="3097" Text="&lt;machine-learning&gt;&lt;bigdata&gt;&lt;data-mining&gt;&lt;python&gt;&lt;r&gt;" />
  <row Id="2711" PostHistoryTypeId="4" PostId="1042" RevisionGUID="c680def1-a502-4614-9cfd-9617970038ee" CreationDate="2014-08-26T17:25:29.930" UserId="3058" Comment="Clarifying title - to make it easier to find in searches" Text="Studying social media platforms - key data analysis methods?" />
  <row Id="2712" PostHistoryTypeId="2" PostId="1045" RevisionGUID="af29426e-c8e4-4932-8ce2-0b694194389d" CreationDate="2014-08-26T21:37:12.107" UserId="3050" Text="Have anyone used Shark as repository from resulting datasets from Apache Spark?&#xD;&#xA;&#xD;&#xA;I'm starting some tests with Spark and read about this database tecnology. Have anyone been using it?" />
  <row Id="2713" PostHistoryTypeId="1" PostId="1045" RevisionGUID="af29426e-c8e4-4932-8ce2-0b694194389d" CreationDate="2014-08-26T21:37:12.107" UserId="3050" Text="Using Shark with Apache Spark" />
  <row Id="2714" PostHistoryTypeId="3" PostId="1045" RevisionGUID="af29426e-c8e4-4932-8ce2-0b694194389d" CreationDate="2014-08-26T21:37:12.107" UserId="3050" Text="&lt;hadoop&gt;" />
  <row Id="2715" PostHistoryTypeId="2" PostId="1046" RevisionGUID="b21210e3-442a-4e79-929d-fd5221e48a01" CreationDate="2014-08-26T21:51:07.247" UserId="3100" Text="Try &lt;http://deeplearning4j.org/word2vec.html&gt;. This has an implementation of Word2Vec used instead of Bag of Words for NER and other NLP tasks." />
  <row Id="2716" PostHistoryTypeId="2" PostId="1047" RevisionGUID="0e002b24-0d6f-4253-8367-d506fdb5a1d3" CreationDate="2014-08-26T22:48:16.617" UserId="3101" Text="1. SPSS is a great tool, but you can accomplish a great deal with resources that you already have on your computer, like Excel, or that are free, like the R-project. Although these tools are powerful, and can help you identify patterns, you'll need to have a firm grasp of your data before running analyses (I'd recommend running descriptive statistics on your data, and exploring the data with graphs to make sure everything is looking normal). In other words, the tool that you use won't offer a &quot;silver bullet&quot;, because the output will only be as valuable as the input (you know the saying... &quot;garbage in, garbage out&quot;). Much of what I'm saying has already been stated in the reply by Aleksandr - spot on. &#xD;&#xA;&#xD;&#xA;2. R can be challenging for those of us who aren't savvy with coding, but the free resources associated with R and its packages are abundant. If you practice learning the program, you'll quickly gain traction. Again, you'll need to be familiar with your data and the analyses you want to run anyway, and that fact remains regardless of the statistical tools you utilize. &#xD;&#xA;&#xD;&#xA;3. I'd begin by getting super familiar with my data (follow the steps outlined in the reply from Aleksandr, for starters). You might consider picking up John Foreman's book called Data Smart. It's a hands-on book, as John provides datasets and you follow along with his examples (using Excel) to learn various ways of navigating and exploring data. For beginners, it's a great resource. " />
  <row Id="2717" PostHistoryTypeId="2" PostId="1048" RevisionGUID="5095a275-69f6-4130-937d-a318578a087b" CreationDate="2014-08-27T03:41:20.317" UserId="3050" Text="I've making some researches last months and I could find more libraries, contente and active community with Python. Actually I'm using it to ETL processes, some minning jobs and to make map/reduce." />
  <row Id="2718" PostHistoryTypeId="5" PostId="671" RevisionGUID="0b98857f-6701-4bdb-b2e7-35f96eb23eb5" CreationDate="2014-08-27T07:15:52.587" UserId="870" Comment="added 534 characters in body" Text="I have a linearly increasing time series dataset of a sensor, with value ranges between 50 and 150. I've implemented a [Simple Linear Regression][1] algorithm to fit a regression line on such data, and I'm predicting the date when the series would reach 120.&#xD;&#xA;&#xD;&#xA;All works fine when the series move upwards. But, there are cases in which the sensor reaches around 110 or 115, and it is reset; in such cases the values would start over again at, say, 50 or 60.&#xD;&#xA;&#xD;&#xA;This is where I start facing issues with the regression line, as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I'm trying to understand if there are any algorithms available that consider this case.&#xD;&#xA;&#xD;&#xA;I'm new to data science, would appreciate any pointers to move further.&#xD;&#xA;&#xD;&#xA;**Edit: nfmcclure's suggestions applied**&#xD;&#xA;&#xD;&#xA;Before applying the suggestions&#xD;&#xA;&#xD;&#xA;![enter image description here][2]&#xD;&#xA;&#xD;&#xA;Below is the snapshot of what I've got after splitting the dataset where the reset occurs, and the slope of two set.&#xD;&#xA;&#xD;&#xA;![enter image description here][3]&#xD;&#xA;&#xD;&#xA;finding the mean of the two slopes and drawing the line from the mean.&#xD;&#xA;&#xD;&#xA;![enter image description here][4]&#xD;&#xA;&#xD;&#xA;Is this OK?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Simple_linear_regression&#xD;&#xA;  [2]: http://i.stack.imgur.com/ZsyyQ.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/OEQCw.png&#xD;&#xA;  [4]: http://i.stack.imgur.com/i2qv5.png" />
  <row Id="2719" PostHistoryTypeId="2" PostId="1049" RevisionGUID="8d3bca9a-62c2-4d58-97df-d6540355cafe" CreationDate="2014-08-27T08:43:51.547" UserId="3106" Text="I am considering the same issue. However my situation is a little bit different. I will be doing a project in my undergrad studies concerned with ML. MATLAB is very popular in academia, however, I think to use Python as whenever I do so far, I have found active community. There is an interesting article in Quora, that also motivates my decision.&#xD;&#xA;&#xD;&#xA;http://www.quora.com/What-skills-are-needed-for-machine-learning-jobs&#xD;&#xA;&#xD;&#xA;I will quote 1. from the recommendations, where it is suggested that Python has similar functionality to MATLAB, but it can also be easily integrated into a web service. You can follow the link and check the full answer." />
  <row Id="2720" PostHistoryTypeId="2" PostId="1050" RevisionGUID="5ab1dbc4-579d-41fd-bf10-0f237ac659d1" CreationDate="2014-08-27T13:01:14.643" UserId="3108" Text="**General description of the problem**&#xD;&#xA;&#xD;&#xA;There is a graph where some of the nodes have a certain type(There are about 3-4 types). For other nodes, type is not known.&#xD;&#xA;I want to predict, based on my graph, for the nodes with unknown type their &quot;most probable&quot; type.&#xD;&#xA;&#xD;&#xA;**Possible framework**&#xD;&#xA;&#xD;&#xA;I guess the general framework for such tasks is called `label propagation algorithm`.&#xD;&#xA;according to literature on the topic.&#xD;&#xA;Here are some examples: [one][1], [two][2]&#xD;&#xA;&#xD;&#xA;Another often mentioned topic is `Frequent Subgraph Mining`, which includes algorithms like `SUBDUE`,`SLEUTH`, and `gSpan`.&#xD;&#xA;&#xD;&#xA;**Found in R**&#xD;&#xA;&#xD;&#xA;The only label propagation algorithm I managed to find in `R` is `label.propagation.community()` from `igraph` library. &#xD;&#xA;However as the name suggests it is mostly for finding communities, not classification of nodes.&#xD;&#xA;&#xD;&#xA;There also seems to be several references to `subgraphMining` library, (here for example)&#xD;&#xA;but looks like it is missing from CRAN.&#xD;&#xA;&#xD;&#xA;**Question**&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If someone could suggest libraries/frameworks for the task described, I would be very grateful.&#xD;&#xA;Made the question as specific as I could, hope that shall do)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://lvk.cs.msu.su/~bruzz/articles/classification/zhu02learning.pdf&#xD;&#xA;  [2]: http://www.csc.ncsu.edu/faculty/samatova/practical-graph-mining-with-R/slides/pdf/Frequent_Subgraph_Mining.pdf" />
  <row Id="2721" PostHistoryTypeId="1" PostId="1050" RevisionGUID="5ab1dbc4-579d-41fd-bf10-0f237ac659d1" CreationDate="2014-08-27T13:01:14.643" UserId="3108" Text="Libraries for (label propagation algorithms/frequent subgraph mining) for graphs in R" />
  <row Id="2722" PostHistoryTypeId="3" PostId="1050" RevisionGUID="5ab1dbc4-579d-41fd-bf10-0f237ac659d1" CreationDate="2014-08-27T13:01:14.643" UserId="3108" Text="&lt;classification&gt;&lt;r&gt;&lt;graphs&gt;" />
  <row Id="2723" PostHistoryTypeId="2" PostId="1051" RevisionGUID="1e74981f-fb4d-438f-b93f-db73ce68d490" CreationDate="2014-08-27T18:33:23.110" UserId="819" Text="&gt; When using the public Google APIs to retrieve results, I was only able to collect 4-10 results per query.&#xD;&#xA;&#xD;&#xA;Here's how to get more than 10 results per query: https://support.google.com/customsearch/answer/1361951?hl=en&#xD;&#xA;&#xD;&#xA;&gt; Google Custom Search and Google Site Search return up to 10 results per query. If you want to display more than 10 results to the user, you can issue multiple requests (using the start=0, start=11 ... parameters) and display the results on a single page. In this case, Google will consider each request as a separate query, and if you are using Google Site Search, each query will count towards your limit.&#xD;&#xA;&#xD;&#xA;There are other search engine APIs as well (e.g., [Bing](http://datamarket.azure.com/dataset/bing/search))" />
  <row Id="2725" PostHistoryTypeId="2" PostId="1053" RevisionGUID="6e7e69d5-c437-49c0-b43d-57f16720c2e1" CreationDate="2014-08-28T01:36:40.540" UserId="1281" Text="I would like to [summarize](http://stat.ethz.ch/R-manual/R-devel/library/base/html/summary.html) (as in R) the contents of a CSV (possibly after [loading](http://www.endmemo.com/program/R/readcsv.php) it, or storing it somewhere, that's not a problem). The summary should contain the quartiles, mean, median, min and max of the data in a CSV file for each numeric (integer or real numbers) dimension. The standard deviation would be cool as well.&#xD;&#xA;&#xD;&#xA;I would also like to generate some plots to visualize the data, for example 3 plots for the 3 pairs of variables that are more correlated ([correlation coefficient](http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient)) and 3 plots for the 3 pairs of variables that are least correlated.&#xD;&#xA;&#xD;&#xA;R requires only a few lines to implement this. Are there any libraries (or tools) that would allow a similarly simple (and efficient if possible) implementation in Java or Scala?&#xD;&#xA;&#xD;&#xA;PD: This is a specific use case for a [previous (too broad) question](http://datascience.stackexchange.com/questions/948/any-clear-winner-for-data-science-in-scala).&#xD;&#xA;" />
  <row Id="2726" PostHistoryTypeId="1" PostId="1053" RevisionGUID="6e7e69d5-c437-49c0-b43d-57f16720c2e1" CreationDate="2014-08-28T01:36:40.540" UserId="1281" Text="Summarize and visualize a CSV in Java/Scala?" />
  <row Id="2727" PostHistoryTypeId="3" PostId="1053" RevisionGUID="6e7e69d5-c437-49c0-b43d-57f16720c2e1" CreationDate="2014-08-28T01:36:40.540" UserId="1281" Text="&lt;tools&gt;" />
  <row Id="2728" PostHistoryTypeId="2" PostId="1054" RevisionGUID="de794cbb-535a-4b5f-a2aa-a75fb99591e8" CreationDate="2014-08-28T06:49:05.210" UserId="1350" Text="There are several reasons why you are getting erroneous results.&#xD;&#xA;First, you should consider using log likelihood instead of likelihood. There are numerical issues with multiplying many small numbers(imagine if you had millions of samples you had to multiply millions of small numbers for the lhd). Also taking gradients for optimization methods that require gradients is often easier when you are dealing with the log likelihood. In general, it is good to have an objective which is a sum rather than a product of variables when dealing with optimization problems.&#xD;&#xA;&#xD;&#xA;Second, fmin is using Nelder-Mead simplex algorithm which has no convergence guarantees according to [scipy documentation][2]. This means the convergence is totally random and you should not expect to find parameters close to the originals. To get around this, I would suggest you to use a gradient based method like stochastic gradient descent or BFGS. Since you know the generative model (rvs are Gaussian distributed) you can write the likelihood and log likelihood as:&#xD;&#xA;![equations][1]&#xD;&#xA;&#xD;&#xA;Where a,b,c and d are your model parameters 5,2,3 and 4 respectively.&#xD;&#xA;Then take the [gradient][3] with respect to [a,b,c,d] and feed that into prime input of fmin_bfgs. Note that due to varying variance what could be solved by just linear regression is now a nastier problem.&#xD;&#xA;&#xD;&#xA;Finally, you may also want to check Generalized least squares on http://en.wikipedia.org/wiki/Linear_regression#Least-squares_estimation_and_related_techniques and http://en.wikipedia.org/wiki/Heteroscedasticity, which talk about your problem and offer several available solutions.&#xD;&#xA;&#xD;&#xA;Good luck!&#xD;&#xA;&#xD;&#xA;[1]: http://i.stack.imgur.com/bfsvr.png&#xD;&#xA;[2]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html#scipy.optimize.fmin&#xD;&#xA;[3]: http://en.wikipedia.org/wiki/Gradient" />
  <row Id="2730" PostHistoryTypeId="2" PostId="1055" RevisionGUID="5d1aec13-6927-476f-9783-bfaf269e8174" CreationDate="2014-08-28T11:24:38.743" UserId="2668" Text="Scala is the only real language that has Big Data at it's core. You have MLLib that sits on Spark, and as Scala is Functional it makes parallel computing really natural.  R, Python and Matlab are not suitable for industry productization, well some would say Python's horrible dynamic typing can be handled a little using special build tools, but really its not type safe and there is no way to solve that problem." />
  <row Id="2731" PostHistoryTypeId="6" PostId="1053" RevisionGUID="87a35c96-2f73-4be5-b717-aa9c8c913e37" CreationDate="2014-08-28T16:28:30.453" UserId="97" Comment="Additional related tags." Text="&lt;tools&gt;&lt;visualization&gt;&lt;scala&gt;&lt;csv&gt;" />
  <row Id="2732" PostHistoryTypeId="24" PostId="1053" RevisionGUID="87a35c96-2f73-4be5-b717-aa9c8c913e37" CreationDate="2014-08-28T16:28:30.453" Comment="Proposed by 97 approved by 1281 edit id of 148" />
  <row Id="2733" PostHistoryTypeId="2" PostId="1056" RevisionGUID="a473ee5b-d514-46d3-93c9-29e579202f79" CreationDate="2014-08-28T16:56:45.577" UserId="3077" Text="It sounds as if you want to use unsupervized learning to create a training set. Am I right? You use your cluster analysis to determine which docs come from UK, US or Oz -- or which docs are talking about Soccer, Football or Australian football respectively? Then feed those tagged docs into a supervized learning algorithm of some sort?&#xD;&#xA;&#xD;&#xA;How well this works will depend entirely on how well you can distinguish UK, US and OZ. I would have thought it would be fairly straightforward to find documents where national origin was known, so that you could build a supervized algorithm for detecting language variant. You wouldn't even need a corpus that talked about football, since dialectical differences show up in other ways that are subject matter independent. (For example, I am clearly from North America, since I just wrote &quot;in ways that are subject matter independent&quot; rather than &quot;Since dialectical differences do not depend on subject matter&quot;).&#xD;&#xA;&#xD;&#xA;However, the answer to your question, &quot;can I use unsupervized learning and then supervized learning&quot; is No. If the results of an unsupervized learning algorithm are fed to a supervized learning algorithm, the net result is unsupervized --- there are still no grown-ups in the room. And the classification errors of the resulting process will contain error terms from both stages. This doesn't mean you shouldn't use the method you propose ... it might still work well ... but it won't be a supervized learning algorithm." />
  <row Id="2734" PostHistoryTypeId="2" PostId="1057" RevisionGUID="77e213eb-f2aa-4724-bf3d-9647303ec910" CreationDate="2014-08-28T18:53:57.297" UserId="3077" Text="You might want to try this book [Mining the Social Web][1] for an overview of different techniques. Obviously, the methods you need will depend on the use case. A lot of people do interesting things with graphs, displaying relationships between users, with respect to certain topics. Or you might simply to a timeline showing how a news topic builds in interest and wanes.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://shop.oreilly.com/product/0636920030195.do" />
  <row Id="2735" PostHistoryTypeId="2" PostId="1058" RevisionGUID="e4a95989-8fb4-467d-985e-3caa32df4696" CreationDate="2014-08-29T01:36:36.320" UserId="3128" Text="Is there any article or any research that made this calculus? How much data space is used by all scientific articles? in pdf, txt, compressed, or something. Is there any way to have a measure of this? &#xD;&#xA;&#xD;&#xA;What would be the better way to realize this study?&#xD;&#xA;&#xD;&#xA;Regards and thanks in advance" />
  <row Id="2736" PostHistoryTypeId="1" PostId="1058" RevisionGUID="e4a95989-8fb4-467d-985e-3caa32df4696" CreationDate="2014-08-29T01:36:36.320" UserId="3128" Text="How much data space is used by all scientific articles?" />
  <row Id="2737" PostHistoryTypeId="3" PostId="1058" RevisionGUID="e4a95989-8fb4-467d-985e-3caa32df4696" CreationDate="2014-08-29T01:36:36.320" UserId="3128" Text="&lt;bigdata&gt;&lt;research&gt;" />
  <row Id="2738" PostHistoryTypeId="2" PostId="1059" RevisionGUID="f4c46038-6135-4633-b90c-24b2878616a7" CreationDate="2014-08-29T06:00:53.420" UserId="3055" Text="I have a question regarding the use of neural network. I am currently working with R (neural net package) and I am facing the following issue.&#xD;&#xA;My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?&#xD;&#xA;Maybe something is wrong in my analysis&#xD;&#xA;&#xD;&#xA;1- I use the daily log return r(t) = ln(s(t)/s(t-1))&#xD;&#xA;2- I normalise my data with the sigmoid function (sigma and mu computed on my whole set)&#xD;&#xA;3- I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.&#xD;&#xA;&#xD;&#xA;I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?&#xD;&#xA;&#xD;&#xA;Thanking you in advance for your help&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2739" PostHistoryTypeId="1" PostId="1059" RevisionGUID="f4c46038-6135-4633-b90c-24b2878616a7" CreationDate="2014-08-29T06:00:53.420" UserId="3055" Text="Neural Network Prediction Foreign Exchange and Lateness" />
  <row Id="2740" PostHistoryTypeId="3" PostId="1059" RevisionGUID="f4c46038-6135-4633-b90c-24b2878616a7" CreationDate="2014-08-29T06:00:53.420" UserId="3055" Text="&lt;r&gt;&lt;neuralnetwork&gt;" />
  <row Id="2741" PostHistoryTypeId="2" PostId="1060" RevisionGUID="8d82f984-37ac-44a9-991e-d75275f18053" CreationDate="2014-08-29T10:42:14.957" UserId="2668" Text="Checkout Breeze and apache commons math for the maths, and ScalaLab for some nice examples of how to plot things in Scala.&#xD;&#xA;&#xD;&#xA;I've managed to get an environment setup where this would just be a couple of lines. I dont actually use ScalaLab, rather borrow some of its code, I use Intellij worksheets instead." />
  <row Id="2742" PostHistoryTypeId="5" PostId="1059" RevisionGUID="d22184f8-356b-447d-a438-c8eba18e9db9" CreationDate="2014-08-29T10:42:29.200" UserId="2961" Comment="improved formatting; edited title" Text="I have a question regarding the use of neural network. I am currently working with R ([neuralnet package][1]) and I am facing the following issue.&#xD;&#xA;My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?&#xD;&#xA;Maybe something is wrong in my analysis&#xD;&#xA;&#xD;&#xA; 1. I use the daily log return r(t) = ln(s(t)/s(t-1))&#xD;&#xA; 2. I normalise my data with the sigmoid function (sigma and mu computed on my whole set)&#xD;&#xA; 3. I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.&#xD;&#xA;&#xD;&#xA;I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://cran.r-project.org/web/packages/neuralnet/index.html" />
  <row Id="2743" PostHistoryTypeId="4" PostId="1059" RevisionGUID="d22184f8-356b-447d-a438-c8eba18e9db9" CreationDate="2014-08-29T10:42:29.200" UserId="2961" Comment="improved formatting; edited title" Text="Forecasting Foreign Exchange with Neural Network - Lag in Prediction" />
  <row Id="2744" PostHistoryTypeId="24" PostId="1059" RevisionGUID="d22184f8-356b-447d-a438-c8eba18e9db9" CreationDate="2014-08-29T10:42:29.200" Comment="Proposed by 2961 approved by 3055 edit id of 149" />
  <row Id="2745" PostHistoryTypeId="2" PostId="1061" RevisionGUID="ace41ebf-5ae7-429e-8f17-8f1f00ef358d" CreationDate="2014-08-29T11:40:28.657" UserId="3133" Text="I'm looking for the best solution to manage and host datasets for journalistic pursuits. I am assessing https://www.documentcloud.org and http://datahub.io/.&#xD;&#xA;&#xD;&#xA;Can anyone explain the differences between them, or recommend a superior solution?" />
  <row Id="2746" PostHistoryTypeId="1" PostId="1061" RevisionGUID="ace41ebf-5ae7-429e-8f17-8f1f00ef358d" CreationDate="2014-08-29T11:40:28.657" UserId="3133" Text="Apps to manage/host data sets" />
  <row Id="2747" PostHistoryTypeId="3" PostId="1061" RevisionGUID="ace41ebf-5ae7-429e-8f17-8f1f00ef358d" CreationDate="2014-08-29T11:40:28.657" UserId="3133" Text="&lt;dataset&gt;&lt;optimization&gt;" />
  <row Id="2748" PostHistoryTypeId="2" PostId="1062" RevisionGUID="76053f2b-fc26-45b4-914c-47609c387c0d" CreationDate="2014-08-29T16:19:06.903" UserId="924" Text="You can definitely try to first cluster your data, and then try to see if the cluster information helps your classification task.&#xD;&#xA;&#xD;&#xA;For example if your data looked like this (in 1D):&#xD;&#xA;&#xD;&#xA;    AA A AA A A      BBB B B B BB BB BB      AA AA A A AAA&#xD;&#xA;&#xD;&#xA;then it may be reasonable to run a clustering algorithm on each class, to obtain *two different kinds of A*, and learn two separate classifiers for A1 and A2, and just drop the cluster distinction for the final output.&#xD;&#xA;&#xD;&#xA;Other common unsupervised techniques used include PCA.&#xD;&#xA;&#xD;&#xA;As for your football example, the problem is that the unsupervised algorithm does not know what it should be looking for. Instead of learning to separate american football and soccer, it may just as well decide to cluster on international vs. national games. Or Europe vs. U.S.; which may look like it learned about american football and soccer at first, but it put american soccer into the same cluster as american football, and american football teams in Europe into the Europe cluster... because it does not have *guidance* on what structure you are interested in; and the continents *are* a valid structure, too!&#xD;&#xA;&#xD;&#xA;So usually, I would not blindly assume that unsupervised techniques yield a distrinction that matches your desired result. They can yield *any* kind of structure, and you will want to carefully inspect what they found before using it. If you use it blindly, make sure you spend enough time on evaluation (e.g. if the clustering improves your classifier performance, then it probably worked as intended ...)" />
  <row Id="2749" PostHistoryTypeId="5" PostId="1056" RevisionGUID="7ef9c4c6-9955-47c5-a27a-535824b91602" CreationDate="2014-08-29T19:26:39.963" UserId="3077" Comment="added 145 characters in body" Text="It sounds as if you want to use unsupervized learning to create a training set. Am I right? You use your cluster analysis to determine which docs come from UK, US or Oz -- or which docs are talking about Soccer, Football or Australian football respectively? Then feed those tagged docs into a supervized learning algorithm of some sort?&#xD;&#xA;&#xD;&#xA;How well this works will depend entirely on how well you can distinguish UK, US and OZ. I would have thought it would be fairly straightforward to find documents where national origin was known, so that you could build a supervized algorithm for detecting language variant. You wouldn't even need a corpus that talked about football, since dialectical differences show up in other ways that are subject matter independent. (For example, I am clearly from North America, since I just wrote &quot;in ways that are subject matter independent&quot; rather than &quot;Since dialectical differences do not depend on subject matter&quot;).&#xD;&#xA;&#xD;&#xA;However, the answer to your question, &quot;can I use unsupervized learning and then supervized learning&quot; is No, if you are looking for supervized learning. If the results of an unsupervized learning algorithm are fed to a supervized learning algorithm, the net result is unsupervized --- there are still no grown-ups in the room. And the classification errors of the resulting process will contain error terms from both stages. You won't get the same performance as you would if you did a SVM with properly tagged training data. This doesn't mean you shouldn't use the method you propose ... it might still work well ... but it won't be a supervized learning algorithm." />
  <row Id="2751" PostHistoryTypeId="2" PostId="1063" RevisionGUID="69e2277b-d98e-433e-9875-b53436508461" CreationDate="2014-08-30T17:05:23.197" UserId="924" Text="If your data is numeric, try loading it into ELKI (Java). With the `NullAlgorithm` it will give you scatterplots, histograms and parallel coordinate plots. It's fast in reading the data; only the current Apache Batik-based visualization is slooow because it's using SVG. :-( I'm mostly using it &quot;headless&quot;.&#xD;&#xA;&#xD;&#xA;It also has classes for various statistics (including higher order moments on data streams), but I havn't seen them in the default UI yet." />
  <row Id="2752" PostHistoryTypeId="2" PostId="1064" RevisionGUID="2ed46644-e79d-4c38-b8da-2576453a5082" CreationDate="2014-08-30T18:14:05.577" UserId="924" Text="MapReduce is not used in searching. It was used a long time ago to build the index; but it is a batch processing framework, and most of the web does not change all the time, so the newer architectures are all *incremental* instead of batch oriented.&#xD;&#xA;&#xD;&#xA;Search in Google will largely work the same it works in Lucene and Elastic Search, except for a lot of fine tuned extra weighting and optimizations. But at the very heart, they will use some form of an **inverted index**. In other words, they do *not* search several terabytes when you enter a search query (even when it is not cached). They likely don't look at the actual documents at all. But they use a lookup table that lists which documents match your query term (with stemming, misspellings, synonyms etc. all preprocessed). They probably retrieve the *list* of the top 10000 documents for each word (10k integers - just a few kb!) and compute the best matches from that. Only if there aren't good matches in these lists, they expand to the next such blocks etc.&#xD;&#xA;&#xD;&#xA;Queries for common words can be easily cached; and via preprocessing you can build a list of the top 10k results and then rerank them according to the user profile.&#xD;&#xA;&#xD;&#xA;Rare terms on the other hand aren't much of a challenge either - one of the lists only contains a few matching documents, and you can immediately discard all others.&#xD;&#xA;&#xD;&#xA;I recommend reading this article:&#xD;&#xA;&#xD;&#xA;&gt; **The Anatomy of a Large-Scale Hypertextual Web Search Engine**&lt;br /&gt;&#xD;&#xA;&gt; Sergey Brin and Lawrence Page&lt;br /&gt;&#xD;&#xA;&gt; Computer Science Department, Stanford University, Stanford, CA 94305&lt;br /&gt;&#xD;&#xA;&gt; http://infolab.stanford.edu/~backrub/google.html&#xD;&#xA;&#xD;&#xA;And yes, that's the Google founders who wrote this. It's not the latest state, but it will already work at a pretty large scale." />
  <row Id="2753" PostHistoryTypeId="2" PostId="1065" RevisionGUID="6b0a47d4-dd83-4b62-8bd4-00142af38530" CreationDate="2014-08-30T18:36:07.490" UserId="924" Text="State of the art as in: used in practise or worked on in theory?&#xD;&#xA;&#xD;&#xA;APRIORI is used everywhere, except in developing new frequent itemset algorithms. It's easy to implement, and easy to reuse in very different domains. You'll find hundreds of APRIORI implementations of varying quality. And it's easy to get APRIORI wrong, actually.&#xD;&#xA;&#xD;&#xA;FPgrowth is much harder to implement, but also much more interesting. So from an academic point of view, everybody tries to improve FPgrowth - getting work based on APRIORI accepted will be very hard by now.&#xD;&#xA;&#xD;&#xA;If you have a good implementation, every algorithm has it's good and it's bad situations in my opinion. A good APRIORI implementation will *only* need to scan the database *k* times to find all frequent itemsets of length *k*. In particular if your data fits into main memory this is cheap. What can kill APRIORI is too many frequent 2-itemsets (in particular when you don't use a Trie and similar acceleration techniques etc.). It works best on large data with a low number of frequent itemsets.&#xD;&#xA;&#xD;&#xA;Eclat works on columns; but it needs to read each column much more often. There is some work on diffsets to reduce this work. If your data does not fit into main memory, Eclat suffers probably more than Apriori. By going depth first, it will also be able to return a first interesting result much earlier than Apriori, and you can use these results to adjust parameters; so you need less iterations to find good parameters. But by design, it cannot exploit pruning as neatly as Apriori did.&#xD;&#xA;&#xD;&#xA;FPGrowth compresses the data set into the tree. This works best when you have lots of duplicate records. You could probably reap of quite some gains for Apriori and Eclat too if you can presort your data and merge duplicates into weighted vectors. FPGrowth does this at an extreme level. The drawback is that the implementation is much harder; and once this tree does not fit into memory anymore it gets a mess to implement.&#xD;&#xA;&#xD;&#xA;As for performance results and benchmarks - don't trust them. There are so many things to implement incorrectly. Try 10 different implementations, and you get 10 very different performance results. In particular for APRIORI, I have the impression that most implementations are broken in the sense of missing some of the main contributions of APRIORI... and of those that have these parts right, the quality of optimizations varies a lot.&#xD;&#xA;&#xD;&#xA;There are actually even papers on how to implement these algorithms efficiently:&#xD;&#xA;&#xD;&#xA;&gt; Efficient Implementations of Apriori and Eclat.&lt;br /&gt; Christian Borgelt&lt;br /&gt;Workshop of Frequent Item Set Mining Implementations (FIMI 2003, Melbourne, FL, USA).&#xD;&#xA;&#xD;&#xA;You may also want to read these surveys on this domain:&#xD;&#xA;&#xD;&#xA;* &gt; Goethals, Bart. &quot;Survey on frequent pattern mining.&quot; Univ. of Helsinki (2003).&#xD;&#xA;&#xD;&#xA;* &gt; Ferenc Bodon, A Survey on Frequent Itemset Mining, Technical Report, Budapest University of Technology and Economic, 2006, &#xD;&#xA;&#xD;&#xA;* &gt; Frequent Item Set Mining&lt;br /&gt;Christian Borgelt&lt;br /&gt;Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 2(6):437-456. 2012&#xD;&#xA;" />
  <row Id="2754" PostHistoryTypeId="5" PostId="1064" RevisionGUID="bdf8db39-5e85-474e-b8a2-145312034e1d" CreationDate="2014-08-30T18:40:02.403" UserId="924" Comment="added 422 characters in body" Text="MapReduce is not used in searching. It was used a long time ago to build the index; but it is a batch processing framework, and most of the web does not change all the time, so the newer architectures are all *incremental* instead of batch oriented.&#xD;&#xA;&#xD;&#xA;Search in Google will largely work the same it works in Lucene and Elastic Search, except for a lot of fine tuned extra weighting and optimizations. But at the very heart, they will use some form of an **inverted index**. In other words, they do *not* search several terabytes when you enter a search query (even when it is not cached). They likely don't look at the actual documents at all. But they use a lookup table that lists which documents match your query term (with stemming, misspellings, synonyms etc. all preprocessed). They probably retrieve the *list* of the top 10000 documents for each word (10k integers - just a few kb!) and compute the best matches from that. Only if there aren't good matches in these lists, they expand to the next such blocks etc.&#xD;&#xA;&#xD;&#xA;Queries for common words can be easily cached; and via preprocessing you can build a list of the top 10k results and then rerank them according to the user profile. There is nothing to be gained by computing an &quot;exact&quot; answer, too. Looking at the top 10k results is likely enough; there is no correct answer; and if a better result somewhere at position 10001 is missed, nobody will know or notice (or care). It likely was already ranked down in preprocessing and would not have made it into the top 10 that is presented to the user at the end (or the top 3, the user actually looks at)&#xD;&#xA;&#xD;&#xA;Rare terms on the other hand aren't much of a challenge either - one of the lists only contains a few matching documents, and you can immediately discard all others.&#xD;&#xA;&#xD;&#xA;I recommend reading this article:&#xD;&#xA;&#xD;&#xA;&gt; **The Anatomy of a Large-Scale Hypertextual Web Search Engine**&lt;br /&gt;&#xD;&#xA;&gt; Sergey Brin and Lawrence Page&lt;br /&gt;&#xD;&#xA;&gt; Computer Science Department, Stanford University, Stanford, CA 94305&lt;br /&gt;&#xD;&#xA;&gt; http://infolab.stanford.edu/~backrub/google.html&#xD;&#xA;&#xD;&#xA;And yes, that's the Google founders who wrote this. It's not the latest state, but it will already work at a pretty large scale." />
  <row Id="2755" PostHistoryTypeId="2" PostId="1066" RevisionGUID="6fdae315-8e4f-4584-bbe5-b90baf947085" CreationDate="2014-09-01T00:01:58.917" UserId="3152" Text="Perhaps you are looking to quantify the amount of filespace used by a specific subset of data that we will label as &quot;academic publications.&quot; &#xD;&#xA;&#xD;&#xA;Well, to estimate, you could find stats on how many publications are housed at all the leading libraries (JSTOR, EBSCO, AcademicHost, etc) and then get the mean average size of each. Multiply that by the number of articles and whamo, you've got yourself an estimate.&#xD;&#xA;&#xD;&#xA;Here's the problem, though: PDF files store the text from string `s` differently (in size) than, say, a text document stores that same string. Likewise, a compressed JPEG will store an amount of information `i` differently than a non-compressed JPEG. So you see we could have two of the same articles containing the same information `i` but taking up different amounts of memory `m`.&#xD;&#xA;&#xD;&#xA;Are you looking to get a wordcount on the amount of scientific literature?&#xD;&#xA;&#xD;&#xA;Are you looking to get an approximation of file system space used to store all academically published content in the world? " />
  <row Id="2756" PostHistoryTypeId="2" PostId="1067" RevisionGUID="dfcbac7f-32ab-49ef-974f-b006c4bdb98f" CreationDate="2014-09-01T00:17:56.070" UserId="3153" Text="&#xD;&#xA;It is likely to be very hard to draw any conclusion if you are training with only 10 input samples.  With more data, your diagnosis that the model is predicting lagged values would have more plausibility.  As it stands, it seems pretty likely that your model is just saying that the last observed value is pretty close to correct.  This isn't the same as a real lag model, but it is a very reasonable thing to guess if you haven't seen enough data.&#xD;&#xA;&#xD;&#xA;" />
  <row Id="2758" PostHistoryTypeId="2" PostId="1068" RevisionGUID="5c506027-97cf-4af7-aa90-bd8b60283d08" CreationDate="2014-09-01T08:10:56.967" UserId="3150" Text="I'd have a closer look at one of Apache Spark's modules: [MLlib][1].&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://spark.apache.org/mllib/ &quot;MLlib&quot;" />
  <row Id="2759" PostHistoryTypeId="2" PostId="1069" RevisionGUID="499fd9ac-ea9d-425c-bd06-e48df968385e" CreationDate="2014-09-02T03:24:12.793" UserId="3169" Text="I am trying to evaluate and compare several different machine learning models built with different parameters (i.e. downsampling, outlier removal) and different classifiers (i.e. Bayes Net, SVM, Decision Tree).&#xD;&#xA;&#xD;&#xA;I am performing a type of cross validation where I randomly select 67% of the data for use in the training set and 33% of the data for use in the testing set. I perform this for several iterations, say, 20.&#xD;&#xA;&#xD;&#xA;Now, from each iteration I am able to generate a confusion matrix and compute a kappa. My question is, what are some ways to aggregate these across the iterations? I am also interested in aggregating accuracy and expected accuracy, among other things.&#xD;&#xA;&#xD;&#xA;For the kappa, accuracy, and expected accuracy, I have just been taking the average up to this point. One of the problems is that when I recompute kappa with the aggregated average and expected average, it is not the same with the aggregated kappa.&#xD;&#xA;&#xD;&#xA;For the confusion matrix, I have been first normalizing the confusion matrix from each iteration and then averaging them, in an attempt to avoid an issue of confusion matrices with different numbers of total cases (which is possible with my cross validation scheme).&#xD;&#xA;&#xD;&#xA;When I recompute the kappa from this aggregated confusion matrix, it is also different from the previous two.&#xD;&#xA;&#xD;&#xA;Which one is most correct? Is there another way of computing an average kappa that is more correct?&#xD;&#xA;&#xD;&#xA;Thanks, and if more concrete examples are in order to illustrate my question please let me know." />
  <row Id="2760" PostHistoryTypeId="1" PostId="1069" RevisionGUID="499fd9ac-ea9d-425c-bd06-e48df968385e" CreationDate="2014-09-02T03:24:12.793" UserId="3169" Text="Kappa From Combined Confusion Matrices" />
  <row Id="2761" PostHistoryTypeId="3" PostId="1069" RevisionGUID="499fd9ac-ea9d-425c-bd06-e48df968385e" CreationDate="2014-09-02T03:24:12.793" UserId="3169" Text="&lt;machine-learning&gt;&lt;confusion-matrix&gt;" />
  <row Id="2762" PostHistoryTypeId="2" PostId="1070" RevisionGUID="1ba0f838-8e37-4bbb-9a49-a7cd874b9058" CreationDate="2014-09-02T08:47:38.737" UserId="2511" Text="I am interested in knowing the differences in **functionality** between SAP HANA and Exasol. Since this is a bit of an open ended question let me be clear. I am not interested in people debating which is &quot;better&quot; or faster. I am only interested in what each was designed to do so please keep your opinions out of it. I suspect it is a bit like comparing HANA to Oracle Exalytics where there is some overlap but the functionality goals are different. " />
  <row Id="2763" PostHistoryTypeId="1" PostId="1070" RevisionGUID="1ba0f838-8e37-4bbb-9a49-a7cd874b9058" CreationDate="2014-09-02T08:47:38.737" UserId="2511" Text="SAP HANA vs Exasol" />
  <row Id="2764" PostHistoryTypeId="3" PostId="1070" RevisionGUID="1ba0f838-8e37-4bbb-9a49-a7cd874b9058" CreationDate="2014-09-02T08:47:38.737" UserId="2511" Text="&lt;bigdata&gt;" />
  <row Id="2765" PostHistoryTypeId="2" PostId="1071" RevisionGUID="76f25cb6-17bd-466b-b7fc-1a2b7a4cea97" CreationDate="2014-09-02T09:48:08.150" UserId="3178" Text="How is the concept of data different for different disciplines? Obviously, for physicists and sociologists, &quot;data&quot; is something different." />
  <row Id="2766" PostHistoryTypeId="1" PostId="1071" RevisionGUID="76f25cb6-17bd-466b-b7fc-1a2b7a4cea97" CreationDate="2014-09-02T09:48:08.150" UserId="3178" Text="How is the concept of data different for different disciplines?" />
  <row Id="2767" PostHistoryTypeId="3" PostId="1071" RevisionGUID="76f25cb6-17bd-466b-b7fc-1a2b7a4cea97" CreationDate="2014-09-02T09:48:08.150" UserId="3178" Text="&lt;definitions&gt;" />
  <row Id="2769" PostHistoryTypeId="2" PostId="1072" RevisionGUID="fead75fd-a6f6-489a-9b00-a17b946fdf9c" CreationDate="2014-09-02T14:01:07.407" UserId="3181" Text="There's not an enormous difference between what you can do with the two databases, it's more a question of the focus and the way the functionality is implemented and that's where it becomes difficult to explain without using words like &quot;better&quot; and &quot;faster&quot; (and for sure words like &quot;cheaper&quot;) &#xD;&#xA;&#xD;&#xA;EXASOL was designed for speed and ease of use with Analytical processing and is designed to run on clusters of commodity hardware. SAP is a more complex, aims to do more than &quot;just&quot; Analytical processing and runs only on a range of &quot;approved&quot; hardware.&#xD;&#xA;&#xD;&#xA;What type of differences did you have in mind ?" />
  <row Id="2770" PostHistoryTypeId="2" PostId="1073" RevisionGUID="da6c98d6-fae0-4fb6-a2d9-0095624bccf5" CreationDate="2014-09-02T19:17:43.210" UserId="802" Text="I am looking for packages (either in python, R, or a standalone package) to perform online learning to predict stock data.&#xD;&#xA;&#xD;&#xA;I have found and read about Vowpal Wabbit (https://github.com/JohnLangford/vowpal_wabbit/wiki),&#xD;&#xA;which seems to be quite promising but I am wondering if there are any other packages out there.&#xD;&#xA;&#xD;&#xA;Thanks in advance." />
  <row Id="2771" PostHistoryTypeId="1" PostId="1073" RevisionGUID="da6c98d6-fae0-4fb6-a2d9-0095624bccf5" CreationDate="2014-09-02T19:17:43.210" UserId="802" Text="Libraries for Online Machine Learning" />
  <row Id="2772" PostHistoryTypeId="3" PostId="1073" RevisionGUID="da6c98d6-fae0-4fb6-a2d9-0095624bccf5" CreationDate="2014-09-02T19:17:43.210" UserId="802" Text="&lt;machine-learning&gt;&lt;online-learning&gt;" />
  <row Id="2773" PostHistoryTypeId="2" PostId="1074" RevisionGUID="a50bb5b2-6eb4-4bc0-8a4a-0f2de27a1ae2" CreationDate="2014-09-02T19:29:07.490" UserId="676" Text="In R's ksvm the polynomial kernel is defined as:&#xD;&#xA;&#xD;&#xA;(scale * crossprod(x, y) + offset)^degree&#xD;&#xA;&#xD;&#xA;How do the scale and offset parameters affect the model and what range should they be in? (intuitively please)&#xD;&#xA;&#xD;&#xA;Are the scale and offset for numeric stability only (that's what it looks like to me), or do they influence prediction accuracy as well?&#xD;&#xA;&#xD;&#xA;Can good values for scale and offset be calculated/estimated when the data is known or is a grid search required? The caret package always sets the offset to 1, but it does a grid search for scale. (Why) is an offset of 1 a good value?&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;PS.: Wikipedia didn't really help my understanding:&#xD;&#xA;&#xD;&#xA;&gt; For degree-d polynomials, the polynomial kernel is defined as&#xD;&#xA;&gt; &#xD;&#xA;&gt; ![][1]&#xD;&#xA;&gt; &#xD;&#xA;&gt; where x and y are vectors in the input space, i.e. vectors of features&#xD;&#xA;&gt; computed from training or test samples, ![][2] **is a constant trading**&#xD;&#xA;&gt; **off the influence of higher-order versus lower-order terms** in the&#xD;&#xA;&gt; polynomial. When ![][3], the kernel is called homogeneous.(**A further**&#xD;&#xA;&gt; **generalized polykernel divides ![][4] by a user-specified scalar**&#xD;&#xA;&gt; **parameter ![][5].**)&#xD;&#xA;&#xD;&#xA;Neither did ?polydot's explanation in R's help system:&#xD;&#xA;&#xD;&#xA;&gt; **scale**: The scaling parameter of the polynomial and tangent kernel is a&#xD;&#xA;&gt; convenient way of normalizing patterns (&lt;-!?) without the need to modify the&#xD;&#xA;&gt; data itself&#xD;&#xA;&gt; &#xD;&#xA;&gt; **offset**: The offset used in a polynomial or hyperbolic tangent kernel (&lt;- lol thanks)&#xD;&#xA;&#xD;&#xA;  [1]: http://i.stack.imgur.com/3stCR.png&#xD;&#xA;  [2]: http://i.stack.imgur.com/34dwi.png&#xD;&#xA;  [3]: http://i.stack.imgur.com/79cDG.png&#xD;&#xA;  [4]: http://i.stack.imgur.com/Ps0qt.png&#xD;&#xA;  [5]: http://i.stack.imgur.com/sjfS1.png&#xD;&#xA;" />
  <row Id="2774" PostHistoryTypeId="1" PostId="1074" RevisionGUID="a50bb5b2-6eb4-4bc0-8a4a-0f2de27a1ae2" CreationDate="2014-09-02T19:29:07.490" UserId="676" Text="Polynomial Kernel Parameters in SVMs" />
  <row Id="2775" PostHistoryTypeId="3" PostId="1074" RevisionGUID="a50bb5b2-6eb4-4bc0-8a4a-0f2de27a1ae2" CreationDate="2014-09-02T19:29:07.490" UserId="676" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;r&gt;&lt;svm&gt;" />
  <row Id="2776" PostHistoryTypeId="5" PostId="1043" RevisionGUID="e3e87dbb-2343-44e6-ab5c-fe473a8cde66" CreationDate="2014-09-02T20:25:23.177" UserId="2511" Comment="added 243 characters in body" Text="I am trying to find out if there have been any A/B tests done by any of the various services with messaging to see if showing when the other user is typing helps your KPIs. I know there are a lot of forum questions about how to disable this feature so the question arises if it is worth the extra effort to add.&#xD;&#xA;&#xD;&#xA;If you do not understand the messaging feature I am referring to there is some background information [here][1]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.slate.com/articles/technology/bitwise/2014/02/typing_indicator_in_chat_i_built_it_and_i_m_not_sorry.html" />
  <row Id="2778" PostHistoryTypeId="2" PostId="1075" RevisionGUID="bde86dd6-cf78-4216-940c-a3f95e062926" CreationDate="2014-09-04T18:13:57.343" UserId="3203" Text="Background:&#xD;&#xA;I run a product that compares sets of data (data matching and data reconciliation).&#xD;&#xA;To get the result we need to compare each row in a data set with every N rows on the opposing data set&#xD;&#xA;Now however we get sets of up to 300 000 rows of data in each set to compare and are getting 90 Billion computations to handle.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;So my question is this:&#xD;&#xA;Even though we dont have the data volumes to use Hadoop, we have the computational need for something distributed. Is Hadoop a good choice for us?" />
  <row Id="2779" PostHistoryTypeId="1" PostId="1075" RevisionGUID="bde86dd6-cf78-4216-940c-a3f95e062926" CreationDate="2014-09-04T18:13:57.343" UserId="3203" Text="Hadoop for grid computing" />
  <row Id="2780" PostHistoryTypeId="3" PostId="1075" RevisionGUID="bde86dd6-cf78-4216-940c-a3f95e062926" CreationDate="2014-09-04T18:13:57.343" UserId="3203" Text="&lt;hadoop&gt;" />
  <row Id="2781" PostHistoryTypeId="2" PostId="1076" RevisionGUID="4d502329-0d0c-43bd-b4a6-e2d63dde961b" CreationDate="2014-09-05T06:17:28.533" UserId="3211" Text="You could look at scikit-learn and orange module in python.&#xD;&#xA;Scikit-learn has a SGD classifier and regressor that could do a partial fit data in case of online learning.&#xD;&#xA;In R take a look at caret package &#xD;&#xA;" />
  <row Id="2782" PostHistoryTypeId="2" PostId="1077" RevisionGUID="f1865d44-dcdf-496f-b346-e0d91114f0ef" CreationDate="2014-09-05T11:00:22.053" UserId="3213" Text="Your job seems like a map-reduce job and hence might be good for Hadoop. Hadoop has a zoo of an ecosystem though. &#xD;&#xA;&#xD;&#xA;Hadoop is a distributed file system. It distributes data on a cluster and because this data is split up it can be analysed in parallel. Out of the box, Hadoop allows you to write map reduce jobs on the platform and this is why it might help with your problem.&#xD;&#xA;&#xD;&#xA;The following technologies work on Hadoop: &#xD;&#xA;&#xD;&#xA;- If the data can be represented in a table format you might want to check out technologies like [hive](https://hive.apache.org/) and [impala](http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html). Impala uses all the distributed memory across a cluster and is very performant while it allows you to still work with a table structure. &#xD;&#xA;- A more new, but promising alternative might also be [spark](https://spark.apache.org/) which allows for more iterative procedures to be run on the cluster. &#xD;&#xA;&#xD;&#xA;Don't underestimate the amount of time setting up and the amount of time needed to understand Hadoop." />
  <row Id="2783" PostHistoryTypeId="2" PostId="1078" RevisionGUID="ea4c7fb0-c9ee-4a33-8d4d-35404d8d2895" CreationDate="2014-09-05T12:08:11.347" UserId="3215" Text="I'm going to classify unstructured text documents, namely web sites of unknown structure. The number of categories is highly limited (at this point I believe, that there are not going to be more than three categories). Any suggestions where to start?&#xD;&#xA;&#xD;&#xA;I'm not sure if the &quot;bag of words&quot; approach would be feasible. Later, I can add another classification stage based on document structure (so finally decision trees are going to be utilised also).&#xD;&#xA;&#xD;&#xA;I can say, that I am somehow familiar with Mahout and Hadoop, therefore Java is preferred. If needed, I can switch to Scala and/or Spark engine (the ML library)." />
  <row Id="2784" PostHistoryTypeId="1" PostId="1078" RevisionGUID="ea4c7fb0-c9ee-4a33-8d4d-35404d8d2895" CreationDate="2014-09-05T12:08:11.347" UserId="3215" Text="Unstructured text classification" />
  <row Id="2785" PostHistoryTypeId="3" PostId="1078" RevisionGUID="ea4c7fb0-c9ee-4a33-8d4d-35404d8d2895" CreationDate="2014-09-05T12:08:11.347" UserId="3215" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;text-mining&gt;" />
  <row Id="2786" PostHistoryTypeId="2" PostId="1079" RevisionGUID="3f3e8b30-558f-4783-bbdd-aa7a355f3b6f" CreationDate="2014-09-05T14:47:52.127" UserId="2920" Text="I'm searching for data sets for evaluating text retrieval quality.&#xD;&#xA;&#xD;&#xA;TF-IDF is a popular similarity measure, but is it the best choice? And which *variant* is the best choice? [Lucenes Scoring][1] for example uses IDF^2, and IDF defined as 1+log(numdocs/(docFreq+1)). TF in lucene is defined as sqrt(frequency)...&#xD;&#xA;&#xD;&#xA;Many more variants exist, including [Okapi BM25][2], which is used by the [Xapian search engine][3]...&#xD;&#xA;&#xD;&#xA;I'd like to study the different variants, and I'm looking for **evaluation data sets**. Thanks!&#xD;&#xA;&#xD;&#xA;  [1]: https://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html&#xD;&#xA;  [2]: https://en.wikipedia.org/wiki/Okapi_BM25&#xD;&#xA;  [3]: http://xapian.org/docs/bm25.html" />
  <row Id="2787" PostHistoryTypeId="1" PostId="1079" RevisionGUID="3f3e8b30-558f-4783-bbdd-aa7a355f3b6f" CreationDate="2014-09-05T14:47:52.127" UserId="2920" Text="Data sets for evaluating text retrieval quality" />
  <row Id="2788" PostHistoryTypeId="3" PostId="1079" RevisionGUID="3f3e8b30-558f-4783-bbdd-aa7a355f3b6f" CreationDate="2014-09-05T14:47:52.127" UserId="2920" Text="&lt;dataset&gt;&lt;text-mining&gt;&lt;similarity&gt;&lt;information-retrieval&gt;" />
  <row Id="2789" PostHistoryTypeId="2" PostId="1080" RevisionGUID="e689cc79-8f9e-41ec-a808-418584b88302" CreationDate="2014-09-05T16:34:55.170" UserId="3216" Text="I was curious about the ANOVA RBF kernel provided by kernlab package available in R.&#xD;&#xA;&#xD;&#xA;I tested it with a numeric dataSet of 34 input variables and one output variable. For each variable I have 700 different values. Comparing with other kernels, I got very bad results with this kernel.&#xD;&#xA;For example using the simple RBF kernel I could predict with 0,88 R2 however with the anova RBF I could only get 0,33 R2.&#xD;&#xA;I thought that ANOVA RBF would be a very good kernel. Any thoughts? Thanks&#xD;&#xA;&#xD;&#xA;The code is as follows:&#xD;&#xA;&#xD;&#xA;    set.seed(100) #use the same seed to train different models&#xD;&#xA;    svrFitanovaacv &lt;- train(R ~ .,&#xD;&#xA;                           data = trainSet,&#xD;&#xA;                           method = SVManova,&#xD;&#xA;                           preProc = c(&quot;center&quot;, &quot;scale&quot;),&#xD;&#xA;                           trControl = ctrl, tuneLength = 10) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = &quot;ROC&quot;&#xD;&#xA;&#xD;&#xA;**define custom model in caret package:**&#xD;&#xA;&#xD;&#xA;    library(caret)&#xD;&#xA;    #RBF ANOVA KERNEL&#xD;&#xA;    SVManova &lt;- list(type = &quot;Regression&quot;, library = &quot;kernlab&quot;, loop = NULL)&#xD;&#xA;    prmanova &lt;- data.frame(parameter = c(&quot;C&quot;, &quot;sigma&quot;, &quot;degree&quot;, &quot;epsilon&quot;),&#xD;&#xA;                         class = rep(&quot;numeric&quot;, 4),&#xD;&#xA;                         label = c(&quot;Cost&quot;, &quot;Sigma&quot;, &quot;Degree&quot;, &quot;Epsilon&quot;))&#xD;&#xA;    SVManova$parameters &lt;- prmanova&#xD;&#xA;    svmGridanova &lt;- function(x, y, len = NULL) {&#xD;&#xA;    library(kernlab)&#xD;&#xA;    sigmas &lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)&#xD;&#xA;    expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,&#xD;&#xA;                C = 2^(-40:len), degree = 1:2) # len = tuneLength in train&#xD;&#xA;    }&#xD;&#xA;    SVManova$grid &lt;- svmGridanova&#xD;&#xA;    svmFitanova &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) {&#xD;&#xA;      ksvm(x = as.matrix(x), y = y,&#xD;&#xA;           kernel = &quot;anovadot&quot;,&#xD;&#xA;           kpar = list(sigma = param$sigma, degree = param$degree),&#xD;&#xA;           C = param$C, epsilon = param$epsilon,&#xD;&#xA;           prob.model = classProbs,&#xD;&#xA;           ...) #default type = &quot;eps-svr&quot;&#xD;&#xA;    }&#xD;&#xA;    SVManova$fit &lt;- svmFitanova&#xD;&#xA;    svmPredanova &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)&#xD;&#xA;      predict(modelFit, newdata)&#xD;&#xA;    SVManova$predict &lt;- svmPredanova&#xD;&#xA;    svmProb &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)&#xD;&#xA;      predict(modelFit, newdata, type=&quot;probabilities&quot;)&#xD;&#xA;    SVManova$prob &lt;- svmProb&#xD;&#xA;    svmSortanova &lt;- function(x) x[order(x$C), ]&#xD;&#xA;    SVManova$sort &lt;- svmSortanova&#xD;&#xA;&#xD;&#xA;**load data:**&#xD;&#xA;&#xD;&#xA;    dataA2&lt;-read.csv(&quot;C:/results/A2.txt&quot;,header = TRUE, &#xD;&#xA;                                 blank.lines.skip = TRUE,sep = &quot;,&quot;)&#xD;&#xA;    set.seed(1)&#xD;&#xA;    inTrainSet &lt;- createDataPartition(dataA2$R, p = 0.75, list = FALSE) #[[1]]&#xD;&#xA;    trainSet &lt;- dataA2[inTrainSet,]&#xD;&#xA;    testSet &lt;- dataA2[-inTrainSet,]&#xD;&#xA;    #-----------------------------------------------------------------------------&#xD;&#xA;    #K-folds resampling method for fitting svr&#xD;&#xA;    ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10,&#xD;&#xA;                         allowParallel = TRUE) #10 separate 10-fold cross-validations&#xD;&#xA;&#xD;&#xA;**link to data:**&#xD;&#xA;&#xD;&#xA;    wuala.com/jpcgandre/Documents/Data%20SVR/?key=BOD9NTINzRHG" />
  <row Id="2790" PostHistoryTypeId="1" PostId="1080" RevisionGUID="e689cc79-8f9e-41ec-a808-418584b88302" CreationDate="2014-09-05T16:34:55.170" UserId="3216" Text="ANOVA RBF kernel returns very poor results" />
  <row Id="2791" PostHistoryTypeId="3" PostId="1080" RevisionGUID="e689cc79-8f9e-41ec-a808-418584b88302" CreationDate="2014-09-05T16:34:55.170" UserId="3216" Text="&lt;machine-learning&gt;&lt;r&gt;" />
  <row Id="2792" PostHistoryTypeId="2" PostId="1081" RevisionGUID="34f9d35a-434a-4770-9f0f-a58a9e6fa616" CreationDate="2014-09-05T16:36:03.227" UserId="3152" Text="Data is, at it's most basic reduction, a raw element of something. Data is a raw &quot;thing&quot; that exists in any form from which we can analyze it and construct intelligence. When I was an Intelligence Analyst, we used to define data as &quot;anything and everything that could be used to construct a hypothesis.&quot; &#xD;&#xA;&#xD;&#xA;Thus, data for any discipline is interchangeable; as a sociologist, I have a vector of discrete variables indicating ethnicity, as an economist I have a vector with housing prices, and as an anthropologist I have a vector of tablet names used in some long-gone civilization. &#xD;&#xA;&#xD;&#xA;Data is data." />
  <row Id="2793" PostHistoryTypeId="2" PostId="1082" RevisionGUID="84e8c21e-115f-4ffd-9c2b-9e5bc58152b3" CreationDate="2014-09-05T19:13:39.993" UserId="819" Text="Here are a couple of really great open source software packages for text classification that should help get you started:&#xD;&#xA;&#xD;&#xA; - [MALLET](http://mallet.cs.umass.edu/) is a CPL-licensed Java-based machine learning toolkit built by UMass for working with text data. It includes implementations of several [classification](http://mallet.cs.umass.edu/classification.php) algorithms (e.g., naïve Bayes, maximum entropy, decision trees).&#xD;&#xA; - The [Stanford Classifier](http://nlp.stanford.edu/software/classifier.shtml) from the Stanford NLP Group is a GPL-licensed Java implementation of a maximum entropy classifier designed to work with text data." />
  <row Id="2799" PostHistoryTypeId="2" PostId="1084" RevisionGUID="936e16a5-1155-4728-923d-d38c4cac8477" CreationDate="2014-09-07T01:08:04.330" UserId="1279" Text="Let's work it out from the ground up. Classification (also known as categorization) is an example of **supervised learning**. In supervised learning you have: &#xD;&#xA;&#xD;&#xA; * **model** - something that approximates internal structure in your data, enabling you to reason about it and make useful predictions (e.g. predict class of an object); normally model has parameters that you want to &quot;learn&quot;&#xD;&#xA; * **training** and **testing datasets** - sets of objects that you use for training your model (finding good values for parameters) and further evaluating &#xD;&#xA; * **training** and **classification algorithms** - first describes how to learn model from training dataset, second shows how to derive class of a new object given trained model&#xD;&#xA;&#xD;&#xA;Now let's take a simple case of spam classification. Your training dataset is a corpus of emails + corresponding labels - &quot;spam&quot; or &quot;not spam&quot;. Testing dataset has the same structure, but made from some independent emails (normally one just splits his dataset and makes, say, 9/10 of it to be used for training and 1/10 - for testing). One way to model emails is to represent each of them as a set (bag) of words. If we assume that words are independent of each other, we can use **Naive Bayes classifier**, that is, calculate prior probabilities for each word and each class (training algorithm) and then apply Bayes theorem to find posterior probability of a new document to belong to particular class. &#xD;&#xA;&#xD;&#xA;So, basically we have: &#xD;&#xA;&#xD;&#xA;    raw model + training set + training algorithm -&gt; trained model&#xD;&#xA;    trained model + classification algorithm + new object -&gt; object label&#xD;&#xA;&#xD;&#xA;Now note that we represented our objects (documents) as a bag of words. But is the only way? In fact, we can extract much more from raw text. For example, instead of words as is we can use their [stems or lemmas](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), throw out noisy [stop words](http://en.wikipedia.org/wiki/Stop_words), add [POS tags](http://nlp.stanford.edu/software/tagger.shtml) of words, extract [named entities](http://nlp.stanford.edu/software/CRF-NER.shtml) or even explore HTML structure of the document. In fact, more general representation of a document (and, in general, any object) is a **feature vector**. E.g. for text: &#xD;&#xA;&#xD;&#xA;    actor, analogue, bad, burn, ..., NOUN, VERB, ADJ, ..., in_bold, ... | label&#xD;&#xA;        0,        0,   1,    1, ...,    5,    7,   2, ...,       2, ... | not spam&#xD;&#xA;        0,        1,   0,    0, ...,    3,   12,  10, ...,       0, ... | spam&#xD;&#xA;&#xD;&#xA;Here first row is a list of possible features and subsequent rows show how many times that feature occurs in a document. E.g. in first document there's no occurrences of word &quot;actor&quot;, 1 occurrence of word &quot;burn&quot;, 5 nouns, 2 adjectives and 2 pieces of text in bold. Last column corresponds to a resulting class label. &#xD;&#xA;&#xD;&#xA;Using feature vector you can incorporate any properties of your texts. Though finding good set of features may take some time. &#xD;&#xA;&#xD;&#xA;And what about model and algorithms? Are we bound to Naive Bayes. Not at all. **logistic regression**, **SVM**, **decision trees** - just to mention few popular classifiers. (Note, that we say &quot;classifier&quot; in most cases we mean model + corresponding algorithms for training and classification). &#xD;&#xA;&#xD;&#xA;As for implementation, you can divide task into 2 parts: &#xD;&#xA;&#xD;&#xA; 1. Features extraction - transforming raw texts into feature vectors. &#xD;&#xA; 2. Object classification - building and applying model.&#xD;&#xA;&#xD;&#xA;First point is well worked out in many [NLP libraries](http://stackoverflow.com/questions/4115526/natural-language-processing). Second is about machine learning, so, depending on your dataset, you can use either [Weka](http://www.cs.waikato.ac.nz/ml/weka/), or [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html). " />
  <row Id="2800" PostHistoryTypeId="2" PostId="1085" RevisionGUID="b4e04688-9fa2-455c-b0d6-a33c93707e1b" CreationDate="2014-09-07T08:02:39.670" UserId="1256" Text="If I understand your description correctly, hadoop seems a huge overhead, for the wrong problem. basically you just need a standard distributed architecture: don't you just have to pass pairs of rows - eg mpi.... ipython.parallel, ..." />
  <row Id="2801" PostHistoryTypeId="2" PostId="1086" RevisionGUID="386dea09-ffd8-4225-9029-d240d6dcbe97" CreationDate="2014-09-07T22:51:27.490" UserId="3232" Text="We currently have some ~500 bio-medical  documents each of some 1-2 MB . We want to use a non query based method to rank the documents in order of their unique content score. I'm calling it as &quot;unique content&quot; bcos our researchers want to know from which document to start reading . All the documents are of the same topic ,in bio medical world we know that there is always a lot of content overlap . So all we want to do is to arrange the documents in the order of their unique content.  &#xD;&#xA;&#xD;&#xA;Most Information Retrieval literature suggest query based ranking which dose not fit our need.  " />
  <row Id="2802" PostHistoryTypeId="1" PostId="1086" RevisionGUID="386dea09-ffd8-4225-9029-d240d6dcbe97" CreationDate="2014-09-07T22:51:27.490" UserId="3232" Text="Document Ranking - Non Query based" />
  <row Id="2803" PostHistoryTypeId="3" PostId="1086" RevisionGUID="386dea09-ffd8-4225-9029-d240d6dcbe97" CreationDate="2014-09-07T22:51:27.490" UserId="3232" Text="&lt;machine-learning&gt;&lt;data-mining&gt;&lt;text-mining&gt;&lt;information-retrieval&gt;" />
  <row Id="2804" PostHistoryTypeId="2" PostId="1087" RevisionGUID="e735cd70-e599-4433-97a6-ad29d8b83d65" CreationDate="2014-09-08T00:47:21.590" UserId="819" Text="Here's a simple initial approach to try:&#xD;&#xA;&#xD;&#xA; 1. Calculate the [TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) score of each word in each document.&#xD;&#xA; 2. Sort the documents by the average TF-IDF score of their words.&#xD;&#xA; 3. The higher the average TF-IDF score, the more unique a document is with respect to the rest of the collection.&#xD;&#xA;&#xD;&#xA;You might also try a clustering-based approach where you look for outliers, or perhaps something with the [Jaccard index](http://en.wikipedia.org/wiki/Jaccard_index) using a bag-of-words model." />
  <row Id="2805" PostHistoryTypeId="2" PostId="1088" RevisionGUID="872e5103-0d50-41d7-8a04-fb983204aa5c" CreationDate="2014-09-08T02:45:42.653" UserId="609" Text="Topic Modeling would be a very appropriate method for your problem.  Topic Models are a form of unsupervised learning/discovery, where a specified (or discovered) number of topics are defined by a list of words that have a high probability of appearing together. In a separate step, you can label each topic using subject matter experts, but for your purposes this isn't necessary since you are only interested in getting to three clusters.&#xD;&#xA;&#xD;&#xA;You treat each document as a bag of words, and pre-process to remove stop words, etc.  With the simplest methods, you pre-specify the number of topics.  In your case, you could either specify &quot;3&quot;, which is your fixed limit on categories, or pick a larger number of topics (between 10 and 100), and then in a separate step, form three clusters for documents with common emphasis on topics. K-means or other clustering methods could be used.  (I'd recommend the latter approach)&#xD;&#xA;&#xD;&#xA;You don't need to code topic modeling software from scratch. Here's a web page with many resources, including software libraries/packages: http://www.cs.princeton.edu/~blei/topicmodeling.html&#xD;&#xA;&#xD;&#xA;None are in Java, but there are ways to run C++ and Python under Java." />
  <row Id="2806" PostHistoryTypeId="2" PostId="1089" RevisionGUID="553f1fe8-a71d-47ff-b119-049257fbfedd" CreationDate="2014-09-08T03:07:07.413" UserId="609" Text="You could use Topic Modeling as described in this paper:&#xD;&#xA;http://faculty.chicagobooth.edu/workshops/orgs-markets/pdf/KaplanSwordWin2014.pdf&#xD;&#xA;&#xD;&#xA;They performed Topic Modeling on abstracts of patents (limited to 150 words).  They identified papers as &quot;novel&quot; if they were the first to introduce a topic, and measured degree of novelty by how many papers in the following year used the same topic. (Read the paper for details).&#xD;&#xA;&#xD;&#xA;I suggest that you follow their lead and only process paper abstracts.  Processing the body of each paper might reveal some novelty that the abstract does not, but you also run the risk of having much more noise in your topic model (i.e. extraneous topics, extraneous words).&#xD;&#xA;&#xD;&#xA;While you say that all 500 papers are on the same &quot;topic&quot;, it's probably safer to say that they are all on the same &quot;theme&quot; or in the same &quot;sub-category&quot; of Bio-medicine. Topic modeling permits decomposition of the &quot;theme&quot; into &quot;topics&quot;.&#xD;&#xA;&#xD;&#xA;The good news is that there are plenty of good packages/libraries for Topic Modeling.  You still have to do preprocessing, but you don't have to code the algorithms yourself.  See this page for many resources:&#xD;&#xA;http://www.cs.princeton.edu/~blei/topicmodeling.html&#xD;&#xA;" />
  <row Id="2807" PostHistoryTypeId="10" PostId="1043" RevisionGUID="904507c1-8a25-4600-bcab-87a6f6925031" CreationDate="2014-09-08T09:15:42.777" UserId="21" Comment="103" Text="{&quot;Voters&quot;:[{&quot;Id&quot;:471,&quot;DisplayName&quot;:&quot;Spacedman&quot;},{&quot;Id&quot;:21,&quot;DisplayName&quot;:&quot;Sean Owen&quot;}]}" />
  <row Id="2808" PostHistoryTypeId="2" PostId="1090" RevisionGUID="05ce02e9-1ad4-454c-885e-ff69e9ec5f49" CreationDate="2014-09-08T14:57:11.223" UserId="387" Text="It may be unlikely that anyone knows this but I have a specific question about Freebase.  Here is the Freebase page from the [Ford Taurus automotive model][1] .  It has a property called &quot;Related Models&quot;.  Does anyone know how this list of related models was compiled.  What is the similarity measure that they use?  I don't think it is only about other wikipedia pages that link to or from this page.  Alternatively, it may be that this is user generated.  Does anyone know for sure?&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.freebase.com/m/014_d3" />
  <row Id="2809" PostHistoryTypeId="1" PostId="1090" RevisionGUID="05ce02e9-1ad4-454c-885e-ff69e9ec5f49" CreationDate="2014-09-08T14:57:11.223" UserId="387" Text="Freebase Related Models" />
  <row Id="2810" PostHistoryTypeId="3" PostId="1090" RevisionGUID="05ce02e9-1ad4-454c-885e-ff69e9ec5f49" CreationDate="2014-09-08T14:57:11.223" UserId="387" Text="&lt;dataset&gt;" />
  <row Id="2811" PostHistoryTypeId="2" PostId="1091" RevisionGUID="dbaab81d-72b5-40e5-a1c5-5f4f1e88eeb2" CreationDate="2014-09-08T19:33:00.253" UserId="3244" Text="What is the best technology to be used to create my custom bag of words with N-grams to apply to. I want to know a functionality that can be achieved over GUI. I cannot use spot fire as it is not available in the organization. Though i can get SAP Hana or R-hadoop. But R-hadoop is bit challenging, any suggessions." />
  <row Id="2812" PostHistoryTypeId="1" PostId="1091" RevisionGUID="dbaab81d-72b5-40e5-a1c5-5f4f1e88eeb2" CreationDate="2014-09-08T19:33:00.253" UserId="3244" Text="Creating Bag of words" />
  <row Id="2813" PostHistoryTypeId="3" PostId="1091" RevisionGUID="dbaab81d-72b5-40e5-a1c5-5f4f1e88eeb2" CreationDate="2014-09-08T19:33:00.253" UserId="3244" Text="&lt;bigdata&gt;&lt;text-mining&gt;" />
  <row Id="2814" PostHistoryTypeId="2" PostId="1092" RevisionGUID="1bea11d2-a90f-455e-8d84-a2ba0ff85026" CreationDate="2014-09-08T21:25:26.183" UserId="2487" Text="Are there any machine learning libraries for Ruby that are relatively complete (including a wide variety of learners for supervised and unsupervised learning), robustly tested, and well-documented? I love Python's [scikit-learn][1] for its incredible documentation, but a client would prefer to write the code in Ruby since that's what they're familiar with.&#xD;&#xA;&#xD;&#xA;Ideally I am looking for a library or set of libraries which, like scikit and numpy, can implement a wide variety of data structures like sparse matrices, as well as learners.&#xD;&#xA;&#xD;&#xA;Some examples of things we'll need to do are binary classification using SVMs, and  implementing bag of words models which we hope to concatenate with arbitrary numeric data, as described in [this][2] Stackoverflow post.&#xD;&#xA;&#xD;&#xA;Thanks in advance!&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/&#xD;&#xA;  [2]: http://stackoverflow.com/q/20106940/1435804" />
  <row Id="2815" PostHistoryTypeId="1" PostId="1092" RevisionGUID="1bea11d2-a90f-455e-8d84-a2ba0ff85026" CreationDate="2014-09-08T21:25:26.183" UserId="2487" Text="Machine learning libraries for Ruby" />
  <row Id="2816" PostHistoryTypeId="3" PostId="1092" RevisionGUID="1bea11d2-a90f-455e-8d84-a2ba0ff85026" CreationDate="2014-09-08T21:25:26.183" UserId="2487" Text="&lt;machine-learning&gt;" />
  <row Id="2820" PostHistoryTypeId="2" PostId="1094" RevisionGUID="25902b9b-c963-46df-a5ad-5fd6f0ad0d2b" CreationDate="2014-09-09T06:33:00.730" UserId="3108" Text="**Problem**&#xD;&#xA;&#xD;&#xA;For my machine learning task, I create a set of predictors.&#xD;&#xA;Predictors come in &quot;bundles&quot; - multi-dimensional measurements (3 or 4 - dimensional in my case).&#xD;&#xA;&#xD;&#xA;The hole &quot;bundle&quot; makes sense only if it has been measured, and taken all together.&#xD;&#xA;&#xD;&#xA;The problem is, different 'bundles' of predictors can be measured only for small part of the sample, and those parts don't necessary intersect for different 'bundles'. &#xD;&#xA;&#xD;&#xA;As parts are small, imputing leads to considerable decrease in accuracy(catastrophical to be more accurate)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;**Possible solutions**&#xD;&#xA;&#xD;&#xA;I could create dummy variables that would mark whether the measurement has taken place for each variable. The problem is, when random forests draws random variables, it does so individually.&#xD;&#xA;&#xD;&#xA;So there are two basic ways to solve this problem:&#xD;&#xA;1) Combine each &quot;bundle&quot; into one predictor. That is possible, but it seems information will be lost. &#xD;&#xA;2) Make random forest draw variables not individually, but by obligatory &quot;bundles&quot;.&#xD;&#xA;&#xD;&#xA;**Problem for random forest**&#xD;&#xA;&#xD;&#xA;As random forest draws variables randomly, it takes features that are useless (or much less useful) without other from their &quot;bundle&quot;. I have a feeling that leads to a loss of accuracy.&#xD;&#xA;&#xD;&#xA;**Example**&#xD;&#xA;&#xD;&#xA;For example I have variables `a`,`a_measure`, `b`,`b_measure`.&#xD;&#xA;The problem is, variables `a_measure` make sense only if variable `a` is present, same for `b`. So I either have to combine `a`and `a_measure` into one variable, or make random forest draw both, in case at least one of them is drawn.&#xD;&#xA;&#xD;&#xA;**Question**&#xD;&#xA;&#xD;&#xA;What are the best practice solutions for problems when different sets of predictors are measured for small parts of overall population, and these sets of predictors come in obligatory &quot;bundles&quot;?&#xD;&#xA;&#xD;&#xA;Thank you!" />
  <row Id="2821" PostHistoryTypeId="1" PostId="1094" RevisionGUID="25902b9b-c963-46df-a5ad-5fd6f0ad0d2b" CreationDate="2014-09-09T06:33:00.730" UserId="3108" Text="Create obligatory combinations of variables for drawing by random forest" />
  <row Id="2822" PostHistoryTypeId="3" PostId="1094" RevisionGUID="25902b9b-c963-46df-a5ad-5fd6f0ad0d2b" CreationDate="2014-09-09T06:33:00.730" UserId="3108" Text="&lt;machine-learning&gt;&lt;r&gt;&lt;random-forest&gt;" />
  <row Id="2823" PostHistoryTypeId="4" PostId="1094" RevisionGUID="5b943376-2090-4e6d-8c71-aab4834da891" CreationDate="2014-09-09T06:45:25.510" UserId="3108" Comment="edited title" Text="Creating obligatory combinations of variables for drawing by random forest" />
  <row Id="2825" PostHistoryTypeId="2" PostId="1095" RevisionGUID="307ec241-59ea-4da8-9ca1-0a10222ff7db" CreationDate="2014-09-09T12:44:16.967" UserId="3250" Text="The problem refers to decision trees building. According to Wikipedia '[Gini coefficient][1]' should not be confused with '[Gini impurity][2]'. However both measures can be used when building a decision tree - these can support our choices when splitting the set of items.&#xD;&#xA;&#xD;&#xA;1) 'Gini impurity' - it is a standard decision-tree splitting metric (see in the link above);&#xD;&#xA;&#xD;&#xA;2) 'Gini coefficient' - each splitting can be assessed based on the AUC criterion. For each splitting scenario we can build a ROC curve and compute AUC metric. According to Wikipedia AUC=(GiniCoeff+1)/2;&#xD;&#xA;&#xD;&#xA;Question is: are both these measures equivalent? On the one hand, I am informed that Gini coefficient should not be confused with Gini impurity. On the other hand, both these measures can be used in doing the same thing - assessing the quality of a decision tree split.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://en.wikipedia.org/wiki/Gini_coefficient&#xD;&#xA;  [2]: http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" />
  <row Id="2826" PostHistoryTypeId="1" PostId="1095" RevisionGUID="307ec241-59ea-4da8-9ca1-0a10222ff7db" CreationDate="2014-09-09T12:44:16.967" UserId="3250" Text="Gini coefficient vs Gini impurity - decision trees" />
  <row Id="2827" PostHistoryTypeId="3" PostId="1095" RevisionGUID="307ec241-59ea-4da8-9ca1-0a10222ff7db" CreationDate="2014-09-09T12:44:16.967" UserId="3250" Text="&lt;data-mining&gt;" />
  <row Id="2828" PostHistoryTypeId="2" PostId="1096" RevisionGUID="76fa47d4-6b7c-40c6-92e8-cea79bce8a7e" CreationDate="2014-09-09T15:05:46.093" UserId="819" Text="&gt; create my custom bag of words with N-grams to apply to&#xD;&#xA;&#xD;&#xA;My initial recommendation would be to use the [NLTK library for Python](http://www.nltk.org/). NLTK offers methods for [easily extracting bigrams from text](http://www.nltk.org/book/ch01.html#bigrams_index_term) or [ngrams of arbitrary length](https://nltk.googlecode.com/svn/trunk/doc/api/nltk.util-module.html#ngrams), as well as methods for analyzing the [frequency distribution of those items](http://www.nltk.org/book/ch01.html#frequency_distribution_index_term). However, all of this requires a bit of programming.&#xD;&#xA;&#xD;&#xA;&gt; a functionality that can be achieved over GUI&#xD;&#xA;&#xD;&#xA;That's tricky. Have you looked into [GATE](https://gate.ac.uk/)? I'm not exactly sure if/how GATE does what you want, but it does offer a GUI." />
  <row Id="2829" PostHistoryTypeId="2" PostId="1097" RevisionGUID="2e83dbfb-9bfa-4b96-8463-b4f36c8f8380" CreationDate="2014-09-09T16:58:10.537" UserId="2487" Text="I'll go ahead and post an answer for now; if someone has something better I'll accept theirs. &#xD;&#xA;&#xD;&#xA;At this point the most powerful option appears to be accessing WEKA using jRuby. We spent yesterday scouring the 'net, and this combination was even used by a [talk at RailsConf 2012][1], so I would guess if there were a comparable pure ruby package, they would have used it.&#xD;&#xA;&#xD;&#xA;Note that if you know exactly what you need, there are plenty of individual libraries that either [wrap standalone packages like libsvm][2] or [re-implement some individual algorithms like Naive Bayes in pure Ruby][3] and will spare you from using jRuby.&#xD;&#xA;&#xD;&#xA;But for a general-purpose library, WEKA and jRuby seem to be the best bet at this time.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://www.confreaks.com/videos/867-railsconf2012-practical-machine-learning-and-rails&#xD;&#xA;  [2]: https://github.com/febeling/rb-libsvm&#xD;&#xA;  [3]: https://github.com/alexandru/stuff-classifier" />
  <row Id="2830" PostHistoryTypeId="5" PostId="1092" RevisionGUID="c39dafd4-4e73-4bc1-93eb-5d59aa0e2ef3" CreationDate="2014-09-09T17:00:05.697" UserId="2487" Comment="added 2 characters in body" Text="Are there any machine learning libraries for Ruby that are relatively complete (including a wide variety of algorithms for supervised and unsupervised learning), robustly tested, and well-documented? I love Python's [scikit-learn][1] for its incredible documentation, but a client would prefer to write the code in Ruby since that's what they're familiar with.&#xD;&#xA;&#xD;&#xA;Ideally I am looking for a library or set of libraries which, like scikit and numpy, can implement a wide variety of data structures like sparse matrices, as well as learners.&#xD;&#xA;&#xD;&#xA;Some examples of things we'll need to do are binary classification using SVMs, and  implementing bag of words models which we hope to concatenate with arbitrary numeric data, as described in [this][2] Stackoverflow post.&#xD;&#xA;&#xD;&#xA;Thanks in advance!&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/&#xD;&#xA;  [2]: http://stackoverflow.com/q/20106940/1435804" />
  <row Id="2831" PostHistoryTypeId="2" PostId="1098" RevisionGUID="97bad0a7-1478-4abc-9710-da5ec0eeaa02" CreationDate="2014-09-10T07:33:06.470" UserId="3259" Text="You can use SKlearn, It is a python library. It is simplest method which i like with minimal code. You can follow this link http://scikit-learn.org/stable/modules/feature_extraction.html " />
  <row Id="2832" PostHistoryTypeId="2" PostId="1099" RevisionGUID="1cd3ba1b-124a-4e7d-8d6e-590e348c07d9" CreationDate="2014-09-10T08:15:17.343" UserId="21" Text="No, despite their names they are not equivalent or even that similar. Gini impurity is a measure of misclassification, which applies in a multiclass classifier context. Gini coefficient applies to binary classification, and requires a classifier that can in some way rank examples according to likelihood of being in the positive class. Both could be applied in some cases, but they are different measures for different things. Impurity is what is commonly used in decision trees." />
  <row Id="2834" PostHistoryTypeId="2" PostId="1100" RevisionGUID="df440c8f-1823-4ae3-b2e5-47b7e33ac1bd" CreationDate="2014-09-10T14:50:13.720" UserId="3263" Text="I'm looking at pybrain for taking server monitor alarms and determining the root cause of a problem. I'm happy with training it using supervised learning and curating the training data sets. The data is structured something like this:&#xD;&#xA;&#xD;&#xA; * Server Type **A** #1&#xD;&#xA;  * Alarm type 1&#xD;&#xA;  * Alarm type 2&#xD;&#xA; * Server Type **A** #2&#xD;&#xA;  * Alarm type 1&#xD;&#xA;  * Alarm type 2&#xD;&#xA; * Server Type **B** #1&#xD;&#xA;  * Alarm type **99**&#xD;&#xA;  * Alarm type 2&#xD;&#xA;&#xD;&#xA;So there are *n* servers, with *x* alarms that can be `UP` or `DOWN`. Both `n` and `x` are variable. &#xD;&#xA;&#xD;&#xA;If Server A1 has *alarm 1 &amp; 2* as `DOWN`, then we can say that *service a* is down on that server and is the cause of the problem.&#xD;&#xA;&#xD;&#xA;If *alarm 1* is down on all servers, then we can say that *service a* is the cause.&#xD;&#xA;&#xD;&#xA;There can potentially be multiple options for the cause, so straight classification doesn't seem appropriate.&#xD;&#xA;&#xD;&#xA;I would also like to tie later sources of data to the net. Such as just scripts that ping some external service.&#xD;&#xA;&#xD;&#xA;All the appropriate alarms may not be triggered at once, due to serial service checks, so it can start with one server down and then another server down 5 minutes later.&#xD;&#xA;&#xD;&#xA;I'm trying to do some basic stuff at first:&#xD;&#xA;&#xD;&#xA;    from pybrain.tools.shortcuts import buildNetwork&#xD;&#xA;    from pybrain.datasets import SupervisedDataSet&#xD;&#xA;    from pybrain.supervised.trainers import BackpropTrainer&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    INPUTS = 2&#xD;&#xA;    OUTPUTS = 1&#xD;&#xA;&#xD;&#xA;    # Build network&#xD;&#xA;&#xD;&#xA;    # 2 inputs, 3 hidden, 1 output neurons&#xD;&#xA;    net = buildNetwork(INPUTS, 3, OUTPUTS)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    # Build dataset&#xD;&#xA;&#xD;&#xA;    # Dataset with 2 inputs and 1 output&#xD;&#xA;    ds = SupervisedDataSet(INPUTS, OUTPUTS)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    # Add one sample, iterable of inputs and iterable of outputs&#xD;&#xA;    ds.addSample((0, 0), (0,))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    # Train the network with the dataset&#xD;&#xA;    trainer = BackpropTrainer(net, ds)&#xD;&#xA;&#xD;&#xA;    # Train 1000 epochs&#xD;&#xA;    for x in xrange(10):&#xD;&#xA;        trainer.train()&#xD;&#xA;&#xD;&#xA;    # Train infinite epochs until the error rate is low&#xD;&#xA;    trainer.trainUntilConvergence()&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    # Run an input over the network&#xD;&#xA;    result = net.activate([2, 1])&#xD;&#xA;&#xD;&#xA;But I[m having a hard time mapping variable numbers of alarms to static numbers of inputs. For example, if we add an alarm to a server, or add a server, the whole net needs to be rebuilt. If that is something that needs to be done, I can do it, but want to know if there's a better way.&#xD;&#xA;&#xD;&#xA;Another option I'm trying to think of, is have a different net for each type of server, but I don't see how I can draw an environment-wide conclusion, since it will just make evaluations on a single host, instead of all hosts at once. &#xD;&#xA;&#xD;&#xA;Which type of algorithm should I use and how do I map the dataset to draw environment-wide conclusions as a whole with variable inputs?" />
  <row Id="2835" PostHistoryTypeId="1" PostId="1100" RevisionGUID="df440c8f-1823-4ae3-b2e5-47b7e33ac1bd" CreationDate="2014-09-10T14:50:13.720" UserId="3263" Text="Neural net for server monitoring" />
  <row Id="2836" PostHistoryTypeId="3" PostId="1100" RevisionGUID="df440c8f-1823-4ae3-b2e5-47b7e33ac1bd" CreationDate="2014-09-10T14:50:13.720" UserId="3263" Text="&lt;machine-learning&gt;&lt;neuralnetwork&gt;" />
  <row Id="2837" PostHistoryTypeId="2" PostId="1101" RevisionGUID="6bc15ac4-6032-4377-9eec-9cb3ac3013a1" CreationDate="2014-09-10T15:52:04.743" UserId="979" Text="I think they both represent the same concept. &#xD;&#xA;&#xD;&#xA;In classification trees, the Gini Index is used to compute the impurity of a data partition. So Assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be:&#xD;&#xA;Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2)&#xD;&#xA;&#xD;&#xA;In CART we perform binary splits. So The gini index will be computed as the weighted sum of the resulting partitions and we select the split with the smallest gini index.&#xD;&#xA;&#xD;&#xA;So the use of Gini Impurity (Gini Index) is not limited to binary situations.&#xD;&#xA;&#xD;&#xA;Another term for Gini Impurity is Gini Coefficient which is used normally as a measure of income distribution.&#xD;&#xA;" />
  <row Id="2838" PostHistoryTypeId="6" PostId="1092" RevisionGUID="7c03bc4d-6a66-4fca-84f9-780088d920d1" CreationDate="2014-09-10T20:58:00.777" UserId="2487" Comment="edited tags" Text="&lt;machine-learning&gt;&lt;ruby&gt;" />
  <row Id="2839" PostHistoryTypeId="2" PostId="1102" RevisionGUID="3159f3f0-114a-4243-ad5b-252b74f5b7f3" CreationDate="2014-09-11T17:09:52.313" UserId="2785" Text="The CoreNLP parts of speech tagger and name entity recognition tagger are pretty good out of the box, but I'd like to improve the accuracy further so that the overall program runs better. To explain more about accuracy -- there are situations in which the POS/NER is wrongly tagged. For instance:&#xD;&#xA;&#xD;&#xA;- &quot;Oversaw car manufacturing&quot; gets tagged as NNP-NN-NN&#xD;&#xA;&#xD;&#xA;Rather than VB* or something similar, since it's a verb-like phrase (I'm not a linguist, so take this with a grain of salt).&#xD;&#xA;&#xD;&#xA;So what's the best way to accomplish accuracy improvement?&#xD;&#xA;&#xD;&#xA; - Are there better models out there for POS/NER that can be incorporated into CoreNLP?&#xD;&#xA; - Should I switch to other NLP tools?&#xD;&#xA; - Or create training models with exception rules?" />
  <row Id="2840" PostHistoryTypeId="1" PostId="1102" RevisionGUID="3159f3f0-114a-4243-ad5b-252b74f5b7f3" CreationDate="2014-09-11T17:09:52.313" UserId="2785" Text="Improve CoreNLP POS tagger and NER tagger?" />
  <row Id="2841" PostHistoryTypeId="3" PostId="1102" RevisionGUID="3159f3f0-114a-4243-ad5b-252b74f5b7f3" CreationDate="2014-09-11T17:09:52.313" UserId="2785" Text="&lt;nlp&gt;&lt;language-model&gt;" />
  <row Id="2842" PostHistoryTypeId="2" PostId="1103" RevisionGUID="df8f5cbb-26be-4982-99d6-44c2b42aa426" CreationDate="2014-09-12T00:40:07.877" UserId="819" Text="Your best best is to train your own models on the kind of data you're going to be working with." />
  <row Id="2843" PostHistoryTypeId="2" PostId="1104" RevisionGUID="f032c4e8-5e18-4853-b87a-660658209cc0" CreationDate="2014-09-12T04:22:38.823" UserId="3279" Text="I have been tasked with creating a pipeline chart with the live data and the budgeted numbers.  &#xD;&#xA;&#xD;&#xA;I know what probability of each phase of reaching the next.  The problem is I have no Idea what to do about the pipeline budgeting with regards to time.  For instance what period of time should I have closed sales in the chart.  &#xD;&#xA;&#xD;&#xA;I have honestly been working on trying to figure it out.  Each successive revision gets me farther from the answer." />
  <row Id="2844" PostHistoryTypeId="1" PostId="1104" RevisionGUID="f032c4e8-5e18-4853-b87a-660658209cc0" CreationDate="2014-09-12T04:22:38.823" UserId="3279" Text="Modeling Pipeline Budget" />
  <row Id="2845" PostHistoryTypeId="3" PostId="1104" RevisionGUID="f032c4e8-5e18-4853-b87a-660658209cc0" CreationDate="2014-09-12T04:22:38.823" UserId="3279" Text="&lt;recommendation&gt;&lt;time-series&gt;" />
  <row Id="2846" PostHistoryTypeId="2" PostId="1105" RevisionGUID="d19c1b64-2f4c-432f-9275-d7a894c2c3fe" CreationDate="2014-09-12T11:06:48.203" UserId="3283" Text="I am currently trying to implement logistic regression with iteratively reweightes LS, according to &quot;Pattern Recognition and Machine Learning&quot; by C. Bishop. In a first approach I tried to implement it in C#, where I used Gauss' algorithm to solve eq. 4.99. For a single feature it gave very promising (nearly exact) results, but whenever I tried to run it with more than one feature my system matrix became singular, and the weights did not converge. I first thought that it was my implementation, but when I implemented it in SciLab the results sustained. The SciLab (more concise due to matrix operators) code I used is&#xD;&#xA;&#xD;&#xA;    phi = [1; 0; 1; 1];&#xD;&#xA;    t = [1; 0; 0; 0];&#xD;&#xA;    w= [1];&#xD;&#xA;&#xD;&#xA;    w' * phi(1,:)'&#xD;&#xA;&#xD;&#xA;    for in=1:100&#xD;&#xA;        y = [];&#xD;&#xA;        R = zeros(size(phi,1));&#xD;&#xA;        R_inv = zeros(size(phi,1));&#xD;&#xA;&#xD;&#xA;        for i=1:size(phi,1)&#xD;&#xA;            y(i) = 1/(1+ exp(-(w' * phi(i,:)')));&#xD;&#xA;            R(i,i) = y(i)*(1 - y(i));&#xD;&#xA;            R_inv(i,i) = 1/R(i,i);&#xD;&#xA;        end&#xD;&#xA;&#xD;&#xA;        z = phi * w - R_inv*(y - t)&#xD;&#xA;        w = inv(phi'*R*phi)*phi'*R*z&#xD;&#xA;    end&#xD;&#xA;&#xD;&#xA;With the values for phi (input/features) and t (output/classes), it yields a weight of  -0.6931472, which is pretty much 1/3, which seems fine to me, for there is 1/3 probability of beeing assigned to class 1, if feature 1 is present (please forgive me, if my terms do not comply with ML-language completely, for I am an software developer). If I now added an intercept feature, which would accord to&#xD;&#xA;&#xD;&#xA;    phi = [1, 1; 1, 0; 1, 1; 1, 1];&#xD;&#xA;    w = [1; 1];&#xD;&#xA;&#xD;&#xA;my R-matrix becomes singular and the last weights value is&#xD;&#xA;&#xD;&#xA;    w  =&#xD;&#xA;      - 5.8151677  &#xD;&#xA;      1.290D+30  &#xD;&#xA;&#xD;&#xA;which - to my reading - would mean, that the probability of belonging to class 1 would be close to 1 if feature 1 is present about 3% for the rest. There has got to be any error I made, but I do not get which one. For both implementations yield the same results I suspect that there is some point I've been missing or gotten wrong, but I do not understand which one." />
  <row Id="2847" PostHistoryTypeId="1" PostId="1105" RevisionGUID="d19c1b64-2f4c-432f-9275-d7a894c2c3fe" CreationDate="2014-09-12T11:06:48.203" UserId="3283" Text="Logistic Regression implementation does not converge" />
  <row Id="2848" PostHistoryTypeId="3" PostId="1105" RevisionGUID="d19c1b64-2f4c-432f-9275-d7a894c2c3fe" CreationDate="2014-09-12T11:06:48.203" UserId="3283" Text="&lt;logistic-regression&gt;" />
  <row Id="2849" PostHistoryTypeId="2" PostId="1106" RevisionGUID="56894379-2f75-484e-adc0-15a500500383" CreationDate="2014-09-12T11:48:21.617" UserId="3284" Text="I am having an HTML string and want to find out if a word I supply is relevant in that string.&#xD;&#xA;&#xD;&#xA;Relevancy could be measured based on frequency in the text.&#xD;&#xA;&#xD;&#xA;An example to illustrate my problem:&#xD;&#xA;&#xD;&#xA;    this is an awesome bike store&#xD;&#xA;    bikes can be purchased online.&#xD;&#xA;    the bikes we own rock.&#xD;&#xA;    check out our bike store now&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Now I want to test a few other words:&#xD;&#xA;&#xD;&#xA;    bike repairs&#xD;&#xA;    dog poo&#xD;&#xA;&#xD;&#xA;`bike repairs` should be marked as relevant whereas `dog poo` should not be marked as relevant.&#xD;&#xA;&#xD;&#xA;Questions:&#xD;&#xA;&#xD;&#xA; * How could this be done?&#xD;&#xA; * How to I filter out ambiguous words like `in` or `or`&#xD;&#xA;&#xD;&#xA;Thanks for your ideas!&#xD;&#xA;&#xD;&#xA;I guess it's something Google does to figure out what keywords are relevant to a website. I am basically trying to reproduce their on-page rankings." />
  <row Id="2850" PostHistoryTypeId="1" PostId="1106" RevisionGUID="56894379-2f75-484e-adc0-15a500500383" CreationDate="2014-09-12T11:48:21.617" UserId="3284" Text="Find out if a word is relevant to a string" />
  <row Id="2851" PostHistoryTypeId="3" PostId="1106" RevisionGUID="56894379-2f75-484e-adc0-15a500500383" CreationDate="2014-09-12T11:48:21.617" UserId="3284" Text="&lt;machine-learning&gt;&lt;data-mining&gt;" />
  <row Id="2852" PostHistoryTypeId="2" PostId="1107" RevisionGUID="35a9ce98-7cc6-4803-9fe2-3cad4780f8d9" CreationDate="2014-09-12T15:20:51.767" UserId="97" Text="I have a classification problem with approximately 1000 positive and 10000 negative samples in training set. So this data set is quite unbalanced. Plain random forest is just trying to mark all test samples as a majority class.&#xD;&#xA;&#xD;&#xA;Some good answers about sub-sampling and weighted random forest are given here: http://datascience.stackexchange.com/questions/454/what-are-the-implications-for-training-a-tree-ensemble-with-highly-biased-datase&#xD;&#xA;&#xD;&#xA;Which classification methods besides RF can handle the problem in the best way?" />
  <row Id="2853" PostHistoryTypeId="1" PostId="1107" RevisionGUID="35a9ce98-7cc6-4803-9fe2-3cad4780f8d9" CreationDate="2014-09-12T15:20:51.767" UserId="97" Text="Quick guide into training highly imbalanced data sets" />
  <row Id="2854" PostHistoryTypeId="3" PostId="1107" RevisionGUID="35a9ce98-7cc6-4803-9fe2-3cad4780f8d9" CreationDate="2014-09-12T15:20:51.767" UserId="97" Text="&lt;machine-learning&gt;&lt;classification&gt;&lt;dataset&gt;&lt;unbalanced-classes&gt;" />
  <row Id="2855" PostHistoryTypeId="2" PostId="1108" RevisionGUID="80256954-6698-4110-a4f5-e6ea4df86169" CreationDate="2014-09-12T16:26:15.827" UserId="97" Text="As mentioned [before][1], I have a classification problem and unbalanced data set. Which contains 9-10 times more negative samples than positive.&#xD;&#xA;I have trained `&quot;gbm&quot;` Generalized Boosted Regression model from `CARET` package in `R` and get the following output:&#xD;&#xA;&#xD;&#xA;      interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD&#xD;&#xA;      1                  50       0.906     0.523  0.00978      0.0512  &#xD;&#xA;      1                  100      0.91      0.561  0.0108       0.0517  &#xD;&#xA;      1                  150      0.91      0.572  0.0104       0.0492  &#xD;&#xA;      2                  50       0.908     0.569  0.0106       0.0484  &#xD;&#xA;      2                  100      0.91      0.582  0.00965      0.0443  &#xD;&#xA;      2                  150      0.91      0.584  0.00976      0.0437  &#xD;&#xA;      3                  50       0.909     0.578  0.00996      0.0469  &#xD;&#xA;      3                  100      0.91      0.583  0.00975      0.0447  &#xD;&#xA;      3                  150      0.911     0.586  0.00962      0.0443  &#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That's clear.&#xD;&#xA;And what is not transparent: how Kappa is calculated.&#xD;&#xA;&#xD;&#xA;- What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance? &#xD;&#xA;- What do `Accuracy SD` and `Kappa SD` stand for?&#xD;&#xA;&#xD;&#xA;  [1]: http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets" />
  <row Id="2856" PostHistoryTypeId="1" PostId="1108" RevisionGUID="80256954-6698-4110-a4f5-e6ea4df86169" CreationDate="2014-09-12T16:26:15.827" UserId="97" Text="Kappa near to 60% in unbalanced (1:10) data set" />
  <row Id="2857" PostHistoryTypeId="3" PostId="1108" RevisionGUID="80256954-6698-4110-a4f5-e6ea4df86169" CreationDate="2014-09-12T16:26:15.827" UserId="97" Text="&lt;r&gt;&lt;unbalanced-classes&gt;&lt;caret&gt;&lt;gbm&gt;&lt;kappa&gt;" />
  <row Id="2858" PostHistoryTypeId="2" PostId="1109" RevisionGUID="44d5239d-3c01-4421-985d-6c5bbbd34225" CreationDate="2014-09-12T20:30:51.740" UserId="816" Text="Undersampling the majority class is usually the way to go in such situations.&#xD;&#xA;&#xD;&#xA;If you think that you have too few instances of the positive class, you may perform oversampling, for example, sample 5n instances with replacement from the dataset of size n.&#xD;&#xA;&#xD;&#xA;Caveats:&#xD;&#xA;&#xD;&#xA; - Some methods may be sensitive to changes in the class distribution, e.g. for Naive Bayes - it affects the prior probabilities. &#xD;&#xA; - Oversampling may lead to overfitting  " />
  <row Id="2860" PostHistoryTypeId="2" PostId="1110" RevisionGUID="75e074d1-641e-43d0-a9db-1b2851bb45ce" CreationDate="2014-09-13T06:33:17.360" UserId="3289" Text="I want to cluster a set of long-tailed /pareto-alike data into several bins (Actually the bin number is not determined yet). &#xD;&#xA;Is there any algorithms or models I can use?" />
  <row Id="2861" PostHistoryTypeId="1" PostId="1110" RevisionGUID="75e074d1-641e-43d0-a9db-1b2851bb45ce" CreationDate="2014-09-13T06:33:17.360" UserId="3289" Text="How to cluster a set of long-tailed / pareto data" />
  <row Id="2862" PostHistoryTypeId="3" PostId="1110" RevisionGUID="75e074d1-641e-43d0-a9db-1b2851bb45ce" CreationDate="2014-09-13T06:33:17.360" UserId="3289" Text="&lt;clustering&gt;&lt;k-means&gt;" />
  <row Id="2863" PostHistoryTypeId="2" PostId="1111" RevisionGUID="881fca58-da9d-45b5-8b42-9004ba0a2940" CreationDate="2014-09-13T11:48:56.133" UserId="3293" Text="Recently, I am trying to investigate the correlation between the content of a Web page and its the social information (e.g., the tweets that linked to a URL). But I found there is no good solution to harvest these data. Now I have hundreds of thousands of fixed URL available. Does anybody know a way to harvest their social data (tweets data or other social information)?&#xD;&#xA;&#xD;&#xA;I would appreciate any useful clues to solve this problem. " />
  <row Id="2864" PostHistoryTypeId="1" PostId="1111" RevisionGUID="881fca58-da9d-45b5-8b42-9004ba0a2940" CreationDate="2014-09-13T11:48:56.133" UserId="3293" Text="How to harvest tweets that contain a specific URL?" />
  <row Id="2865" PostHistoryTypeId="3" PostId="1111" RevisionGUID="881fca58-da9d-45b5-8b42-9004ba0a2940" CreationDate="2014-09-13T11:48:56.133" UserId="3293" Text="&lt;data-mining&gt;" />
  <row Id="2866" PostHistoryTypeId="2" PostId="1112" RevisionGUID="b78e5cab-dc65-407d-9482-88e8acbf8f2f" CreationDate="2014-09-13T15:36:19.867" UserId="3294" Text="- Max Kuhn covers this well in Ch16 of *Applied Predictive Modeling*.     &#xD;&#xA;- As mentioned in the linked thread, imbalanced data is essentially a cost sensitive training problem. Thus any cost sensitive approach is applicable to imbalanced data.&#xD;&#xA;- There are a large number of such approaches. Not all implemented in R: C50, weighted SVMs are options. Jous-boost. Rusboost I think is only available as Matlab code. &#xD;&#xA;- I don't use Weka, but believe it has a large number of cost sensitive classifiers. &#xD;&#xA;-  *Handling imbalanced datasets: A review*: Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas'&#xD;&#xA;-  *On the Class Imbalance Problem*: Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang, Guangtong Zhou&#xD;&#xA;       " />
  <row Id="2867" PostHistoryTypeId="2" PostId="1113" RevisionGUID="b310df29-2ee1-40dc-b59a-89d1d0565a01" CreationDate="2014-09-13T17:13:23.373" UserId="36" Text="I have a general methodological question. I have two columns of data, with one a column a numeric variable for age and another column a short character variable for text responses to a question. &#xD;&#xA;&#xD;&#xA;My goal is to group the age variable (that is, create cut points for the age variable), based on the text responses. I'm unfamiliar with any general approaches for doing this sort of analysis. What general approaches would you recommend? Ideally I'd like to categorize the age variable based on linguistic similarity of the text responses." />
  <row Id="2868" PostHistoryTypeId="1" PostId="1113" RevisionGUID="b310df29-2ee1-40dc-b59a-89d1d0565a01" CreationDate="2014-09-13T17:13:23.373" UserId="36" Text="General approahces for grouping a continuous variable based on text data?" />
  <row Id="2869" PostHistoryTypeId="3" PostId="1113" RevisionGUID="b310df29-2ee1-40dc-b59a-89d1d0565a01" CreationDate="2014-09-13T17:13:23.373" UserId="36" Text="&lt;bigdata&gt;&lt;text-mining&gt;" />
  <row Id="2870" PostHistoryTypeId="2" PostId="1114" RevisionGUID="de7976f6-99d2-4f8f-acec-7259342a6c23" CreationDate="2014-09-13T18:17:33.253" UserId="92" Text="Gradient boosting is also a good choice here.  You can use the gradient boosting classifier in sci-kit learn for example.  Gradient boosting is a principled method of dealing with class imbalance by constructing successive training sets based on incorrectly classified examples." />
</posthistory>