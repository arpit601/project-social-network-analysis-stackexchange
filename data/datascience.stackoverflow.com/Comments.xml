<?xml version="1.0" encoding="utf-8"?>
<comments>
  <row Id="5" PostId="5" Score="9" Text="this is a super theoretical AI question. An interesting discussion! but out of place..." CreationDate="2014-05-14T00:23:15.437" UserId="34" />
  <row Id="6" PostId="7" Score="4" Text="List questions are usually not suited for Stack Exchange websites since there isn't an &quot;objective&quot; answer or a way to measure the usefulness of an answer. Having said that, one of my recommendations would be MacKay's &quot;Information Theory, Inference, and Learning Algorithms.&quot;" CreationDate="2014-05-14T00:38:19.510" UserId="51" />
  <row Id="9" PostId="7" Score="3" Text="This question appears to be off-topic because it is asks for a favorite resource.  On other SE sites, this would immediately be closed.  Since this is a new site, we still have to decide if this is a valid question here" CreationDate="2014-05-14T01:16:12.623" UserId="66" />
  <row Id="12" PostId="15" Score="3" Text="This question is far too broad. It may be salvaged by restricting the question to a particular use case." CreationDate="2014-05-14T02:00:22.797" UserId="51" />
  <row Id="13" PostId="10" Score="2" Text="Nice one, @Nicholas... Another book from Hastie and Tibshirani is [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), which is a bit gentler of an entry compared to ESL." CreationDate="2014-05-14T02:16:20.503" UserId="24" />
  <row Id="14" PostId="7" Score="0" Text="Fair enough regarding what constitutes a &quot;valid&quot; question, although on other SE sites this question would **not** be immediately closed as you've stated: e.g., [2495 votes](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books), [1440 votes](http://stackoverflow.com/questions/1711/what-is-the-single-most-influential-book-every-programmer-should-read), [168 votes](http://tex.stackexchange.com/questions/11/what-is-the-best-book-to-start-learning-latex), and so on. There's great interest for these kinds of questions, even if this isn't deemed the right place." CreationDate="2014-05-14T02:35:50.090" UserId="36" />
  <row Id="17" PostId="22" Score="1" Text="Yes, using 1-of-n encoding is valid too." CreationDate="2014-05-14T06:47:00.223" UserId="21" />
  <row Id="19" PostId="19" Score="4" Text="A nice article about when your data starts to be too big for normal usage chrisstucchio.com/blog/2013/hadoop_hatred.html" CreationDate="2014-05-14T07:48:10.370" UserId="115" />
  <row Id="20" PostId="14" Score="0" Text="As to the second part of your question, I have proposed a discussion in meta: http://meta.datascience.stackexchange.com/questions/5/should-we-adopt-a-self-study-tag-like-stats-se-in-scope-and-approach How that gets received could shape whether your proficiency concern is answerable or within scope." CreationDate="2014-05-14T08:00:35.420" UserId="53" />
  <row Id="21" PostId="31" Score="1" Text="What do you mean under &quot;interesting groups&quot;? Do you have some predefined important feature list?" CreationDate="2014-05-14T09:08:56.007" UserId="120" />
  <row Id="22" PostId="31" Score="0" Text="Interesting groups are any groups of size greater than some threshold an that are much bigger than other possible clusters." CreationDate="2014-05-14T09:11:09.143" UserId="118" />
  <row Id="24" PostId="31" Score="1" Text="It isn't clear how you will perform preparement steps of your data. But you should look at algorithms described at http://en.wikipedia.org/wiki/Anomaly_detection . If I were you, I've checked SVM method first" CreationDate="2014-05-14T09:31:01.977" UserId="120" />
  <row Id="26" PostId="37" Score="1" Text="+1 I pretty much appreciate the stress out on big data being not about *what is the size*, and rather about *what is the content (characteristics of)*." CreationDate="2014-05-14T10:49:41.527" UserId="84" />
  <row Id="29" PostId="51" Score="0" Text="Excellent! Anything similar on Windows? I suppose if I want to condition it on something like the size of the database I can have my R script check that at the beginning and either stop or continue generating the report. What if I add this to the beginning of the R script and then I want to run it every 15 minutes? Do I need 24x4 `crontab` commands? Or can I iterate it somehow?" CreationDate="2014-05-14T14:58:48.107" UserId="151" />
  <row Id="30" PostId="51" Score="1" Text="@rnorberg You can set a cron job to repeat in any periodic time pattern as long as the time period is constant, and greater than 1 minute. Read the man page for more details, or such google &quot;crontab&quot;. Lots of stuff available." CreationDate="2014-05-14T15:14:00.130" UserId="62" />
  <row Id="31" PostId="37" Score="1" Text="That is a very refreshing perspective. I have never heard this before, but it is very true. This suggests that SQL and NoSQL technologies are not competetive, but complementary." CreationDate="2014-05-14T15:16:37.650" UserId="157" />
  <row Id="32" PostId="46" Score="0" Text="Thanks for your answer. It looks like different from SVM. I'll survey it. :)" CreationDate="2014-05-14T15:32:16.760" UserId="63" />
  <row Id="35" PostId="31" Score="0" Text="I've read about SVM and I think its more about classification of newly created data after manual training on existing dataset - not about clustering existing data and finding abnormally big clusters. Am I right? If I am then this method isn't what I want." CreationDate="2014-05-14T16:15:01.610" UserId="118" />
  <row Id="37" PostId="46" Score="4" Text="Just a reminder that we don't encourage linking off-site to an answer because its easy for links to break, causing an otherwise useful community resource to instead turn into a dead end. It's always best to put the answer directly into your post." CreationDate="2014-05-14T17:30:36.967" UserId="41" />
  <row Id="38" PostId="41" Score="3" Text="In addition to the answers below a good thing to remember is the fact that most of the things you need from R regarding Big Data can be done with summary data sets that are very small in comparison to raw logs.  Sampling from the raw log also provides a seamless way to use R for analysis without the headache of parsing lines and lines of a raw log.  For example, for a common modelling task at work I routinely use map reduce to summarize 32 gbs of raw logs to 28mbs of user data for modelling." CreationDate="2014-05-14T17:45:41.430" UserId="92" />
  <row Id="40" PostId="24" Score="4" Text="If your scale your numeric features to the same range as the binarized categorical features then cosine similarity tends to yield very similar results to the Hamming approach above.  I don't have a robust way to validate that this works in all cases so when I have mixed cat and num data I always check the clustering on a sample with the simple cosine method I mentioned and the more complicated mix with Hamming.  If the difference is insignificant I prefer the simpler method." CreationDate="2014-05-14T17:53:54.897" UserId="92" />
  <row Id="41" PostId="57" Score="0" Text="hm - get data, format data(awk sed grep stuff), remove noise as first step, then go deeper. so is't *hard preprocessing* comes at first, if use your therms" CreationDate="2014-05-14T18:09:54.360" UserId="146" />
  <row Id="42" PostId="52" Score="1" Text="probably it's good idea to clear a bit, what you mean under *cleaning data* , looks a bit confusing for my opinion" CreationDate="2014-05-14T18:11:45.363" UserId="146" />
  <row Id="43" PostId="57" Score="0" Text="@MolbOrg Yes, that's what I meant. I called *hard preprocessing* the *scripting side*, and *soft preprocessing* the use of data mining algorithms that generally reduce the *size* of the problem (cleans up the database). I also noted that the *second part, hard preprocessing, actually comes prior to any other process*. If it's not very clear with such terms, I'd gladly consider any other suggestions to improve the answer." CreationDate="2014-05-14T18:20:28.510" UserId="84" />
  <row Id="44" PostId="57" Score="1" Text="ah yes, did not paid enough attention, *raw data preprocessing*. Tested atm - yes perl oneliner is 3times slower then grep ) for 3.5kk strings in 300MB, for perl it took 1.1 sec, for grep 0.31 sec . I saw article where points that perl regexp is slow, much slower then it may be in practice, (i suspect that is also for grep too) [http://swtch.com/~rsc/regexp/regexp1.html](http://swtch.com/~rsc/regexp/regexp1.html)" CreationDate="2014-05-14T18:34:15.603" UserId="146" />
  <row Id="46" PostId="57" Score="0" Text="@MolbOrg Nice reference! AFAIK, `grep` uses POSIX basic regex by default, and allows for extended POSIX regex when run as `grep -E`, and for PCRE when run as `grep -P`." CreationDate="2014-05-14T18:44:15.457" UserId="84" />
  <row Id="47" PostId="57" Score="0" Text="yes, I used both variants, default was 6 times faster(just two attempts because lost a window with results, and second pattern was not suitable for default grep), and in the link above states that grep uses different(rigth?) approach, did't mean it's new for me, but new angle on that stuff for sure, have to think about ))" CreationDate="2014-05-14T18:49:49.287" UserId="146" />
  <row Id="48" PostId="24" Score="1" Text="That sounds like a sensible approach, @cwharland.  On further consideration I also note that one of the advantages Huang gives for the k-modes approach over Ralambondrainy's -- that you don't have to introduce a separate feature for each value of your categorical variable -- really doesn't matter in the OP's case where he only has a single categorical variable with three values.  Better to go with the simplest approach that works." CreationDate="2014-05-14T19:54:45.137" UserId="14" />
  <row Id="49" PostId="69" Score="0" Text="Can you give an example of a workflow which is reproducible without being a replication?" CreationDate="2014-05-14T21:00:16.777" UserId="157" />
  <row Id="50" PostId="46" Score="1" Text="Agree with that. At this point, it barely exists as more than that link anyhow. I will add a link to the underlying project." CreationDate="2014-05-14T21:02:21.930" UserId="21" />
  <row Id="51" PostId="69" Score="0" Text="@JayGodse: Good question, first I'd suggest reading the update I posted to the question, which might make my use of the terms a bit clearer. To answer your question, **no reproduction of a workflow is a replication of the original workflow**; which is to say, I don't have an answer to your question, since it is unclear to me how to you execute a data science workflow that allows for reproducibility without simply being a replication (that is, an exact copy) of what has already been done?" CreationDate="2014-05-14T21:46:39.713" UserId="158" />
  <row Id="52" PostId="70" Score="0" Text="Right, I get that, so maybe my question is unclear. The intent of my question is how to build a workflow that allows for reproducibility. Meaning if replication requires a step-by-step guide for taking a given input and reaching a given output, how do you execute a data science workflow that allows for reproducibility without simply being a replication of what has already been done?  Meaning you run the build, and it executes the code pulls the input creates the output AND generates an abstraction in text that would allow the build to be reproducible." CreationDate="2014-05-14T22:14:32.813" UserId="158" />
  <row Id="53" PostId="70" Score="0" Text="Think of it as a [reproducibility documentation generator](http://en.wikipedia.org/wiki/Comparison_of_documentation_generators)." CreationDate="2014-05-14T22:16:44.943" UserId="158" />
  <row Id="54" PostId="70" Score="0" Text="+1 So, I've thought about it, and unable to think of a way to make the question more clear without make your answer invalid, nor would it be fair for me to ask you delete your answer so I'm able to delete my question. So, just going to leave things be, and sorry my question was unclear. Cheers!" CreationDate="2014-05-14T22:32:23.857" UserId="158" />
  <row Id="55" PostId="71" Score="2" Text="Snarky answer: almost always. There's a huge incentive to create Type 1 errors (i.e., &quot;false alarms&quot;) when analysts examine data, so almost all p-values you'll encounter are &quot;too&quot; small." CreationDate="2014-05-14T23:07:08.427" UserId="36" />
  <row Id="56" PostId="71" Score="7" Text="Just throwing this out there, but wouldn't this sort of question best be posed on [Cross Validated](http://stats.stackexchange.com/)?" CreationDate="2014-05-14T23:47:53.803" UserId="24" />
  <row Id="57" PostId="58" Score="1" Text="Open source and some wiki. It looks good. Thanks for your suggestion. :)" CreationDate="2014-05-15T01:05:14.973" UserId="63" />
  <row Id="58" PostId="70" Score="0" Text="@blunders: please change the question.  I would rather have my answer be invalid and your question answered :)." CreationDate="2014-05-15T01:37:11.243" UserId="178" />
  <row Id="59" PostId="70" Score="0" Text="Done, thank you!" CreationDate="2014-05-15T02:02:21.317" UserId="158" />
  <row Id="60" PostId="76" Score="1" Text="About how much tweets a &quot;run&quot; are we talking?" CreationDate="2014-05-15T07:02:00.873" UserId="115" />
  <row Id="61" PostId="71" Score="1" Text="@buruzaemon: Maybe.  I did a search, this is the closest match: http://stats.stackexchange.com/questions/67320/hypothesis-testing-with-big-data  There don't seem to be more than a handful of questions that touch on this." CreationDate="2014-05-15T08:55:18.250" UserId="26" />
  <row Id="64" PostId="78" Score="0" Text="thank you for posting a link to that paper" CreationDate="2014-05-15T11:01:32.950" UserId="59" />
  <row Id="65" PostId="76" Score="0" Text="It's hard to know without a range of tweets, 1000, 100,000 full datahose etc." CreationDate="2014-05-15T13:01:58.240" UserId="59" />
  <row Id="66" PostId="76" Score="1" Text="@Johnny000: [500 million Tweets a day](https://blog.twitter.com/2013/new-tweets-per-second-record-and-how); my understanding is that Twitter limits streams to vendors based on trust/need, but to insure the solution covers the current daily averages, the solution should account what is the max, or you're able to reference as the max via a reliable source other than yourself." CreationDate="2014-05-15T13:24:51.407" UserId="158" />
  <row Id="68" PostId="85" Score="0" Text="I would add that FDR and FER are used when you have many hypotheses tested simultaneously." CreationDate="2014-05-15T14:19:14.513" UserId="178" />
  <row Id="69" PostId="93" Score="0" Text="The context would be info trackable within a single site with a 3rd party cookie, via an iframe. The site would be ecommerce. I find google analytics mostly looks at IP, sometimes at useragent, and I am able to get very similar numbers from looking only at IP in a timeframe. But google analytics is known to over-report by 30% ish, depending on context" CreationDate="2014-05-15T14:57:50.803" UserId="116" />
  <row Id="70" PostId="93" Score="0" Text="Looking at visited product pages doesn't help much either, as the structure of the shop is such that it leads users down predetermined paths, leading to very similar behaviour" CreationDate="2014-05-15T14:59:59.857" UserId="116" />
  <row Id="71" PostId="93" Score="1" Text="Also, I am aware that ML does not fit in the context of this question. Rather, hard coded algorithms are used by most tracking solutions that offer sensible results. The last few degrees of accuracy, that would be achievable with ML are of less relevance, since this info is rather used for observing trends." CreationDate="2014-05-15T15:03:21.477" UserId="116" />
  <row Id="78" PostId="76" Score="1" Text="Here's more information, appear the &quot;firehose&quot; data feed is pricy, so guessing it would be more relevant to limit input to the volume produce via the [Twitter Streaming APIs](https://dev.twitter.com/docs/api/streaming) via the Public Stream; [guide to processing the data is here](https://dev.twitter.com/docs/streaming-apis/processing#Scaling)." CreationDate="2014-05-15T20:48:23.770" UserId="158" />
  <row Id="79" PostId="76" Score="0" Text="Public Stream is a random sample of the &quot;firehose&quot; and appears to 1% of its total volume; meaning I estimate the feed to be 5 million tweets per day, and spikes might reach 1432 tweets per second; appears the spike must be account for, otherwise the feed gets discounted." CreationDate="2014-05-15T20:49:10.247" UserId="158" />
  <row Id="80" PostId="7" Score="1" Text="@statsRus: Try posting a question like that to SO, and it'll be closed; these questions exists because they have historical significance, but they are not considered good, on-topic questions for Stack Exchange sites, so **please do not use them as evidence that you can ask similar questions here.**" CreationDate="2014-05-15T21:08:13.933" UserId="158" />
  <row Id="82" PostId="91" Score="0" Text="I edited the question to add another query." CreationDate="2014-05-16T02:48:03.413" UserId="189" />
  <row Id="83" PostId="92" Score="0" Text="First, I edited the question to add another query. Also: I imagine even with a significant minority pre-computed, the rest of the query should still take long time to complete. Besides, when the process is delegated from one machine to 100 machines, isn't the latency actually increased (network latency between machines, and total latency is maximum of the latencies of all machines)?" CreationDate="2014-05-16T02:52:12.253" UserId="189" />
  <row Id="84" PostId="91" Score="0" Text="@namehere I tried to address your edit; hope it helps answering the question." CreationDate="2014-05-16T04:29:25.523" UserId="84" />
  <row Id="85" PostId="101" Score="0" Text="A very good answer! For those reading, I'd like to add that in the case of 3rd party cookies, many safari mobile versions will not take those by default, and other browsers have the same in their pipelines. Keep those in mind and treat them separately." CreationDate="2014-05-16T05:10:18.347" UserId="116" />
  <row Id="86" PostId="52" Score="1" Text="Explaining further what cleaning data means would be helpful. In the context where I work, cleaning has nothing to do with formatting - I'd just call that parsing/importing - But rather it would mean talking noisy user data and verifying it for coherence. The techniques use are dataset specific, from simple statistical rules, to fuzzy algorithms, especially when the data is sparse." CreationDate="2014-05-16T05:17:27.677" UserId="116" />
  <row Id="87" PostId="102" Score="1" Text="Alot of browsergames use a documentdatabase like Mongo DB or Couch DB" CreationDate="2014-05-16T06:15:20.240" UserId="115" />
  <row Id="88" PostId="102" Score="1" Text="As @Johnny000 mentioned, there are ones like MongoDB and CouchDB, that are widely used for that purpose.  I would add that you should consider developer time as well when making the decision." CreationDate="2014-05-16T12:35:21.483" UserId="178" />
  <row Id="89" PostId="92" Score="0" Text="I mean that answering the query &quot;spaghetti diamond&quot;, which is a weird rare query, might be sped up by precomputed results for &quot;spaghetti&quot; and &quot;diamond&quot; individually. Intra-DC connections are very fast and low latency. An extra hop or two inside is nothing compared to the ~20 hops between your computer and the DC. The dominating problem in distributing work is the straggler problem; you have to drop results from some subset if they don't respond in time. These are all gross generalizations but point in the right direction." CreationDate="2014-05-16T13:07:54.197" UserId="21" />
  <row Id="90" PostId="61" Score="1" Text="Is your question actually whether there is a case where it's impossible to overfit?" CreationDate="2014-05-16T13:09:01.880" UserId="21" />
  <row Id="91" PostId="61" Score="0" Text="@SeanOwen: No, how would it be impossible to overfit?" CreationDate="2014-05-16T13:13:46.987" UserId="158" />
  <row Id="92" PostId="61" Score="0" Text="Agree, just checking as you asked if overfitting caused models to become worse regardless of the data" CreationDate="2014-05-16T13:14:24.237" UserId="21" />
  <row Id="93" PostId="61" Score="0" Text="Just to be clear, to me, you asking if it's &quot;impossible to overfit&quot; and &quot;overfitting caused models to become worse regardless of the data&quot; are completely different in my opinion; that said, as far as I know, both are impossible." CreationDate="2014-05-16T13:49:13.207" UserId="158" />
  <row Id="94" PostId="101" Score="1" Text="Cookie churn is quite the problem for services that don't require log in. Many users simply don't understand cookies though so you are likely to have some cohort that you can follow for an appreciable amount of time." CreationDate="2014-05-16T14:44:37.800" UserId="92" />
  <row Id="95" PostId="103" Score="2" Text="I wonder why you emphasized that you do **not** have a distance. I'm not an expert here, but wonder whether it should not be possible to convert such a similarity into a distance, if required, basically by considering its inverse. Regardless of that, I doubt that there are clustering algorithms that are completely free of parameters, so some tuning will most likely be necessary in all cases. When you considered k-Means, can one assume that you have real-valued properties (particularly, that you *can* take the &quot;mean&quot; of several elements)?" CreationDate="2014-05-16T16:28:18.287" UserId="156" />
  <row Id="96" PostId="103" Score="4" Text="You don't need to know k to perform k means. You can cluster with varying k and check cluster variance to find the optimal.  Alternatively you might think about going for Gaussian mixture models or other restaraunt process like things to help you cluster." CreationDate="2014-05-17T00:12:00.940" UserId="92" />
  <row Id="97" PostId="102" Score="1" Text="You may want to consider a graph database that will help you manage relationships much faster. My favorite is OrientDB, which we use to manage a social network." CreationDate="2014-05-17T03:51:36.990" UserId="70" />
  <row Id="98" PostId="20" Score="2" Text="For anything social networking, I would **highly** recommend a graph database like [Neo4j](http://neo4j.org) or [OrientDB](http://orientechnologies.com)" CreationDate="2014-05-17T04:21:42.693" UserId="70" />
  <row Id="99" PostId="112" Score="0" Text="Thanks for the suggestion. I've looked at neo4j earlier but never at orientdb. Currently I can't envision a lot of benefit in modelling leadeboard data as graph but I will still look at streaming options in orientdb" CreationDate="2014-05-17T08:15:16.490" UserId="200" />
  <row Id="100" PostId="113" Score="1" Text="This page http://www.mongodb.com/nosql-explained provides some details about it" CreationDate="2014-05-17T08:29:53.633" UserId="211" />
  <row Id="102" PostId="115" Score="2" Text="I doubt there's any general API for this. You can try crawling various services likes Academia.edu, publishers' sites and so on. Nevertheless, it would be easier to build a local database of documents first, and then experiment with extracting the abstracts." CreationDate="2014-05-17T08:55:28.927" UserId="173" />
  <row Id="104" PostId="103" Score="0" Text="@Marco13 good point. Sometimes it works like that, but not in all cases, making sure that you keep the properties (e.g. triangle inequality). Regarding the mean of several elements, I can't think how I can do that, or if it is possible. Good point, too!" CreationDate="2014-05-17T09:01:14.093" UserId="113" />
  <row Id="105" PostId="103" Score="0" Text="@cwharland Veru useful comment. Doesn't the initial choice of k, though influence the result? I will look at the Gaussian mixture models. Thanks!" CreationDate="2014-05-17T09:02:59.783" UserId="113" />
  <row Id="106" PostId="115" Score="0" Text="Thanks for your answer! I have already built a local database for this. The problem of crawling from various services is that I have to make parse rules for each website." CreationDate="2014-05-17T09:05:02.373" UserId="212" />
  <row Id="107" PostId="115" Score="0" Text="So, how about converting PDFs to TXTs and then extracting the abstracts with regular expressions?" CreationDate="2014-05-17T09:35:04.733" UserId="173" />
  <row Id="108" PostId="115" Score="0" Text="thx! However, the contract states that massive download of papers are not allowed. This creates some headache." CreationDate="2014-05-17T11:39:36.970" UserId="212" />
  <row Id="109" PostId="115" Score="2" Text="I think this stack-overflow answer [link](http://stackoverflow.com/questions/14530019/avoiding-google-scholar-block-for-crawling) gives the best answer I can get. Maybe people who encounter this problem could also have a look at this page." CreationDate="2014-05-17T11:55:48.903" UserId="212" />
  <row Id="111" PostId="103" Score="2" Text="I asked the questions for a specific reason: **If** you could apply k-Means, but the only problem was finding the initial &quot;k&quot;, then you could consider a http://en.wikipedia.org/wiki/Self-organizing_map as an alternative. It has some nice properties, and basically behaves &quot;similar&quot; to k-Means, but does not require the initial &quot;k&quot; to be set. It's probably not an out-of-the-box solution, because it has additional tuning parameters (and the training may be computationally expensive), but worth a look nevertheless." CreationDate="2014-05-17T16:07:09.590" UserId="156" />
  <row Id="112" PostId="103" Score="2" Text="The initial choice of k does influence the clustering results but you can define a loss function or more likely an accuracy function that tells you for each value of k that you use to cluster, the relative similarity of all the subjects in that cluster.  You choose the k that minimizes variance in that similarity.  GMM and other dirichlet processes take care of the not-knowing-k problem quite well.  One of the best resources I've ever seen on this is [Edwin Chen's tutorial](http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/)." CreationDate="2014-05-17T16:30:06.077" UserId="92" />
  <row Id="113" PostId="116" Score="1" Text="When you say &quot;many more features than samples&quot; I assume you mean the unique number of liked sites is &gt;&gt; num users.  Is that also the case for the root domain of the sites?  i.e. are they a number of youtube.com or cnn.com urls in the sites or are they already stemmed to domain?  I'm leaning towards dimensionality reduction by collapsing URLs to domain roots rather than specific pages if it's possible." CreationDate="2014-05-17T18:12:20.767" UserId="92" />
  <row Id="114" PostId="116" Score="0" Text="Thanks for answer. The number of features (unique liked sites) is 32k, while the number of samples (users) is 12k. The features are Facebook Pages, so there's no need to stem the URLs. A user may either like facebook.com/cnn or not. I like the idea of trying to estimate users' age based on the links they share, though :)" CreationDate="2014-05-17T18:29:41.743" UserId="173" />
  <row Id="115" PostId="116" Score="0" Text="Ahhh, I misread the liked sites description.  Thanks for the clarification." CreationDate="2014-05-17T18:47:06.270" UserId="92" />
  <row Id="116" PostId="103" Score="4" Text="Just a thought: If your similarity score is normalized to _1_, than `1-sim(ei, ej) = Distance`. With distance metric you may apply for example hierarchical clustering. Going down from the root you will see at what level of granularity clusters would make sense for your particular problem." CreationDate="2014-05-18T02:16:46.537" UserId="31" />
  <row Id="118" PostId="76" Score="1" Text="I'd suggest Apache Kafka as message store and any stream processing solution of your choice like Apache Camel or Twitter Storm" CreationDate="2014-05-18T11:31:19.250" UserId="118" />
  <row Id="119" PostId="121" Score="0" Text="Hi, your answer is quite similar to my actual strategy. I used `sklearn.neighbors.KNeighborsRegressor` with cosine metric on SVD-reduced space (after applying SVD the average estimation error went down from ~6 years to ~4).&#xA;&#xA;Users in my database are aged 18-65 (older users were filtered out), so there are 48 possible classes. I wonder whether that's not too many classes for kNN, and whether I should treat it as regression or a classification problem (I think both are applicable)." CreationDate="2014-05-18T12:09:17.057" UserId="173" />
  <row Id="120" PostId="76" Score="0" Text="+1 @KonstantinV.Salikhov: Please post your comment as an answer, thanks!" CreationDate="2014-05-18T13:09:02.180" UserId="158" />
  <row Id="122" PostId="113" Score="0" Text="It depends on what kind of performance you are interested in: being able to handle a very large number of concurrent requests, being able to retrieve a specific record among a very large amount of records, being able to compute complex summary values from the data, etc?" CreationDate="2014-05-18T13:43:52.580" UserId="172" />
  <row Id="123" PostId="134" Score="2" Text="http://blog.mongodb.org/post/57611443904/mongodb-connector-for-hadoop" CreationDate="2014-05-18T14:16:42.150" UserId="118" />
  <row Id="124" PostId="126" Score="0" Text="Sorry for duplicate question, I search with a phrase and don't see other question, excuse me. Thanks for you answer. And, do you have a use case, personal experience, etc?" CreationDate="2014-05-18T16:07:01.423" UserId="109" />
  <row Id="125" PostId="138" Score="1" Text="Can you clarify the exact question? Maybe some possible answers you have in mind can also help." CreationDate="2014-05-18T19:10:48.500" UserId="227" />
  <row Id="127" PostId="159" Score="0" Text="Although this is a very nice and interesting question, I guess it will raise rather primarily opinion-based answers." CreationDate="2014-05-18T19:51:54.947" UserId="84" />
  <row Id="128" PostId="159" Score="0" Text="Please define long-term, computer science itself really is not that old." CreationDate="2014-05-18T20:00:18.810" UserId="158" />
  <row Id="129" PostId="155" Score="7" Text="This question might be more appropriate on the dedicated [opendata.SE](http://opendata.stackexchange.com/). That said, I cross my fingers for [dat](http://usodi.org/2014/04/02/dat), which aspires to become a &quot;Git for data&quot;." CreationDate="2014-05-18T21:23:52.687" UserId="216" />
  <row Id="130" PostId="135" Score="2" Text="R is a pleasure to work with for data manipulation (`reshape2`, `plyr`, and now `dplyr`) and I don't think you can do better than `ggplot2`/`ggvis` for visualization" CreationDate="2014-05-18T21:52:42.273" UserId="236" />
  <row Id="131" PostId="161" Score="0" Text="Sorry for the misunderstanding. My intention was to bring up answers concerning the importance of having control over an application, and how this control is *loosened* by libraries. Of course you can assume things about them (people don't normally rewrite pthreads), but if the data changes (load, throughput, ...), you may need to access the lib source to grant performance. And yes, it is not necessarily C/C++ -- although they're usually the languages chosen for hpc. May I delete my question, or would you like to change it into something more specific? I accept any suggestions to improve it." CreationDate="2014-05-18T21:56:12.430" UserId="84" />
  <row Id="132" PostId="138" Score="0" Text="@AmirAliAkbari SeanOwen posted an answer and I noticed the lack of specificity in my question. I've added a comment to his post. Please, feel free to suggest any improvements on the post -- I'm planing to delete it, otherwise." CreationDate="2014-05-18T21:59:09.827" UserId="84" />
  <row Id="133" PostId="159" Score="0" Text="While the &quot;Data&quot; subject still alive in digital world, It's scientific improvements / researchs stay alive ;)" CreationDate="2014-05-18T23:38:07.320" UserId="229" />
  <row Id="134" PostId="131" Score="0" Text="thanks for your answer." CreationDate="2014-05-19T07:18:05.963" UserId="212" />
  <row Id="135" PostId="120" Score="0" Text="thanks a lot. However arXiv does provide the papers I need." CreationDate="2014-05-19T07:18:44.240" UserId="212" />
  <row Id="136" PostId="140" Score="0" Text="I've read some basics about Apache Storm, it looks like it's concerned about issues related to scalability/reliability of stream processing, leaving you to handle the actual algorithms. Esper on the other handle process data for you based on your queries" CreationDate="2014-05-19T07:36:33.670" UserId="200" />
  <row Id="137" PostId="135" Score="0" Text="@pearpies As said in the beginning of my answer, I admit the good libraries available for R, but as a whole, when considering all areas needed for big data (which I as said a few of them in the answer), R is no match for the mature and huge libraries available for Python." CreationDate="2014-05-19T08:08:53.207" UserId="227" />
  <row Id="138" PostId="155" Score="0" Text="@ojdo Thanks, I never heard of opendata.SE before, I also found [this](http://opendata.stackexchange.com/q/266/2872) interesting (and very similar) question there." CreationDate="2014-05-19T08:28:56.713" UserId="227" />
  <row Id="139" PostId="165" Score="0" Text="Thank you very much Rapaio :) The points you gave me are very useful and gets something clearer..Since I'm a .NET developer and curious one on plain C (i start to learn) and new, fast, reliable, scalable ancd of course fully controllable -in a short term : very excited- techniques..So i need to learn very much..To learn, i try to read so many documents but as you can guess i'm at the start-line..&#xA;I didn't know that BTree has advantages on disk (In .Net world, so many writers explain it like : A hierarchical data structure like Linked-List..No More!) Thank you very much again" CreationDate="2014-05-19T11:41:09.810" UserId="229" />
  <row Id="140" PostId="165" Score="0" Text="And if you permit me, until there is a higher quality explanation / answer than yours, i want to accept this as answer.. And BTW, Lucene.NET is a .NET implementation of Java's Lucene" CreationDate="2014-05-19T11:45:21.380" UserId="229" />
  <row Id="141" PostId="84" Score="1" Text="While Bonferroni is definitely old-school it is still pretty popular. Related to it is a method called Šidák correction ( https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction ). I am calling it out, because in a large scale targeting advertising system I worked on we were able to implement this approach as a UDF in Hive. However this only works better when you have independence between tests. If not you have to fall back to Bonferroni or another method." CreationDate="2014-05-19T23:38:38.263" UserId="249" />
  <row Id="142" PostId="174" Score="1" Text="With LR you would have to make multiple models for age bins i think. How would compare two models for different age bins that predict the same prob on inclusion for a user?" CreationDate="2014-05-20T15:18:13.717" UserId="92" />
  <row Id="143" PostId="174" Score="1" Text="Note that LR fails when there are more variables than observations and performs poorly if the assumptions of the model is not met.  To use it, dimensionality reduction must be a first step." CreationDate="2014-05-20T17:06:04.223" UserId="178" />
  <row Id="144" PostId="59" Score="0" Text="Also see [Is the R language suitable for Big Data](http://datascience.stackexchange.com/q/41/227)." CreationDate="2014-05-20T17:42:19.287" UserId="227" />
  <row Id="145" PostId="135" Score="1" Text="[Peter](http://continuum.io/our-team) from Continuum Analytics (one of the companies on the [DARPA project referenced above](http://www.computerworld.com/s/article/9236558/Python_gets_a_big_data_boost_from_DARPA)) is working on some very impressive [opensource code](http://continuum.io/developer-resources) for [data visualization that simply do things that other sets of code are not able to do](http://bokeh.pydata.org/)." CreationDate="2014-05-20T18:46:45.713" UserId="158" />
  <row Id="146" PostId="174" Score="1" Text="@cwharland you should not consider the response variable to be categorical as it is continuous by nature, and discretized by the problem definition. Considering it categorical would mean telling the algorithm that predicting age 16 when it actually is 17 is as a serious error as predicting 30 when it actually is 17. Considering it continuous ensures that small errors (16 vs 17) are considered small and large errors (30 vs 17) are considered large. The logistic regression is used in this case to predict the continuous value and not estimate posterior probabilities." CreationDate="2014-05-20T19:28:01.027" UserId="172" />
  <row Id="147" PostId="174" Score="0" Text="@ChristopherLouden You are right that the vanilla version of logistic regression is not suitable for the 'large p small n' case, I should have mentioned that regularization is important in the present case. I update my answer. But  L1-regularized LR is a sort of feature selection so I consider no need for a preliminary FS step." CreationDate="2014-05-20T19:33:50.253" UserId="172" />
  <row Id="148" PostId="174" Score="0" Text="@damienfrancois: Agreed, thank you." CreationDate="2014-05-20T19:48:42.710" UserId="178" />
  <row Id="149" PostId="175" Score="1" Text="It may be easier to help if you show some two or three entries of your input file." CreationDate="2014-05-20T22:38:19.070" UserId="84" />
  <row Id="151" PostId="179" Score="1" Text="Hi, thanks for this tip, but I would prefer something that's easily publicable on the Web in a dynamic form. Also, I prefer free solutions, while Tableau - correct me if I am wrong - is only available as a trial version." CreationDate="2014-05-21T12:08:21.993" UserId="173" />
  <row Id="152" PostId="183" Score="0" Text="Thanks, I like this Python/networkx/matplotlib solution since it's my default working environment, and it's easy to make a gif out of this code. Still, something that looks nicer on the Web would beat this solution :)" CreationDate="2014-05-21T12:10:22.683" UserId="173" />
  <row Id="153" PostId="179" Score="0" Text="It also has &quot;Public&quot; edition, which means you have to store/share your results in the web, and can't save it locally." CreationDate="2014-05-21T12:10:59.243" UserId="97" />
  <row Id="154" PostId="187" Score="1" Text="Thanks, I misunderstood it. Now it is clear." CreationDate="2014-05-21T17:39:11.487" UserId="133" />
  <row Id="155" PostId="174" Score="0" Text="@damienfrancois: I definitely agree.  I'm just a little concerned that in this case LR will penalize intermediate values too harshly.  There's seem to be no motivation to map to a sigmoidal like curve given that you are not particularly interested in extreme age values.  Perhaps I'm misinterpreting the use though." CreationDate="2014-05-22T20:32:52.293" UserId="92" />
  <row Id="156" PostId="121" Score="0" Text="I can say, anecdotally, that I have use per class Random Forests to fit a number of classes individually then combined the results of each of those models in various ways.  In this case you might even think about assigning prior probabilities to each user's age with the kNN, then run through each class based model, use those scores to update the prior probabilities for each class and choose the most probable class from those posteriors.  It sounds like over complicating a bit but at worst you would have the kNN accuracy." CreationDate="2014-05-22T20:36:20.637" UserId="92" />
  <row Id="158" PostId="204" Score="1" Text="One interesting aspect of using the top 5000 sites is the fact that they may not be good at segmenting users on age.  The top sites, by construction, are ones that everyone visits.  They therefore are not very good at segmenting your users since all possible classifications (ages) have engaged with those sites.  This is a similar notion to the idf part of tf-idf.  idf helps filter out the &quot;everyone has this feature&quot; noise.  How do the most visited sites rank as features in your variable importance plots with your RF?" CreationDate="2014-05-24T04:59:04.420" UserId="92" />
  <row Id="159" PostId="209" Score="0" Text="There's no problem that -1 doesn't mean dislike. It's simply a way to differentiate that someone saw the item. In that sense it carries more info than a missing value. It may actually increase the accuracy of your recommendation. Depending on your distance metric in recommending you may consider changing it from a -1 to a slight metrics value so it doesn't influence the distance as much." CreationDate="2014-05-25T15:10:34.063" UserId="92" />
  <row Id="160" PostId="155" Score="2" Text="See http://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public." CreationDate="2014-05-26T17:08:57.360" UserId="289" />
  <row Id="161" PostId="209" Score="0" Text="The canonical paper for implicit feedback is [Hu, Koren, and Volinsky](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.167.5120). Lots of good recommendations in there, including estimating your confidence in which -1 indicates a dislike or merely a &quot;didn't see.&quot;" CreationDate="2014-05-27T00:00:55.560" UserId="159" />
  <row Id="162" PostId="213" Score="0" Text="I see. I did study the README file, however I still can't figure out how the data it self can be read. For example, the train/X_train.txt'file represents training set (the sample data that I showed in post comes from this file)." CreationDate="2014-05-27T12:30:31.233" UserId="295" />
  <row Id="163" PostId="204" Score="1" Text="Good point. An easy fix for this would be to stratify the training dataset into J age bins (e.g., 13-16, 17-20, 21-24, etc.) and take the top (K/J) pages for each group. That would ensure you have significant representation for each group. There will certainly be some overlap across groups, so if you were really picky you might want to take the top (K/J) unique pages for each group, but I think that might be overkill." CreationDate="2014-05-27T13:37:09.423" UserId="250" />
  <row Id="164" PostId="213" Score="0" Text="It seems to me that the data set is rather wide, there are 561 variables per row which are listed inside of the features.txt file. I believe that is what you are referring to." CreationDate="2014-05-27T13:38:22.867" UserId="59" />
  <row Id="165" PostId="197" Score="0" Text="I had experimented with C45/J48 in a previous project. I did not realize there were rules I could retrieve from it. I'll also check out RIPPER. Thanks!" CreationDate="2014-05-27T13:53:22.987" UserId="275" />
  <row Id="166" PostId="213" Score="0" Text="So each of the variable from 'features' file corresponds to each column in e.g 'x-test.txt' file, or Am I wrong?" CreationDate="2014-05-27T14:40:54.283" UserId="295" />
  <row Id="167" PostId="213" Score="0" Text="That is how I am taking it. From what I could gather from the README is that is what the file contains." CreationDate="2014-05-27T14:53:03.520" UserId="59" />
  <row Id="168" PostId="213" Score="0" Text="@Jakubee Yes. There are 561 rows/variable names in the `features.txt` file, and 561 columns in the `X_train.txt` file, one for each variable." CreationDate="2014-05-27T14:53:30.540" UserId="156" />
  <row Id="169" PostId="213" Score="0" Text="Ok. I do get it now! Thanks!! But what about the data itself. What kind of analytics I can process on this? (That might be stupid question) You think I can apply this for future analysis with python, using e.g hadoop?" CreationDate="2014-05-27T15:08:46.697" UserId="295" />
  <row Id="170" PostId="213" Score="0" Text="You can definitely use Python, I don't know if Hadoop will be necessary for files of this size, you could also use R and install the package Rattle with the following command `install.packages(&quot;rattle&quot;)` it is a good data mining package for R. In the future if you are going to be using massive data sets then for sure Hadoop will be good to know and use" CreationDate="2014-05-27T15:12:05.937" UserId="59" />
  <row Id="171" PostId="212" Score="0" Text="I think imputing slight negative values for items that have been seen many times but never chosen is reasonable. The OP doesn't indicate they have access to data that qualifies these negative imputations but I wouldn't rule that tactic out entirely. The optimal magnitude of the negative value can be determined from the data.  I've had small gains from doing this in recsys scenarios. In any case...would you suggest other ways of differentiating between item seen once and not chosen vs seen N times and never chosen besides negative imputing?" CreationDate="2014-05-28T02:37:14.990" UserId="92" />
  <row Id="172" PostId="216" Score="1" Text="+1 automation doesn't necessarily make things better, only more consistently and often faster!" CreationDate="2014-05-29T13:52:18.137" UserId="297" />
  <row Id="173" PostId="161" Score="1" Text="No it's a fine question, you can reflect your comments here in edits to the question if you like." CreationDate="2014-05-29T13:57:43.450" UserId="21" />
  <row Id="174" PostId="161" Score="0" Text="Please, check if the question makes sense now. I've added a small case in order to make it more straightforward. In case you want to add some consideration in the question, please, feel free to edit it." CreationDate="2014-05-29T15:04:26.473" UserId="84" />
  <row Id="175" PostId="175" Score="0" Text="For next time, use [ASCII Delimited Text](https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/) if you are unsure of which delimiter to select." CreationDate="2014-05-30T06:16:20.733" UserId="227" />
  <row Id="176" PostId="179" Score="2" Text="@WojciechWalczak Maybe [gephi](http://gephi.org/) can be used instead of Tableau." CreationDate="2014-05-30T06:31:29.873" UserId="227" />
  <row Id="177" PostId="179" Score="0" Text="@Amir Ali Akbari, looks great thanks" CreationDate="2014-05-30T06:49:36.470" UserId="97" />
  <row Id="178" PostId="220" Score="0" Text="Thanks for your answer mate. unfortunately, if I transpose the matrix it will make the false calculation." CreationDate="2014-05-30T11:46:02.917" UserId="273" />
  <row Id="179" PostId="223" Score="0" Text="One of SGML's major purposes (the same holds for its offspring, XML) was to provide the means for tagging text documents (POS **and** semantic tags)." CreationDate="2014-05-30T20:47:32.157" UserId="318" />
  <row Id="180" PostId="175" Score="0" Text="@Rubens yes I will mock up some entries and edit this weekend." CreationDate="2014-05-31T06:56:25.183" UserId="249" />
  <row Id="181" PostId="175" Score="0" Text="@AmirAliAkbari unfortunately I cannot get the source system to change the format otherwise I wouldn't have the issue, and SASB on AIX has limited options, but thank you for the link." CreationDate="2014-05-31T07:00:48.387" UserId="249" />
  <row Id="182" PostId="182" Score="0" Text="Besides the metadata used to declare the hive table there is also information on the original informat from the SAS environment. Even if the case is more obscure than int, string, int I am hoping that that additional information could be used to get there if the table metadata is insufficient due to potential ambiguity (e.g. string, string, string)" CreationDate="2014-05-31T07:03:29.753" UserId="249" />
  <row Id="184" PostId="229" Score="0" Text="I referred to the processing via MapReduce in Hadoop Echo system as simply Hadoop because that's the term commonly used (Though technically wrong and I have changed the question accordingly)." CreationDate="2014-06-01T21:50:16.317" UserId="339" />
  <row Id="185" PostId="229" Score="0" Text="May be I am wrong but I think there is more to that than to just have near-real-time processing. If there were no trade-offs between them, everyone would have like to do things in near-real-time. A hybrid approach allows for getting the best of both worlds (to some extent). That's why Summingbird was created." CreationDate="2014-06-01T21:55:28.310" UserId="339" />
  <row Id="186" PostId="223" Score="0" Text="Could be more specific/restrictive about what kind of metadata you want to add? With your two examples, I doubt that there is a less verbose way that has the same generic expressiveness as XML tags." CreationDate="2014-06-01T22:28:33.520" UserId="216" />
  <row Id="188" PostId="229" Score="0" Text="A major difference is that a stream processing system can just touch data once, and by itself has no long-term state. Some problems can't be solved this way. For problems for which this is OK, it's faster to use a system that does not require first persisting data into (re-readable) storage. MapReduce is not inherently slower than Storm; both are containers. They are different paradigms for different problems." CreationDate="2014-06-01T22:53:58.930" UserId="21" />
  <row Id="189" PostId="229" Score="0" Text="By not having long-term persistent state does it mean that such near-real-time systems can not accumulate input updates over a long duration? Can you refer me to any resources that discuss further on this?" CreationDate="2014-06-01T23:07:22.743" UserId="339" />
  <row Id="190" PostId="229" Score="0" Text="This is kind of the definition of a streaming system. If you imagine a system that can access long-term state at will, it's not really streaming." CreationDate="2014-06-02T00:39:08.850" UserId="21" />
  <row Id="191" PostId="223" Score="0" Text="@ojdo The most of the meta-data is either for disambiguation (like the relative times), or for specifying special entities (i.e. FKs)." CreationDate="2014-06-02T16:59:48.337" UserId="227" />
  <row Id="192" PostId="220" Score="1" Text="Can you explain why you have to calculate such a giant covariance matrix? Would be easier to find a workaround if you could shed light on the motivation." CreationDate="2014-06-03T15:01:16.907" UserId="250" />
  <row Id="193" PostId="197" Score="0" Text="Also check out the C50 package in R." CreationDate="2014-06-06T14:56:59.903" UserId="375" />
  <row Id="194" PostId="232" Score="1" Text="When you say, when you sum results you can not interpret totals, you mean that each classification can have a different weight and its contribute can be over/under estimated in the total? If I suppose to run, e.g., 4 independent tests, may I assume that each classification has the same weight and interpret (painlessly) the totals? Hope it is clear.." CreationDate="2014-06-08T16:56:17.433" UserId="133" />
  <row Id="1194" PostId="224" Score="1" Text="Seems to be just a text reformatting problem, have you written any code yet?" CreationDate="2014-06-09T10:43:11.727" UserId="227" />
  <row Id="1196" PostId="223" Score="1" Text="I have used http://brat.nlplab.org/ in the past.  There is a nice interface for many different types of annotations.  The annotations are stored in a separate .annot file which is a list of the words that are annotoated and their position in the document." CreationDate="2014-06-09T13:09:03.373" UserId="387" />
  <row Id="1197" PostId="224" Score="2" Text="what have you tried so far?" CreationDate="2014-06-09T19:54:01.820" UserId="59" />
  <row Id="1198" PostId="236" Score="0" Text="I would check out research by faculty and number of citations. Generally that is a good way to rank programs." CreationDate="2014-06-09T20:58:14.230" UserId="141" />
  <row Id="1199" PostId="208" Score="0" Text="I think you should note that for processes on a single machine you can memory map variables with joblib/Numpy. You lose that ability for processes on different machines." CreationDate="2014-06-09T21:55:20.937" UserId="403" />
  <row Id="1200" PostId="44" Score="0" Text="But does using R with Hadoop overcome this limitation (having to do computations in memory)?" CreationDate="2014-06-09T23:07:41.553" UserId="413" />
  <row Id="1201" PostId="188" Score="1" Text="That would still have mongo doing the processing, which I believe from the question is to be avoided in the final solution.  Giving you an upvote anyways for bringing up an important piece of knowledge." CreationDate="2014-06-10T04:15:12.013" UserId="434" />
  <row Id="1203" PostId="232" Score="1" Text="What I meant to convey is that we lose track of what the actual numbers mean. E.g., if I have 4 in a specific entry in run 1 and get 5 in that same entry on run 2, it's hard to say exactly what 4+5=9 means. I'd rather look at a distribution (%'s) or averages of where individuals fall across the matrix. It seems much more intuitive." CreationDate="2014-06-10T06:11:12.453" UserId="375" />
  <row Id="1204" PostId="251" Score="1" Text="+1 That may be my fault for being not very clear in my post, so others had not got it before. This is surely the kind of answer I was looking for. Thanks." CreationDate="2014-06-10T07:06:17.757" UserId="84" />
  <row Id="1205" PostId="218" Score="0" Text="Isn't this a programming question that should be on Stack OVerflow? Getting an error when computing a covariance is not data science." CreationDate="2014-06-10T08:15:35.330" UserId="471" />
  <row Id="1206" PostId="256" Score="1" Text="Thank you for the detailed answer. That is relaxing!" CreationDate="2014-06-10T08:29:43.067" UserId="456" />
  <row Id="1207" PostId="191" Score="0" Text="http://en.wikipedia.org/wiki/Winner-take-all" CreationDate="2014-06-10T08:45:00.260" UserId="11" />
  <row Id="1208" PostId="271" Score="0" Text="Interesting.  The reason I asked the question is that I wondered if something similar to the &quot;gambler's fallacy&quot; (or even gf itself).  I thought there might be a chance it had already been proven to be a fruitless venture.  Still - these other answers are intriguing." CreationDate="2014-06-10T11:56:17.943" UserId="434" />
  <row Id="1210" PostId="274" Score="4" Text="From personal experience I'd add that built-in documentation/label is huge. Now all my datasets can can be stored with explicit records of where they came from, sampling frequency, anomalies, etc. etc." CreationDate="2014-06-10T13:04:36.180" UserId="403" />
  <row Id="1212" PostId="272" Score="2" Text="+1 For Andrew Ng's course.  It is very well done." CreationDate="2014-06-10T15:12:40.480" UserId="533" />
  <row Id="1216" PostId="245" Score="0" Text="You are right, the data science PhD is not yet listed.  Machine learning has a lot math behind it, we can't just use an algorithm without proving the theory part why it converges, optimizes, etc..." CreationDate="2014-06-10T16:38:53.567" UserId="386" />
  <row Id="1217" PostId="241" Score="0" Text="As Andrew Gelman [mentions HERE](http://andrewgelman.com/2014/05/25/decided-physicist/), it's pointless to be a math major if you are the best.  I am not even that great at math, actually.  That's what led me to think of data science." CreationDate="2014-06-10T16:41:45.913" UserId="386" />
  <row Id="1218" PostId="291" Score="0" Text="Thanks, see my edit. I have a lot of coding experience and have taken MOOCs. I have a masters in Statistics and a minor in applied mathematics, I would consider math my biggest strength. I am really looking for things to put on a PhD application." CreationDate="2014-06-10T19:23:53.090" UserId="560" />
  <row Id="1219" PostId="248" Score="0" Text="You have very good points.  But, imagine you are Data Science PhD grad, which journal will you publish in?  Stat, CS, journal of machine learning?  Each one of those fields has its own experts." CreationDate="2014-06-10T19:31:58.813" UserId="386" />
  <row Id="1220" PostId="135" Score="1" Text="This answer seems to be wholly anecdotal and hardly shows anywhere where R is weak relative to Python." CreationDate="2014-06-10T20:31:38.787" UserId="598" />
  <row Id="1221" PostId="291" Score="1" Text="Then write some papers and get them published in a good conference: that's the best signal that you are fit for research--and a PhD program. Maybe you can use your economics background to write a paper on [multi-agent learning](http://mitpress.mit.edu/books/theory-learning-games). You don't have to stick to the same subject once you get accepted; it's just to demonstrate your ability." CreationDate="2014-06-10T20:59:51.017" UserId="381" />
  <row Id="1222" PostId="295" Score="0" Text="Thanks for the answer. I have a minor in applied mathematics and a masters in statistics. I have been taking graduate math courses for the last two years, as I did my masters in statistics. Are there any specific classes I should take? I have taken my calc sequence, linear algebra, differential equations, fourier analysis, stochastic processes, advanced probability, statstical inference, bayesian analysis, time series and a few others. Any others in particular" CreationDate="2014-06-10T21:42:57.950" UserId="560" />
  <row Id="1226" PostId="289" Score="2" Text="He said it specifically about research job though." CreationDate="2014-06-10T23:58:32.280" UserId="615" />
  <row Id="1228" PostId="295" Score="0" Text="Statistics MS/MA is offered everywhere these days, they don't help you get into a stat PhD. Stat PhD is looking for solid math undergrads: real analysis, optimization, numerical analysis. CS PhD is looking for cs and math undergrad. Why don't you continue on economics?" CreationDate="2014-06-11T00:12:25.993" UserId="386" />
  <row Id="1230" PostId="253" Score="1" Text="This class of questions is being discussed on meta. You can voice your opinion on this [meta post.](http://meta.datascience.stackexchange.com/q/41/62)" CreationDate="2014-06-11T02:19:34.887" UserId="62" />
  <row Id="1232" PostId="234" Score="0" Text="This class of questions is being discussed on meta. You may voice your opinion [here.](http://meta.datascience.stackexchange.com/q/41/62)" CreationDate="2014-06-11T02:22:39.393" UserId="62" />
  <row Id="1234" PostId="44" Score="0" Text="RHadoop does overcome this limitation.  The tutorial here: https://github.com/RevolutionAnalytics/rmr2/blob/master/docs/tutorial.md spells it out clearly.  You need to shift into a mapreduce mindset, but it does provide the power of R to the hadoop environment." CreationDate="2014-06-11T06:34:50.310" UserId="434" />
  <row Id="1235" PostId="308" Score="1" Text="Thank you. This is very helpful. I know its a bug space and there is no one right answer. I am very interested to know how one selects big data tools and technology to suit their needs. I am not marking this as the right answer for now but it certainly deserve lot of UP votes. Cheers :)" CreationDate="2014-06-11T07:40:08.317" UserId="496" />
  <row Id="1236" PostId="306" Score="3" Text="I assume that the OP's reference to TBs means &quot;for data on the small end of what you might use Hadoop for.&quot;  If you have multiple petabytes or more, Redshift clearly isn't suitable. (I believe it's limited to a hundred 16TB nodes.)" CreationDate="2014-06-11T07:47:01.247" UserId="14" />
  <row Id="1237" PostId="309" Score="2" Text="`easier to develop because of Redshift's maturity` contradicts with `Redshift isn't that mature yet` so what is your verdict?" CreationDate="2014-06-11T08:41:41.310" UserId="646" />
  <row Id="1238" PostId="309" Score="0" Text="@M.Mimpen: Edited answer to be more specific" CreationDate="2014-06-11T08:56:18.213" UserId="638" />
  <row Id="1239" PostId="287" Score="0" Text="Thanks for your answer, I will read it carefully." CreationDate="2014-06-11T09:09:30.187" UserId="133" />
  <row Id="1240" PostId="287" Score="0" Text="What do you mean for misclassification ratio in the fourth paragraph?" CreationDate="2014-06-11T09:14:36.593" UserId="133" />
  <row Id="1241" PostId="287" Score="0" Text="misclassification ratio = (number of instances correctly classified)/(total number of instances); in that paragraph we have 0.33 = proportion of each class (let's name labels as c1, c2, c3); we have 0.33*1.0 (c1 are all correctly classified), + 0.33*0.5 (c2 are random classified as c2 or c3) + 0.33*0.5 (c3 are random classified as c2 or c3) = 0.33 + 0.166 + 0.166 = 0.66 (instances classified correctly/total number of instances)" CreationDate="2014-06-11T09:36:33.673" UserId="108" />
  <row Id="1242" PostId="19" Score="2" Text="&quot;Anything too big to load into Excel&quot; is the running joke." CreationDate="2014-06-11T12:07:51.477" UserId="471" />
  <row Id="1244" PostId="313" Score="0" Text="Asking about &quot;good&quot; books will attract opinion-based answers and so this is off-topic. Flagged." CreationDate="2014-06-11T14:32:37.843" UserId="471" />
  <row Id="1245" PostId="313" Score="2" Text="I've changed it so I am just looking for books. Nothing opinion-based." CreationDate="2014-06-11T14:34:15.243" UserId="663" />
  <row Id="1246" PostId="313" Score="0" Text="It's spelled S-t-a-t-i-s-t-i-c-s :)   Stick with something pragmatic that focuses on prediction rather than inference. Both _Elements of Statistical Learning_ and _An Introduction to Statistical Learning_ are on most people's lists." CreationDate="2014-06-11T15:17:28.750" UserId="515" />
  <row Id="1248" PostId="291" Score="0" Text="Thank you, that is the best advice I have received." CreationDate="2014-06-11T16:07:35.017" UserId="560" />
  <row Id="1249" PostId="295" Score="0" Text="When I left undergrad I was 12 credit hours short of a math major. After I finished my MS in stats I could have pursued a PhD where I got my MS(top 30 school), however I am more interested in ML. I really don't think my math background will be a problem, as I feel it is very strong. I left economics and went to pure statistics in graduate school because economics no longer interested me, so that is definitely out. So do you think I should try to finish a math undergrad? It would take less than two semesters" CreationDate="2014-06-11T16:15:30.020" UserId="560" />
  <row Id="1250" PostId="316" Score="1" Text="+1 Thank you for taking the time to improve upon the existing answer, and given in my opinion your answer is now the better answer, I've selected your answer as the answer. Cheers!" CreationDate="2014-06-11T17:35:10.527" UserId="158" />
  <row Id="1251" PostId="316" Score="0" Text="Glad to answer!" CreationDate="2014-06-11T19:01:09.080" UserId="514" />
  <row Id="1252" PostId="305" Score="0" Text="Do you mean Hadoop or do you mean a specific counterpart to Redshift, like Impala?" CreationDate="2014-06-11T19:17:18.213" UserId="21" />
  <row Id="1253" PostId="311" Score="1" Text="I've already implemented feature extraction, and a toolkit for it (publication awaits some bugchecking)." CreationDate="2014-06-11T19:18:26.090" UserId="555" />
  <row Id="1254" PostId="272" Score="0" Text="John Hopkins also has a data science certificate track (9 classes) that started last week at Coursera.  https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage - it's not all machine learning, but worth sharing.  Coursera is full of awesomeness (and Andrew Ng is a great lecturer)." CreationDate="2014-06-11T20:49:17.350" UserId="434" />
  <row Id="1257" PostId="324" Score="1" Text="RODBC is for connecting to relational databases - often ones that are on your premises. You question alludes to  connecting to public API - that is something else." CreationDate="2014-06-12T03:33:12.170" UserId="366" />
  <row Id="1258" PostId="303" Score="0" Text="actually the ppt and the SO site you've linked to have nothing to do with the phrase alignments. It only achieves the word alignments that I have showed in the original post. =(" CreationDate="2014-06-12T04:38:42.697" UserId="122" />
  <row Id="1259" PostId="324" Score="2" Text="You're asking two questions. Please post them separately. You'd better flesh out the part that comes before sentiment analysis too." CreationDate="2014-06-12T05:12:19.767" UserId="381" />
  <row Id="1265" PostId="339" Score="1" Text="R hardly beats python in visualization. I think it's rather the reverse; not only does python have [ggplot](https://github.com/yhat/ggplot/) (which I don't use myself, since there are more pythonic options, like [seaborn](http://stanford.edu/~mwaskom/software/seaborn/)), it can even do interactive visualization in the browser with packages like [bokeh](http://bokeh.pydata.org)." CreationDate="2014-06-12T15:57:49.973" UserId="381" />
  <row Id="1266" PostId="343" Score="0" Text="Deep learning is about increasing the _number_ of hidden layers. Otherwise it would be called fat learning :)" CreationDate="2014-06-12T16:00:20.453" UserId="381" />
  <row Id="1267" PostId="24" Score="2" Text="Good answer. Potentially helpful: I have implemented Huang's k-modes and k-prototypes (and some variations) in Python: https://github.com/nicodv/kmodes" CreationDate="2014-06-12T16:08:03.140" UserId="554" />
  <row Id="1271" PostId="203" Score="1" Text="Thank you for your post, but can you include some of the essential information to answer the question here? The material you linked is a great way to support the information in your answer, but this site was created to compile a great collection of questions with answers. Posts that send users elsewhere to find that information are not really considered &quot;answers&quot; here. Thanks." CreationDate="2014-06-12T16:56:07.723" UserId="50" />
  <row Id="1272" PostId="320" Score="0" Text="Good answer. Realize that 2, 3, 4 can interact in complex ways, though. Debugging could be done by checking the activation values of the ANN, the magnitude of the weights of the ANN, keeping an eye on the in-sample and out-of-sample error and convergence of the optimizer, etc." CreationDate="2014-06-12T16:59:43.233" UserId="554" />
  <row Id="1274" PostId="339" Score="0" Text="ggplot was built in 2005 and it continues to be the favorite of many researchers due to its intuitive nature and well defined grammar of visualizations. Seaborn is built on top of matplotlib and it could be adopted by many in the coming years but ggplot still leads the charts in statistical data analysis visualizations. ggplot for python is so unpythonic in nature. The API is directly taken from the R implementation. Coming to interactive visualisations part, both python and R have d3js and d3py to take leverage of interactive visualisations." CreationDate="2014-06-12T17:12:15.527" UserId="514" />
  <row Id="1275" PostId="343" Score="0" Text="@Emre definitely meant that. Curse my punctuation!" CreationDate="2014-06-12T18:08:53.677" UserId="754" />
  <row Id="1276" PostId="341" Score="0" Text="Apparently most of the people are using Hadoop for analytics. What I am thinking is do I need something like that or knowledge about database, ML, statistics is enough?" CreationDate="2014-06-12T18:26:34.837" UserId="456" />
  <row Id="1278" PostId="295" Score="0" Text="No, you should not back dig for that math major, but take courses you need like real analysis, and optimization.  I know these courses sound irrelevant, but PhD programs want to see it, please them.  They want to know do you have the theories down.  They don't worry if you don't understand neural network well.  As Prof. LeCun said, take as many math courses as you can." CreationDate="2014-06-12T20:46:56.670" UserId="386" />
  <row Id="1280" PostId="339" Score="3" Text="Also R has the ability to interactive viz with Shiny." CreationDate="2014-06-12T22:04:23.300" UserId="598" />
  <row Id="1281" PostId="350" Score="0" Text="This is an example of a career question. Please join in on the [discussion here](http://meta.datascience.stackexchange.com/q/41/62) and voice your opinion if you feel this should be on-topic." CreationDate="2014-06-13T01:41:26.040" UserId="62" />
  <row Id="1282" PostId="334" Score="2" Text="This is too broad and primarily opinion based. Please take a look at http://datascience.stackexchange.com/help/dont-ask" CreationDate="2014-06-13T01:44:05.283" UserId="62" />
  <row Id="1283" PostId="334" Score="2" Text="@AsheeshR - We're averaging 2 questions a day and 2 answers per question.  At this point the focus needs to be on encouraging participation and increasing interest." CreationDate="2014-06-13T04:29:41.007" UserId="434" />
  <row Id="1284" PostId="334" Score="7" Text="Engagement at the expense of site quality is not the solution. Engagement is transient. Quality is much harder to alter later on." CreationDate="2014-06-13T05:05:00.603" UserId="62" />
  <row Id="1285" PostId="334" Score="0" Text="@AsheeshR I'll agree with you if you can point me to a single instance of a QA site that is considered authoritative on less than 10 questions a day." CreationDate="2014-06-13T05:11:38.660" UserId="434" />
  <row Id="1286" PostId="334" Score="3" Text="[bicycles.se], [workplace.se], [money.se], [skeptics.se], [gamedev.se] all launched with less than 10 questions per day. [Bicycles](http://area51.stackexchange.com/proposals/2305/bicycles) was launched with 4 per day because it was considered to be a high quality site." CreationDate="2014-06-13T05:18:36.150" UserId="62" />
  <row Id="1287" PostId="334" Score="2" Text="Well... I guess I have to declare you the winner at this point.  :)" CreationDate="2014-06-13T05:37:04.533" UserId="434" />
  <row Id="1288" PostId="339" Score="2" Text="Librariers - I do not agree at all with that. R is by far the richest tool set, and more than that it provides the information in a proper way, partly by inheriting S, partly by one of the largest community of reputed experts." CreationDate="2014-06-13T06:11:10.367" UserId="108" />
  <row Id="1289" PostId="349" Score="2" Text="Sometimes experience is hard to gain if your current job is not focused on data science but on some related field (in my case statistics). I use the courses to gain some knowledge and stay on topic, which I cannot do in my daytime job." CreationDate="2014-06-13T07:16:19.973" UserId="791" />
  <row Id="1290" PostId="235" Score="5" Text="Voted down as question does not show any research effort. Its not that hard." CreationDate="2014-06-13T07:26:14.520" UserId="471" />
  <row Id="1291" PostId="38" Score="0" Text="Voted down for lack of research effort. Hadoop and noSQL are well-defined elsewhere." CreationDate="2014-06-13T07:29:24.280" UserId="471" />
  <row Id="1293" PostId="354" Score="5" Text="Career-question, off-topic: http://meta.datascience.stackexchange.com/questions/41/is-a-question-about-future-career-paths-appropriate" CreationDate="2014-06-13T07:33:53.157" UserId="471" />
  <row Id="1294" PostId="354" Score="0" Text="Thank you, I was wondering if this was an feasible question, but did not see the meta thread. maybe adding an description to this site?" CreationDate="2014-06-13T07:34:51.107" UserId="791" />
  <row Id="1297" PostId="351" Score="2" Text="And what about stats.SE, datascience.SE profiles. Do you think they can say much about relevant level of knowledge?" CreationDate="2014-06-13T08:26:14.040" UserId="97" />
  <row Id="1299" PostId="327" Score="0" Text="But what about the libraries? There are advanced R packages (think Ranfom Forest or Caret) that would be utterly impractical to reimplement in a general purpose language such us C or Java" CreationDate="2014-06-13T09:35:46.040" UserId="457" />
  <row Id="1300" PostId="352" Score="0" Text="This is a good question.  The answer has been laid out in papers relating to various graphing libraries, but it will always depend on the assumptions and restrictions that you put in place.  Are the circles area appropriate? Will they always have at least X points of intersection?  Are you limited in size or quantity of categories?  More importantly is the question of whether or not a Euler diagram will be useful.  More than a dozen intersecting circles is hard to interpret, but if there's a mostly hierarchical relationship that you are depicting a much larger quantity can work." CreationDate="2014-06-13T09:48:54.630" UserId="434" />
  <row Id="1301" PostId="352" Score="0" Text="I could be wrong and there's an easier test, but with my experience with visualizations, the question has always been a practical one first, and honestly if I'm looking at intersecting categories I usually have a small collection of them." CreationDate="2014-06-13T09:53:37.330" UserId="434" />
  <row Id="1302" PostId="345" Score="1" Text="+1 for the bonus paper. Great read" CreationDate="2014-06-13T10:00:56.180" UserId="457" />
  <row Id="1303" PostId="350" Score="0" Text="There is no easy answer to this question, IMO.  Workloads and project goals vary significantly.  Think of immunology, for example.  An immunologist's tasks will vary throughout his career and at the senior level the focus will be very specialized.  Data science is like that, but even more broad because the technologies they work with and the data they work with can vary so much.  A data scientist at Facebook is going to have a very different workload and type of data to analyze than one at Baxter." CreationDate="2014-06-13T10:16:04.927" UserId="434" />
  <row Id="1304" PostId="327" Score="0" Text="mahout i.e. supports random forest for java" CreationDate="2014-06-13T10:17:04.727" UserId="115" />
  <row Id="1305" PostId="327" Score="0" Text="ok, maybe RF wasn't a good example, but you get my meaning: there are hundreds of statistical packages in R not implemented in other platforms" CreationDate="2014-06-13T10:21:46.037" UserId="457" />
  <row Id="1306" PostId="327" Score="0" Text="Yeah maybe, but R doesn't bring the performance at all that you need for proccessing big sets of data and most of the time you have really big datasets in industrial use." CreationDate="2014-06-13T10:41:21.297" UserId="115" />
  <row Id="1307" PostId="351" Score="0" Text="What do dropouts have to do with it? Presumably, certification is contingent upon completing the course, not merely registering…" CreationDate="2014-06-13T10:49:41.657" UserId="762" />
  <row Id="1309" PostId="355" Score="1" Text="I thinkt that's a good answer, thank you for the hints. &#xA;I especially like the idea to build up from a better fitting job - I had the impression that I had to know all, which is not the case.+" CreationDate="2014-06-13T11:15:12.490" UserId="791" />
  <row Id="1310" PostId="358" Score="1" Text="That's pretty close to my initial thought. (Which was Export, hadoop m/r the merge, import to temp, atomically replace table).  Export/Import/Indexing will still be very slow, but it will scale beyond 100M rows in 24hrs on everything but DB2.  DB2s indexing speed is a bear.  I'm wondering if there might be a high speed algorithm for finding a changed column.  Right now I'm thinking of hashing the row and hashing some row segments to at least narrow down changes without having to iterate each column for comparison.  I'm also wondering about in-memory caching of the production dataset." CreationDate="2014-06-13T12:46:46.617" UserId="434" />
  <row Id="1311" PostId="358" Score="0" Text="future questions for when I get moving on it, I suppose :)." CreationDate="2014-06-13T12:47:26.107" UserId="434" />
  <row Id="1312" PostId="358" Score="1" Text="Yeah. If you go down this route then pre-process step after exporting is probably where you wrangle it to a good state for some kind of comparison algorithm. Sounds like a really fun project tbh :)" CreationDate="2014-06-13T13:20:10.287" UserId="587" />
  <row Id="1313" PostId="349" Score="0" Text="I agree fully, the courses are very valuable for giving you a starting point, and some structure to gain that experience.  To get the most out of the Mooc I suggest taking a very specific example, lets say logistic regression, and really working through it with a different data set, double bonus if you do it in a language other than the one the course is taught in." CreationDate="2014-06-13T13:36:51.167" UserId="780" />
  <row Id="1314" PostId="349" Score="0" Text="That's a good idea. &#xA;What#s missing for statistics in general is a training website. E.g. a set of databases, along with goals and possible results at the end.&#xA;Something like khancademy, but more powerful ;)" CreationDate="2014-06-13T14:07:52.810" UserId="791" />
  <row Id="1315" PostId="339" Score="0" Text="I am not sure if the number of libraries for R is a purely good thing. There are too many ways to accomplish a goal, which confuses beginners and advanced students alike (ok, how can I summarize my data? summary() or describe() or...)&#xA;Also, often packages have theor own styles, which leads to headaches when you are working with multiple ones. Third, the naming is atrochious, a software engineer would be flogged for methods like read.csv2 oder cut2..." CreationDate="2014-06-13T14:10:40.397" UserId="791" />
  <row Id="1316" PostId="361" Score="0" Text="I guess you meant, &quot;*Logic often states that by (over)underfitting a model, it's capacity to generalize is increased.*&quot;" CreationDate="2014-06-13T16:51:41.793" UserId="84" />
  <row Id="1317" PostId="361" Score="0" Text="@Rubens: Correct, thanks, updated the text!" CreationDate="2014-06-13T16:56:24.207" UserId="158" />
  <row Id="1318" PostId="360" Score="0" Text="+1 Thanks, as a result of your answer, I've posted a followup to the question above, &quot;[When is a Model Underfitted?](http://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)&quot;" CreationDate="2014-06-13T16:59:09.830" UserId="158" />
  <row Id="1319" PostId="351" Score="0" Text="There are many people who mention that they are undergoing certification by doing a course on these MOOCs. You need to be careful with that." CreationDate="2014-06-13T17:02:53.243" UserId="735" />
  <row Id="1320" PostId="352" Score="1" Text="I am not familiar with this topic, but I spent in the past a lot of time studying graphs. I think that the property that an Euler diagram could be drawn is related with the planarity of the graph where the sets are the nodes. This paper seems to shade some light on this relation: [Ensuring the Drawability of Extended Euler Diagrams for up to 8 Sets](http://www.google.ie/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0CDMQFjAC&amp;url=http://www-rocq.inria.fr/~verroust/diagrams04.pdf&amp;ei=eJKaU9nPAYXH7Ab7g4HQBQ&amp;usg=AFQjCNF_H6ptg8ApWGmrWed4TN6FIH5mAA&amp;sig2=5ZDb8tEOrWG1xv-O7ckn7w)." CreationDate="2014-06-13T06:04:40.787" UserId="108" />
  <row Id="1321" PostId="362" Score="0" Text="This works well in Oracle, but not on SQL Server (at least some older versions).  For some reason I see bad hash comparisons (false positives, unflagged mismatches).  Pulling the data into an external environment should alleviate that." CreationDate="2014-06-13T18:32:29.797" UserId="434" />
  <row Id="1322" PostId="365" Score="0" Text="Part of the question is meant to gauge whether or not the community placed value on certification.  In some areas, certification is an absolute necessity.  In others, certification doesn't matter at all.  In still others, certifications by a particular company are held in high regard and competitive certifications are not.  The other part was meant to understand the difference in topical focus of the certifications that are out there.  Data Science is a broad term.  Certifications are normally more focused.  This is a bad question for QA format - it's more of a discussion, subject to opinion." CreationDate="2014-06-14T03:26:47.133" UserId="434" />
  <row Id="1323" PostId="365" Score="0" Text="My purpose in noting that I chose the answer by votes was to make it plain that all of the answers deserved reading.  Everybody makes good points, including you way down here at the bottom.  Somebody who is wondering about these things shouldn't limit themselves to the top one or two answers." CreationDate="2014-06-14T03:29:39.203" UserId="434" />
  <row Id="1324" PostId="365" Score="0" Text="Voting to find the right answer is a horrible idea.  It is the wrong way to approach math.  You clearly missed my point." CreationDate="2014-06-14T04:04:26.623" UserId="386" />
  <row Id="1325" PostId="223" Score="0" Text="@user1893354 Very helpful! Specially the &quot;[brat standoff format](http://brat.nlplab.org/standoff.html)&quot; used by it seems very suitable to my needs. I suggest posting an answer if you like." CreationDate="2014-06-14T08:09:09.150" UserId="227" />
  <row Id="1326" PostId="50" Score="1" Text="This is off-topic and has probably already been answered a hundred times on more general computing forums. Flagged." CreationDate="2014-06-14T09:31:19.403" UserId="471" />
  <row Id="1327" PostId="379" Score="1" Text="Thank you !!! This is helpful.." CreationDate="2014-06-15T05:30:48.820" UserId="496" />
  <row Id="1330" PostId="388" Score="1" Text="+1 Thanks for the references. Do you have any small example, or could show just an intuition behind one of the approaches? I'm not acquainted to neural networks, but I can check it out if the example requires such knowledge base." CreationDate="2014-06-15T15:25:05.307" UserId="84" />
  <row Id="1331" PostId="388" Score="0" Text="Could you be more specific about your use case?  Strategy can vary wildly depending on how you tend to implement the solution." CreationDate="2014-06-15T20:26:30.203" UserId="780" />
  <row Id="1332" PostId="388" Score="0" Text="I mean, I just would like to see what is the *idea* behind a different approach. For example, if I was to tell you what is done by using blacklists (which we know is not good), I could describe the *algorithm* as: scan the dataset looking for entries containing &quot;viagra&quot;; add such entries to the blacklist. I just would like to see a high-level description of a *methodology/algorithm*. Do they gather spam network usage information and put on a neural network classifier, or what do they do?" CreationDate="2014-06-15T20:35:17.443" UserId="84" />
  <row Id="1333" PostId="305" Score="0" Text="@SeanOwen in my question, I was referring to Apache Hadoop. Although it would be interesting to make the Impala comparison as well." CreationDate="2014-06-16T03:46:01.843" UserId="534" />
  <row Id="1334" PostId="395" Score="0" Text="good, thanks! Though not sure yet if I can use R package &quot;randomForest&quot; (http://cran.r-project.org/web/packages/randomForest/randomForest.pdf) to generate ERF. Probably not." CreationDate="2014-06-16T08:05:55.243" UserId="97" />
  <row Id="1335" PostId="399" Score="0" Text="I agree that I'm still only scratching the surface. That's mainly the result of being in the early stages of my project-design. (I had initially ruled out R since I was more interested in using a general purpose language)" CreationDate="2014-06-16T09:57:46.783" UserId="872" />
  <row Id="1336" PostId="61" Score="1" Text="Overfitting is bad by definition. If it weren't it wouldn't be *over*-fitting." CreationDate="2014-06-16T10:51:52.677" UserId="762" />
  <row Id="1337" PostId="351" Score="0" Text="@Kunal It makes sense but your answer jumps from the “certification” to “dropouts” (who presumably *don't have a certification*). The key here is *undergoing*. It's a bit like being registered as a student or having a Kaggle account. None of this tells us whether you should value someone who did actually get a degree, complete a course or participate in a competition to the end." CreationDate="2014-06-16T11:02:19.743" UserId="762" />
  <row Id="1338" PostId="400" Score="1" Text="Your point 2 is very important. Juggling with different tools means importing data. And data importing is an enormously error prone step and feels extremely unproductive - so avoid it, whereever you can." CreationDate="2014-06-16T13:25:03.337" UserId="791" />
  <row Id="1339" PostId="61" Score="0" Text="-1 for not being stated clearly. I would propose a fix, but the only ones I can think of would fundamentally change the meaning of the question. I think blunders might be conflating &quot;overfitting&quot; with &quot;adding model complexity&quot;." CreationDate="2014-06-16T14:06:19.127" UserId="675" />
  <row Id="1340" PostId="398" Score="9" Text="All these questions have been beaten to death on StackOverflow.  What value is there in rehashing it here?" CreationDate="2014-06-16T14:21:06.007" UserId="515" />
  <row Id="1341" PostId="404" Score="0" Text="Being a web developer, JSON seems completely reasonable to me, but, can you elaborate on the exact format of mapping words to entities?" CreationDate="2014-06-16T15:49:04.183" UserId="227" />
  <row Id="1342" PostId="61" Score="0" Text="@NathanGould: Thanks for commenting, though appears you're both inferring a meaning that is simply not present in the question, and quoting text that is also not present; meaning no where in the text are the words &quot;adding model complexity.&quot;" CreationDate="2014-06-16T15:57:44.980" UserId="158" />
  <row Id="1343" PostId="404" Score="0" Text="@AmirAliAkbari Updated answer to include more details." CreationDate="2014-06-16T17:35:52.767" UserId="548" />
  <row Id="1344" PostId="408" Score="1" Text="Another very useful package for forecasting and time series analysis is [forecast](http://cran.r-project.org/web/packages/forecast/index.html) by Prof. Rob J. Hyndman." CreationDate="2014-06-16T18:05:32.453" UserId="554" />
  <row Id="1345" PostId="409" Score="0" Text="Yes, visualization is an essential first step in any analysis." CreationDate="2014-06-16T18:42:16.387" UserId="178" />
  <row Id="1346" PostId="398" Score="0" Text="@DirkEddelbuettel Because they are off-topic on SO, hence often closed?" CreationDate="2014-06-16T18:56:18.513" UserId="843" />
  <row Id="1347" PostId="412" Score="0" Text="It would be useful to have a `pii` or `confidentiality` tag for this question." CreationDate="2014-06-16T19:50:06.313" UserId="322" />
  <row Id="1348" PostId="411" Score="11" Text="There is no question here.  If you need to do basic research on programming language, you are better off reading Wikipedia than to wait for someone to pop up here to push his hobby-horse." CreationDate="2014-06-16T20:05:09.750" UserId="515" />
  <row Id="1349" PostId="412" Score="0" Text="I added some tags." CreationDate="2014-06-16T20:07:44.737" UserId="381" />
  <row Id="1350" PostId="411" Score="0" Text="@DirkEddelbuettel Very good point. Thought it was better to try producing content than refining it at this point in the Beta, but I don't know a huge amount about SE betas. Was that a good move on my part or not?" CreationDate="2014-06-16T20:11:36.353" UserId="548" />
  <row Id="1351" PostId="61" Score="1" Text="I didn't mean to quote you on &quot;adding model complexity&quot; -- I was just highlighting the phrase. Anyhow I guess my issue is basically the same as @GaLa, which is that overfitting means fitting too much. So it seems you are asking us to confirm a tautology. So, I would tend to think that you actually meant to ask a different question. E.g., does increasing model complexity cause models to become worse? Or, how does complexity of the data relate to the tendency of a model to overfit?" CreationDate="2014-06-16T20:19:32.450" UserId="675" />
  <row Id="1352" PostId="411" Score="1" Text="Look at [these](http://economics.sas.upenn.edu/~jesusfv/comparison_languages.pdf) numbers." CreationDate="2014-06-16T20:20:29.817" UserId="381" />
  <row Id="1353" PostId="132" Score="0" Text="Both of these fall into the category of feature engineering as they involve manually creating or selecting features. Dimensionality reduction typically involves a change of basis or some other mathematical re-representation of the data" CreationDate="2014-06-16T21:05:47.767" UserId="890" />
  <row Id="1354" PostId="411" Score="0" Text="@DirkEddelbuettel you're not wrong, but my hope was to foster a discussion about the useful characteristics and tools associated with various languages. The language you use is an important tool in data science, so my thinking was that people could discuss the tools they preferred and there objective benefits here, as a resource for those looking to attempt similar work." CreationDate="2014-06-16T21:08:55.900" UserId="890" />
  <row Id="1355" PostId="411" Score="0" Text="This area is subject to such rapid change that it's probably best to strictly limit the scope of this question to &quot;what qualities make for the best tool&quot; rather than &quot;which tools exist and what are their qualities&quot; - even as a community wiki, the latter would require an intimidating amount of continued maintenance in order to be net useful" CreationDate="2014-06-16T21:54:22.960" UserId="322" />
  <row Id="1357" PostId="408" Score="0" Text="Do you know if this is already implemented in any other language?  I'm not exactly a pro with R.  I will definitely read the paper at least." CreationDate="2014-06-17T01:18:21.087" UserId="886" />
  <row Id="1358" PostId="409" Score="0" Text="I did add the month, day of month, day of week, and year as features.  I even tried a linearly decreasing &quot;Recentness&quot; value.  I don't think I have tried OLS.  I'm observing a time frame that could range anywhere from a couple weeks to multiple years.  As far as visualizing it goes, I did try to do that.  The problem is, we want the software to be able to predict automatically, without human intervention, for different customers." CreationDate="2014-06-17T01:24:29.210" UserId="886" />
  <row Id="1359" PostId="426" Score="0" Text="I am not 100% convinced by the topics on that course. For example, are SVMs actually of practical use these days? You never see a winning Kaggle entry that used SVMs as the main part." CreationDate="2014-06-17T07:45:32.770" UserId="910" />
  <row Id="1360" PostId="411" Score="0" Text="You need to learn either R  or Python properly and also be able to understand the other.  R has an unbeatable number of stats packages and nothing will ever be able to compare to that." CreationDate="2014-06-17T07:48:23.087" UserId="910" />
  <row Id="1361" PostId="405" Score="2" Text="R is unbeatable in terms of number of stats packages. So if you go for python you still need to learn enough R to be able to use rpy2 to call the missing packages." CreationDate="2014-06-17T08:07:24.300" UserId="910" />
  <row Id="1362" PostId="426" Score="1" Text="I think OP's question is specifically about *online* techniques - i.e. where system is expected to learn at least partially &quot;on the job&quot;. Not *online tutorials*" CreationDate="2014-06-17T08:40:15.450" UserId="836" />
  <row Id="1363" PostId="421" Score="2" Text="Could you clarify key aspects of &quot;online&quot; that you are interested in? Do you have a specific form for the data, or any options to pre-train your algorithm before the online part?" CreationDate="2014-06-17T08:41:49.577" UserId="836" />
  <row Id="1364" PostId="426" Score="0" Text="I agree with @NeilSlater since the OP mentioned &quot;compared to normal machine learning methods&quot;." CreationDate="2014-06-17T09:45:23.663" UserId="343" />
  <row Id="1365" PostId="426" Score="3" Text="lol, &quot;online&quot; is ambiguous" CreationDate="2014-06-17T10:19:41.480" UserId="122" />
  <row Id="1366" PostId="430" Score="0" Text="Did you make any research about this? There are many youtube videos and slideshare presentations describing different architectures" CreationDate="2014-06-17T10:53:46.840" UserId="478" />
  <row Id="1367" PostId="430" Score="1" Text="Hey Stanpol, thanks for your response - I did some initial searches and didn't really find anything besides AWS and cloudera stuff - maybe if you can give me some search terms that a promising, I'll be happy to take it from there." CreationDate="2014-06-17T11:28:41.010" UserId="913" />
  <row Id="1368" PostId="421" Score="0" Text="do you mean to analyze datastreams?" CreationDate="2014-06-17T11:42:23.613" UserId="115" />
  <row Id="1369" PostId="424" Score="0" Text="My guess is that this is based on learning topics in a large corpus." CreationDate="2014-06-17T12:49:06.043" UserId="241" />
  <row Id="1370" PostId="408" Score="0" Text="I am not familiar with one.  If you would like to use python, you can use the [rpy2](http://rpy.sourceforge.net/) package to call the glarma function while doing most of the rest of the programming in python.  Most other languages have such a connector as well." CreationDate="2014-06-17T12:59:46.167" UserId="178" />
  <row Id="1371" PostId="433" Score="0" Text="That's pretty awesome, exactly what I was looking for! Thanks a lot :)" CreationDate="2014-06-17T13:35:17.570" UserId="913" />
  <row Id="1372" PostId="433" Score="0" Text="@chrshmmmr You're welcome. Don't forget to upvote/mark as accepted if this helped!" CreationDate="2014-06-17T13:37:02.053" UserId="241" />
  <row Id="1373" PostId="433" Score="3" Text="These links seem very useful indeed, but then again, they are links, and I guess we should strive to maintain the answers independent of the stability of outer sources. Thus, it'd be nice if you could take some two or three minutes to add, for example, the diagram from [this link](http://blog.twitch.tv/2014/04/twitch-data-analysis-part-1-the-twitch-statistics-pipeline/), posting it along with a quick description. Something in the lines of: &quot;For example, this is the workflow of a ... system. &lt;img&gt;. Further info may be found in &lt;link&gt;.&quot;" CreationDate="2014-06-17T13:40:48.977" UserId="84" />
  <row Id="1374" PostId="427" Score="3" Text="There is no question here." CreationDate="2014-06-17T13:41:37.477" UserId="515" />
  <row Id="1375" PostId="433" Score="1" Text="@Rubens I will propose an edit in a bit.&#xA;fgnu: Will do so, just need a bit more reputation to actually upvote answers, but I certainly will honor your contribution :)" CreationDate="2014-06-17T13:42:47.237" UserId="913" />
  <row Id="1376" PostId="433" Score="0" Text="@Rubens That would be no more than reproducing the information at the link. I would if there were something I felt would add to the explanation already given there." CreationDate="2014-06-17T13:53:34.867" UserId="241" />
  <row Id="1377" PostId="433" Score="0" Text="The point is that the information would be here, instead. Of course, I'm not asking you to deliberately copy the whole of the references, but it'd be nice to have a small example in the answer itself, and not just in a link. I see this [*rule of thumb*](http://meta.stackexchange.com/questions/8231/are-answers-that-just-contain-links-elsewhere-really-good-answers) to be very praised in [stackoverflow](http://meta.stackoverflow.com/questions/251006/flagging-link-only-answers), but then again, we're not stackoverflow... I'll add a discussion about this on meta, anyway." CreationDate="2014-06-17T14:01:49.917" UserId="84" />
  <row Id="1378" PostId="411" Score="0" Text="@Lembik I appreciate the evangelism, but there isn't a single piece of statistical functionality that `R` has that `Python` doesn't. There are on the other hand, a large number of scientific and machine learning functions that `R` lacks, which are preset in `python`. Good `opencv` bindings are a great example." CreationDate="2014-06-17T14:45:46.360" UserId="548" />
  <row Id="1379" PostId="433" Score="0" Text="I've opened a discussion [here](http://meta.datascience.stackexchange.com/questions/63/flagging-link-only-answers). You're very welcome to participate. Thanks for raising this topic." CreationDate="2014-06-17T14:45:59.170" UserId="84" />
  <row Id="1380" PostId="411" Score="0" Text="@indico I am not sure if  I understand your point. There are literally thousands of packages in CRAN which don't exist for python. See http://cran.r-project.org/web/packages/available_packages_by_name.html ." CreationDate="2014-06-17T14:54:54.170" UserId="910" />
  <row Id="1381" PostId="411" Score="0" Text="@Lembik packages yes, but I am referring to functionality. The vast majority of the statistical functionality present in those `R` packages is contained simply within `scipy`" CreationDate="2014-06-17T14:56:48.123" UserId="548" />
  <row Id="1382" PostId="411" Score="0" Text="@indico This simply isn't true I am afraid.  People who do PhDs in stats, for example, still routinely write an R package as part of that. These very very rarely get implemented in scipy. How many new stats functions does scipy add every year? Try just picking a few of the R packages at random to see what I mean." CreationDate="2014-06-17T14:58:16.727" UserId="910" />
  <row Id="1383" PostId="405" Score="1" Text="@Lembik I would disagree with both of those points actually. As someone who has very frequently used both `R` and `python`, I've never had the need to use `rpy2`, and even though `R` wins in pure number of packages, `python` has decidedly more functionality across data science." CreationDate="2014-06-17T14:58:30.103" UserId="548" />
  <row Id="1384" PostId="411" Score="0" Text="@Lembik I don't really want to argue on this one. Could you please provide an example? `scipy` adds a huge amount of statistical functions every year, not really keeping track I would guess a few hundred each year." CreationDate="2014-06-17T15:00:20.027" UserId="548" />
  <row Id="1385" PostId="411" Score="0" Text="@indico Try http://cran.r-project.org/web/packages/overlap/index.html which is just the first one I happened to pick at random.  But really, I have personally known many statisticians who have written R packages. Not one of them has yet written a python one.  To broaden the conversation a little, http://www.kdnuggets.com/2013/08/languages-for-analytics-data-mining-data-science.html is interesting." CreationDate="2014-06-17T15:08:22.267" UserId="910" />
  <row Id="1386" PostId="411" Score="0" Text="@Lembik and my rebuttal in the form of a blog post: http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/" CreationDate="2014-06-17T15:11:36.837" UserId="548" />
  <row Id="1387" PostId="431" Score="0" Text="I thank you very much for posting this reference, but I was expecting the answers here to point a publicly available dataset/API for social network, *andalso* describe what is provided by such source (either the download rate of posts, or what kind of information about users). As your answer is, I guess it would be very welcome to the list of [publicly available datasets](http://datascience.stackexchange.com/questions/155/publicly-available-datasets) we have." CreationDate="2014-06-17T15:12:51.027" UserId="84" />
  <row Id="1388" PostId="411" Score="0" Text="@indico Oh I read that before. If you are the author of that blog I greatly admire it. However it is in no way a rebuttal to the fact that there are thousands of R packages written by statisticians which have no python equivalent :)" CreationDate="2014-06-17T15:13:00.223" UserId="910" />
  <row Id="1389" PostId="411" Score="0" Text="@Lembik I'm not trying to argue about a direct 1-to-1 correlation here. I'm trying to argue that in terms of raw functionality, here referring to kernel density estimation, there is nothing fundamentally missing from `python`, if we're going down the missing functionality line, there are an entire order of magnitude more python packages, and bindings to amazing academic projects such as opencv, which do not exist in R" CreationDate="2014-06-17T15:15:23.390" UserId="548" />
  <row Id="1390" PostId="411" Score="0" Text="@indico  You are right in terms of non stats packages. My comment was only meant to refer to statistics.  You may also be right about kernel density estimation although it looked like the overlap package had more in it than your blog post referred to. It would perhaps be interesting for someone to go through the thousands of R packages and see what is missing from python but I still claim that the fact is that PhD students from the best stats departments write R packages for their work and put them on CRAN. So if you have an interest in cutting edge stats you need to use those packages." CreationDate="2014-06-17T15:21:57.660" UserId="910" />
  <row Id="1391" PostId="411" Score="0" Text="@indico Just picking some more at random which have the word &quot;kernel&quot; in them. See http://cran.r-project.org/web/packages/bark/index.html , http://cran.r-project.org/web/packages/bbefkr/index.html , http://cran.r-project.org/web/packages/bpkde/index.html , http://cran.r-project.org/web/packages/DBKGrad/DBKGrad.pdf" CreationDate="2014-06-17T15:25:56.273" UserId="910" />
  <row Id="1392" PostId="405" Score="0" Text="I think that may be right for data science in general although I would argue that for cutting edge stats you still need to use R packages." CreationDate="2014-06-17T15:27:51.723" UserId="910" />
  <row Id="1393" PostId="405" Score="3" Text="@indico then you must be using the definition of &quot;Data Science&quot; that excludes statistics." CreationDate="2014-06-17T16:15:13.170" UserId="539" />
  <row Id="1394" PostId="416" Score="0" Text="We do quite a bit of processing already to link records in this way. I made some edits to the question, hopefully making it more clear what I'm after." CreationDate="2014-06-17T17:08:46.563" UserId="322" />
  <row Id="1395" PostId="405" Score="0" Text="I've accepted this answer since it gives the most direct answer to my question; not because it is the one true answer (read: check the other answers as well)" CreationDate="2014-06-17T17:30:31.490" UserId="872" />
  <row Id="1399" PostId="439" Score="0" Text="What interests me about the paper I linked is that it claims to show a method for performing this sort of computation *without knowledge of both input strings*. In the paper, each actor has knowledge of *one* string, which isn't useful for my purposes; I would need one actor to be able to perform the calculation without knowledge of *either* string. Calculating them beforehand is only feasible for very small datasets or very limited products; a full cross product of integer distances on my dataset would take ~10 PB of storage." CreationDate="2014-06-17T19:44:59.190" UserId="322" />
  <row Id="1400" PostId="439" Score="0" Text="That's why I brought up the idea of a substitution cipher (ROT13) since it preserves the distance between strings; but it's not secure, and I suspect it may be impossible to securely encrypt the strings while preserving the edit distance. (Would love to be wrong!)" CreationDate="2014-06-17T19:48:14.150" UserId="322" />
  <row Id="1401" PostId="416" Score="0" Text="Appreciate the k-anonymity reference, and the bin suggestion - that gives me some new things to think about." CreationDate="2014-06-17T19:57:32.907" UserId="322" />
  <row Id="1402" PostId="439" Score="0" Text="Right, I would just filter the matrix to only include Levenshteins below a certain cutoff, so you are only populating where there is high likelihood of overlap.  Additionally, when it comes to PII I am of the mindset that if you include enough information to determine a relationship among disparate entities in your datasets, its very unlikely you are preserving the customers anonymity.  The point of anonymizing the data is to avoid potential PII related regulatory headaches down that line, (standards can always be tightened), so personally I wouldn't take the risk." CreationDate="2014-06-17T20:27:56.733" UserId="780" />
  <row Id="1403" PostId="303" Score="0" Text="Maybe so.  As I said, this isn't my area.  Did you try going through all the steps in the Tutorials and also the &quot;Training&quot; section?  It appears that they take you through all the steps, starting with source texts.  Best wishes." CreationDate="2014-06-17T22:14:55.200" UserId="609" />
  <row Id="1404" PostId="303" Score="0" Text="it's the steps to get word alignments not phrases..." CreationDate="2014-06-17T22:30:15.820" UserId="122" />
  <row Id="1405" PostId="303" Score="0" Text="My impression was that the Tutorials and Training sections take you through all these steps, including phrase alignment.  I suggest going through all of them and you might discover which stage and tool accomplishes this task." CreationDate="2014-06-17T22:42:58.523" UserId="609" />
  <row Id="1406" PostId="449" Score="0" Text="thanks for the clarification. Keeping data in memory sounds like it has some interesting implications -I'll read up on Spark's Resilient Distributed Dataset concept a bit more." CreationDate="2014-06-18T10:30:55.527" UserId="426" />
  <row Id="1407" PostId="445" Score="0" Text="This is a pretty good idea. The Masters is in Statistics." CreationDate="2014-06-18T12:30:41.317" UserId="839" />
  <row Id="1408" PostId="453" Score="1" Text="Can you link to/reference where you read that the unary method is not universal? Context may help." CreationDate="2014-06-18T16:06:03.520" UserId="322" />
  <row Id="1409" PostId="453" Score="2" Text="I... am not sure how this relates to data science. It seems off-topic for this stack exchange. Could you possibly relate this back to data science?" CreationDate="2014-06-18T16:14:01.900" UserId="869" />
  <row Id="1410" PostId="453" Score="0" Text="@SlaterTyranus I... am not sure too (and that made me think about some two other questions I posted). My idea was to add this question since compression methods are largely used in information retrieval (mainly during indexing). In general, I find this related to efficiency, and it may be put in the *hacking skills* area of this [Venn diagram](http://en.wikipedia.org/wiki/Data_science). Anyway, I guess it'd be nice to discuss whether this kind of question is on topic." CreationDate="2014-06-18T16:33:25.077" UserId="84" />
  <row Id="1411" PostId="455" Score="1" Text="I think more details about your problem are necessary before anyone could recommend a dataset." CreationDate="2014-06-18T16:40:58.183" UserId="836" />
  <row Id="1412" PostId="451" Score="1" Text="how about some compression on the messages?" CreationDate="2014-06-18T17:54:09.643" UserId="743" />
  <row Id="1413" PostId="451" Score="0" Text="@user798196 That's one of the answers I was expecting. I'm just not sure about why it came as just a comment :D" CreationDate="2014-06-18T17:55:37.270" UserId="84" />
  <row Id="1414" PostId="453" Score="0" Text="@Rubens That seems like a reasonable discussion, in my mind efficiency talk fits much more into something like theoretical CS than explicit *hacking skills*. In my mind, hacking skills are much more related to things like databases, deployment, and knowledge of tools." CreationDate="2014-06-18T18:08:48.673" UserId="869" />
  <row Id="1415" PostId="451" Score="0" Text="I would totally suggest using zmq, what are you using for your message passing layer right now?" CreationDate="2014-06-18T18:10:56.937" UserId="869" />
  <row Id="1416" PostId="451" Score="0" Text="@SlaterTyranus I'm using a platform built on OpenMPI, but it does not perform message compression. Does zeromq do so? In fact, my intention here was to bring up such concepts (caching, compression, aggregation), and I was expecting them to come as answers. I'm also adding this question to a meta post, to discuss whether it is on topic." CreationDate="2014-06-18T18:22:29.890" UserId="84" />
  <row Id="1417" PostId="451" Score="1" Text="@Rubens They considered adding it, but decided it didn't make sense for a couple reasons. Discussion here: https://github.com/JustinTulloss/zeromq.node/issues/285" CreationDate="2014-06-18T18:26:11.227" UserId="869" />
  <row Id="1418" PostId="451" Score="0" Text="Also, @indico totally beat me to it. OpenMPI is very good for parallelism, but generally not optimal for actual distributed systems. My impression is that message compression adds more latency than it prevents." CreationDate="2014-06-18T18:27:44.977" UserId="869" />
  <row Id="1419" PostId="455" Score="3" Text="For what purpose? Spam filtering? Sentiment analysis? Without a clear purpose it is *very* difficult to suggest a dataset." CreationDate="2014-06-18T18:37:12.887" UserId="553" />
  <row Id="1420" PostId="453" Score="0" Text="@SlaterTyranus Hope you join the discussion [here](http://meta.datascience.stackexchange.com/questions/65/are-hacking-skills-questions-off-topic). I've tried to point what I consider to be *hacking skills*. Although I understand theoretical CS to be &quot;the place for algorithms&quot;, there are some *pratical* concepts that may would be nice to be added to this site." CreationDate="2014-06-18T18:41:55.673" UserId="84" />
  <row Id="1421" PostId="457" Score="0" Text="That's a very nice suggestion, thanks. I'm actually using a framework implemented upon OpenMPI, and therefore I won't be able to make such changes -- although they seem to be a very promissing improvement. Btw, I very much enjoyed the citation :D" CreationDate="2014-06-18T18:50:20.903" UserId="84" />
  <row Id="1422" PostId="452" Score="0" Text="Could you elaborate on how you would use this feature matrix?  Are you trying to learn based on change of traffic across days?" CreationDate="2014-06-18T19:19:17.557" UserId="886" />
  <row Id="1423" PostId="352" Score="1" Text="I'm assuming that by &quot;simple&quot; you mean the diagram uses only circles and that by &quot;proportional&quot; you mean that the area of each section of the diagram is proportional to the population it represents from the total set. It might help to make these definitions explicit in the question." CreationDate="2014-06-18T23:04:18.013" UserId="322" />
  <row Id="1424" PostId="466" Score="2" Text="What do you mean, bad results? Describe your process, your results, and how they differ from what you expected, instead of only linking to the git repository. Otherwise this discussion will be of no use to anyone." CreationDate="2014-06-18T23:08:41.183" UserId="322" />
  <row Id="1425" PostId="466" Score="0" Text="It's also true this :D. I added the description in the page &quot;The results has a 14.14 RMSE, so it can't predict so well the gas consumptions, consecutevely I can't run a good outlier detection mechanism. I see that in some papers that even if they predict daily or hourly consumption in the electric power, they have errors like MSE = 0.01.&quot;" CreationDate="2014-06-18T23:25:19.710" UserId="989" />
  <row Id="1426" PostId="466" Score="1" Text="@marcodena This is a QA site, and others need to know what you're trying to solve, so that they'll understand the answers, and will hopefully be able to use them in their own problems. That's what AirThomas meant, and is also why it'd be nice if you could describe what you're doing and what exactly you think is wrong. If the link to your git-hub page changes, the link here will be invalid, and others won't be able to understand what the problem is. Please, take a minute to make your question self-contained. Thanks." CreationDate="2014-06-19T00:35:40.470" UserId="84" />
  <row Id="1427" PostId="466" Score="0" Text="@Rubens you are right, but as you see in the github link, the description is more than 6 A4 pages long, so here it would be very difficult to explain everything. I'll try." CreationDate="2014-06-19T08:50:22.207" UserId="989" />
  <row Id="1429" PostId="477" Score="4" Text="This question appears to cross posted across multiple sites.  While it may be on topic here, it is also on topic (and has gained an answer) on Data Science.SE.  Questions should exist in one place only unless there are extenuating circumstances." CreationDate="2014-06-19T02:43:14.847" UserDisplayName="MichaelT" />
  <row Id="1430" PostId="477" Score="0" Text="As a computing problem it's hard to even make a start without some idea of scale and the success criteria. Is a cross-product possible? Should the answer be optimal, or just good enough? More info please." CreationDate="2014-06-19T06:30:51.003" UserDisplayName="david.pfx" />
  <row Id="1432" PostId="477" Score="0" Text="You need to define the question better, e.g., what is &quot;better suit their needs&quot;" CreationDate="2014-06-19T11:01:24.493" UserId="743" />
  <row Id="1434" PostId="61" Score="0" Text="@NathanGould: I think you're over thinking the question, or somehow expecting that I would ask a question that on a topic that I completely understood. Every single reference by me to complexity within the question related to the data being modeled, not the complexity of the model." CreationDate="2014-06-19T11:50:57.280" UserId="158" />
  <row Id="1437" PostId="468" Score="0" Text="I suggest you add the R tag as well." CreationDate="2014-06-19T13:28:55.910" UserId="178" />
  <row Id="1438" PostId="452" Score="0" Text="I edited the response to hopefully give more clarity." CreationDate="2014-06-19T14:32:56.907" UserId="403" />
  <row Id="1440" PostId="485" Score="0" Text="Your answer is very nice, and nice also would that be if you added a little example, so as to emphasize your point on data mining being more related to *detecting something new* rather than trying to *solve and reach results*." CreationDate="2014-06-19T16:54:42.093" UserId="84" />
  <row Id="1441" PostId="474" Score="1" Text="In what context did you see these terms? It seems that `p` and `k` simply show the size of the subgraph, not a special category of them." CreationDate="2014-06-19T17:07:13.287" UserId="227" />
  <row Id="1442" PostId="382" Score="0" Text="Great question! I think this is an important and non-trivial problem." CreationDate="2014-06-19T17:24:09.717" UserId="1011" />
  <row Id="1443" PostId="455" Score="0" Text="@lsdr Looking at the answers, it seems that the question does not necessarily need more details." CreationDate="2014-06-19T17:25:47.420" UserId="227" />
  <row Id="1445" PostId="455" Score="0" Text="@AmirAliAkbari I think they came after an edit. I retracted my close-vote, anyway." CreationDate="2014-06-19T17:30:36.410" UserId="84" />
  <row Id="1447" PostId="403" Score="0" Text="I don't know much about SAS, but your math is clearly right. Perhaps it's a bug in how you called the model predicted output?" CreationDate="2014-06-19T17:49:57.240" UserId="1011" />
  <row Id="1448" PostId="172" Score="0" Text="How large are your list of values ? Have you tried to pass it as a set ? For parallelism, you may be interested in Joblib. It is easy to use and can speed up computations. Use it with large chunks of data." CreationDate="2014-06-19T18:22:28.557" UserId="1023" />
  <row Id="1449" PostId="489" Score="0" Text="Many thanks for your answer." CreationDate="2014-06-19T18:22:56.117" UserId="1021" />
  <row Id="1451" PostId="61" Score="0" Text="@blunders Maybe the question should simply be “What is overfitting?” or “How can a model fitting the data more precisely be worse?”" CreationDate="2014-06-19T20:27:23.540" UserId="762" />
  <row Id="1452" PostId="492" Score="0" Text="Have you tried training the Stanford NER on your corpus? There's a tutorial [here](http://www.linguisticsweb.org/doku.php?id=linguisticsweb:tutorials:linguistics_tutorials:automaticannotation:stanford_ner)." CreationDate="2014-06-19T20:58:53.030" UserId="381" />
  <row Id="1454" PostId="488" Score="0" Text="I think machine learning is usually an application of statistical modelling, so I'd say it is both." CreationDate="2014-06-19T21:08:37.097" UserId="922" />
  <row Id="1455" PostId="492" Score="0" Text="I have not -- should give that a go to see how it fares." CreationDate="2014-06-19T21:30:42.020" UserId="684" />
  <row Id="1456" PostId="475" Score="0" Text="Logistic regression can be used as a binary classifier, but it isn't inherently one.  You could be using it to estimate odds or determine the relationship of a predictor variable to the outcome." CreationDate="2014-06-19T21:32:33.223" UserId="953" />
  <row Id="1457" PostId="492" Score="0" Text="I'd like to use word2vec features or similar, though, since I have access to a relatively small labeled dataset and need to make the most of unlabeled data I have on hand." CreationDate="2014-06-19T21:37:43.327" UserId="684" />
  <row Id="1459" PostId="61" Score="0" Text="@GaLa: Thanks for the feedback, though I wanted to know why overfitting is bad, not what overfitting is, or how fitting the model to the data more precisely is bad. Beyond that, at this point, given there are 3 answers, and 40+ votes, I do not feel that it would either be fair or a good idea to change the meaning and/or request made by the question. If you believe that the question might be better expressed, my suggestion would be just to post a question yourself; please feel free to link to it in the comments here. Again, thanks!" CreationDate="2014-06-19T21:56:03.637" UserId="158" />
  <row Id="1460" PostId="466" Score="1" Text="When you find that your problem takes a very long time to explain, that is when it's *most important* to spend the time to explain your question to others, explicitly and with plenty of details and discussion of your research/attempts. Often during that process you will find some or all of the answers yourself. Not only is that a great feeling, if what you find is useful to others, you can still post that question you spend so much time on, *and* the answer(s) you came up with." CreationDate="2014-06-19T23:53:47.010" UserId="322" />
  <row Id="1461" PostId="474" Score="0" Text="please see the edit" CreationDate="2014-06-20T02:27:44.880" UserId="957" />
  <row Id="1462" PostId="481" Score="0" Text="please see the edit" CreationDate="2014-06-20T02:29:46.337" UserId="957" />
  <row Id="1463" PostId="495" Score="0" Text="Thanks for the input. I have taken a very similar approach. So, its good that I am proceeding in the right direction. I am going to keep the question open for a few more days to see if anyone comes up with other ideas or more complex analysis techniques." CreationDate="2014-06-20T02:56:48.260" UserId="1028" />
  <row Id="1464" PostId="495" Score="2" Text="That's very reasonable and I will be curious to read other peoples' approaches to this problem. I would just add that the goal of good data science should be to use data to answer the question at hand, not to concoct the most complex analysis technique." CreationDate="2014-06-20T03:56:30.570" UserId="1011" />
  <row Id="1465" PostId="202" Score="0" Text="We home/hope the same about tex support! :D" CreationDate="2014-06-20T06:15:57.220" UserId="84" />
  <row Id="1466" PostId="191" Score="0" Text="Please, add information/reference link on that *MNIST data*, so as to make your post self-contained. Thanks." CreationDate="2014-06-20T06:19:47.490" UserId="84" />
  <row Id="1467" PostId="500" Score="0" Text="Using that logic neural networks are statistical models since the architecture is decided in advance. I don't think attempts to define a clear cut between statistics and machine learning are possible nor necessary." CreationDate="2014-06-20T06:54:45.653" UserId="119" />
  <row Id="1468" PostId="415" Score="0" Text="To add to this answer: in terms of scalability, `Scala` and `Go` are worth mentioning." CreationDate="2014-06-20T07:17:43.250" UserId="119" />
  <row Id="1469" PostId="61" Score="0" Text="@blunders We are back to my original comment but it seems to me that if you know what overfitting is, you know why it's bad. Since you seemed to suggest earlier you didn't fully understand the topic (which is fine), I would think the first thing to ask is simply what overfitting is." CreationDate="2014-06-20T07:37:44.590" UserId="762" />
  <row Id="1470" PostId="61" Score="0" Text="Also, the answers seem quite messy, perhaps because the question was so confusing to begin with but some of them do seem to address it in the way I suggest." CreationDate="2014-06-20T07:39:37.753" UserId="762" />
  <row Id="1471" PostId="449" Score="1" Text="+1 for a really clear and useful answer for a lot of people who had this question, like me." CreationDate="2014-06-20T09:20:30.793" UserId="113" />
  <row Id="1473" PostId="500" Score="0" Text="This is exactly the reason why I have mentioned the word 'rarely' in machine learning paragraph. I haven't said that you absolutely don't! Well, to the people who start exploring these things, it's good to know the nuances between statistical learning and machine learning" CreationDate="2014-06-20T10:17:59.630" UserId="514" />
  <row Id="1474" PostId="61" Score="0" Text="@GaLa: If you want to ask your own question, as stated above, please do. If you have any issues with the answers, please address them within the comments to the answer. The top answer, which I selected when it had I believe one or two votes, now has 18+ votes, and the question received a number of valid answers; meaning nothing to see, moving on." CreationDate="2014-06-20T11:49:26.910" UserId="158" />
  <row Id="1475" PostId="481" Score="0" Text="an n-clique is something else altogether from a k-clique or a p-clique. The n-clique refers to a maximal distance between two subgroups, where a k-clique refers to a clique of size k, where k is some constant chosen, and a p-clique is simply a clique composed of p nodes again where p is a chosen constant" CreationDate="2014-06-20T11:50:47.167" UserId="59" />
  <row Id="1476" PostId="463" Score="0" Text="Wright. The main issue here is the number of dimensions, i.e the level of detail I need to use. Could you clarify to me  “IR approach”." CreationDate="2014-06-20T14:49:06.473" UserId="986" />
  <row Id="1478" PostId="502" Score="0" Text="Thank you very much for the answer and references." CreationDate="2014-06-20T15:02:47.023" UserId="1021" />
  <row Id="1479" PostId="500" Score="0" Text="Thanks for the further clarifications!" CreationDate="2014-06-20T15:03:33.823" UserId="1021" />
  <row Id="1481" PostId="442" Score="0" Text="Another vote for dimensionality reduction. Just some additions: `Principal Component Analysis` or `Non-Negative Matrix Factorization` will reduce number of variables, enrich sparse data, and transform all variables to quantitative. Moreover, evaluating quality of dimensionality reduction model, question author can estimate usefulness of textual variables." CreationDate="2014-06-20T16:13:35.127" UserId="941" />
  <row Id="1482" PostId="463" Score="1" Text="By IR, I meant Information Retrieval. You might think that the documents (sellers) in your collection and the query (a buyer) are all vectors embedded in a term (attribute) space. As I said, such an approach needs a preset number of dimensions to work with." CreationDate="2014-06-20T16:38:49.233" UserId="984" />
  <row Id="1483" PostId="510" Score="0" Text="Sheer awesomeness! I was actually expecting something like this dissolved into many answers, and you came carrying the whole :D Thanks for the answer. Nice job! :)" CreationDate="2014-06-20T18:08:24.743" UserId="84" />
  <row Id="1485" PostId="504" Score="0" Text="Can you clarify you question by defining the goals of this analysis?  What is the nature of the 10 to 15 categories?  Are these categories you define a priori or are they clusters suggested by the data itself?  I appears that your question is centered on the choosing a good data encoding/transformation process rather than on data analysis methods (e.g. discriminant analysis, classification)." CreationDate="2014-06-20T18:25:53.873" UserId="609" />
  <row Id="1486" PostId="511" Score="1" Text="I guess a prior question is: doesn't this selection depend on the algorithm (mainly its behavior) being evaluated?" CreationDate="2014-06-20T18:28:44.200" UserId="84" />
  <row Id="1487" PostId="504" Score="1" Text="If your documents range from single words to full page of text, and you aim to have any combination of document lengths/types in any category, then you'll need to use a very simple encoding method such as Bag of Words.  Anything more complicated (e.g. grammar style) won't scale across that range." CreationDate="2014-06-20T18:29:26.120" UserId="609" />
  <row Id="1488" PostId="511" Score="1" Text="@Rubens, I've updated the question: I'm intersted in RF and logistic regression" CreationDate="2014-06-20T18:34:39.000" UserId="97" />
  <row Id="1489" PostId="510" Score="1" Text="I left behind the scope LinkedIn, YouTube, Secret. Maybe other regional networks (QQ?). And would be glad to get any info about them." CreationDate="2014-06-20T18:36:03.220" UserId="941" />
  <row Id="1491" PostId="403" Score="0" Text="Ben, thanks for the reply. I called the Scoring node directly from SAS Enterprise Miner, not much options we can change there. I compared the scoring output with what R predicted, and they perfectly matched. Then used the estimates manually , R's estimates theoretically matched with the output. It's just the problem with what SAS is giving as output, I am unable to understand that." CreationDate="2014-06-20T18:45:56.357" UserId="880" />
  <row Id="1492" PostId="509" Score="0" Text="No, it has nothing to do with spatial data in particular. It's applicable to temporal, spatio-temporal, and other sorts of data too." CreationDate="2014-06-21T06:10:06.757" UserId="381" />
  <row Id="1493" PostId="516" Score="3" Text="What is this algorithm *fmincg*? Please, add some references to such algorithm in your question." CreationDate="2014-06-21T06:51:15.523" UserId="84" />
  <row Id="1494" PostId="61" Score="0" Text="@blunders I just think this question is a bit of a mess and since the comments seemed to confuse you, I tried to explain them a bit." CreationDate="2014-06-21T09:17:29.030" UserId="762" />
  <row Id="1495" PostId="464" Score="0" Text="Thanks :), I already visited it before but I found it's classifications are weak not abstract enough or it may be not related to my content" CreationDate="2014-06-21T10:25:19.050" UserId="960" />
  <row Id="1496" PostId="466" Score="1" Text="Just a clarification, when you mention that &quot;in some papers  they have errors like MSE = 0.01&quot;, do you refer to the same dataset you are using? Or is it a different dataset altogether?" CreationDate="2014-06-21T17:29:42.433" UserId="1085" />
  <row Id="1497" PostId="507" Score="0" Text="@1 for crazy marketing guys. Working in market research and the twisting done to poor statistics makes me sad..." CreationDate="2014-06-22T05:18:22.043" UserId="791" />
  <row Id="1498" PostId="389" Score="0" Text="Wouldn't that also mean that my *bug* is stable across subsamples?" CreationDate="2014-06-22T06:13:51.587" UserId="846" />
  <row Id="1499" PostId="516" Score="0" Text="What is your alternate cost function? The optimal optimization algorithm depends on it :)" CreationDate="2014-06-22T12:03:45.373" UserId="1061" />
  <row Id="1500" PostId="403" Score="0" Text="Well I'm glad you were able to get a reasonable result with R! Hopefully someone who knows why this issue exists in SAS can answer the question!" CreationDate="2014-06-22T13:04:26.187" UserId="1011" />
  <row Id="1501" PostId="518" Score="0" Text="Your question is very weak, since you are neither presenting any problem, nor trying to understand something specific. Asking for comparisons between [online/batch k-means](http://datascience.stackexchange.com/questions/458/k-means-vs-online-k-means), or for how to efficiently implement some part of the algorithm is ok; asking for simple descriptions is too broad -- a google search may be a better fit." CreationDate="2014-06-22T16:56:52.783" UserId="84" />
  <row Id="1502" PostId="518" Score="0" Text="@Rubens I'm asking for a recommendation so what's wrong about that ?, I already googled it but I couldn't find something useful." CreationDate="2014-06-22T17:27:57.603" UserId="960" />
  <row Id="1503" PostId="389" Score="0" Text="That is a possible outcome, but you'll only knew once you try. And if so, you could at least debug on smaller data sets." CreationDate="2014-06-22T18:15:10.680" UserId="515" />
  <row Id="1505" PostId="527" Score="2" Text="It might be easier to answer your question if you gave an example of a matrix that you would consider similar to the first, and explained what qualities you are looking for in terms of similarity. Or if there is a general goal here, what is the task you mean to accomplish?" CreationDate="2014-06-23T02:25:12.900" UserId="322" />
  <row Id="1506" PostId="466" Score="0" Text="@AirThomas that's why I created a repository with a full explanation, with graphs etc." CreationDate="2014-06-23T09:38:09.920" UserId="989" />
  <row Id="1507" PostId="466" Score="0" Text="@insys no.. different :)" CreationDate="2014-06-23T09:38:25.643" UserId="989" />
  <row Id="1508" PostId="415" Score="0" Text="I would add *clarity and brevity* (related to syntax and language architecture, but not only). Being able to write fast and read without pain makes a huge difference (as programmers time is more expensive than machine time)." CreationDate="2014-06-23T11:14:14.553" UserId="289" />
  <row Id="1509" PostId="536" Score="1" Text="The question you are asking is vague. Please correct me if I'm wrong but the question essentially sums up to &quot;what to do if I have a small amount of data entries that need processing?&quot;. There are about a gazillion different answers to that depending on an equally large number of factors. You might at least present an example or make your question more specific. Things to consider: - What kind of data/fixes/errors are we talking about? -What prevents you from using a script exactly? -What are the &quot;unintended consequences&quot; you mention?" CreationDate="2014-06-23T11:15:13.220" UserId="1085" />
  <row Id="1510" PostId="536" Score="0" Text="@insys I gave examples (with city names) and modified question to address your doubts." CreationDate="2014-06-23T11:21:30.430" UserId="289" />
  <row Id="1511" PostId="536" Score="1" Text="About your phrase &quot;is not 100% clean&quot;: get used to it - it will always be like that :) Other than that, you have multiple questions in one. For example, named entities (cities) disambiguation reserves to be a question on its own. Overall there is no best practice, there's a whole field called data cleaning..." CreationDate="2014-06-23T11:29:16.800" UserId="418" />
  <row Id="1513" PostId="536" Score="0" Text="@iliasfl It's not my first day (but where data is far from being clear manual fixes usually make no sense). Point of this question is not city disambiguation (which is a fascinating topic on its own, and depends on a lot of things). It is how to incorporate manual changes (of, say, a few entries) into the workflow." CreationDate="2014-06-23T12:36:51.537" UserId="289" />
  <row Id="1514" PostId="483" Score="0" Text="I was looking into Neural Networks as an option, but didn't know what kind of parameters I would use.  I'll have to give those a shot." CreationDate="2014-06-23T12:54:17.227" UserId="886" />
  <row Id="1515" PostId="493" Score="0" Text="I didn't learn or get anything useful from this answer.  You assume I don't have ML experience, and you also assume my data is in an SQL database.  Most of all, the answer is way too generic." CreationDate="2014-06-23T12:58:06.640" UserId="886" />
  <row Id="1516" PostId="408" Score="0" Text="I will look into this option. Thanks." CreationDate="2014-06-23T12:59:31.563" UserId="886" />
  <row Id="1520" PostId="533" Score="0" Text="Adjusting the threshold for logistic regression did the trick. Thanks for the list of sources." CreationDate="2014-06-23T15:13:12.467" UserId="793" />
  <row Id="1521" PostId="538" Score="3" Text="off topic - discussion, opinion, and fist-fights will result." CreationDate="2014-06-23T15:15:53.153" UserId="471" />
  <row Id="1522" PostId="538" Score="2" Text="['To prevent your question from being flagged and possibly removed, avoid asking subjective questions where every answer is equally valid: “What’s your favorite ______?”'](http://datascience.stackexchange.com/help/dont-ask)" CreationDate="2014-06-23T15:29:06.037" UserId="322" />
  <row Id="1524" PostId="540" Score="4" Text="This is too broad - please read [What types of questions should I avoid asking?](http://datascience.stackexchange.com/help/dont-ask) in the help center." CreationDate="2014-06-23T16:58:38.603" UserId="322" />
  <row Id="1525" PostId="541" Score="0" Text="Thanks, but you answered a different question. Mine is on workflow, not - science. I mean, in this particular case my goal is not making numerical estimations (then removing 'exceptions' by hand, or any other form of censoring, needs serious justification; and even with it is is easy to delude oneself or others), but rather (say) modifying data for visualization so end-user don't see misspelled cities (or worse: data split between copies of the same city, one with misspelled name). In any case you are right that explicit documentation of changes is crucial." CreationDate="2014-06-23T17:15:46.740" UserId="289" />
  <row Id="1526" PostId="541" Score="0" Text="My point is that the same principle applies to *how* you change the data, even if only how it is displayed. You need to make clear to the end user that some data has been modified. If what you are asking is exactly how one should modify their data, there is no good answer that will apply to every circumstance; it is entirely dependent on your use case, your software, your data structure, etc. etc. etc." CreationDate="2014-06-23T17:25:05.013" UserId="322" />
  <row Id="1527" PostId="540" Score="0" Text="Depends what type of dataset it is and more importantly what do you want to do with it? Ask yourself these questions... i) what is my objective; ii) if i'm going to train a model, will it be supervised or unsupervised?; iii) if supervised, do i have a train-test split? iv) do i have the reference class labels? etc." CreationDate="2014-06-23T17:28:50.353" UserId="984" />
  <row Id="1530" PostId="497" Score="0" Text="There is a whole class of statistical analytics devoted to modeling longitudinal data. If you had repeated measures on the same subjects then mixed models are used often as state of the art in social sciences to determine if there is impact of an intervention. If you have a time series only something like an Arima can be used." CreationDate="2014-06-23T18:14:57.430" UserId="1138" />
  <row Id="1532" PostId="527" Score="0" Text="Yeah, I'd like to see an example of what a 1 would look like and what a 0 would look like." CreationDate="2014-06-23T18:58:14.737" UserId="1011" />
  <row Id="1533" PostId="546" Score="3" Text="And you can formalize this and make it &quot;prettier&quot; by constructing in your script a set of known cities and an `N:1` mapping of known misspellings." CreationDate="2014-06-23T19:11:23.767" UserId="322" />
  <row Id="1534" PostId="497" Score="0" Text="A RDD approach might also be useful for you: http://austinclemens.com/blog/2014/06/08/436/" CreationDate="2014-06-23T19:30:58.427" UserId="1138" />
  <row Id="1535" PostId="549" Score="0" Text="My concern with this answer is that PCA doesn't recognize the clear dependency between the series t and t+1." CreationDate="2014-06-24T01:50:57.443" UserId="1138" />
  <row Id="1541" PostId="559" Score="1" Text="Perhaps this question is better suited to the cross validation SE site, now that I think of it. The distinction is somewhat unclear to me..." CreationDate="2014-06-24T12:36:56.940" UserId="1147" />
  <row Id="1542" PostId="559" Score="4" Text="I think the question is very much fitting to this site, since it discusses a practical application of machine learning. btw, silly question, why so few photos of cats? Do they only come around for just five seconds?" CreationDate="2014-06-24T12:45:46.473" UserId="1085" />
  <row Id="1545" PostId="552" Score="0" Text="Thanks for the link. I had also considered using DTW and hierachical clustering. I have experimented with the R package for DWT. http://www.jstatsoft.org/v31/i07/paper" CreationDate="2014-06-24T13:58:49.467" UserId="1138" />
  <row Id="1546" PostId="552" Score="1" Text="I considered specifically creating n clusters and using the clustering membership as a feature." CreationDate="2014-06-24T13:59:40.057" UserId="1138" />
  <row Id="1547" PostId="550" Score="0" Text="Nice suggestions! Can you flesh out the use of derivatives more?" CreationDate="2014-06-24T14:04:44.013" UserId="1138" />
  <row Id="1548" PostId="550" Score="0" Text="I agree completely with your first statement. I would LOVE to see a box written which collected case studies on feature engineering / extraction. The adage is that feature creation is much more important than the latest greatest algorithm in predictive model performance." CreationDate="2014-06-24T14:07:06.180" UserId="1138" />
  <row Id="1550" PostId="523" Score="0" Text="I tried your FFT method but I really don't get how to set the frequency threshold and amplitude with my data. I'll keep looking, but if u could help me..." CreationDate="2014-06-24T15:28:00.807" UserId="989" />
  <row Id="1552" PostId="559" Score="0" Text="@insys, rumors about my vigilance with the soaker appears to have spread in the feline community. They tend not to linger like they used to. I guess that's a good thing w/r/t the actual objective of ridding my garden of cats, even though it complicates my preferred, more sophisticated solution." CreationDate="2014-06-24T19:54:37.957" UserId="1147" />
  <row Id="1553" PostId="561" Score="0" Text="This is a nice idea for a controlled environment but I'm not sure of it's applicability in this case, since we are dealing with the natural environment where there is continuous change, i.e. change in weather, position of sun, plants and trees because of wind, seasons etc. I believe the region of change as you describe would grow close to the size of the whole image in any case." CreationDate="2014-06-24T20:05:18.477" UserId="1085" />
  <row Id="1555" PostId="567" Score="0" Text="thanks but I'm aiming this at non technical business users, ive updated question. I think a colormap would go over their heads" CreationDate="2014-06-24T20:34:33.083" UserId="237" />
  <row Id="1556" PostId="567" Score="0" Text="@blue-sky Updated my answer. May not be what you're looking for, but that's all I've got =)" CreationDate="2014-06-24T20:37:47.817" UserId="1163" />
  <row Id="1557" PostId="561" Score="0" Text="@insys - I see your point but I disagree - I believe it makes the detector more resilient to change. The time difference between relative frames should be small (~ seconds to a minute) so sun, season, weather should be negligible. I agree that wind will cause plants to move but the classification step can avoid those since their size/shape/color are different than a cat. Plus, using two frames at similar times enables normalizing pixel intensities to better handle varying illumination conditions (e.g., a cat on a sunny vs. cloudy day)." CreationDate="2014-06-24T20:55:20.650" UserId="964" />
  <row Id="1558" PostId="559" Score="2" Text="Seems like the obvious next step (after you get the cat detection working) is a raspberry pi controlled super soaker :-)" CreationDate="2014-06-24T20:56:38.683" UserId="1167" />
  <row Id="1559" PostId="561" Score="0" Text="Actually, I am more confused about your answer now that I read through your comment :) Perhaps I misunderstood, but if you actually use the &quot;extracted change regions&quot; to form your positive samples, as mentioned in your question, how do you even make sure they are cats? They could be anything. As such, your classification step would fail to detect anything but what is taught to detect – that is, changes of any kind. So it is actually repeating the job of the &quot;change&quot; detector." CreationDate="2014-06-24T21:21:19.717" UserId="1085" />
  <row Id="1560" PostId="561" Score="0" Text="Furthermore, illumination conditions are definitely of concern, but, if I get your point right, it is unclear what two similar images, taken with a difference of 1 minute would offer towards normalising pixel intensities?" CreationDate="2014-06-24T21:22:00.297" UserId="1085" />
  <row Id="1561" PostId="561" Score="0" Text="The extracted regions can represent either positive or negative examples - they are what you would use to train the cat classifier. With regard to intensities, Suppose the classifier is trained from regions extracted primarily from sunny images. The classifier might then easily find cats with bright white fur but that won't work well later on a cloudy day (when the white fur isn't nearly as bright) or near dusk. Performing a normalization of the two images helps mitigate that problem (i.e., a pair of bright images and a pair of dim images would appear similar to the classifier)." CreationDate="2014-06-24T22:00:18.433" UserId="964" />
  <row Id="1562" PostId="446" Score="0" Text="Marking this as the accepted answer because out of the approaches suggested, it's the most promising for my particular use case." CreationDate="2014-06-24T22:00:36.210" UserId="322" />
  <row Id="1564" PostId="512" Score="0" Text="I don't think this suggestion addresses the problem as it's presented in the question. Where is the flexibility post-encryption? How do I refine your analysis without access to the original data?" CreationDate="2014-06-24T22:17:32.963" UserId="322" />
  <row Id="1565" PostId="568" Score="0" Text="would this work on a manchester encoded signal where the value is encoded in the state transitions?" CreationDate="2014-06-24T22:21:32.167" UserId="890" />
  <row Id="1566" PostId="559" Score="1" Text="@Kryten related: YouTube [Militarizing Your Backyard with Python: Computer Vision and the Squirrel Hordes](https://www.youtube.com/watch?v=QPgqfnKG_T4)" CreationDate="2014-06-24T22:26:40.920" UserId="1170" />
  <row Id="1570" PostId="512" Score="0" Text="@AirThomas I'm sorry but I don't understand your two questions.  What do you mean by &quot;flexibility post-encryption&quot;? I didn't see anything in your question/description like that.  What do you mean &quot;refine your analysis without access to the original data&quot;? I didn't see anything about &quot;refining&quot;." CreationDate="2014-06-25T02:53:14.440" UserId="609" />
  <row Id="1571" PostId="566" Score="0" Text="So can I say that your question is about &quot;Is combining two consecutive NNP Trees a good approach?&quot;" CreationDate="2014-06-25T03:43:55.783" UserId="1177" />
  <row Id="1572" PostId="512" Score="0" Text="I tried to identify the problem in the second paragraph of the **Motivation** section. Imagine, for example, that you want to release your data set to various researchers who want to do some modeling. There are any number of clever and effective methodologies that could be applied, and each researcher works a little differently. You can't disclose the names of private individuals in your data set. If you perform that portion of the analysis before releasing the data, it forces your choice of methodology on everyone." CreationDate="2014-06-25T03:49:58.180" UserId="322" />
  <row Id="1573" PostId="512" Score="0" Text="If you additionally provide hashes of the names, the benefit is that third parties can distinguish exact identity, but no more. So the question is, how might you provide more information about the data that you can't release? For example, is there a method that preserves in the hashing/encryption output the edit distance between arbitrary inputs? I have found at least one method that at least approximates that functionality (for more information, see my own answer). I hope that makes things more clear." CreationDate="2014-06-25T03:57:55.367" UserId="322" />
  <row Id="1574" PostId="47" Score="0" Text="Another package is distributedR which allows you to work with distributed files in RAM." CreationDate="2014-06-25T07:03:56.810" UserId="1155" />
  <row Id="1575" PostId="560" Score="0" Text="And what would happen if your block splits the cut into two or more slices? The blocking strategy is a very common approach, but when having a camera completely fixed to a certain position, motion detection is a better and less time-consuming approach, from my point of view." CreationDate="2014-06-25T07:08:37.443" UserId="1155" />
  <row Id="1576" PostId="568" Score="0" Text="It depends on the Manchester codification but I would say so. Nonetheless, previous to a HMM training, I'd suggest to use a zero-crossing algorithm to detect flanks of the signal. With this, you could detect the minimum time a change occurs which can give you a hint on the clock speed." CreationDate="2014-06-25T07:11:45.257" UserId="1155" />
  <row Id="1578" PostId="565" Score="1" Text="Any chance of a reproducible example with code and data?" CreationDate="2014-06-25T07:29:35.100" UserId="471" />
  <row Id="1579" PostId="560" Score="0" Text="@adesantos – What you say may well be true, and for prediction differentiating between moving and non-moving parts has it's advantages. But for training, the way it is described by bogatron, it is unclear what benefits it brings to the table. Overall, my opinion is it adds complexity, which lengthens the debugging time significantly. The advantage of moving window is in it's simplicity." CreationDate="2014-06-25T07:36:28.230" UserId="1085" />
  <row Id="1580" PostId="560" Score="0" Text="Btw, regarding the split that you mention, an obvious strategy is to let your windows overlap, so that split position does not affect your classifier." CreationDate="2014-06-25T07:36:50.183" UserId="1085" />
  <row Id="1581" PostId="570" Score="0" Text="please see question update, so for my use case instead of a movie display a user (that is similar) , the stars indicate the degree of similarity ?" CreationDate="2014-06-25T08:01:22.837" UserId="237" />
  <row Id="1584" PostId="560" Score="0" Text="I would add to my proposal (motion detection) the use of SIFT algorithm with a cat texture. The SIFT method can also be used with that strategy of blocks, but in that case you will compare more blocks than require.&#xA;&#xA;Notice that a cat moves, but a tree or a bush not that much." CreationDate="2014-06-25T09:06:43.937" UserId="1155" />
  <row Id="1585" PostId="570" Score="0" Text="Exactly - instead of a movie poster, you can display a user profile picture.  So several rows - Users similar to A, Users similar to B, etc. and across each row, user profile pictures.  Below each user profile picture a star rating.  You indicate that the similarity is stronger the closer to 0, so .2-.0 = 5 stars, .4-.2 = 4 stars, etc.  If there are no profile pictures available, a user name would do, but people will respond to photos better." CreationDate="2014-06-25T09:08:57.833" UserId="434" />
  <row Id="1586" PostId="221" Score="0" Text="I do not agree trees are of help here. It is a matter of filtering rules, and that can be achieved with the _arules_ package in R." CreationDate="2014-06-25T10:13:45.667" UserId="1155" />
  <row Id="1587" PostId="566" Score="0" Text="True that... good approach for Named Entity Recognition (NER) ?" CreationDate="2014-06-25T14:12:52.660" UserId="1165" />
  <row Id="1588" PostId="568" Score="0" Text="Why would I need clock speed? Manchester encoding is self clocking. Timing should be unimportant." CreationDate="2014-06-25T14:21:50.763" UserId="890" />
  <row Id="1590" PostId="435" Score="0" Text="What about rules mining? It is not clear to me what is your aim." CreationDate="2014-06-25T14:32:26.520" UserId="1155" />
  <row Id="1591" PostId="24" Score="0" Text="I do not recommend converting categorical attributes to numerical values. Imagine you have two city names: NY and LA. If you apply NY number 3 and LA number 8, the distance is 5, but that 5 has nothing to see with the difference among NY and LA." CreationDate="2014-06-25T14:38:17.337" UserId="1155" />
  <row Id="1592" PostId="568" Score="0" Text="I though it could be helpful to know the clock speed in order to know how fast are the transitions between low/high values." CreationDate="2014-06-25T14:39:49.943" UserId="1155" />
  <row Id="1594" PostId="559" Score="0" Text="@MichaelT Yeah, I've been looking at that to solve a pigeon problem at my house" CreationDate="2014-06-25T15:39:56.180" UserId="1167" />
  <row Id="1596" PostId="559" Score="0" Text="There is open-source software for this, known as &quot;motion&quot;: http://www.lavrsen.dk/foswiki/bin/view/Motion/WebHome" CreationDate="2014-06-25T15:54:51.600" UserId="924" />
  <row Id="1597" PostId="565" Score="2" Text="which playground competition is it?" CreationDate="2014-06-25T16:01:01.027" UserId="153" />
  <row Id="1598" PostId="264" Score="1" Text="Can you be more specific? EM refers to an optimization algorithm that can be used for clustering. There are many ways to do this and it is not obvious what you mean." CreationDate="2014-06-25T19:18:45.110" UserId="1193" />
  <row Id="1600" PostId="559" Score="0" Text="@Anony-Mousse I tried the motion software before posting the question here, but it failed miserably in detecting any kind of animals, instead reacting only to leaves/trees moving, sunlight intensity changes and such – _except_ in the sample image above where it actually detected a bee flying close to the camera. Might work better in other environments." CreationDate="2014-06-25T20:43:05.533" UserId="1147" />
  <row Id="1601" PostId="518" Score="0" Text="This question appears to be off-topic because Data Science Stack Exchange is not a link recommendation service. Please use Google for basic information and tutorials." CreationDate="2014-06-26T01:41:16.660" UserId="62" />
  <row Id="1603" PostId="586" Score="1" Text="This online book will be useful http://www.nltk.org/book/" CreationDate="2014-06-26T00:40:36.690" UserId="1196" />
  <row Id="1604" PostId="200" Score="0" Text="Thank you for your post, but can you include here an overall description of the solution linked? The link may have the answer to the question, but link-only answers are discouraged." CreationDate="2014-06-26T04:41:26.317" UserId="84" />
  <row Id="1607" PostId="555" Score="1" Text="+1 This is one of the best answers I've ever read." CreationDate="2014-06-26T09:14:03.430" UserId="1155" />
  <row Id="1609" PostId="595" Score="1" Text="What do you mean by 'restoring parameters'? Do you mean populating those users who didn't indicate gender/age and so forth?" CreationDate="2014-06-26T13:00:25.503" UserId="1155" />
  <row Id="1611" PostId="595" Score="0" Text="@adesantos Yes. Is it possible?" CreationDate="2014-06-26T13:09:04.630" UserId="1207" />
  <row Id="1612" PostId="598" Score="0" Text="Which classification algorithm do you recommend in my case? Will be Naive Bayes classifier good choice?" CreationDate="2014-06-26T14:23:49.747" UserId="1207" />
  <row Id="1613" PostId="598" Score="0" Text="I suggest, as I said, not to populate and keep a variable called unknown." CreationDate="2014-06-26T14:25:01.953" UserId="1155" />
  <row Id="1614" PostId="598" Score="0" Text="I have no choice. It's must be done. But haven't expirience enough." CreationDate="2014-06-26T14:32:36.413" UserId="1207" />
  <row Id="1615" PostId="595" Score="0" Text="You may wish to look up the term data imputation as well" CreationDate="2014-06-26T14:46:13.673" UserId="1085" />
  <row Id="1616" PostId="598" Score="0" Text="Then use the second approach I told you: Create a distribution. But if you want to keep it simple, populate with the most common value. That's the easiest option." CreationDate="2014-06-26T14:53:21.123" UserId="1155" />
  <row Id="1617" PostId="600" Score="3" Text="[Cross posting](http://stats.stackexchange.com/q/104868) seems in poor form. Why is this tagged SQL?" CreationDate="2014-06-26T15:34:29.440" UserId="322" />
  <row Id="1618" PostId="600" Score="0" Text="Solve the (approximate) travelling salesman problem (TSP)..." CreationDate="2014-06-26T16:08:01.983" UserId="984" />
  <row Id="1620" PostId="597" Score="0" Text="Well, *someone* knows for sure what Google uses, and there are plenty of Google employees with SE accounts. Perhaps one of them will come along and provide us all with some interesting (non-proprietary) information." CreationDate="2014-06-26T17:28:18.160" UserId="322" />
  <row Id="1621" PostId="596" Score="1" Text="I think you could improve this question by defining &quot;gold standard dataset&quot; in a more objective fashion. What makes it &quot;must-know&quot;? Should it be referenced in a number of textbooks? Used in a number of published models? Etc. Otherwise the answers will be subjective AND they will change as time passes. A bad combination here." CreationDate="2014-06-26T17:33:37.880" UserId="322" />
  <row Id="1623" PostId="605" Score="0" Text="Normalize which matrix? These similarity measures do not require arguments in [0,1], by the way." CreationDate="2014-06-26T22:33:11.973" UserId="21" />
  <row Id="1624" PostId="599" Score="0" Text="Thanks, but it's not exactly what I'm looking for. See update for more details." CreationDate="2014-06-27T05:23:06.967" UserId="941" />
  <row Id="1625" PostId="612" Score="0" Text="Yes I still intend to cross-validate to check for over-fitting, my question is more basic than that - I cannot find any references to using regularisation with a per-item weight adjustment in MLP at all, and am concerned there is a good reason for that - e.g. it does not work in that learning mode, or needs adjustment. The crossvalidated question *is* very similar though and gives me more confidence, thank you. The SGD algorithm page seems to have a different, stochastic method for introducing regularisation, which might be a bit advanced for me, but is exactly what I am looking for." CreationDate="2014-06-27T07:30:40.650" UserId="836" />
  <row Id="1626" PostId="612" Score="0" Text="Regularization is relevant in per-item learning as well. I would still suggest to start with a basic validation approach for finding out lambda. This is the easiest and safest approach. Try manually with a number of different values. e.g. 0.001. 0.003, 0.01, 0.03, 0.1 etc. and see how your validation set behaves. Later on you may automate this process by introducing a linear or local search method." CreationDate="2014-06-27T09:38:04.807" UserId="1085" />
  <row Id="1627" PostId="612" Score="0" Text="If your comment above was edited in and replaced the first sentence/question in your answer, then I think I could accept it." CreationDate="2014-06-27T09:40:46.120" UserId="836" />
  <row Id="1628" PostId="612" Score="0" Text="Thanks for pointing out, I agree. Edited it in. Hope it is more clear." CreationDate="2014-06-27T09:55:29.610" UserId="1085" />
  <row Id="1629" PostId="468" Score="1" Text="Given that this is a question about the statistical model, you may want to go to [CrossValidated](http://stats.stackexchange.com) website, but keep in mind that it is a terrible practice to cross-post the questions: you would either want to formulate it to highlight the methodological issues you are facing, or migrate the whole question." CreationDate="2014-06-27T13:23:38.907" UserId="1237" />
  <row Id="1633" PostId="468" Score="0" Text="Without really explaining why, [ISL](http://www-bcf.usc.edu/~gareth/ISL) notes (on p 137) that discriminant analysis (like LDA, QDA) is more often used than multiple class extensions of logistic regression. Packages like [penalizedLDA](http://cran.r-project.org/web/packages/penalizedLDA/index.html) may therefore be worth examining." CreationDate="2014-06-27T21:03:57.007" UserId="953" />
  <row Id="1636" PostId="607" Score="1" Text="First question you should ask here is do you need a virtual machine as an instrument for your data science practice? (Maybe even rephrase the original question.) And there are more than the two mentioned VM to choose from. Here is a link to a [post comparing four VMs for data science.](http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html)" CreationDate="2014-06-28T09:51:37.420" UserId="454" />
  <row Id="1637" PostId="619" Score="0" Text="Thank u for ur kind answer..." CreationDate="2014-06-28T16:20:27.867" UserId="1235" />
  <row Id="1638" PostId="619" Score="0" Text="Can we do Regresssion,clustering,classifications using Rmr...... If it is possible then we can do using R directly.. only because of Parallelism we are using Rmr.. If i am correct.. Is there any main Difference between Hadoop Mapreduce and R mapreduce (apart from Parallelism)..." CreationDate="2014-06-28T16:21:39.303" UserId="1235" />
  <row Id="1639" PostId="619" Score="0" Text="rmr is a framework for running R functions in MapReduce. That is the thing it lets you do that you could not do before. It is not a library of new statistical functions of course." CreationDate="2014-06-28T17:32:36.687" UserId="21" />
  <row Id="1640" PostId="607" Score="0" Text="This would even be sufficient as a reply. Fantastic. Even though the poster is the creator of the data science tool box so it is not an unbiased report, but it's the best thing I've seen - so thanks." CreationDate="2014-06-28T20:54:44.647" UserId="1223" />
  <row Id="1641" PostId="561" Score="0" Text="Have a look at background subtraction in opencv...it is doing anomaly detection on pixels, by basically calculating historical mean, variance (in fact gaussian mixture model)." CreationDate="2014-06-28T23:06:53.930" UserId="1256" />
  <row Id="1642" PostId="623" Score="0" Text="If you have different classifiers trained on different datasets, how can you compare them in a meaningful way? Apples and oranges, chalk and cheese come to mind. Also, if you have multiclass classifiers, how do you calculate precision and recall?  Even knowing N=1 is not necessarily helpful - if there is only one egg in the world, your egg classifier is fine." CreationDate="2014-06-29T06:58:18.617" UserId="1230" />
  <row Id="1643" PostId="623" Score="0" Text="They're different classifiers trained on the same datasets, e.g. we know we have a document that's about apples and oranges, so we run an apple classifier on it to determine  type of apple it's talking about, and an orange classifier to determine type of orange it talks about. If our documents are 99% about apples, 1% about oranges, and both classifiers have the same prec/rec (summing rows/cols over confusion matrix), is there any info we can present that takes into account the differences in quantities of each? (it might be that no, there isn't, which is an answer I'd be happy with)" CreationDate="2014-06-29T09:06:28.753" UserId="474" />
  <row Id="1644" PostId="619" Score="0" Text="Then any other  specific feature where hadoop mapreduce can't handle?....." CreationDate="2014-06-29T11:32:23.643" UserId="1235" />
  <row Id="1645" PostId="414" Score="0" Text="In practice, you will use a learning rate with adadelta. On some problems it does not work without." CreationDate="2014-06-29T18:18:55.930" UserId="1193" />
  <row Id="1646" PostId="535" Score="0" Text="This seems to work pretty well except the scaling of the numbers between 0 and 1. Not sure whether the python version is as intended?" CreationDate="2014-06-29T22:08:50.460" UserId="1107" />
  <row Id="1647" PostId="534" Score="0" Text="True, I'd forgotten about norms, I'll look into this thanks." CreationDate="2014-06-29T22:09:23.323" UserId="1107" />
  <row Id="1648" PostId="603" Score="0" Text="I agree with Steve K that the key to tackling this is to aim for approximately optimal or just good route strategies. Many times the difference between &quot;best&quot; and &quot;good enough&quot; isn't much." CreationDate="2014-06-30T03:55:22.573" UserId="609" />
  <row Id="1651" PostId="535" Score="0" Text="I simplified your python version. And what's wrong with scaling? Assuming, that all values in original matrix are between 0 and 1, result should also be of the same scale." CreationDate="2014-06-30T05:38:40.697" UserId="941" />
  <row Id="1652" PostId="586" Score="0" Text="@RajaPasupuleti Ah yes this is very useful. Thanks!" CreationDate="2014-06-30T07:38:22.640" UserId="1192" />
  <row Id="1653" PostId="203" Score="0" Text="Sorry, I take your point.  I thought &quot;Symphony / COIN-OR seem to be the dominant suggestions&quot; answered the question &quot;is there an open source [...] alternative [...]?&quot; - the links were for backup as to why. I did caveat that the OP's scenario is on a bigger scale than I'm used to - I cannot give any extra information from personal experience but as this is a new stackexchange site and no-one had jumped in, I thought I'd try to assist. Will delete if you think it's not helpful / an answer." CreationDate="2014-06-30T10:45:17.277" UserId="265" />
  <row Id="1654" PostId="535" Score="0" Text="Nothing is wrong with the scaling now... I must of had a bug in my code. Thanks for the help this works great on my dataset" CreationDate="2014-06-30T11:38:42.077" UserId="1107" />
  <row Id="1655" PostId="636" Score="0" Text="That is a lot of open ended questions to ask all at once. One of the goals of this site is that questions and their answers should be useful to future visitors. Can you be more specific? What looks bad in your linear model? When you tried the logistic model, how did it come out? Were there specific problems?" CreationDate="2014-06-30T15:52:28.900" UserId="322" />
  <row Id="1656" PostId="30" Score="1" Text="If the total amount of data in the world doubles every 40 months, then surely it *can* get bigger than that. ;p" CreationDate="2014-06-30T16:00:06.990" UserId="322" />
  <row Id="1660" PostId="636" Score="0" Text="Sorry,an eg:&#xA;&#xA;On trying logit in R with these values:&#xA; &#xA;day-28, hour-11,day of the week-7, state-New Mexico, OS-iOS, OS Version-7.0, browser family-Mobile Safari,  browser version-7.0, device manufacturer-apple, IP carrier- Comcast, user age-20, gender-male, click happened-yes, predicted probability from GLM in R-0.000000001.&#xA;&#xA;In another instance,day-28, hour-12th, day of the week-7, state-Connecticut, OS-iOS, OS version-7.0, browser-mobile safari, browser version-7.0, device manufacturer-apple, IP carrier-Comcast, user age-22, user gender-female,click happened-no, predicted probability-.046" CreationDate="2014-06-30T17:02:06.487" UserId="1273" />
  <row Id="1662" PostId="636" Score="0" Text="Thanks. There's an &quot;edit&quot; link at the bottom of your question so that you can update it with this additional information." CreationDate="2014-06-30T17:13:58.197" UserId="322" />
  <row Id="1663" PostId="636" Score="0" Text="That is what I meant by how the model does not look good. I think this is due to the sparsity of instances of clicks happening and random nature of event of clicks happening.&#xA;&#xA;So how can I go about sampling the data? Also how should I add weights to the parameters for improving the prediction? Sorry for adding more open-ended questions but encountering issues as they come, I also want to ask how to work with large datasets in R?" CreationDate="2014-06-30T17:44:17.230" UserId="1273" />
  <row Id="1664" PostId="641" Score="0" Text="Recommend asking on http://opendata.stackexchange.com/" CreationDate="2014-06-30T21:28:41.943" UserId="322" />
  <row Id="1665" PostId="520" Score="0" Text="I added some graphs, after having improved the model A LOT. In github there are the new steps. May I ask you how I can apply linear regression in a time series problem? :(" CreationDate="2014-06-30T22:59:25.760" UserId="989" />
  <row Id="1666" PostId="523" Score="0" Text="I added the sources also" CreationDate="2014-06-30T23:00:24.603" UserId="989" />
  <row Id="1667" PostId="519" Score="0" Text="I added some graphs and you can check also about the parameters now :)" CreationDate="2014-06-30T23:01:05.820" UserId="989" />
  <row Id="1668" PostId="466" Score="0" Text="Project updated!" CreationDate="2014-06-30T23:01:21.583" UserId="989" />
  <row Id="1669" PostId="603" Score="0" Text="Of course the optimum can be found, it might just take longer than the age of the universe to iterate over all the possibilities. Your answer fails to mention this." CreationDate="2014-07-01T13:23:16.427" UserId="471" />
  <row Id="1670" PostId="600" Score="0" Text="Beyond lat-long, what's the geography like? A gridded city? An almost tree-shaped suburb with smaller roads into cul-de-sacs? Has a MASSIVE influence." CreationDate="2014-07-01T13:24:47.337" UserId="471" />
  <row Id="1671" PostId="24" Score="0" Text="@adesantos Yes, that's a problem with representing multiple categories with a single numeric feature and using a Euclidean distance.  Using the Hamming distance is one approach; in that case the distance is 1 for each feature that differs (rather than the difference between the numeric values assigned to the categories).  Making each category its own feature is another approach (e.g., 0 or 1 for &quot;is it NY&quot;, and 0 or 1 for &quot;is it LA&quot;)." CreationDate="2014-07-01T14:36:16.047" UserId="14" />
  <row Id="1672" PostId="642" Score="0" Text="Is this the Gelman paper referenced: http://www.stat.columbia.edu/~gelman/research/published/rsquared.pdf?" CreationDate="2014-07-01T17:51:00.910" UserId="684" />
  <row Id="1673" PostId="641" Score="0" Text="Thanks for the recommendation!" CreationDate="2014-07-01T17:51:26.133" UserId="684" />
  <row Id="1674" PostId="644" Score="0" Text="I didn't quite get your question. Do you intend to compute pairwise cosine similarities between every pair of row vectors (observations)?" CreationDate="2014-07-01T18:37:05.693" UserId="984" />
  <row Id="1675" PostId="644" Score="0" Text="Moreover, I didn't quite get why are you taking the average?" CreationDate="2014-07-01T18:37:53.627" UserId="984" />
  <row Id="1677" PostId="642" Score="0" Text="That works, but I highly recommend his [book on multilevel modeling](http://www.stat.columbia.edu/~gelman/arm/)." CreationDate="2014-07-01T19:01:15.310" UserId="159" />
  <row Id="1678" PostId="642" Score="0" Text="Ah, awesome, thanks!" CreationDate="2014-07-01T19:14:50.183" UserId="684" />
  <row Id="1679" PostId="644" Score="0" Text="If your observations are just 0s and 1s, then I don't think cosine similarity will work. Perhaps you should consider using Tanimoto similarity, which considers similarity across bitmaps." CreationDate="2014-07-02T01:30:01.480" UserId="24" />
  <row Id="1680" PostId="644" Score="0" Text="@Debasis yes, I do plan to compute similarities between each row. And I'm only taking the average as an example strategy to show what my computations might involve." CreationDate="2014-07-02T03:30:08.680" UserId="754" />
  <row Id="1681" PostId="644" Score="0" Text="@buruzaemon my observations aren't actually 0s and 1s, though I do like the simplicity of those kind of similarities. I actually asked this question because I'm interested in what the right answer for cosine similarities is. But yeah, I do appreciate the advice." CreationDate="2014-07-02T03:35:10.417" UserId="754" />
  <row Id="1682" PostId="649" Score="1" Text="Thanks! I would upvote this, but I don't have the reputation necessary. Maybe someone else can do that for me?" CreationDate="2014-07-02T03:43:26.020" UserId="754" />
  <row Id="1683" PostId="555" Score="1" Text="+1, and you should always put the original data into some kind of source control. Very sound advice, @Steve" CreationDate="2014-07-02T04:48:19.950" UserId="24" />
  <row Id="1684" PostId="628" Score="0" Text="Did you mean to say &quot;increase reduce the model complexity&quot; on the last bullet point? I think just &quot;increase the model complexity&quot; . . . BTW good timing I am enrolled in that course and had only just watched the video you are referring to." CreationDate="2014-07-02T10:24:36.813" UserId="836" />
  <row Id="1685" PostId="654" Score="1" Text="So, kNN is not training but lazy learning, because it does not abstract and generalize but, in case of classification, for each new observation it learns from the nearest k-labeled observations. While NTP is kNN-1 between an observation and the signature (that can be considered a pseudo-observation). Am I right?" CreationDate="2014-07-02T14:57:33.457" UserId="133" />
  <row Id="1686" PostId="616" Score="0" Text="Note to readers: [This question is also under discussion on Stack Overflow, where a high-quality solution has been offered.](http://stackoverflow.com/q/24455620/2359271)" CreationDate="2014-07-02T15:23:50.970" UserId="322" />
  <row Id="1687" PostId="654" Score="2" Text="While the kNN classifier is referred to as a &quot;lazy learner&quot;, that term is somewhat misleading and it is more accurately described as a &quot;lazy classifier&quot;. With regard to your comment, it doesn't actually *learn* when you give it a new observation to classify - it simply calculates the appropriate class. If you give it that same observation later, it would perform the same computation the second time because it doesn't actually learn from the first observation. In short, kNN doesn't produce/learn a model - it simply stores training data, then uses those data when it is time to classify." CreationDate="2014-07-02T16:48:34.320" UserId="964" />
  <row Id="1688" PostId="628" Score="0" Text="@NeilSlater Thanks, good catch, there was indeed a typo :)" CreationDate="2014-07-02T16:56:18.073" UserId="843" />
  <row Id="1690" PostId="655" Score="0" Text="I'm not sure if you'll do better than predicting 1/f noise when using past values as indicators for future ones. http://www.scholarpedia.org/article/1/f_noise#Stock_markets_and_the_GNP - your results so far seem consistent with that. Probably you should look at other possible features that have some reason to correlate with future exchange rates. If this were easy, there would be more rich data scientists." CreationDate="2014-07-02T23:32:03.263" UserId="836" />
  <row Id="1691" PostId="636" Score="0" Text="The class imbalance is such that you don't really want to predict click vs no click.  Since almost no one clicks your best result is to simply predict they don't click. Instead, think about predicting for all the users that click...which ad will they click on. This is a much more useful problem." CreationDate="2014-07-03T03:49:44.793" UserId="92" />
  <row Id="1692" PostId="660" Score="0" Text="You need to find the log output from your R script, which would indicate the error. &quot;hadoop streaming failed with error code 1&quot; just means &quot;the script failed for some reason&quot;" CreationDate="2014-07-03T11:19:25.687" UserId="21" />
  <row Id="1694" PostId="660" Score="0" Text="Sometimes the folder where you write must be deleted before writing (if it exists). Check that out." CreationDate="2014-07-03T11:41:17.003" UserId="1155" />
  <row Id="1695" PostId="646" Score="1" Text="Shouldn't any old spatial index that supports containment queries work? i.e. an r-tree?" CreationDate="2014-07-03T12:40:20.623" UserId="1283" />
  <row Id="1696" PostId="660" Score="0" Text="thank u for ur answer ... but i have check all the possibilites what u have mentioned... i doubt that there is some problem with code itself...can someone please rectify..." CreationDate="2014-07-03T15:19:02.067" UserId="1235" />
  <row Id="1697" PostId="661" Score="1" Text="try `sudo hadoop fs -ls -l /` and see what comes back.  Usually a permissions error comes back explicitly, but lets have a gander at what the root path looks like.  I wonder if the file system was never formatted." CreationDate="2014-07-03T17:39:19.290" UserId="434" />
  <row Id="1698" PostId="661" Score="1" Text="This might be better at serverfault.com" CreationDate="2014-07-03T21:36:59.580" UserId="21" />
  <row Id="1699" PostId="667" Score="0" Text="Algorithm-wise: what would you recommend ?" CreationDate="2014-07-03T21:59:30.367" UserId="1315" />
  <row Id="1700" PostId="663" Score="0" Text="I would love to see your example." CreationDate="2014-07-03T22:03:13.677" UserId="1315" />
  <row Id="1701" PostId="663" Score="0" Text="Updated with quick example." CreationDate="2014-07-03T22:52:43.353" UserId="375" />
  <row Id="1702" PostId="653" Score="2" Text="Other keywords that might be useful for what you are looking are *clustering* and the more general *unsupervised learning*." CreationDate="2014-07-04T07:24:06.803" UserId="113" />
  <row Id="1703" PostId="667" Score="0" Text="you mean algorithm for computing the most similar resume vectors given a query job vector? you can use any standard algorithm such as BM25 or Language Model..." CreationDate="2014-07-04T11:35:38.297" UserId="984" />
  <row Id="1704" PostId="667" Score="0" Text="I have never heard of these algorithms at all. Are these NLP algorithms or ML algo ?" CreationDate="2014-07-04T13:32:43.553" UserId="1315" />
  <row Id="1705" PostId="667" Score="0" Text="these are standard retrieval models... a retrieval model defines how to compute the similarity between a document (resume in your case) and a query (job in your case)." CreationDate="2014-07-04T15:23:49.700" UserId="984" />
  <row Id="1706" PostId="667" Score="0" Text="I have no knowledge about information retrieval, do you think machine learning algorithms like clustering / nearest neighbour will also work in my case ?" CreationDate="2014-07-04T16:37:18.267" UserId="1315" />
  <row Id="1707" PostId="629" Score="0" Text="This. Is. Fantastic! Thanks @AsheeshR" CreationDate="2014-07-05T08:07:48.490" UserId="1223" />
  <row Id="1708" PostId="669" Score="0" Text="great thanks. I will evaluate robust regression and see how it goes." CreationDate="2014-07-05T11:04:54.390" UserId="1300" />
  <row Id="1709" PostId="655" Score="0" Text="yes, Maybe other variables are contributing to the next value more than the time series values it self.. I will experiment with that too. Thank you for the pointers." CreationDate="2014-07-05T11:08:20.760" UserId="1300" />
  <row Id="1710" PostId="667" Score="0" Text="yes, nearest neighbour is in fact very similar in objective to that of information retrieval... that is that of finding the most similar k points given a query point... the only problem is that when N (the number of data points) is too large, IR is more efficient" CreationDate="2014-07-05T12:00:14.867" UserId="984" />
  <row Id="1712" PostId="675" Score="0" Text="Say I am analyzing linkedin data, do you think it would be a good idea for me to merge the previous work experience, educations recommendations and skills of one profile into one text file and extract keywords from it ?" CreationDate="2014-07-05T14:46:08.110" UserId="1315" />
  <row Id="1713" PostId="675" Score="0" Text="LinkedIn now has skill tags that people assign themselves and other users can endorse, so basically there's no need to extract keywords manually. But in case of less structured data - yes, it may be helpful to merge everything and then retrieve keywords. However, remember main rule: **try it out**. Theory is good, but only practical experiments with different approaches will reveal best one." CreationDate="2014-07-05T20:33:05.633" UserId="1279" />
  <row Id="1714" PostId="675" Score="0" Text="true said. Thanks a lot" CreationDate="2014-07-05T23:33:49.547" UserId="1315" />
  <row Id="1715" PostId="679" Score="1" Text="Search queries are not noisier (there are very few words in a query not actually related to the search), but may contain misspellings, ambiguity, slang and other stuff that you have to deal with separately. Beyond these issues, queries and documents may be processed pretty much the same way." CreationDate="2014-07-06T00:13:02.033" UserId="1279" />
  <row Id="1716" PostId="677" Score="1" Text="Your second import is not correctly indented.  I would correct the code myself if the edit was long enough." CreationDate="2014-07-07T10:08:28.323" UserId="1367" />
  <row Id="1717" PostId="684" Score="0" Text="Thanks. I wouldn't have known what had caused it. I only know most of the time it's my work that's at fault :)" CreationDate="2014-07-07T12:56:31.427" UserId="974" />
  <row Id="1718" PostId="679" Score="0" Text="maybe you can extract keyword vectors from queries, and then compute the distance between those vectors, and how the similarity is defined, i think this is still an open question:)" CreationDate="2014-07-06T06:15:21.697" UserId="1006" />
  <row Id="1719" PostId="684" Score="0" Text="@elksie5000 : I have added how to debug the call. I hope the last call is what you would expect from a successful call to the function (?). Otherwise, it is always good to know how to step into the code with `pdb` :)" CreationDate="2014-07-07T13:44:45.997" UserId="1367" />
  <row Id="1720" PostId="684" Score="0" Text="I must admit pdb was something I was looking at again after working through the Python for Data Analysis book by Wes McKinney. I already work in IPython, but had been reasonably happy with print statements. Thank you again." CreationDate="2014-07-07T15:03:33.867" UserId="974" />
  <row Id="1721" PostId="684" Score="0" Text="As a side note, the debugger prompt says &quot;ipdb&quot; because it is the ipython debugger - this is an extra install in my setup. Under normal circumstances, it would be the regular pdb that is called. Just noticed this difference." CreationDate="2014-07-07T15:27:48.683" UserId="1367" />
  <row Id="1722" PostId="680" Score="1" Text="Note that when doing LSA, typically you use the cosine distance on the LSA projections of the original dataset. Just to clarify." CreationDate="2014-07-07T18:09:18.477" UserId="1301" />
  <row Id="1723" PostId="691" Score="0" Text="How exactly have you used LSA? It's worth noting that LDA is actually a pretty thin wrapper around LSA (it's pLSA with a dirichlet prior) that has been empirically shown to greatly increase generalization. You would almost certainly see better accuracies with LSA, but that's generally a result of overfitting, which is a very notable problem with LSA. Also, what exactly do you mean by scaling here? doc2vec does not actually require a new model for each document, and for computation there's no notable difference between LSA and LDA, both being very scalable." CreationDate="2014-07-07T19:36:26.987" UserId="869" />
  <row Id="1725" PostId="697" Score="2" Text="This is quite a broad question. You may be memory-bound (dataset doesn't fit into memory, and thus swap is used extensively, making it deadly slow) or CPU-bound (memory is ok, but operations just take too long). In latter case you can try to use vectorization more widely or write extensions in C directly. You can also try to compile your functions with `cmpfun()` or parallelize code to use several cores. Finally, you can rent Amazon web server to run your experiments, which will cost much less than buying hardware yourself. Anyway, try to determine your bottleneck first." CreationDate="2014-07-08T06:18:59.317" UserId="1279" />
  <row Id="1726" PostId="701" Score="0" Text="I just added an answer assuming that you want to cluster the samples `id_1`...`id_n` *based on their distances*. If you do want to cluster *the distances themselves*, you just need to use them as a 1-dimensional array." CreationDate="2014-07-08T09:28:49.813" UserId="1367" />
  <row Id="1729" PostId="679" Score="1" Text="Both of your questions are broad, subjective and will require significant maintenance to avoid becoming obsolete. Since the community appreciates that sort of question, keeping one of them might be reasonable - but certainly not both, when this discussion is a proper subset of the other. Please review [What types of questions should I avoid asking?](http://datascience.stackexchange.com/help/dont-ask)" CreationDate="2014-07-08T15:13:07.897" UserId="322" />
  <row Id="1731" PostId="693" Score="0" Text="Do you still use SVM when you have 3 or more classes ? And what features do you want to extract using a natural language parser? For what purpose ?" CreationDate="2014-07-08T15:40:48.113" UserId="1315" />
  <row Id="1733" PostId="694" Score="0" Text="See also http://stackoverflow.com/q/2276933/2359271" CreationDate="2014-07-08T20:14:34.267" UserId="322" />
  <row Id="1734" PostId="691" Score="0" Text="I have not observed over fitting with LSA, and like I said, I have met multiple other people who have seen better performance over LDA. Also, I have seen LSA used in many winning entries in semeval competitions, I have never seen LDA used in a winning entry. That is the academic conference for comparing semantic similarity between documents, so I assume they know what they are doing. Doc2vec, if you are referring to Mikolov's paragraph vector implementation, does SGD on each document separately. So it's very slow." CreationDate="2014-07-08T20:48:55.170" UserId="1301" />
  <row Id="1735" PostId="691" Score="0" Text="Also, pLsa uses a very different model than LSA, and saying one is a wrapper for another is like saying that a neural network is a wrapper for Linear Regression, as they use similar training techniques. That's not really an argument for what is better. All of this is empirical, what works better depends on the dataset, as stated in the no free lunch theorem." CreationDate="2014-07-08T20:50:41.470" UserId="1301" />
  <row Id="1736" PostId="686" Score="0" Text="Note my comments below about paragraph vector scalability. This technique looks very promising, but is hard to implement, and does not scale well at all, as you are doing a separate SGD for each document, which is very costly, if I remember the paper correctly" CreationDate="2014-07-08T20:52:43.710" UserId="1301" />
  <row Id="1737" PostId="700" Score="0" Text="I can't fully answer your question, but I know of one way I have selected the k-value before.  You can look at minimizing a function of (Sum of squares within clusters)/(Sum of Squares between clusters), or maximizing the inverse." CreationDate="2014-07-08T23:15:07.113" UserId="375" />
  <row Id="1738" PostId="694" Score="0" Text="I saw thanks, but it is outdated, and closed." CreationDate="2014-07-09T00:06:23.383" UserId="989" />
  <row Id="1739" PostId="712" Score="2" Text="Thank you for this detailed answer. I have one question about it: The graph you included has no axis labels at the moment. I can see that RVM is tracking some theoretical result better than SVM in the graph, but I cannot figure out the context." CreationDate="2014-07-09T13:10:48.787" UserId="836" />
  <row Id="1740" PostId="687" Score="0" Text="Thanks for your answer. Do I understand right: your starting point is to merge the 2 datasets ?" CreationDate="2014-07-09T14:31:34.480" UserId="906" />
  <row Id="1741" PostId="690" Score="0" Text="Thanks for your answer. It's a great example of what generative models can do that discriminative models cannot." CreationDate="2014-07-09T14:34:15.320" UserId="906" />
  <row Id="1742" PostId="712" Score="0" Text="Sorry for missing out on the figure description... i've now added a couple of lines..." CreationDate="2014-07-09T17:44:00.177" UserId="984" />
  <row Id="1743" PostId="712" Score="1" Text="I don't mean to be overly critical, but SVMs are NOT efficient. They have a cubic complexity in most cases, which is why there is a lot of phasing out happening." CreationDate="2014-07-09T19:05:12.953" UserId="548" />
  <row Id="1744" PostId="713" Score="1" Text="This is probably better for serverfault.com" CreationDate="2014-07-09T20:01:26.083" UserId="21" />
  <row Id="1745" PostId="712" Score="0" Text="yes, standard convergence methods takes O(n^3)... but i think i've seen somewhere (may be from the home page of T. Joachims) that it's been reduced to O(n^2)" CreationDate="2014-07-09T20:21:05.123" UserId="984" />
  <row Id="1746" PostId="713" Score="1" Text="@SeanOwen Yes, or [cloudera support](http://www.cloudera.com/content/cloudera/en/about/contact-form.html), even" CreationDate="2014-07-09T21:13:50.973" UserId="322" />
  <row Id="1747" PostId="705" Score="0" Text="okay, can you elaborate what does interaction mean? I added classes like wifi enabled, gps enabled. now lasso performed slightly better than ridge, I also added more days for computation. However, the AUC is now in range of .51 to .55 only. Is there anything else i can do? Will try adding quadratic features, and what else?" CreationDate="2014-07-10T05:34:55.647" UserId="1273" />
  <row Id="1748" PostId="705" Score="0" Text="For every feature, you can create quadratic features (x_i ^ 2) and interaction features (x_i * x_j). Also be aware that there are hyper-parameters for both methods of regularization that should be tuned rather than left at their defaults" CreationDate="2014-07-10T08:19:26.393" UserId="1399" />
  <row Id="1749" PostId="636" Score="0" Text="for selecting what users will click on, we will have to search our system for keywords etc as relevant. so if there is a way to reject a request without delaying time in keyword lookup, it will save a lot of time in processing for us" CreationDate="2014-07-10T11:03:43.300" UserId="1273" />
  <row Id="1750" PostId="716" Score="1" Text="A non-random correlation might be an indicator that the feature *is* useful. But I'm not so sure about pre-training tests that could rule ideas out. The paper you link makes it clear that non-linear correlations are not well detected by the available tests, but a neural net has a chance of finding and using them." CreationDate="2014-07-10T11:28:15.503" UserId="836" />
  <row Id="1751" PostId="711" Score="0" Text="See also http://stats.stackexchange.com/questions/tagged/svm" CreationDate="2014-07-10T11:55:01.303" UserId="1237" />
  <row Id="1752" PostId="705" Score="0" Text="i took alpha in between 0 and 1 as well for trying regression, is that what you mean?" CreationDate="2014-07-10T14:47:05.807" UserId="1273" />
  <row Id="1753" PostId="700" Score="0" Text="Look up the Chinese Restaurant Process to help deal with dynamic numbers of clusters." CreationDate="2014-07-10T15:49:54.777" UserId="684" />
  <row Id="1754" PostId="718" Score="0" Text="I always thought to compare the value to predict with the features, you are talking about correlation between features. Is your answer applicable also to my case? in theory I should add only new features that are correlated to the value to predict, right?" CreationDate="2014-07-10T19:06:57.880" UserId="989" />
  <row Id="1755" PostId="718" Score="0" Text="That's also a valuable metric -- just updated my answer  to address that as well." CreationDate="2014-07-10T19:18:34.210" UserId="684" />
  <row Id="1756" PostId="718" Score="0" Text="In short, strong correlations with the value to predict is a great sign, but weak correlation with the value to predict is not necessarily a bad sign." CreationDate="2014-07-10T19:19:17.200" UserId="684" />
  <row Id="1757" PostId="718" Score="0" Text="Thanks. I'm writing a report and I wanted to show the linear/non-linear correlations in order to justify the features (even before the results). Does it make any sense?&#xA;From your answer I could make a matrix of correlations but maybe it's nosense" CreationDate="2014-07-10T19:27:33.073" UserId="989" />
  <row Id="1758" PostId="718" Score="0" Text="A matrix of correlations would make more sense if you were performing linear regression, but it could also be a useful metric for neural nets.  Give it a go and see what you get!" CreationDate="2014-07-10T21:29:06.873" UserId="684" />
  <row Id="1759" PostId="718" Score="1" Text="I would use non-linear correlations, but ok thanks" CreationDate="2014-07-10T23:17:47.063" UserId="989" />
  <row Id="1760" PostId="687" Score="0" Text="@cafe876 That is certainly one way to start, and then trying to basically recreate a clustering that closely approximates the original." CreationDate="2014-07-11T00:36:54.230" UserId="548" />
  <row Id="1761" PostId="720" Score="0" Text="Thank you for your answer. I already know these tools. Unfortunatly none of them are neither able to summarize a collection of documents nor perform query-based summarization" CreationDate="2014-07-11T09:23:52.577" UserId="979" />
  <row Id="1765" PostId="713" Score="0" Text="Could you provide the list of misconfigurations (that are just below the yellow bar)?" CreationDate="2014-07-12T00:35:10.563" UserId="2460" />
  <row Id="1766" PostId="730" Score="0" Text="This post was researched and presented well. It makes a poor question for an SE network site but it would be a great topic to start on a discussion forum." CreationDate="2014-07-12T19:38:44.743" UserId="322" />
  <row Id="1767" PostId="730" Score="0" Text="@AirThomas Thanks for the warning. I tried to save the post by making a proper question out of it." CreationDate="2014-07-13T03:06:30.877" UserId="84" />
  <row Id="1768" PostId="705" Score="0" Text="I suspect by alpha you mean the step size? If so, this is not the parameter I'm referring to---there's another parameter that regulates how much the regularization term is multiplied by in the objective, which allows you to balance model fit (likelihood) with sparseness as measured by the regularizer. This needs tuning." CreationDate="2014-07-14T10:04:29.207" UserId="1399" />
  <row Id="1769" PostId="704" Score="1" Text="Ah, the joys of deployment ... enhanced by the joys of distributed systems :)" CreationDate="2014-07-14T10:44:38.130" UserId="1367" />
  <row Id="1770" PostId="658" Score="0" Text="Please clarify: you say you want to use Column 2 as a feature, but then you say you want to predict/classify Column 2. Also, you call this feature 'non-atomic' ... do you mean it is not categorical?" CreationDate="2014-07-14T13:12:22.047" UserId="1367" />
  <row Id="1771" PostId="729" Score="0" Text="I think your answer is useful to the problem.  Just some suggestions: I would move the data generation code to the bottom, or even to an external Gist, since it is not really part of the proposed solution.  And I would elaborate a bit more on the fact that you are using 4 standard deviations to detect resets: right now, it is just a comment lost in the code, and it is the core of your solution." CreationDate="2014-07-14T13:26:23.830" UserId="1367" />
  <row Id="1772" PostId="729" Score="0" Text="Good ideas.  Will do." CreationDate="2014-07-14T15:18:33.427" UserId="375" />
  <row Id="1773" PostId="737" Score="0" Text="Thanks for the reply! I have actually tried numerous other algorithms/kernels and still have the same type of problem. So I am looking for more of an approach like undersampling or some way to even out the classes." CreationDate="2014-07-14T16:40:26.047" UserId="802" />
  <row Id="1774" PostId="737" Score="0" Text="Ok, you might also want to try replicating rows for classes containing sparse data, although its useful only if the features of the sparse data are really good." CreationDate="2014-07-14T17:25:06.797" UserId="2485" />
  <row Id="1775" PostId="740" Score="0" Text="thanks for the advice, do you know if libsvm automatically does this or do I need to manually pass in the class weights?" CreationDate="2014-07-14T19:24:14.940" UserId="802" />
  <row Id="1776" PostId="740" Score="0" Text="You have to manually pass in the class weights. The way to do that is different based on the interface you are using (python, java, matlab, c). It is well documented in the read me files if you download the tool from http://www.csie.ntu.edu.tw/~cjlin/libsvm/.&#xA;Also your data size seems to be large and the default multi-class implementation of libsvm will use one-vs-one classification which may take too long to run. You can try training 50 one-vs-all binary classifiers specifying the weights appropriately." CreationDate="2014-07-14T19:57:27.733" UserId="1350" />
  <row Id="1777" PostId="734" Score="0" Text="Thanks very much! Your comments are very helpful and provide a basis on which I may be able to tease more out of the article. John" CreationDate="2014-07-14T20:24:37.367" UserId="2458" />
  <row Id="1778" PostId="301" Score="1" Text="As a side comment: I think spotting errenous data caused by some problem further up the pipeline is a gold skill. Many a time I have wondered why my analysis produced weird results and when I looked at the pipeline i found some kind of error . E.g: I wondered why all my data where heavily skewed towards high prices - WAY out of my mental model. When I asked around, I found out that some subcontractor misunderstood the briefing and delivered data for high income groups, while we whanted mixed data..." CreationDate="2014-07-15T09:33:38.703" UserId="791" />
  <row Id="1779" PostId="742" Score="1" Text="+1 You do need a lot of engineering experience to be an effective data science, but you don't get that at school. Use school for the theory and use jobs for engineering skill." CreationDate="2014-07-15T11:07:06.440" UserId="21" />
  <row Id="1780" PostId="742" Score="1" Text="+1 for the role definition and the overall good advice" CreationDate="2014-07-15T14:03:13.123" UserId="1367" />
  <row Id="1781" PostId="506" Score="0" Text="Thank you very much for such a thorough explanation! I really appreciate it. Between yourself and neone4373 I was able to solve the problem! This community rocks!&#xA;&#xA;Thanks!" CreationDate="2014-07-15T15:06:43.223" UserId="1047" />
  <row Id="1782" PostId="301" Score="0" Text="Yes! Data errors are frequently signs of process problems. Knowing where in the process the errors were introduced and also the mechanism, will greatly help with the cleaning process.  But better still is to fix the process problems so that they produce clean (or cleaner) data." CreationDate="2014-07-15T19:12:57.530" UserId="609" />
  <row Id="1783" PostId="679" Score="0" Text="Thanks, AirThomas!  ffriend's post certainly seems to indicate that this is clearly a duplicate.  I'll see what I can do about this." CreationDate="2014-07-16T00:14:25.850" UserId="1097" />
  <row Id="1784" PostId="746" Score="1" Text="My vote is also for R. As far as I know, there is no time series decomposition functions in `statsmodel` (Python). Though in this case decomposition could be crucial to improving prediction. I see notable seasonal peaks." CreationDate="2014-07-16T08:45:53.197" UserId="941" />
  <row Id="1786" PostId="746" Score="0" Text="@sobach: Thank you for R solidarity. In regard to the rest of your comment, similarly to now famous tweet, I can neither confirm, nor deny that :-). [Since it's beyond my current level of knowledge on the subject.]" CreationDate="2014-07-16T10:38:45.793" UserId="2452" />
  <row Id="1787" PostId="755" Score="0" Text="I belive you are referring to OpenCV nactive_vars parameter (not max_depth), which I set to default sqrt(N) value, that is nactive_vars=sqrt(16) for first dataset and sqrt(200) for other two. max_depth determines whether trees grow to full depth (25 is its maximum value) and balances between underfitting and overfitting, more about it here: &#xA;http://stats.stackexchange.com/questions/66209/opencv-parameters-of-random-trees&#xA;Not sure about min_sample_count but I tried various values and setting it to 1 worked best." CreationDate="2014-07-17T07:05:44.933" UserId="1387" />
  <row Id="1788" PostId="755" Score="0" Text="OpenCV documentation gives brief explanation of parameters:&#xA;http://docs.opencv.org/modules/ml/doc/random_trees.html#cvrtparams-cvrtparams&#xA;For now I would like to make random trees work reasonably well and keep things simple because I want to focus on working with a multiple classifier system." CreationDate="2014-07-17T07:06:32.730" UserId="1387" />
  <row Id="1789" PostId="755" Score="0" Text="About kNN - these are all really good suggestions, but what I meant to say is that kNN performed better than random trees classifier and I think there is lots of room for improvement with random trees." CreationDate="2014-07-17T07:15:51.477" UserId="1387" />
  <row Id="1790" PostId="732" Score="0" Text="Thank you! Your answer really helped me!" CreationDate="2014-07-17T08:43:42.207" UserId="2471" />
  <row Id="1791" PostId="755" Score="0" Text="yes, i'm not sure why random forest is not performing as well (or better) than the simplistic k-NN approach... it just might be the case that a kernel based approach where you directly try to estimate P(y|D) (output given data) such as in k-NN without estimating P(theta|D) (latent model given data) such as in the parametric models." CreationDate="2014-07-17T09:28:49.887" UserId="984" />
  <row Id="1792" PostId="760" Score="1" Text="Transform your data to be a list of number of month between events (in Matlab this would be `diff(find(V))` where `V` is your current time series vector. Then try to fit an exponential distribution to this by estimating the *rate* parameter. *rate*, would be a decent metric of the frequency of job changes. The exponential distribution should show how the probability will increase with time since the last event. You also might want to [test for a goodness of fit](http://stats.stackexchange.com/questions/76994/how-do-i-check-if-my-data-fits-an-exponential-distribution) after estimating *rate*:" CreationDate="2014-07-17T09:41:18.727" UserId="2532" />
  <row Id="1793" PostId="760" Score="0" Text="what is the equivalent of `diff(find(V))` in R ?" CreationDate="2014-07-17T10:22:02.283" UserId="1315" />
  <row Id="1794" PostId="760" Score="1" Text="I don't know much `R` but I'll explain the Matlab code: `find` return the element number of the `1`s, `diff` returns the difference between each consecutive number. Hence that line just returns a vector of the number of months between each job change." CreationDate="2014-07-17T10:46:10.830" UserId="2532" />
  <row Id="1797" PostId="760" Score="0" Text="I'll point back to this link again: http://stats.stackexchange.com/questions/76994/how-do-i-check-if-my-data-fits-an-exponential-distribution looks like r has a `fitdistr` function" CreationDate="2014-07-17T16:00:12.900" UserId="2532" />
  <row Id="1798" PostId="753" Score="0" Text="Answers that rely on external resources to be useful are strongly discouraged on Stack Exchange. If a link stops working, or a reference book is not available, these answers become useless. Please *do* explain with words, either by paraphrasing or quoting your sources directly, and upload images (with attribution) as necessary and appropriate, using external links and references only for citation and &quot;further reading.&quot;" CreationDate="2014-07-17T16:01:42.867" UserId="322" />
  <row Id="1799" PostId="766" Score="0" Text="Essentially what I could do is to set self.error = error at the end of _tsne(), in order to retrieve it from the TSNE instance afterwards. Yes, but that would mean changing sklearn.manifold code, and I was wondering if the developers thought of some other ways to get the information or if not why they didn't (i.e.: is 'error' considered useless by them?). Furthermore, if I changed that code I would need all the people running my code to have the same hack on their sklearn installations. Is that what you suggest, or did I get it wrong?" CreationDate="2014-07-17T16:07:30.343" UserId="131" />
  <row Id="1800" PostId="760" Score="0" Text="actually, i just manually computed the MLE of $\lambda$ of exponential distribution." CreationDate="2014-07-17T16:08:29.123" UserId="1315" />
  <row Id="1801" PostId="769" Score="0" Text="Yes, indeed error rates on training data set are around 0. Changing parameters to reduce overfitting didn't result in higher accuracy on test dataset in my case. I will look into techniques you mention as soon as possible and comment, thank you." CreationDate="2014-07-17T16:16:51.433" UserId="1387" />
  <row Id="1802" PostId="769" Score="0" Text="What are the relative proportions of training and test dataset btw? Something line 70:30, 60:40, or 50:50?" CreationDate="2014-07-17T16:38:07.737" UserId="2556" />
  <row Id="1803" PostId="766" Score="0" Text="Yes, that is what I suggested as a possible solution. Since scikit-learn is open source, you could also submit your solution as a pull request and see if the authors would include that in future releases. I can't speak to why they did or didn't include various things." CreationDate="2014-07-17T17:10:10.463" UserId="159" />
  <row Id="1804" PostId="768" Score="1" Text="I like the watermark magic. For those who are unaware, GitHub now offers up to 5 free private repositories for users associated with academic institutions." CreationDate="2014-07-17T17:22:06.340" UserId="964" />
  <row Id="1805" PostId="773" Score="1" Text="&quot;distance metric&quot; is commonly used as an opposite of &quot;similarity&quot; in literature: the larger distance, the smaller similarity, but basically they represent same idea." CreationDate="2014-07-17T20:52:20.197" UserId="1279" />
  <row Id="1806" PostId="757" Score="0" Text="Do you have any advice related to your first sentence?" CreationDate="2014-07-17T23:42:37.857" UserId="989" />
  <row Id="1807" PostId="769" Score="0" Text="First dataset - UCI letter recognition is set to 50:50 (10000:10000), Digits is about 51:49 (1893:1796) and MNIST is about 86:14 (60000:10000)." CreationDate="2014-07-18T01:35:18.207" UserId="1387" />
  <row Id="1808" PostId="751" Score="0" Text="Why the dot product alone (equivalent to not normalizing) *not* account for features' data and frequency? I don't know that this is the difference." CreationDate="2014-07-18T11:27:57.540" UserId="21" />
  <row Id="1809" PostId="744" Score="0" Text="Note that neither of these are proper distance metrics, even if you transform them to be a value that is small when points are &quot;similar&quot;. It may or may not matter for your use case." CreationDate="2014-07-18T11:34:09.417" UserId="21" />
  <row Id="1810" PostId="751" Score="0" Text="Perhaps, I wasn't clear. I was talking about data diversity. E.g., we have two pairs of documents. Within each pair docs are identical, but pair-1 documents are shorter, than pair-2 ones. And we computing similarity within each pair. Dot product would produce different numbers, though in both cases maximum similarity estimate is expected." CreationDate="2014-07-18T12:36:29.073" UserId="941" />
  <row Id="1811" PostId="757" Score="0" Text="Create a dataset for each day of the week and fit a model to all seven of them." CreationDate="2014-07-18T12:58:53.813" UserId="325" />
  <row Id="1812" PostId="765" Score="0" Text="What do people generally due with predicted negative values if they know that the true target function cannot output negative values?" CreationDate="2014-07-18T14:05:20.120" UserId="1162" />
  <row Id="1813" PostId="758" Score="2" Text="Where is the *question* in this question? Please take a moment to review [the Help Center guidelines](http://datascience.stackexchange.com/help/dont-ask), specifically: &quot;If your motivation for asking the question is 'I would like to participate in a discussion about ______', then you should not be asking here.&quot;" CreationDate="2014-07-18T15:05:56.783" UserId="322" />
  <row Id="1814" PostId="766" Score="2" Text="Thanks. If anybody else is interested in this, https://github.com/scikit-learn/scikit-learn/pull/3422." CreationDate="2014-07-18T15:44:03.873" UserId="131" />
  <row Id="1815" PostId="764" Score="1" Text="How are points close to each other on the wrap-around point handled?" CreationDate="2014-07-18T17:00:53.687" UserId="2587" />
  <row Id="1816" PostId="765" Score="0" Text="I don't think it's applicable to what we're talking about, but if you have a strictly positive target, for example, you'd probably want to model the log of the target.  Then you'd exponentiate the predictions." CreationDate="2014-07-18T19:15:21.783" UserId="2543" />
  <row Id="1817" PostId="753" Score="0" Text="@keisZn, the pdf was a good explanation but there is no algorithm to explain how he parse through the alignment matrix to get consistent phrases..." CreationDate="2014-07-18T21:22:46.960" UserId="122" />
  <row Id="1818" PostId="785" Score="2" Text="It's over 4 GB of data. I should plot it by reading from stdin or something similar. It's a bad idea to load everything to RAM. I'll take a look at what you said in a couple of days - and hopefully, any other ideas that may arise - and I'll let you know how it went, thanks!" CreationDate="2014-07-20T01:39:58.163" UserId="2604" />
  <row Id="1819" PostId="787" Score="0" Text="Are you using &quot;poor&quot; Apache Hadoop or some other distribution like [HDP](http://hortonworks.com/hdp/) or [CDH](http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html). I would heavily recommend using automated tools like these two instead of messing up with native settings. In addition to easy installation, they provide tools for monitoring and managing your cluster later." CreationDate="2014-07-20T11:28:51.477" UserId="1279" />
  <row Id="1821" PostId="787" Score="0" Text="This is better at serverfault.com" CreationDate="2014-07-20T17:32:17.743" UserId="21" />
  <row Id="1822" PostId="784" Score="0" Text="https://www.otexts.org/fpp/6/5" CreationDate="2014-07-20T18:02:37.283" UserId="989" />
  <row Id="1823" PostId="784" Score="0" Text="I think your question would be better stated as, What is the importance of seasonality for forecasting. As it is, it seems someone told you to use &quot;STL&quot;, but you don't say who told you so, neither why (which is probably what you're trying to find out)." CreationDate="2014-07-20T19:37:34.577" UserId="84" />
  <row Id="1825" PostId="769" Score="0" Text="I experimented with PCA, still didn't get good results with random forrest, but boost and Bayes now give results similar to other classifiers. I found a discussion about random forrest here: &#xA;http://stats.stackexchange.com/questions/66543/random-forest-is-overfitting&#xA;It is possible I am actually not overfitting but couldn't find the out-of-bag (OOB) prediction error mentioned there. Running experiment now with a large number of trees to see if accuracy will improve." CreationDate="2014-07-21T16:04:50.647" UserId="1387" />
  <row Id="1826" PostId="782" Score="1" Text="Great answer, Aleksandr, very informative!" CreationDate="2014-07-21T16:18:43.513" UserId="2599" />
  <row Id="1827" PostId="769" Score="0" Text="Okay, sounds you are making a little bit of progress :) A trivial question, but have you standardized your features (z-score) so that they are centered around the mean with standard deviation=1?" CreationDate="2014-07-21T16:19:55.410" UserId="2556" />
  <row Id="1828" PostId="769" Score="0" Text="Actually no, I usually would scale features to range 0-1 but now I see I didn't even do that correctly before PCA. So that would not be the right thing to do anyway? After PCA mean = 0, std = 0.5754." CreationDate="2014-07-21T16:41:29.853" UserId="1387" />
  <row Id="1829" PostId="782" Score="0" Text="@DaveKay: Thank you for kind words, Dave! Glad to help." CreationDate="2014-07-21T16:46:43.350" UserId="2452" />
  <row Id="1830" PostId="795" Score="0" Text="I'm aware of edit distance like Levenshtein distance, but I'm looking for something like semantic similarity." CreationDate="2014-07-21T16:55:04.707" UserId="921" />
  <row Id="1831" PostId="795" Score="0" Text="That's significantly harder.  The only way I know to do something like this is to be able to access a dictionary.  Then you could look into text mining definitions of the words.  Try looking into accessing 'wordnet', maybe that could help. http://wordnet.princeton.edu/wordnet/" CreationDate="2014-07-21T17:16:23.593" UserId="375" />
  <row Id="1832" PostId="787" Score="0" Text="@ffriend I am using &quot;poor&quot; Hadoop. I actually didn't know HDP or CDH existed. Is HDP an add-on or would I have to reinstall Hadoop entirely?" CreationDate="2014-07-21T17:27:07.070" UserId="2614" />
  <row Id="1833" PostId="795" Score="1" Text="-0 for suggesting Levenshtein distance." CreationDate="2014-07-21T19:08:36.797" UserId="869" />
  <row Id="1834" PostId="787" Score="0" Text="@BigDataDude: You will have to reinstall it entirely, but unlike manual installation, automated way will take only 10-15 minutes even for large clusters (at least, this is true for CDH - I haven't used Hortonworks' manager). So unless you have already pushed too much unique data to existing HDFS, migrating to maintained cluster should be pretty easy and painless." CreationDate="2014-07-21T20:38:09.437" UserId="1279" />
  <row Id="1835" PostId="794" Score="0" Text="I'm was also a bit thrown by the YAML files at first, but have since come to love the clean separation between configuration and code.  You can choose to use Pylearn2 without YAML files, although this option is not well documented." CreationDate="2014-07-21T20:52:39.473" UserId="684" />
  <row Id="1836" PostId="794" Score="0" Text="In short, however, I wouldn't discard the library because of this simple design decision." CreationDate="2014-07-21T20:53:05.997" UserId="684" />
  <row Id="1837" PostId="758" Score="0" Text="&quot;You should only ask practical, answerable questions based on actual problems that you face.&quot;" CreationDate="2014-07-21T21:04:02.473" UserId="895" />
  <row Id="1838" PostId="758" Score="0" Text="This is practical, answerable and based on an actual problem in much the same way that &quot;Tell me how to perform data science&quot; is practical, answerable and based on an actual problem." CreationDate="2014-07-21T21:44:19.357" UserId="322" />
  <row Id="1839" PostId="799" Score="0" Text="This looks quite like what I am looking for. I am studying for finals now and am unable to take time to think about this again, but as soon as I can I'll let you know.&#xA;&quot;A 256 byte periodic pattern would have manifested as vertical lines.&quot; -- exactly what I was thinking of. I can also show an image where I put all 256 bytes in the same line, and that is already obvious in text. I'm quite curious about what will come out of it :)" CreationDate="2014-07-22T00:08:30.030" UserId="2604" />
  <row Id="1840" PostId="807" Score="0" Text="Could you add how AUC compares to an F1-score?" CreationDate="2014-07-22T07:00:40.660" UserId="2532" />
  <row Id="1841" PostId="798" Score="0" Text="But the problem is that 'senior' and 'primary' don't occur in one title. How can I even compare this two words using list of job titles ?" CreationDate="2014-07-22T08:57:25.677" UserId="921" />
  <row Id="1842" PostId="798" Score="0" Text="Yes, this might help you learn that &quot;senior&quot; and &quot;developer&quot; go together, but not that &quot;senior&quot; and &quot;lead&quot; have similar semantic content." CreationDate="2014-07-22T09:35:35.613" UserId="21" />
  <row Id="1843" PostId="791" Score="0" Text="I think you will need some extra information to learn this. For example, do you have salary info, industry, and number of direct reports? This defines when two roles should be considered similar. Then you can ask what terms seem to be synonymous among similar roles. But if you don't know anything about what makes things similar I'm not sure what you can do." CreationDate="2014-07-22T09:36:47.343" UserId="21" />
  <row Id="1844" PostId="793" Score="2" Text="The major use is storing data and retrieving data. In fact, that's about the only use for a NOSQL database, or any database. Want to make your question better?" CreationDate="2014-07-22T15:09:41.543" UserId="471" />
  <row Id="1845" PostId="811" Score="1" Text="_If you're trying to build a representative model -- one that describes the data rather than necessarily predicts_ ... who builds a model which doesn't predcit?? Didn't get you there..." CreationDate="2014-07-22T15:23:14.600" UserId="2661" />
  <row Id="1846" PostId="812" Score="0" Text="you can try Graphviz... not sure if it scales up to millions of vertices...." CreationDate="2014-07-22T15:29:54.013" UserId="984" />
  <row Id="1847" PostId="797" Score="0" Text="You are right. NOSQL databases are mainly used for storing unstructured or semi-structured data like json. Can you  explain some of the types of data analysis we can do with them. What are the tools built into mongodb that can used for data analysis?" CreationDate="2014-07-22T15:32:12.733" UserId="2643" />
  <row Id="1848" PostId="793" Score="0" Text="Yes, database is mainly used for storing and retrieveing data. How can they be used for data analysis? What are the tools  built into  NOSQL databases like mongodb  which makes data analysis easy and powerful?" CreationDate="2014-07-22T16:56:17.863" UserId="2643" />
  <row Id="1849" PostId="793" Score="1" Text="Improve your question by editing it, not adding to the comments." CreationDate="2014-07-22T16:57:58.290" UserId="471" />
  <row Id="1850" PostId="798" Score="0" Text="@Mher, They're not supposed to occur in the same title; the terms _following_ them are supposed to occur in both, e.g., senior *developer*, or primary *developer*." CreationDate="2014-07-22T17:12:30.653" UserId="381" />
  <row Id="1851" PostId="798" Score="0" Text="@SeanOwen, If the titles are semantically similar, you would expect their co-occurrence vectors to be similar too since they would be used interchangeably." CreationDate="2014-07-22T17:14:14.703" UserId="381" />
  <row Id="1852" PostId="797" Score="0" Text="@jithinjustin there aren't data analysis tools built into mongo, or really any database. Also, `json` is totally structured data. You can technically do any kind of data analysis on it, using a NOSQL database is actually not related. There are tools built *on top of* mongo, like analytica though." CreationDate="2014-07-22T17:39:42.677" UserId="548" />
  <row Id="1853" PostId="811" Score="3" Text="Unsupervised learning would be an example where you build a model that isn't necessarily geared to predict. In some instances you might want to explore or summarize your data." CreationDate="2014-07-22T18:07:02.207" UserId="2513" />
  <row Id="1854" PostId="801" Score="0" Text="Related methods like LDA may also be a good bet." CreationDate="2014-07-22T18:39:23.373" UserId="684" />
  <row Id="1855" PostId="807" Score="1" Text="@Dan- The biggest difference is that you don't have to set a decision threshold with AUC (it's essentially measuring the probability spam is ranked above non-spam). F1-score requires a decision threshold. Of course, you could always set the decision threshold as an operating parameter and plot F1-scores." CreationDate="2014-07-22T19:14:58.447" UserId="2513" />
  <row Id="1856" PostId="815" Score="1" Text="Just as a comment, it looks like you have a pretty strict schema for your data at this. Depending on how many new &quot;columns&quot; you expect to appear at a later time, SQL may actually the best solution for you. But again, I'm just speculating." CreationDate="2014-07-22T22:38:43.677" UserId="1163" />
  <row Id="1857" PostId="815" Score="2" Text="As someone who is a huge fan of NOSQL, SQL is probably the right choice for this project." CreationDate="2014-07-23T06:16:20.823" UserId="869" />
  <row Id="1858" PostId="819" Score="0" Text="thanks for this, it is really useful." CreationDate="2014-07-23T09:32:40.020" UserId="906" />
  <row Id="1859" PostId="798" Score="0" Text="@Emre yes but you also 'learn' that &quot;strategist&quot; and &quot;ballerina&quot; and &quot;chef&quot; are similar because they follow &quot;head&quot; or something. I am not sure if this is enough to learn on by itself?" CreationDate="2014-07-23T10:52:17.920" UserId="21" />
  <row Id="1860" PostId="802" Score="1" Text="I think the structure you want to implement is a &quot;trie&quot; - whether you can find a DB that efficiently works with that structure, or need to roll your own in RDBMS of your choice I cannot say." CreationDate="2014-07-23T13:43:04.077" UserId="836" />
  <row Id="1861" PostId="822" Score="0" Text="Thanks for the answer, it also makes me think about the ethics of when you do an experiment in order to see its influence such as the recent [fabeook](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840) issue, perhaps that itself would make a good question as to the moral implications." CreationDate="2014-07-23T13:55:09.943" UserId="95" />
  <row Id="1862" PostId="821" Score="0" Text="This was a generic question. I did not include code because I did not want to solve a specific problem. I never stated I'm a data scientist and... IMHO it is not necessary to be so &quot;rude&quot; :). Thank you anyway for the explanation." CreationDate="2014-07-23T13:56:06.313" UserId="989" />
  <row Id="1863" PostId="822" Score="0" Text="You're welcome.  Ethics is something we deal with a lot in Biostatistics due to the history of medical research.  As a statistician/data scientist, I would argue that it is ethical to give an accurate portrait of the data and to not torture it into confessing.  A good place to start for the ethics of trials, from the medical point of view is the [Nurenburg Code](http://en.wikipedia.org/wiki/Nuremberg_Code).  It certainly has application for what Facebook did." CreationDate="2014-07-23T13:59:51.797" UserId="178" />
  <row Id="1864" PostId="823" Score="0" Text="I don't have enough rep to add an ethics tag or perhaps social experimentation and facebook, if someone could oblige" CreationDate="2014-07-23T14:06:20.877" UserId="95" />
  <row Id="1865" PostId="798" Score="0" Text="@SeanOwen: perhaps some constraints/context should be specified in this case. For me &quot;head chief&quot; and &quot;head developer&quot; sound similar because they both lead their teammates. If you want to distinguish between professions only, you can use only keywords meaning profession titles. But question clearly refers to employees level as well as his profession." CreationDate="2014-07-23T15:27:10.410" UserId="1279" />
  <row Id="1866" PostId="798" Score="1" Text="Yeah it must be about level and role. Two &quot;head&quot;s are similar, but that's obvious because both have the word &quot;head&quot;. My point was that &quot;chef&quot; and &quot;ballerina&quot; aren't necessarily similar just because you see &quot;head chef&quot; and &quot;head ballerina&quot; which is how I understood the cooccurrence idea. How do you learn that &quot;lead developer&quot; and &quot;senior developer&quot; are similar but &quot;junior developer&quot; is not? I think some other data has to enter the picture to tell us that the first two are supposed to be similar, then we can figure out why terms explain it." CreationDate="2014-07-23T15:40:02.637" UserId="21" />
  <row Id="1867" PostId="769" Score="0" Text="It depends on your data whether you want to do a Min-max normalization to unit range (e.g., 0-1) or Z-score normalization/standardization to unit variance (variance=1, mean=0). Sorry, but I forgot that you are doing text classification. I think normalization after you stemmed the words and used a vectorizer function would not be necessary" CreationDate="2014-07-23T16:22:00.253" UserId="2556" />
  <row Id="1868" PostId="823" Score="0" Text="I'm a bit confused as to what you're actually asking here. The title of this question seems inconsistent with the body and how ethics &quot;should&quot; be applied is way beyond the scope of this site. Academic programs are pretty diverse, but a question about professional standards accountability seems like it would be both answerable and reasonably scoped. Can you clarify or refocus your question?" CreationDate="2014-07-23T19:40:09.743" UserId="322" />
  <row Id="1869" PostId="823" Score="1" Text="You may also find [this meta discussion](http://meta.datascience.stackexchange.com/q/7/322) interesting, if you haven't read it already." CreationDate="2014-07-23T19:40:25.260" UserId="322" />
  <row Id="1870" PostId="823" Score="0" Text="@AirThomas Thanks for that meta link, I wasn't sure if this would be n topic or not. I guess what I'd like to know is whether ethics are taught or imbued academically and in the professional workplace and whether a moral code of practice exists for data science, this is a naive and broad question I admit" CreationDate="2014-07-23T20:00:22.007" UserId="95" />
  <row Id="1871" PostId="808" Score="0" Text="possible duplicate of [Starting my career as Data Scientist, is Software Engineering experience required?](http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require)" CreationDate="2014-07-23T21:03:31.483" UserId="553" />
  <row Id="1872" PostId="798" Score="0" Text="@SeanOwen: My answer below should clarify the idea, but in short it's all about context. If &quot;head&quot; is the only common word in chef and ballerina profiles, their semantic vectors will still be quite far from each other. Two chefs will have many common words about cooking, and two _head_ chefs will additionally refer to same ratings/certificates/management techniques, etc. Same thing with developers: lead and senior devs frequently use words &quot;client&quot;, &quot;strategy&quot;, etc., while juniors normally mention few most commonly known technologies." CreationDate="2014-07-23T21:49:55.393" UserId="1279" />
  <row Id="1873" PostId="826" Score="0" Text="I don't mean to be negative, but regular expressions are an extremely poor choice for this problem. Between state-by-state variation, vanity plates, and differing formats, regular expressions are a poor choice." CreationDate="2014-07-24T02:07:45.667" UserId="548" />
  <row Id="1874" PostId="829" Score="0" Text="What error are you getting? It might actually help us to know..." CreationDate="2014-07-24T06:46:00.500" UserId="471" />
  <row Id="1875" PostId="825" Score="2" Text="Any particular country? Different countries have different license plate formats. Its a big world out there." CreationDate="2014-07-24T11:48:14.770" UserId="471" />
  <row Id="1876" PostId="831" Score="1" Text="It looks like there are no prerequisites for membership, without which I see no reason why membership should carry any meaning. Membership is supposed to be a &quot;Valuable credential to signal to clients and employers&quot;, but this will only be true if there is some sort of qualification requirement/entrance exam and also some industry experience requirements. With a free for all membership, being a member doesn't distinguish you from a non-member." CreationDate="2014-07-24T11:56:29.887" UserId="2532" />
  <row Id="1877" PostId="826" Score="0" Text="How many different variations are there? I'm from a different country." CreationDate="2014-07-24T12:53:56.637" UserId="325" />
  <row Id="1878" PostId="826" Score="1" Text="A conservative estimate would be 51, probably close to a few hundred." CreationDate="2014-07-24T16:18:06.910" UserId="548" />
  <row Id="1879" PostId="826" Score="0" Text="Ah right I see the problem. I was thinking that I could find all the plates registered here with 3 regular expressions." CreationDate="2014-07-24T16:55:11.487" UserId="325" />
  <row Id="1880" PostId="825" Score="0" Text="@Spacedman Any country is fine :)" CreationDate="2014-07-24T17:38:07.027" UserId="1192" />
  <row Id="1881" PostId="826" Score="0" Text="@germcd Thanks for the attempt though! Glad to hear I wasn't the only person to think of regex too." CreationDate="2014-07-24T17:42:29.130" UserId="1192" />
  <row Id="1882" PostId="827" Score="0" Text="Never seen brat before; thanks for the tip!" CreationDate="2014-07-24T17:52:43.600" UserId="1192" />
  <row Id="1883" PostId="832" Score="0" Text="Do you want to be able to print the plot?" CreationDate="2014-07-24T19:54:46.143" UserId="2575" />
  <row Id="1884" PostId="799" Score="0" Text="I can't seem to run this on Debian Linux. I installed the packages `python-scitools` and `ipython`. The error message is `ValueError: invalid literal for int() with base 10: '#'`. I'll see if I can make it work anyway..." CreationDate="2014-07-25T02:41:49.133" UserId="2604" />
  <row Id="1885" PostId="797" Score="0" Text="I don't know about all that. MongoDB can perform better than MySQL. You'd have a better argument if you said PostgreSQL (which, by the way can accept JSON). Either way, I wouldn't consider some arbitrary &quot;performance&quot; (we don't know what the use case is) to be a reason not to use NoSQL. Also don't discount using multiple databases. Remember, MongoDB has amazing aggregation features that SQL does not have." CreationDate="2014-07-25T02:56:24.790" UserId="2711" />
  <row Id="1886" PostId="799" Score="0" Text="I succeeded (by running the code directly inside `ipython`, and changing `map(int, line)` to `map(ord, line)`, and updated the question with the new picture." CreationDate="2014-07-25T03:30:23.273" UserId="2604" />
  <row Id="1887" PostId="783" Score="0" Text="An example/excerpt of the data (maybe only a few MB) could be interesting." CreationDate="2014-07-25T09:43:55.557" UserId="156" />
  <row Id="1889" PostId="835" Score="1" Text="I don't see anything in that link or here that proves you can't do sentiment analysis in an SQL database. the mongoDB examples benefit from Javascript in the DB, so you could use any embedded language in an SQL database. For example Postgres + R." CreationDate="2014-07-25T12:54:27.213" UserId="471" />
  <row Id="1890" PostId="785" Score="0" Text="Don't load it in and treat it like a dataframe, its not a dataframe, its a stream of bytes." CreationDate="2014-07-25T12:56:20.203" UserId="471" />
  <row Id="1891" PostId="835" Score="0" Text="Would love to see where you could execute code and map/reduce in those databases. In all seriousness (especially Postgres). ...and even if you could, that still doesn't make the answer any less valid by the way. One simply just might want to use NoSQL. It does work." CreationDate="2014-07-25T14:07:06.143" UserId="2711" />
  <row Id="1892" PostId="835" Score="1" Text="Postgres + C, Python, Perl, R, feed your Postgres DB into the latest machine learning algorithms. Easy: http://www.postgresql.org/docs/9.0/static/xplang.html" CreationDate="2014-07-25T15:00:05.993" UserId="471" />
  <row Id="1893" PostId="835" Score="0" Text="Nice. I'll have to try that out sometime. How about MySQL?" CreationDate="2014-07-25T15:16:52.210" UserId="2711" />
  <row Id="1894" PostId="783" Score="0" Text="If you're interested in the periodic nature of the data taking a look at the DFT of the data could be revealing." CreationDate="2014-07-25T17:44:44.230" UserId="2724" />
  <row Id="1895" PostId="840" Score="0" Text="Your code please." CreationDate="2014-07-25T17:52:56.020" UserId="381" />
  <row Id="1896" PostId="832" Score="1" Text="@dsign, If you mean do I need to print it on a physical medium--no. But I would like a graphical representation of the data." CreationDate="2014-07-25T18:28:05.447" UserId="2702" />
  <row Id="1897" PostId="833" Score="0" Text="Unfortunately this type of plot doesn't work because it doesn't scale to show how many actions are taking place at the same time. I would estimate that at any given time there could be 100 actions taking place over the whole data set there are millions of actions." CreationDate="2014-07-25T18:32:48.153" UserId="2702" />
  <row Id="1898" PostId="833" Score="0" Text="you can change the thickness of the lines e.g. `geom_line(size = 2)`" CreationDate="2014-07-25T18:56:38.250" UserId="325" />
  <row Id="1899" PostId="797" Score="0" Text="@Tom on performance, you'll find that the only task that mongo actually outperforms mysql on is inserts (http://www.moredevs.ro/mysql-vs-mongodb-performance-benchmark/), which is a comparatively small part of data analysis. SQL's aggregation features are FAR more mature than Mongo's. As far as MYSQL versus Postgres, the numbers are very temporily skewed and both tend to offer similar performance. MYSQL is more common, which is why I mentioned that instead, but the two are quite similar." CreationDate="2014-07-25T21:59:38.130" UserId="548" />
  <row Id="1901" PostId="845" Score="0" Text="Thank you so much, that helped clear up a lot of things." CreationDate="2014-07-26T01:14:10.963" UserId="2726" />
  <row Id="1902" PostId="783" Score="0" Text="@mrmcgreg: I'll have to re-learn how the DFT works. I should've paid more attention to the signals &amp; systems classes :)" CreationDate="2014-07-26T03:10:46.017" UserId="2604" />
  <row Id="1903" PostId="797" Score="0" Text="I've always seen better performance on MongoDB when things fit into memory. I take benchmarks with a gain of salt because if you Google a bit you're gonna find a bunch of benchmarks showing MongoDB as faster. It truly depends on your needs. That said, to help answer the original question - I think there's plenty of uses for NoSQL in big data science and analytics." CreationDate="2014-07-26T03:29:46.807" UserId="2711" />
  <row Id="1904" PostId="797" Score="0" Text="@Tom If things fit into memory, one should take advantage of that and use something like `redis` or `memchached`. I'm not saying that benchmarks should be viewed as very authoritative, but I would certainly be interested in seeing benchmarks where mongo beats nosql in a read-heavy situation. I'm trying to be objective here, and I'm not saying that there's no place for NoSQL, I'm saying that SQL is an excellent, mature technology and there's no reason to move to NoSQL unless you have a specific use case that calls for a NoSQL database." CreationDate="2014-07-26T03:51:09.707" UserId="548" />
  <row Id="1906" PostId="797" Score="0" Text="ok, there's nosql. redis is key/value. i was just pointing out some actual use cases in my answer below and then all of a sudden i'm getting down voted by people who, i can only assume, have never used any other database other than MySQL and are afraid of doing so. a question like this wouldn't even be on normal stack exchange. it's just frustrating. though i do understand your point of view. don't get me wrong there." CreationDate="2014-07-26T04:22:40.623" UserId="2711" />
  <row Id="1907" PostId="797" Score="0" Text="http://stackoverflow.com/questions/9702643/mysql-vs-mongodb-1000-reads/9703513#9703513 is another great answer and thing to think about. it depends on how you're using these databases. if you're doing all these joins, then i just can't see how it's faster. mongodb of course can be faster than mysql and easily vica versa. and i always find mongodb useful when working with unpredictable and changing data." CreationDate="2014-07-26T04:40:30.683" UserId="2711" />
  <row Id="1908" PostId="797" Score="0" Text="Oh, another good one to look at is InfluxDB. It's newer. NoSQL technically, but has a SQL-like syntax. It's great for time series. I had a bit of fun with it." CreationDate="2014-07-26T05:29:29.660" UserId="2711" />
  <row Id="1909" PostId="841" Score="0" Text="Thanks, I was looking for something on these lines. It would be extremely helpful if you could give reference to a tutorial or ipython notebook discussing this." CreationDate="2014-07-26T06:41:43.930" UserId="1131" />
  <row Id="1910" PostId="844" Score="0" Text="See http://stackoverflow.com/questions/22592811/scala-spark-task-not-serializable-java-io-notserializableexceptionon-when/22594142#22594142" CreationDate="2014-07-26T09:58:00.053" UserId="2668" />
  <row Id="1911" PostId="753" Score="0" Text="does anyone have the algorithm to get the &quot;consistent phrases&quot;?" CreationDate="2014-07-26T10:48:48.253" UserId="122" />
  <row Id="1912" PostId="525" Score="0" Text="Thanks, this is awesome!" CreationDate="2014-07-27T07:08:12.487" UserId="913" />
  <row Id="1913" PostId="806" Score="1" Text="Consider a highly unbalanced problem. That is where ROC AUC is very popular, because the curve balances the class sizes. It's easy to achieve 99% accuracy on a data set where 99% of objects is in the same class." CreationDate="2014-07-27T10:26:44.380" UserId="924" />
  <row Id="1914" PostId="851" Score="2" Text="There are basically 2 possible ways to go. Simple one is to simply compare colour histograms. [This question](http://stackoverflow.com/questions/6499491/comparing-two-histograms) give pretty good description of several good measures/methods. But if you are going to use it in image search engine or something like that, it makes sense to also mimic human _perception_ of colours, which is much harder task. [This paper](http://ect.bell-labs.com/who/emina/papers/pfl.pdf) provides some cues for better human-aware comparison." CreationDate="2014-07-27T22:18:12.620" UserId="1279" />
  <row Id="1915" PostId="851" Score="0" Text="That's a perfectly adequate answer, @ffriend." CreationDate="2014-07-28T01:27:55.820" UserId="381" />
  <row Id="1916" PostId="851" Score="0" Text="This is a cross post from Cross Validated. Anyone know what to do?" CreationDate="2014-07-28T02:23:36.587" UserId="1241" />
  <row Id="1917" PostId="831" Score="0" Text="@AsheeshR, yes this question is a matter of opinion but I think that the community can really benefit from it. How do we proceed?" CreationDate="2014-07-28T04:53:52.953" UserId="366" />
  <row Id="1919" PostId="850" Score="0" Text="Yes this is what I was looking for. I would be interested to see if this reduces to something like a Welch t-test because it may illuminate some complications. Eq. 52 in the paper you gave is nearly exactly what I want except it is not just a multinomial but a multinomial called a number of times for each user. I think the problem boils down to a generalized Behrens–Fisher problem. I am however a little out of my depth. Do you think you could sketch out a solution for this particular case? I know there are others on **Cross Validated** interested in the solution." CreationDate="2014-07-28T11:12:26.290" UserId="2511" />
  <row Id="1921" PostId="764" Score="0" Text="You need to find an algorithm that takes a pre-computed distance matrix or allows you to supply a distance-function that it can call when it needs to compute distances. Otherwise it wont work." CreationDate="2014-07-28T13:45:35.470" UserId="471" />
  <row Id="1922" PostId="823" Score="0" Text="Please, re-edit your question. I feel you have more than one question in your mind. Can you appropriately list them (first paragraph for introducing what you have read, second paragraph for your opinion about it, and third paragraph for listing your questions)?" CreationDate="2014-07-28T17:12:41.130" UserDisplayName="user1361" />
  <row Id="1925" PostId="831" Score="0" Text="@power Please take a look at http://datascience.stackexchange.com/help/dont-ask. If you still disagree, please raise the issue on [meta]." CreationDate="2014-07-29T02:08:03.853" UserId="62" />
  <row Id="1926" PostId="831" Score="0" Text="@AsheeshR Okay, I will re-word the question shortly." CreationDate="2014-07-29T03:56:38.243" UserId="366" />
  <row Id="1931" PostId="823" Score="1" Text="I'd suggest removing the word &quot;moral&quot; from the title of the question. Ethics, by definition, implies moral aspect as foundational." CreationDate="2014-07-29T12:38:08.847" UserId="2452" />
  <row Id="1932" PostId="823" Score="1" Text="@AleksandrBlekh sure will do, thanks for feedback, I noted today that okcupid have just admitted experimenting on users." CreationDate="2014-07-29T12:39:48.540" UserId="95" />
  <row Id="1934" PostId="846" Score="0" Text="Getting the data is often one of the biggest challenge :)" CreationDate="2014-07-29T19:35:55.310" UserId="737" />
  <row Id="1935" PostId="865" Score="0" Text="This question is very difficult to understand. Can you please consider rewriting it with a better description of what you are looking for and what you have already tried." CreationDate="2014-07-30T12:22:03.380" UserId="802" />
  <row Id="1936" PostId="866" Score="0" Text="Can you provide some example data (in plain English, no codes)?" CreationDate="2014-07-30T13:49:20.293" UserId="1279" />
  <row Id="1937" PostId="866" Score="0" Text="I added some example data to my original post.  In this version, each condition is denoted by a three letter code." CreationDate="2014-07-30T13:59:47.707" UserId="2781" />
  <row Id="1941" PostId="866" Score="1" Text="R is cool, but not very human-readable. Could you please reformat sample of your data as a table (e.g. using CSV or TSV format; 5-6 columns is ok)? Also, some explanation of variables (what &quot;anx.any&quot;, &quot;flu.isbefore.ckd&quot;, etc. actually mean and what is to be predicted) will help a lot." CreationDate="2014-07-30T19:49:21.060" UserId="1279" />
  <row Id="1942" PostId="871" Score="1" Text="Let me know if you have further questions and I'll do my best to provide some more detail." CreationDate="2014-07-30T20:09:29.443" UserId="684" />
  <row Id="1943" PostId="870" Score="0" Text="Worth noting that this is not explicitly a problem in all of machine learning, but only a problem when it comes to generating feature vectors, which are not ubiquitous in machine learning." CreationDate="2014-07-30T20:12:37.283" UserId="869" />
  <row Id="1944" PostId="870" Score="0" Text="What kind of machine learning doesn't use features?" CreationDate="2014-07-30T20:22:26.113" UserId="381" />
  <row Id="1945" PostId="870" Score="0" Text="Random forest is a good example of something for which getting a feature vector of the sort you see in neural nets is not an issue. A lot of unsupervised methods also work on raw words rather than feature vectors. Note: I didn't say there are methods that don't use features, only that there are methods which do not rely on strictly structured vectors." CreationDate="2014-07-30T23:00:17.087" UserId="869" />
  <row Id="1946" PostId="870" Score="0" Text="I don't know what you mean by &quot;strictly structured&quot;." CreationDate="2014-07-30T23:05:35.013" UserId="381" />
  <row Id="1947" PostId="870" Score="0" Text="strictly structured is a 1d vector of unint8's as opposed to a list containing a dictionary, a weight matrix, and a series of strings" CreationDate="2014-07-30T23:48:32.817" UserId="869" />
  <row Id="1948" PostId="769" Score="0" Text="It took me a while to try everything out, I had an error earlier with PCA, now I see I just get much lower accuracy when using it. I reduce dimensions to 100, and that should be fine, but SVM gives me 4% error on MNIST (0.6% without PCA) and over 20% error on DIGITS (4% without PCA). Same for other classifiers. Earlier I somehow made the error of doing PCA on the whole dataset (train and test sets) which gave me too optimistic results." CreationDate="2014-07-31T05:29:44.877" UserId="1387" />
  <row Id="1949" PostId="875" Score="0" Text="Thanks, I'll try it" CreationDate="2014-07-31T08:13:05.463" UserId="988" />
  <row Id="1950" PostId="863" Score="0" Text="I'm not so sure about what you say on MapReduce, especially `So basically any problem that doesn't break data locality principle may be efficiently implemented using MapReduce`. From my understanding, you can only solve problems which can be expressed with the MapReduce pattern." CreationDate="2014-07-31T09:33:22.557" UserId="883" />
  <row Id="1951" PostId="863" Score="0" Text="@fxm: MapReduce framework may be used for pretty much different tasks. For example, Oozie - workflow scheduler for different Hadoop components - has so-called Java action, that simply creates one mapper and runs custom Java code in it. With this approach you can essentially run _any_ code. This, however, won't give you any MR advantages compared to simple Java app. The only way to gain these advantages is to run computations in parallel (using Map) on *nodes with data*, that is, locally. To summarize: you can run _any_ code with MR, but to obtain performance, you need to hold  data locality." CreationDate="2014-07-31T11:23:04.177" UserId="1279" />
  <row Id="1952" PostId="863" Score="0" Text="I agree, but what I meant is a developer should not worry about managing data locality (which is managed by hadoop), but rather about expressing the wanted algorithms with the MapReduce pattern." CreationDate="2014-07-31T12:00:27.257" UserId="883" />
  <row Id="1953" PostId="866" Score="0" Text="Understandable.  I was trying to provide a &quot;reproducible example.&quot;  I will update the question now.  Thanks." CreationDate="2014-07-31T12:11:59.803" UserId="2781" />
  <row Id="1954" PostId="863" Score="0" Text="@fxm: developer ensures locality by using mapper. Roughly speaking, &quot;map(f, data)&quot; means &quot;transfer f() to data nodes and run it locally&quot;.  If developer doesn't consider locality (e.g. puts all computations to reducer), he loses all advantages of MR. So expressing algo as (efficient) MR job implies utilization of data locality anyway. That said, I would still suggest using more flexible tools than pure MR such as Spark." CreationDate="2014-07-31T13:29:35.030" UserId="1279" />
  <row Id="1955" PostId="769" Score="0" Text="Which programming language are you using btw? If you are a Python guy, I'd have some examples here where I used PCA, maybe it helps: http://sebastianraschka.com/Articles/2014_about_feature_scaling.html http://sebastianraschka.com/Articles/2014_scikit_dataprocessing.html http://sebastianraschka.com/Articles/2014_pca_step_by_step.html Usually I prefer LDA since I am mostly working with supervised datasets (class labels), a separate article (like the step by step PCA) is in the works ;)" CreationDate="2014-07-31T13:47:17.910" UserId="2556" />
  <row Id="1956" PostId="871" Score="0" Text="Thanks, that gives me some great terms to continue exploring with!" CreationDate="2014-07-31T15:21:58.400" UserId="2790" />
  <row Id="1957" PostId="769" Score="0" Text="I am using C++ for classification and Matlab to prepare datasets. I will check out your links and try LDA too." CreationDate="2014-07-31T16:04:48.890" UserId="1387" />
  <row Id="1958" PostId="876" Score="0" Text="Thanks, Andy.  Could you elaborate a little? Is it because the variables don't capture enough detail?" CreationDate="2014-07-31T16:42:18.460" UserId="2781" />
  <row Id="1959" PostId="876" Score="0" Text="I have no idea. I guess it depends on how different models work." CreationDate="2014-07-31T16:54:46.747" UserId="1241" />
  <row Id="1960" PostId="876" Score="0" Text="Could you suggest some of the solutions you tried or considered?" CreationDate="2014-07-31T17:25:51.437" UserId="2781" />
  <row Id="1961" PostId="876" Score="0" Text="So far I haven't done either, so no help there. Sorry." CreationDate="2014-07-31T17:38:05.417" UserId="1241" />
  <row Id="1962" PostId="641" Score="0" Text="@Madison May. Did you find a data set? I'm looking for something similar. Thanks." CreationDate="2014-07-31T22:49:26.447" UserId="2507" />
  <row Id="1963" PostId="641" Score="0" Text="I had to make do with the twitter ner corpus from U. Washington (linked to in original post)." CreationDate="2014-07-31T23:01:20.507" UserId="684" />
  <row Id="1964" PostId="865" Score="0" Text="Not a data science question, its a programming question. Go ask on StackOverflow" CreationDate="2014-08-01T09:08:50.180" UserId="471" />
  <row Id="1965" PostId="840" Score="0" Text="Have you checked the contents of `df`?" CreationDate="2014-08-01T12:17:52.767" UserId="172" />
  <row Id="1966" PostId="881" Score="0" Text="Thanks for this! It confirms some of the steps I have already taken (exploratory analysis, hypothesis testing, etc.)." CreationDate="2014-08-01T15:15:02.910" UserId="2781" />
  <row Id="1967" PostId="883" Score="4" Text="What class is the Date column? It looks like it might be sorting by character (1 comes before 9) rather than date value." CreationDate="2014-08-01T20:13:47.650" UserId="2802" />
  <row Id="1968" PostId="883" Score="0" Text="Base R read.csv() converts strings to factor. I agree with @user1683454; you will find that your dates are in alphabetical order." CreationDate="2014-08-02T04:59:48.967" UserId="2666" />
  <row Id="1969" PostId="722" Score="1" Text="I would also add `dplyr`, which is an optimized rephrasing of certain `plyr` tools, and `data.table` which is a completely different approach to manipulating data. Both by Hadley Wickham." CreationDate="2014-08-02T05:22:07.200" UserId="1156" />
  <row Id="1970" PostId="722" Score="0" Text="@ssdecontrol: I agree - updated the answer. Hadley is the author of the `dplyr` package, but not of the `data.table` one." CreationDate="2014-08-02T08:22:18.307" UserId="2452" />
  <row Id="1971" PostId="722" Score="1" Text="Funny, I just kind of assumed he was. Thanks." CreationDate="2014-08-02T11:46:36.047" UserId="1156" />
  <row Id="1972" PostId="890" Score="0" Text="The computations I perform are both cpu and disk intense, which may occur concurrently. Using gpu's would surely speedup the cpu step, but disk access would still limit the performance." CreationDate="2014-08-02T16:53:34.197" UserId="84" />
  <row Id="1973" PostId="823" Score="1" Text="http://www.datakind.org/blog/meetup-recap-untangling-ethical-questions-in-data-science/" CreationDate="2014-08-02T19:58:25.010" UserId="381" />
  <row Id="1974" PostId="890" Score="1" Text="@Rubens you've to have good reason to use GPU for your computation. It expects fine-grained matrix type data, and well written parallel works, kernel implementations. For your disk problem, you'ld stream your data and feed them into RAM so that both (especially) GPU and CPU will benefits. Then, you can use your data in any, arithmetic, manner e.g. you can use map+reduce approach on your stream. To note, CUDA has built-in support for Map+Reduce+filter operations. However, testing performance on GPU based calculation is tricky (You can use manual calculation putting watcher around your code)." CreationDate="2014-08-02T20:51:25.250" UserDisplayName="user1361" />
  <row Id="1975" PostId="890" Score="0" Text="@Rubens Some patterns in [Patterns for Parallel Programming book](http://www.amazon.com/Patterns-Parallel-Programming-paperback-Software/dp/0321940784) will probably solve your disk problem." CreationDate="2014-08-02T20:53:10.027" UserDisplayName="user1361" />
  <row Id="1976" PostId="739" Score="0" Text="This question appears to be off-topic because it is about career advice. Career advice has been proven to result in opinion-oriented, broad questions or sometimes extremely restricted questions, most of which result in no useful discourse. If you disagree with this opinion, please raise the issue on [meta]." CreationDate="2014-08-03T06:18:00.257" UserId="62" />
  <row Id="1977" PostId="808" Score="0" Text="This question appears to be off-topic because it is about career advice. Career advice has been proven to result in opinion-oriented, broad questions or sometimes extremely restricted questions, most of which result in no useful discourse. If you disagree with this opinion, please raise the issue on [meta]." CreationDate="2014-08-03T06:18:34.900" UserId="62" />
  <row Id="1978" PostId="769" Score="0" Text="I tried using LDA but can't get it working with my data. Matlab function classify should perform LDA but it works only up to 20 dimensions, at least on my data. Also I found that maximum dimensions given by LDA should be number_of_classes-1, which is too little." CreationDate="2014-08-03T09:20:08.827" UserId="1387" />
  <row Id="1981" PostId="893" Score="4" Text="I'd say [CV.SE](http://stats.stackexchange.com/) is a better place for questions about more theoretical statistics like this. If not, I'd say that the answer to your questions depend on context. Sometimes it makes sense to flatten multiple levels into dummy variables, other times it's worth to model your data according to multinomial distribution, etc." CreationDate="2014-08-03T14:00:11.460" UserId="1279" />
  <row Id="1985" PostId="876" Score="0" Text="I'm now on vacation for the next few weeks, but when I get back I'll look into it because it really has piqued my interest." CreationDate="2014-08-03T20:29:59.447" UserId="1241" />
  <row Id="1986" PostId="878" Score="0" Text="Thanks for the response, do you have some source explaining more thoroughly about what kind of data is collected and methods of user tracking?" CreationDate="2014-08-03T20:37:39.297" UserId="2798" />
  <row Id="1987" PostId="878" Score="0" Text="I believe tasks are too diverse to be described in a single source. For example, in Facebook you would be interested in what user likes, who he talks to most frequently, etc. In commercial website you'd like to know what actions led to conversion and what forced user to leave site. In entertainment software (like games or funny web pages) you would most likely want to optimize user experience and thus look for UI component usage, time spent on page, etc. Different tasks require different data and different methods. Just determine your use case and look for appropriate approach." CreationDate="2014-08-03T21:15:16.970" UserId="1279" />
  <row Id="1988" PostId="769" Score="0" Text="I just uploaded the LDA article, although I used Python for the step-wise implementation, the Intro might still be interesting and helpful: http://sebastianraschka.com/Articles/2014_python_lda.html" CreationDate="2014-08-03T21:40:36.477" UserId="2556" />
  <row Id="1989" PostId="883" Score="1" Text="You should probably ask this on stackoverflow, its a basic R programming question, not really data science." CreationDate="2014-08-03T22:15:35.913" UserId="471" />
  <row Id="1990" PostId="883" Score="0" Text="@Spacedman good point. I will put it there next time." CreationDate="2014-08-04T14:23:22.043" UserId="2614" />
  <row Id="1991" PostId="902" Score="3" Text="There's one way to get &quot;generalization&quot; that is good enough for any scenario - sample the entire population. In all other cases your best option is to select confidence level and take sample large enough to give reasonable confidence interval." CreationDate="2014-08-04T20:06:43.500" UserId="1279" />
  <row Id="1993" PostId="886" Score="1" Text="As an addendum, I'd recommend taking a look at the `lubridate` package, which can make certain date manipulation tasks simpler." CreationDate="2014-08-04T23:30:59.173" UserId="1156" />
  <row Id="1994" PostId="812" Score="0" Text="Hopefully an answer to this question can touch on how graphs like these were made: https://medium.com/i-data/israel-gaza-war-data-a54969aeb23e?_ga=1.106579909.790909978.1407183841" CreationDate="2014-08-04T23:39:21.430" UserId="1156" />
  <row Id="1995" PostId="821" Score="0" Text="I would downvote this answer if I could. Modeling a feature of the data for its own sake is absurd. Models exist to answer questions, and if seasonality does not affect your question or its answer, then it is not only allowable but desirable to ignore it. With that in mind, @marcodena, the only answer to your question is &quot;because sometimes ignoring it will bias your predictions.&quot; I think a better question would ask which times those are. I also disagree with implication here that data always precedes a model in statistics." CreationDate="2014-08-04T23:44:04.903" UserId="1156" />
  <row Id="1996" PostId="811" Score="1" Text="I'd say it's safer to balance your sample, but also collect sampling weights so you can later re-weight your data for representativeness if you need to. @pnp plenty of social scientists build non-predictive models, e.g. for confirming theories." CreationDate="2014-08-04T23:56:14.120" UserId="1156" />
  <row Id="1997" PostId="886" Score="0" Text="@ssdecontrol: Agree, thanks for the comment. Upvoted." CreationDate="2014-08-05T00:15:21.147" UserId="2452" />
  <row Id="1998" PostId="904" Score="0" Text="I suggest replacing `untagged` tag with `r`, `dashboards`, `reports` or similar." CreationDate="2014-08-05T07:22:27.870" UserId="2452" />
  <row Id="1999" PostId="769" Score="0" Text="Finally I found out what was going on... My function in Matlab that writes features to a file would add spaces sometimes and only on some datasets and then my reader function in C would apparently read wrong values... PCA actually helped, boost classifier is still bad but will try to play with parameters some more to make it work. Still didn't try LDA but will do that too." CreationDate="2014-08-05T08:49:26.577" UserId="1387" />
  <row Id="2000" PostId="907" Score="0" Text="I didn't know about sparkTable, looks like a great tool for the job." CreationDate="2014-08-05T12:16:28.987" UserId="1156" />
  <row Id="2003" PostId="57" Score="1" Text="This is a very good post. R is excellent for data *manipulation* but can be pretty cumbersome with data *cleaning* because of its verbose syntax for string manipulation and fairly rigid adherence to lists and matrices for data structures." CreationDate="2014-08-05T13:01:12.313" UserId="1156" />
  <row Id="2004" PostId="871" Score="0" Text="Incidentally, I can relate to &quot;feature hashing&quot; since that seems very similar to a [bloom filter](http://en.wikipedia.org/wiki/Bloom_filter), which I'm familiar with from working with cryptocurrency code. I wonder if it's more effective to have a hashing function relate an input feature to multiple index positions (bloom-filter-style) rather than need a second hash function to set the sign of an index..." CreationDate="2014-08-05T13:44:28.797" UserId="2790" />
  <row Id="2005" PostId="769" Score="0" Text="Nice! I am glad to here that it was &quot;just&quot; a technical problem :). For supervised training samples, LDA is often (but not always) a better choice than PCA. There is a research article where the authors discuss this point: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=908974" CreationDate="2014-08-05T15:04:04.100" UserId="2556" />
  <row Id="2006" PostId="907" Score="0" Text="@ssdecontrol: Until recently, I didn't know about it, too. Looks like a great tool, for sure. I look forward to trying it in my project, if I will have a need and opportunity." CreationDate="2014-08-05T17:50:51.177" UserId="2452" />
  <row Id="2007" PostId="913" Score="0" Text="Nice answer! Any thoughts on my related questions? 1) http://stats.stackexchange.com/questions/101251/determining-characteristics-of-sampling-sets-for-efa-cfa-sem; 2) http://stats.stackexchange.com/questions/90386/optimal-sampling-strategy-for-efa-cfa-and-sem." CreationDate="2014-08-05T17:55:47.023" UserId="2452" />
  <row Id="2008" PostId="909" Score="0" Text="Hi, Alexei! It seems that you're proficient in R, so I'm wondering, if you have any advice on the problem I'm currently stuck with: http://stackoverflow.com/questions/25101444/errors-related-to-data-frame-columns-during-merging. Beyond that, I'd be glad to connect with you (see aleksandrblekh.com for my profiles on professional social networks), as it seems that we have some common interests (including the native language :-)." CreationDate="2014-08-05T19:53:25.950" UserId="2452" />
  <row Id="2009" PostId="891" Score="0" Text="I've always wondered about the difference between measures and metrics. According the government (NIST): &quot;...We use measure for more concrete or objective attributes and metric for more abstract, higher-level, or somewhat subjective attributes. ... Robustness, quality (as in &quot;high quality&quot;), and effectiveness are important attributes that we have some consistent feel for, but are hard to define objectively. Thus these are metrics.&quot; But the context is software engineering, not mathematics. What's your take?" CreationDate="2014-08-05T20:55:59.590" UserId="2507" />
  <row Id="2010" PostId="891" Score="1" Text="Wikipedia was more helpful. distance(x,y) is must be non-negative; d(x,y)=0 only if x=y; d(x,y) = d(y,x); and satisfy triangle inequality- d(x, z) ≤ d(x, y) + d(y, z)" CreationDate="2014-08-05T21:03:19.577" UserId="2507" />
  <row Id="2011" PostId="915" Score="0" Text="This is a good question. A lot of statistics books talk about the theoretical aspects of high-dimensional data and not the computational aspects." CreationDate="2014-08-06T00:23:33.773" UserId="1156" />
  <row Id="2012" PostId="896" Score="0" Text="Can you add a full citation for the paper? The link doesn't work." CreationDate="2014-08-06T00:47:34.150" UserId="1156" />
  <row Id="2013" PostId="917" Score="1" Text="Typo in the title: spare =&gt; sparse." CreationDate="2014-08-06T02:52:16.923" UserId="2452" />
  <row Id="2014" PostId="891" Score="0" Text="That's pretty much it: a metric has to meet certain axioms and a measure is less strictly defined." CreationDate="2014-08-06T03:34:29.120" UserId="2809" />
  <row Id="2015" PostId="893" Score="0" Text="Are your categorical variables ordered ? If yes, this can influence the type of correlation you want to look for." CreationDate="2014-08-06T06:58:25.637" UserId="906" />
  <row Id="2016" PostId="918" Score="2" Text="Thank you for the great answer! I am hesitant to classify the problem as sparse regression since I am not really trying to model and predict but rather solve for a set of coefficients. The reason I am using Genetic Algorithms is because I can also employ constraints on the equation. If no other answers come through I will gladly accept this though." CreationDate="2014-08-06T12:27:51.970" UserId="802" />
  <row Id="2017" PostId="919" Score="2" Text="With k=5 you will get 20k observations in training set and 5k in testing set. With k=25 you'll get 24k for training and 1k for testing. If you believe that additional 4k records will affect generalization a lot, use larger k. If you think that even, say, 10k records already give good generalization, use smaller k. If you are unsure, just use standard 10-fold cross validation, which is a good compromise in most cases." CreationDate="2014-08-06T13:02:48.630" UserId="1279" />
  <row Id="2018" PostId="924" Score="0" Text="Thanks a lot. As I see, it's all about learning Caret package.." CreationDate="2014-08-06T13:14:47.953" UserId="97" />
  <row Id="2019" PostId="927" Score="0" Text="I split my test/train sets this way:  http://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas  Maybe this can help." CreationDate="2014-08-06T17:09:21.220" UserId="375" />
  <row Id="2020" PostId="918" Score="0" Text="@mike1886: My pleasure! I have updated my answer, based on your comment. Hope it helps." CreationDate="2014-08-06T21:34:14.897" UserId="2452" />
  <row Id="2021" PostId="926" Score="0" Text="Stephan, I appreciate your kind words! Upvoted your nice answer. You might be interested in the update I made to my answer, based on comment by the question's author." CreationDate="2014-08-06T21:44:31.100" UserId="2452" />
  <row Id="2022" PostId="896" Score="0" Text="@ssdecontrol the paper is called &quot;Class-Based n-gram Models of Natural Language&quot; by Brown et al. I have actually managed to resolve this. I will post details as an answer." CreationDate="2014-08-07T03:02:32.407" UserId="2817" />
  <row Id="2023" PostId="685" Score="0" Text="Instead of &quot;saving the coefficients&quot; you could save the whole model to a file, later load it again and use the predict() function. Should make the process a bit easier and less error prone." CreationDate="2014-08-07T08:41:22.423" UserId="676" />
  <row Id="2024" PostId="685" Score="0" Text="How much data did you use for training? Did you tune glmnet's lambda parameter and how? Cross validation?" CreationDate="2014-08-07T08:43:01.317" UserId="676" />
  <row Id="2025" PostId="933" Score="1" Text="Also, his other publications are at publications at http://research.microsoft.com/en-us/people/cyl/publication.aspx" CreationDate="2014-08-07T09:34:05.100" UserId="2861" />
  <row Id="2026" PostId="934" Score="1" Text="I asked a somewhat related question : http://datascience.stackexchange.com/q/810/2661" CreationDate="2014-08-07T12:24:39.557" UserId="2661" />
  <row Id="2027" PostId="934" Score="1" Text="Also, did you try the `BayesNet` (Bayesian Networks) algorithm in Weka and tried tuning the `MaxNrOfParents` argument in K2 search algorithm? I found it of good help in class imbalance problems." CreationDate="2014-08-07T12:28:24.147" UserId="2661" />
  <row Id="2028" PostId="930" Score="0" Text="the first suggestion will work but it is kind of &quot;manual&quot;.I was looking for train_test_split way to get dataframe. The second explanation does what i normally do but it returns nparray to which it is hard to impute values since there are no column names." CreationDate="2014-08-07T13:57:43.967" UserId="2854" />
  <row Id="2029" PostId="929" Score="0" Text="I can do that but it does not preserve any column names from the original dataframe. Then I need to impute values by indexes which can be very painful and can be a source of errors." CreationDate="2014-08-07T14:00:36.443" UserId="2854" />
  <row Id="2030" PostId="929" Score="0" Text="You can simply get the columns from the first dataframe in &#xA;cols = list(df_orig.columns.values)&#xA;and then set the new column names with this&#xA;new_df = pf.DataFrame(a,columns=cols)" CreationDate="2014-08-07T14:06:47.757" UserId="802" />
  <row Id="2031" PostId="936" Score="1" Text="Have you tried running that code in pure R, not Knitr? It seems to be more of an R problem and rather than a rendering to graphics problem." CreationDate="2014-08-07T15:14:37.097" UserId="802" />
  <row Id="2032" PostId="929" Score="0" Text="That really worked! Thanks a lot!" CreationDate="2014-08-07T15:27:42.750" UserId="2854" />
  <row Id="2033" PostId="936" Score="0" Text="The code produces no error when run in R." CreationDate="2014-08-07T15:40:41.360" UserId="2792" />
  <row Id="2034" PostId="936" Score="1" Text="As it stands, this example is not reproducible and therefore nobody can help you. Can you provide a minimal working example (MWE) of your .Rmd file that replicates the error? Chances are, constructing the MWE will also help you figure out what the problem is. However, my guess is that you aren't loading your data anywhere in the code. Knitr searches a new environment and not the current global environment, so you will need to re-load all packages and objects you plan to use." CreationDate="2014-08-07T16:45:11.010" UserId="1156" />
  <row Id="2035" PostId="922" Score="0" Text="Hi @Ali, it would help if you added some more detail. What do you mean by &quot;similarity words&quot;? are you talking about some preprocessing step? SVMs don't use a similarity measure." CreationDate="2014-08-07T23:39:38.887" UserId="21" />
  <row Id="2036" PostId="777" Score="0" Text="I think this might be more suitable for StackOverflow if you're looking for how to use the Github API? Maybe ServerFault?" CreationDate="2014-08-07T23:40:33.203" UserId="21" />
  <row Id="2037" PostId="910" Score="0" Text="This answer is exactly what I was hoping to get here. Thank you very much." CreationDate="2014-08-08T07:18:47.767" UserId="1192" />
  <row Id="2038" PostId="866" Score="0" Text="Can you provide more information on the parameters used in the data set so that we can understand if there are any correlations. Some of the abbreviations mentioned by you are not clear to me. It would be great if you could share your email-id for us to collaborate offline. Thanks!" CreationDate="2014-08-08T06:04:45.150" UserId="1094" />
  <row Id="2039" PostId="748" Score="0" Text="Most of your question here is describing a problem with your question. The text does not contain your question. Please in-line it and improve the text if needed. I don't think you need to summarize the history on the other site." CreationDate="2014-08-08T12:20:17.923" UserId="21" />
  <row Id="2040" PostId="812" Score="0" Text="Hello @Cici, usually questions about recommended tools are discouraged on this and other SE sites, as they just invite a lot of opinion." CreationDate="2014-08-08T12:21:22.990" UserId="21" />
  <row Id="2041" PostId="821" Score="0" Text="I know but I'm a student, so I'm learning. The question is made for this reason :)" CreationDate="2014-08-08T13:27:54.967" UserId="989" />
  <row Id="2042" PostId="942" Score="0" Text="Thanks, that's very helpful! If you do a literature review for various predictive modelling techniques, I'm sure it would get referenced a lot. It would be very helpful for people who want to differentiate between which algorithms to use in large n or large p cases, or for medium values of those for more precise calculations. Do you happen to know how some of the more obscure techniques scale? (Like Cox proportional hazard regression or confirmatory factor analysis)" CreationDate="2014-08-08T13:31:01.120" UserId="2841" />
  <row Id="2044" PostId="934" Score="0" Text="http://cs229.stanford.edu/proj2005/AltendorfBrendeDanielLessard-FraudDetectionForOnlineRetailUsingRandomForests.pdf&#xA;A good read that involves a similar 'rare-event' problem.  The authors use a random forest and optimize based on the ratio of class occurrence in the training set. (I'm not affiliated, but was just reading this a few days ago for a problem I'm working on)." CreationDate="2014-08-08T16:12:01.837" UserId="375" />
  <row Id="2045" PostId="947" Score="0" Text="That's really helpful. In our case we have currently hundreds of patterns so we'd end up having vectors with hundreds of features. Do these algorithms behave well in this situation? Other thing, each time a totally new exception comes up we'd need to classify it manually, possibly add new features and launch the train process to get a new model. Is this an appropriate approach?" CreationDate="2014-08-09T09:35:12.643" UserId="2878" />
  <row Id="2046" PostId="947" Score="0" Text="NB and SVM should work fine, decision trees over hundreds of variables may be hard to visualize and interpret, but I don't expect degradation in accuracy (though **random forests** are often suggested in such settings). As for new exceptions, standard way is to keep special `&lt;UNKOWN&gt;` variable, and really add new variables only when percentage of unknowns exceeds, say, 5%." CreationDate="2014-08-09T12:09:35.123" UserId="1279" />
  <row Id="2047" PostId="947" Score="0" Text="Great. Thanks!!" CreationDate="2014-08-09T13:50:19.260" UserId="2878" />
  <row Id="2048" PostId="937" Score="0" Text="I created my own class for that but very surprised that sklearn doesn't have that." CreationDate="2014-08-09T14:36:09.090" UserId="2854" />
  <row Id="2049" PostId="713" Score="0" Text="FWIW, I just started the 5.1 QuickStart VM and there was no such warning." CreationDate="2014-08-10T10:08:52.010" UserId="21" />
  <row Id="2052" PostId="715" Score="0" Text="There's no configuration that is required, or certainly there shouldn't be. The 5.1 VM does not exhibit this." CreationDate="2014-08-11T14:18:47.123" UserId="21" />
  <row Id="2053" PostId="954" Score="1" Text="possible duplicate of [Coreference Resolution for German Texts](http://datascience.stackexchange.com/questions/955/coreference-resolution-for-german-texts)" CreationDate="2014-08-11T15:19:58.320" UserId="1279" />
  <row Id="2054" PostId="757" Score="0" Text="Have you tried a model using temperatures?" CreationDate="2014-08-11T19:03:57.133" UserId="325" />
  <row Id="2055" PostId="959" Score="0" Text="Thanks for the helpful suggestions. I will definitely check out that book! Though I have access to lab values, the data is unreliable and sporadic, so i am trying to stick to data I can get from claims. The variable abbreviations are actually AHRQ Clinical Classification Software groupings of diagnosis codes." CreationDate="2014-08-11T22:01:36.267" UserId="2781" />
  <row Id="2056" PostId="955" Score="1" Text="Your question is very concise, yet it'd be nice to hear from you what you've tried so far, or even see some example of what you're trying to achieve. Would you mind adding some further information to your post?" CreationDate="2014-08-12T01:55:23.210" UserId="84" />
  <row Id="2057" PostId="955" Score="0" Text="possible duplicate of [OpenNLP Coreference Resolution (German)](http://datascience.stackexchange.com/questions/954/opennlp-coreference-resolution-german)" CreationDate="2014-08-12T08:00:42.737" UserId="2920" />
  <row Id="2058" PostId="954" Score="0" Text="It is not a duplicate. It is a question specific to OpenNLP and the other question is about coreference resolution tools in general" CreationDate="2014-08-12T10:57:39.147" UserId="979" />
  <row Id="2059" PostId="955" Score="0" Text="I already added additional information to the post" CreationDate="2014-08-12T11:00:45.350" UserId="979" />
  <row Id="2060" PostId="954" Score="0" Text="I can't see the difference between these questions. Both ask for a coreference tool besides OpenNLP, because OpenNLP doesn't support it." CreationDate="2014-08-12T11:20:12.657" UserId="21" />
  <row Id="2061" PostId="965" Score="1" Text="I think you'll probably need to be more specific about what your data is like, what your tool requirements are, what you've tried, in order to get more specific replies." CreationDate="2014-08-12T11:22:30.637" UserId="21" />
  <row Id="2062" PostId="967" Score="0" Text="Actually I disagree. Since the author likes them being in trouble, it is a positive sentiment there. It's a negative comment on the company, but nevertheless a positive sentiment by the author. In this simpler scenario (I'm not saying this is the complete goal), **predicting which emojis a user would add to his post** sounds like a reasonable task to me. In fact you can construct many cases where the emoji will be essential.. Consider &quot;Got f_cked :-)&quot; as opposed to &quot;Got f_cked. :-(&quot;" CreationDate="2014-08-12T13:32:24.477" UserId="2920" />
  <row Id="2063" PostId="963" Score="0" Text="Don't believe that something like this exists currently, but would love it if you put something together for this!" CreationDate="2014-08-12T13:57:53.260" UserId="548" />
  <row Id="2064" PostId="967" Score="0" Text="In case you try to estimate person's emotion as opposed to person's attitude to a subject, then yes, this example doesn't work. But there are many others. Sarcasm is common case. Consider sentence &quot;oh yeah, you are real 'master' ;)&quot;. Human can catch negative context, but positive emoticon will point to positive emotion. But I haven't really got it: do you want to extract subjective information from tweets or just predict possible emojis? Even though they sound similar, second task is not really about sentiment analysis. Not directly, at least." CreationDate="2014-08-12T14:10:36.103" UserId="1279" />
  <row Id="2065" PostId="954" Score="0" Text="This question asks about training methods to add the coref resolution functionality to OpenNLP and the other one asks for available tools for German coreference resolution. This is a big difference!" CreationDate="2014-08-12T15:11:32.730" UserId="979" />
  <row Id="2066" PostId="967" Score="0" Text="The &quot;wink&quot; smiley is usually not considered to be &quot;positive&quot;, but &quot;ironic&quot;... which is why a good dictionary such as SentiWordNet makes sense. If you look up funny in SentiWordNet, is has more than one meaning, too! http://sentiwordnet.isti.cnr.it/search.php?q=funny&#xA;(So it *is* not trivial to annotate them manually, because it's not as simple as positive/negative; but you should do the usual interrater-agreement validation etc.)" CreationDate="2014-08-12T16:03:50.307" UserId="2920" />
  <row Id="2067" PostId="967" Score="0" Text="Now I see your idea. But I don't really think it will work, just because (most) emojis don't really sound like a good predictors to me, and you explicitly don't want to use other features. Anyway, this is just an opinion based on my experience, only data can give real answers. Good luck!" CreationDate="2014-08-12T21:39:06.460" UserId="1279" />
  <row Id="2068" PostId="967" Score="0" Text="Who said I don't want to use other features? But for these I have seen databases..." CreationDate="2014-08-13T10:13:26.760" UserId="2920" />
  <row Id="2069" PostId="954" Score="0" Text="OK. I suggest rewording these to make them more distinct. I think many readers will not get the nuance of what you are asking in each case." CreationDate="2014-08-13T12:17:52.483" UserId="21" />
  <row Id="2071" PostId="973" Score="0" Text="Can you clarify? It sounds like you are looking for feature importance, but the link you give talks about writing weights as a linear combination of input, which is not the same thing at all." CreationDate="2014-08-14T10:22:23.980" UserId="21" />
  <row Id="2072" PostId="973" Score="0" Text="@SeanOwen, actually, what I wanted to say was since you can compute the weight vector using the dual variables and the input examples (in a binary SVM), and the actual weights corresponding to features tell us the relative importance of the features [Gene Selection for Cancer Classification using&#xA;Support Vector Machines&#xA;](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.9598&amp;rep=rep1&amp;type=pdf)" CreationDate="2014-08-14T14:48:25.540" UserId="2949" />
  <row Id="2074" PostId="957" Score="0" Text="If this does not get answered here, consider migrating it to the statistics site (stats.stackexchange.com) where you will be more likely to find experts in [time series](http://stats.stackexchange.com/questions/tagged/time-series)." CreationDate="2014-08-14T17:08:25.320" UserId="1237" />
  <row Id="2075" PostId="976" Score="2" Text="Math formatting does not work on DataScience? Really? May be we should ask to get it." CreationDate="2014-08-14T17:29:51.060" UserId="1237" />
  <row Id="2077" PostId="976" Score="0" Text="Good point about numerical accuracy." CreationDate="2014-08-15T00:24:26.790" UserId="1156" />
  <row Id="2078" PostId="980" Score="1" Text="Thanks a lot for such detailed response. I will go through all your suggestions, a lot of work to try and test! Also I have found that carrot2 tool is really doing great job on unsupervised clustering of textual data. Posting link for future reference [http://project.carrot2.org/](http://project.carrot2.org/)" CreationDate="2014-08-15T19:20:33.777" UserId="2958" />
  <row Id="2079" PostId="980" Score="0" Text="@MaximGalushka: You're very welcome! I'm curious to learn about your findings and the progress that you will achieve eventually. Feel free to post here or connect directly with me." CreationDate="2014-08-16T01:25:37.757" UserId="2452" />
  <row Id="2080" PostId="977" Score="0" Text="Cross-post from stats http://stats.stackexchange.com/questions/111911/solutions-for-continuous-online-cluster-identification AND stackoverflow: http://stackoverflow.com/questions/24970702/algorithm-for-identifying-non-ambiguous-clusters" CreationDate="2014-08-16T10:33:48.610" UserId="924" />
  <row Id="2082" PostId="915" Score="0" Text="In many cases, the original literature will discuss complexity. But often theoretical complexity is useless. QuickSort has a worst-case of O(n^2), but often is the fastest - faster than HeapSort, which has worst case O(n log n).&#xA;If you do a little research, you will find out complexity results for many algorithms - if known. E.g. PCA being O(n d^3), k-means being O(n k i d) etc." CreationDate="2014-08-16T10:39:50.540" UserId="924" />
  <row Id="2083" PostId="984" Score="0" Text="I don't think it's possible for people to help you without the data set, and it sounds like that is intentionally not public. I am not sure it's a good thing to ask for it then, and questions asking for links or resources are not considered on-topic for StackExchange sites." CreationDate="2014-08-16T17:31:37.270" UserId="21" />
  <row Id="2084" PostId="985" Score="0" Text="Sure, you can. It's common to do dimensionality reduction as a preprocessing step." CreationDate="2014-08-16T17:39:49.320" UserId="381" />
  <row Id="2085" PostId="898" Score="0" Text="Thanks Alexey for the details. Based on more research i found about polyserial and polychloric correlation. How is your approach better than these? Please explain" CreationDate="2014-08-17T09:58:57.173" UserId="1151" />
  <row Id="2086" PostId="898" Score="0" Text="I'm not aware of these things, sorry." CreationDate="2014-08-17T14:04:27.123" UserId="816" />
  <row Id="2087" PostId="936" Score="0" Text="As @ssdecontrol suggests, you probably haven't created or loaded `sample` in the R in your document subsequent to this line." CreationDate="2014-08-17T14:21:23.310" UserId="2978" />
  <row Id="2088" PostId="961" Score="0" Text="Please share some info on the data you are looking at." CreationDate="2014-08-17T19:22:03.863" UserId="1193" />
  <row Id="2089" PostId="961" Score="0" Text="@bayer: please check out my update." CreationDate="2014-08-17T19:48:04.687" UserId="1279" />
  <row Id="2090" PostId="961" Score="0" Text="This looks like a poor training procedure. Can you add information on the number of CD steps, learning rate/momentum, batch size etc?" CreationDate="2014-08-18T11:34:15.643" UserId="1193" />
  <row Id="2091" PostId="961" Score="0" Text="@bayer: at the moment of these experiments I used CD-1, batch size of 10 images, learning rate of 0.01 (0.1 / batch_size) and no momentum at all. I also noticed that weight initialization has some impact: with weights initialized from N(0, 0.01) I have almost never seen described issue, but with weights from N(0, 0.001) I get the issue almost each time." CreationDate="2014-08-18T11:58:30.507" UserId="1279" />
  <row Id="2092" PostId="989" Score="0" Text="Could you provide the code? Also, does it training or testing takes so much time? How about smaller training/testing datasets?" CreationDate="2014-08-18T12:09:04.657" UserId="1279" />
  <row Id="2093" PostId="989" Score="0" Text="I am just reading data from a csv file into a pandas dataframe and passing it to the scikit learn function. That's all! Providing code wouldn't really help here" CreationDate="2014-08-18T12:49:25.843" UserId="793" />
  <row Id="2094" PostId="989" Score="1" Text="sklearn's SVM implementation implies at least 3 steps: 1) creating SVR object, 2) fitting a model, 3) predicting value. First step describes kernel in use, which helps to understand inner processes much better. Second and third steps are pretty different, and we need to know at least which of them takes that long. If it is training, then it may be ok, because learning is slow sometimes. If it is testing, then there's probably a bug, because testing in SVM is really fast. In addition, it may be CSV reading that takes that long and not SVM at all. So all these details may be important." CreationDate="2014-08-18T13:22:28.580" UserId="1279" />
  <row Id="2095" PostId="995" Score="0" Text="I just edited my question to add that the dependent variable is binary. Hence, a linear model isn't suitable." CreationDate="2014-08-18T17:36:56.743" UserId="1241" />
  <row Id="2096" PostId="995" Score="0" Text="&quot; you should not expect different models to perform nearly identically, unless they all provide the same predictive bias.&quot; I used MAE and the ratio of actual to predicted outcomes as validation measures and the ratios were very close." CreationDate="2014-08-18T17:40:01.437" UserId="1241" />
  <row Id="2097" PostId="995" Score="1" Text="Andy, I would include logistic regression (and linear SVM) as 'linear' model. They are all just separating the data by a weighted sum of the inputs." CreationDate="2014-08-18T17:59:17.967" UserId="1256" />
  <row Id="2098" PostId="995" Score="1" Text="@seanv507 Exactly - the decision boundary is still linear. The fact that binary classification is being performed doesn't change that." CreationDate="2014-08-18T18:07:36.887" UserId="964" />
  <row Id="2099" PostId="961" Score="0" Text="Do you randomly initialize your weights before training?" CreationDate="2014-08-18T18:18:08.873" UserId="403" />
  <row Id="2100" PostId="961" Score="0" Text="@gallamine: yes, I initialize them from `N(0, 0.01)` or `N(0, 0.001)` with later case producing smoother images but giving described issues more frequently." CreationDate="2014-08-18T19:07:31.317" UserId="1279" />
  <row Id="2101" PostId="961" Score="0" Text="I recommend playing around with the learning rate and adding momentum. Another thing you should check is if the features (i.e. $p(h|v)$) are saturating. That is the case if all the features are always close to 0 or 1--learning gets harder then." CreationDate="2014-08-18T19:43:41.783" UserId="1193" />
  <row Id="2102" PostId="997" Score="0" Text="Could you be more specific into what you're looking for? does it matter?" CreationDate="2014-08-19T13:48:19.933" UserId="2969" />
  <row Id="2103" PostId="997" Score="0" Text="I just want to have a look at the dataset such that I can perform data visualization , or even apply time series models to the dataset." CreationDate="2014-08-19T14:44:20.317" UserId="2972" />
  <row Id="2104" PostId="997" Score="0" Text="Would EEG data work for you? It's spatial (channel locations on the head) time series data." CreationDate="2014-08-19T14:47:49.047" UserId="2969" />
  <row Id="2105" PostId="997" Score="0" Text="Sure. Thanks a lot" CreationDate="2014-08-19T14:49:30.277" UserId="2972" />
  <row Id="2106" PostId="995" Score="0" Text="What about trees? They really don't seem linear to me." CreationDate="2014-08-19T14:52:54.113" UserId="1241" />
  <row Id="2107" PostId="995" Score="0" Text="Random forests can approximate a linear boundary pretty well. But there are no details in your question regarding characteristics of the data, model parameters, or model performance such that anyone can give a definitive answer as to why they all perform similarly." CreationDate="2014-08-19T15:10:19.713" UserId="964" />
  <row Id="2108" PostId="995" Score="0" Text="With regard to oberservations-to-variables ratio, consider a 2-variable problem where `x * y &gt; 0` is classified as `True` and `x * y &lt;= 0` is `False`. Even as sample size goes to infinity, a logistic regression model will, on average, have no better than accuracy of 0.5, whereas a 2x2x2 neural network will have accuracy approaching 1.0. One can easily construct such examples where sufficiently high ratio of sample size to number of variables will not guarantee similar performance of classifiers." CreationDate="2014-08-19T15:19:36.383" UserId="964" />
  <row Id="2109" PostId="942" Score="0" Text="Unfortunately no, but if I ever do that review I will try to be comprehensive. I'd hardly call Cox regression &quot;obscure,&quot; at least in my field." CreationDate="2014-08-19T16:31:00.380" UserId="1156" />
  <row Id="2110" PostId="995" Score="0" Text="bogatron- could you elaborate? I'm not quite sure what you're asking for." CreationDate="2014-08-19T18:37:12.850" UserId="1241" />
  <row Id="2111" PostId="999" Score="0" Text="I'll ask my boss if I can get the company to pay for it." CreationDate="2014-08-19T18:39:02.327" UserId="1241" />
  <row Id="2112" PostId="699" Score="0" Text="On a related note, I found [this relevant paper](http://research.microsoft.com/en-us/um/people/sdumais/ecir07-metzlerdumaismeek-final.pdf)." CreationDate="2014-08-19T19:11:38.010" UserId="1097" />
  <row Id="2113" PostId="995" Score="0" Text="It *may* be that for your data set, each of the models can accurately approximate the optimal decision boundary (see answer by @StasK), though I wouldn't say that for sure without looking at the model parameters and doing a comparative error analysis. My main point was simply that a high sample-to-variable ratio does not guarantee similar performance for a given set of classifiers because they each have different biases/limitations on the kind of solutions they can produce." CreationDate="2014-08-19T19:17:08.837" UserId="964" />
  <row Id="2114" PostId="991" Score="0" Text="Thank you for your answer! I have two follow-up questions :) 1) How are SVM (with a linear kernel) and Naive Bayes different in that they do not sum up their features and corresponding weights (i.e. what you call an &quot;additive model&quot;)? Both effectively create a separating hyperplane so isn't the result is always some kind of adding features multiplied by corresponding weights? 2) I'd like to try random forests, but unfortunately the feature space is too large to represent it in dense format (I'm using sklearn). Is there an implementation that can handle that?" CreationDate="2014-08-19T22:07:18.420" UserId="2979" />
  <row Id="2115" PostId="987" Score="1" Text="Some update: I was able to achieve acceptable results by l2-normalizing the additional dense vectors. I wrongfully assumed the sklearn StandardScaler would do that. I am still looking for more complex methods, though, that would allow me to model label dependencies or incorporate confidence of sub-classifiers." CreationDate="2014-08-19T22:11:42.240" UserId="2979" />
  <row Id="2116" PostId="961" Score="0" Text="@bayer: thanks, I'll try these parameters. I'm more interested, however, in _why_ this may happen. That is, I already can fix described effect by setting larger number of hidden units, but I'm curious why at all weights in random Markov field could tend to change synchronously." CreationDate="2014-08-19T23:37:53.633" UserId="1279" />
  <row Id="2117" PostId="993" Score="1" Text="Hm why is that? more observations seems to increase the chance that the decision boundary is more complex -- i.e. definitely not linear. And these models do different things in complex cases, and tend to do the same in simple ones." CreationDate="2014-08-20T09:53:57.613" UserId="21" />
  <row Id="2118" PostId="993" Score="0" Text="@SeanOwen: I think I'm not understanding your comment. What part of my answer does &quot;why is that&quot; refer to? The OP said nothing about using linear decision boundaries - after all, he might by transforming predictors in some way." CreationDate="2014-08-20T10:31:49.017" UserId="2853" />
  <row Id="2119" PostId="993" Score="0" Text="Why would more observations make different classifiers give more similar decisions? my intuition is the opposite. Yes, I'm not thinking of just linear decision boundaries. The more complex the optimal boundary the less likely they will all fit something similar to that boundary. And the boundary tends to be more complex with more observations." CreationDate="2014-08-20T10:51:28.617" UserId="21" />
  <row Id="2120" PostId="909" Score="0" Text="This is a great explanation, and is in fact the method that I ended up using. I like to think of this method as splitting the entire sample set into smaller sub-samples and using the means (average with CLT) of each sub-sample as the distribution of the data set. Thanks for the answer!" CreationDate="2014-08-20T11:00:40.643" UserId="2830" />
  <row Id="2121" PostId="999" Score="1" Text="ESL is 'free' as a pdf from their homepage...also worth downloading is ISL (by many of same authors) - more practical http://www-bcf.usc.edu/~gareth/ISL/" CreationDate="2014-08-20T12:02:47.173" UserId="1256" />
  <row Id="2122" PostId="961" Score="0" Text="If your learning rate is too high, the first sample (or the mean of the batch) will be what the RBM overfits to. If the &quot;neurons&quot; (i.e. p(h|v)) then saturate, learning stalls--the gradients of these neurons will be close to zero. This is one way of this happening." CreationDate="2014-08-20T15:11:44.423" UserId="1193" />
  <row Id="2123" PostId="991" Score="0" Text="1) In linear regression you are interested in points _on_ hyperplane, thus you add up weighted features to get predicted point. In SVM, on other hand, you are looking for points _on the sides_ of hyperplane. You do classification by simple checking on which side is your example, no summation involved during prediction. Naive Bayes may incorporate different kinds of models (e.g. binomial or multinomial), but basically you multiply probabilities, not add them." CreationDate="2014-08-20T15:33:08.117" UserId="1279" />
  <row Id="2124" PostId="991" Score="0" Text="2) I have seen some research in this topic, but never encountered implementation (probably googling will give some links here). However, you can always go another way - reduce dimensionality with, say, PCA and then run random forest based on reduced dataset." CreationDate="2014-08-20T15:35:33.363" UserId="1279" />
  <row Id="2125" PostId="1007" Score="3" Text="Why not point us to the page that you are trying to scrape? The &quot;best tool&quot; is hard to define otherwise." CreationDate="2014-08-20T15:55:06.153" UserId="471" />
  <row Id="2126" PostId="977" Score="0" Text="Is the problem that you are trying to maintain the identity of the clusters as much as possible at each time step? So that at N+1 you can say how a cluster has changed because there is some relation between clusters at N and those at N+1? And the tricky bit is what happens if clusters split and merge?" CreationDate="2014-08-20T16:01:32.980" UserId="471" />
  <row Id="2127" PostId="977" Score="0" Text="@Spacedman: BINGO :) http://www.joyofdata.de/blog/reasonable-inheritance-of-cluster-identities-in-repetitive-clustering/" CreationDate="2014-08-20T16:31:32.067" UserId="725" />
  <row Id="2128" PostId="1006" Score="1" Text="+1 for all the links too! I see my road is long!" CreationDate="2014-08-20T19:10:45.143" UserId="2861" />
  <row Id="2133" PostId="858" Score="0" Text="Hey, fist I want to thank you for your answer. I have one more question. Word vector that are returned from word2vec algorithm have float values, so words like big and bigger will have vectors that are close in vector space, but the values of vectors could be completely different. For example big = [0.1, 0.2, 0,3] and bigger = [0.11, 0.21, 0.31]. Isn't that a problem for CRF algorithm, because this algorithm would treat them as not simillar? Is there any addional processing that sould be done before using this word vectors in CRF? I hope my question is clear enough." CreationDate="2014-08-21T08:10:01.407" UserId="2750" />
  <row Id="2134" PostId="961" Score="0" Text="@bayer: now it makes sense to me, thanks!" CreationDate="2014-08-21T11:00:44.460" UserId="1279" />
  <row Id="2136" PostId="1017" Score="0" Text="This is a quite open-ended question. Can you be more specific about what problem you are considering, how RL applies, what you have done so far and what your specific confusion is?" CreationDate="2014-08-21T11:53:25.470" UserId="21" />
  <row Id="2138" PostId="1010" Score="0" Text="Some sites create their HTML from Javascript and then you need a Javascript-powered scraper (Selenium). Or you reverse engineer their JSON API and get the data directly..." CreationDate="2014-08-21T15:20:35.523" UserId="471" />
  <row Id="2139" PostId="1010" Score="0" Text="some sites do, but the majority do not. As OP didn't specify, one assumes it's a 'normal' site." CreationDate="2014-08-21T15:22:06.923" UserId="2861" />
  <row Id="2140" PostId="1007" Score="2" Text="On StackExchange sites, when you say &quot;doesn't work&quot; or &quot;not satisfied&quot;, you really *have* to say what that means. Was it too slow? expensive? feature missing? Otherwise, how do you expect a suggestion for something &quot;better&quot;?" CreationDate="2014-08-21T15:49:30.967" UserId="21" />
  <row Id="2145" PostId="1017" Score="0" Text="Please ask just one question per post, unless the questions are closely related.  Your questions are quite different." CreationDate="2014-08-22T15:25:42.997" UserId="609" />
  <row Id="2146" PostId="811" Score="0" Text="How would a balanced model compare to a representative model using weighted observations to mimic a balanced model?" CreationDate="2014-08-22T22:25:21.753" UserId="1241" />
  <row Id="2147" PostId="1022" Score="0" Text="This is more of a comment than answer." CreationDate="2014-08-23T09:22:35.743" UserId="21" />
  <row Id="2148" PostId="1021" Score="2" Text="PS I think the term you're looking for is &quot;canonical&quot;" CreationDate="2014-08-23T09:23:00.063" UserId="21" />
  <row Id="2149" PostId="1028" Score="0" Text="All algorithms will overfit to some degree. It's not about picking something that doesn't overfit, it's about carefully considering the amount of overfitting and the form of the problem you're solving to maximize more relevant metrics." CreationDate="2014-08-23T18:16:45.767" UserId="548" />
  <row Id="2150" PostId="1030" Score="0" Text="The reason I'm using regression is that I need the per year rate of change for reasons I'd rather not get into right now." CreationDate="2014-08-23T20:24:15.167" UserId="1241" />
  <row Id="2151" PostId="1030" Score="1" Text="@AndyBlankertz: I just updated my answer." CreationDate="2014-08-23T20:33:27.433" UserId="2452" />
  <row Id="2152" PostId="1030" Score="0" Text="Thanks. I'd love to delve into the resources, but I'm time limited- the report I'm working on is due on Friday. I also have some slack in statistical rigorousness, because the target audience is Management :) Hopefully next week." CreationDate="2014-08-23T20:44:59.527" UserId="1241" />
  <row Id="2153" PostId="1030" Score="1" Text="@AndyBlankertz: You're welcome. I understand, as I'm not a statistician myself :-). But I'm trying to learn wherever and whenever I can." CreationDate="2014-08-23T20:54:16.963" UserId="2452" />
  <row Id="2154" PostId="1029" Score="4" Text="This is probably not a great question for StackExchange. It's going to be mostly opinions and speculation." CreationDate="2014-08-24T09:24:29.747" UserId="21" />
  <row Id="2155" PostId="1022" Score="0" Text="Will move it to comments ASAP." CreationDate="2014-08-24T12:59:11.480" UserId="941" />
  <row Id="2156" PostId="1020" Score="0" Text="AFAIK you have to buy that data. The FB api is more for interacting with the platform not for retrieving personal info of users that use the service." CreationDate="2014-08-24T16:49:27.613" UserId="92" />
  <row Id="2157" PostId="1021" Score="0" Text="Is the &quot;most probable&quot;/&quot;most consensual&quot; string you are looking to idendify a regular expression?  Or one of the strings on the list?" CreationDate="2014-08-24T21:47:38.487" UserId="609" />
  <row Id="2158" PostId="1021" Score="0" Text="@MrMeritology I am not looking for a regular expression. I have shown a regular expression in my question just to illustrate how flexible I am in the kind of strings I would consider to be correct." CreationDate="2014-08-25T08:07:58.927" UserId="3047" />
  <row Id="2159" PostId="1037" Score="0" Text="Thanks!  I thought of #1, but maybe that there was a highly visited site that might be doing it.  And you're right about #2- heuristic optimization is not guaranteed to come up with a interpretable solution." CreationDate="2014-08-25T16:32:56.073" UserId="375" />
  <row Id="2160" PostId="1021" Score="0" Text="OK.  Then the answer I gave below should work for you." CreationDate="2014-08-25T18:53:19.470" UserId="609" />
  <row Id="2161" PostId="1030" Score="0" Text="This isn't time series analysis (unless you throw away loads of data). I think the data records are individual sales records with a year attached as a covariate. Time series analysis is used when the variable of interest (eg *total sales*) has a unique time point. You could compute total sales within years and do time series analysis, but that would mean losing all the other information from each sales record (eg item purchased, buyer age etc). Regression is the right thing here." CreationDate="2014-08-26T08:51:42.180" UserId="471" />
  <row Id="2162" PostId="1030" Score="0" Text="@Spacedman: The term I've emphasized in my answer is **time series regression**. Thus, in my view, it could be considered as a **special case** of either of the two approaches, depending on the **perspective**." CreationDate="2014-08-26T09:12:48.210" UserId="2452" />
  <row Id="2163" PostId="1030" Score="0" Text="All I'm saying is that individual sales records data are not time series data. So you can't treat them like time series data. So reading about fitting AR(1) models and time series regression approaches is a waste of the OP's time here when all they have to do is convert year to numeric and run the model again. My concern now is wondering exactly what the OP means by &quot;per-year rate of change&quot;, which may imply something more than a linear term in year is required (some kind of smoother or polynomial term perhaps)." CreationDate="2014-08-26T09:40:26.557" UserId="471" />
  <row Id="2164" PostId="1030" Score="0" Text="@Spacedman: I see. Thank you for the clarification. However, my initial impression was that for this particular task, the OP is only interested in future values of a **single aggregate** *outcome variable* (keeping the model's **full information** for *regression analysis*). That would be the case for *time series forecasting*, wouldn't it? Perhaps, I misunderstood the question." CreationDate="2014-08-26T10:01:11.867" UserId="2452" />
  <row Id="2165" PostId="1044" Score="5" Text="Asking for tool recommendations is usually considered off topic for StackExchange. I think it could be a better question if you could narrow down requirements and ask for how to accomplish this in the few tools you are considering using." CreationDate="2014-08-26T15:56:43.573" UserId="21" />
  <row Id="2166" PostId="1041" Score="0" Text="Hmm... it never occurred to be to make year a continuous variable. In retrospect it seems obvious." CreationDate="2014-08-26T18:51:31.157" UserId="1241" />
  <row Id="2167" PostId="729" Score="0" Text="Hi nfmcclure, I've applied your suggestion and updated the post. Please provide your comments." CreationDate="2014-08-27T07:16:43.810" UserId="870" />
  <row Id="2168" PostId="1045" Score="1" Text="Use SparkSQL, Shark is deprecated." CreationDate="2014-08-27T09:21:59.780" UserId="2668" />
  <row Id="2169" PostId="1044" Score="2" Text="The answer to your question is &quot;all of them&quot;. Depending on where the real detailed challenges to your problem are, one language might give you better support for what you want to do. But to answer that, the question needs a *lot* more detail." CreationDate="2014-08-27T10:29:04.177" UserId="836" />
  <row Id="2170" PostId="671" Score="1" Text="You have the right idea, except when plotting it you should start where the series starts every reset.  For estimating where it will hit, say 120, see my first edit in my answer." CreationDate="2014-08-27T16:13:41.443" UserId="375" />
  <row Id="2171" PostId="1028" Score="1" Text="ISTR that Breiman had a proof based on the Law of Large Numbers. Has someone discovered a flaw in that proof?" CreationDate="2014-08-28T01:18:43.393" UserId="1241" />
  <row Id="2172" PostId="1044" Score="0" Text="I think you need to decide how good is good enough." CreationDate="2014-08-28T16:36:20.693" UserId="1241" />
  <row Id="2173" PostId="1046" Score="0" Text="I've had no problem finding word2vec implementations, but I have been unable to find a working recursive net to use." CreationDate="2014-08-28T16:57:14.207" UserId="684" />
  <row Id="2174" PostId="1045" Score="0" Text="@samthebeast can you provide a reference?" CreationDate="2014-08-28T17:53:20.473" UserId="403" />
  <row Id="2175" PostId="1045" Score="1" Text="I tried Shark a few weeks ago. I was able to get it running using their documentation and a lot of sweat. I ultimately gave up on it as I was trying to pull/push data from s3 and I couldn't find a way to do that. We since switched to Impala." CreationDate="2014-08-28T17:54:23.883" UserId="403" />
  <row Id="2176" PostId="985" Score="0" Text="I don't get the point of the question. What would prevent you from doing such a thing? The data science police? (I don't mean to be disrespectful, I simply don't understand the reason for the question)." CreationDate="2014-08-28T18:20:24.147" UserId="1281" />
  <row Id="2177" PostId="1045" Score="0" Text="I got it! I'll try to make some tests with SparkSQL to see if I feel confortable with that." CreationDate="2014-08-28T20:30:35.243" UserId="3050" />
  <row Id="2178" PostId="1049" Score="0" Text="Part of this isn't an answer to the question, and link-only answers are generally discouraged. I'd suggest elaborating or moving this to a comment." CreationDate="2014-08-29T05:59:27.097" UserId="21" />
  <row Id="2179" PostId="1058" Score="1" Text="Although this is about science, and data, I'm not sure the 'data science' StackExchange is the best place for this. Can you elaborate why this is of interest?" CreationDate="2014-08-29T06:00:38.573" UserId="21" />
  <row Id="2180" PostId="1045" Score="0" Text="@gallamine http://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html" CreationDate="2014-08-29T22:58:59.587" UserId="21" />
  <row Id="2181" PostId="1045" Score="0" Text="This doesn't make a great question. I'd rephrase to ask about specific issues you have in mind." CreationDate="2014-08-29T22:59:40.270" UserId="21" />
  <row Id="2182" PostId="1060" Score="0" Text="Thx a lot. I've seen [Breeze can read a CSV](http://www.scalanlp.org/api/breeze/#breeze.io.CSVReader$) and can calculate several statistics like [mean and variance](http://www.scalanlp.org/api/breeze/index.html#breeze.linalg.meanAndVariance$). The quartiles are missing, but this may be the best in the state of the art for Scala, a bit far from R." CreationDate="2014-08-30T02:14:50.483" UserId="1281" />
  <row Id="2183" PostId="1060" Score="0" Text="@Trylks R is a DSL for statistics, so out of box it will have a lot of helper functions for every day work for statiticians, but its totally inappropriate for building a product, doing big data or writting complicated machine laerning algorithms in a scalable way.  ScaLa is a Scalable Language and is designed to be expanded to whatever people want.  You can do just about anything in Scala, but it wont all be out-of-box. The payoff is massive though, once u climb a little way up the learning curve one realizes how much more powerful it is." CreationDate="2014-08-30T05:42:33.703" UserId="2668" />
  <row Id="2184" PostId="1061" Score="1" Text="I think recommendations, and for web-services, are mostly off-topic, even though this does concern hosting data." CreationDate="2014-08-30T16:13:10.113" UserId="21" />
  <row Id="2186" PostId="92" Score="0" Text="@Anony-Mousse That's a good topic for meta. I would like the site to be what it can be. I think I've contributed for my part as a user and moderator. I'm not sure what you suggest should be done better." CreationDate="2014-08-30T18:44:11.267" UserId="21" />
  <row Id="2187" PostId="1061" Score="0" Text="I was marginally aware of that though I find it perhaps a tad perverse. Where does one draw the line between the relevance of finding a suitable function, programming pattern, library, framework, or an app/web-service? Any of these may serve to solve data science issues, albeit at different resolutions." CreationDate="2014-08-30T20:12:14.117" UserId="3133" />
  <row Id="2188" PostId="1038" Score="0" Text="This isn't a great fit for the Data Science site. I'd suggest posting this in serverfault.com perhaps." CreationDate="2014-08-30T22:55:15.140" UserId="21" />
  <row Id="2189" PostId="1045" Score="0" Text="This is my issue! I nerd a technology to stage my resulting data from spark as I asked before. Thanks folks! I'm getting great results from it!" CreationDate="2014-08-30T23:54:34.620" UserId="3050" />
  <row Id="2190" PostId="1063" Score="0" Text="Looks cool, lots of stuff and I had no idea about that library. It seems a bit confusing, though, there is a textwriter but nothing named &quot;reader&quot;. In particular, I don't see anything about quartiles. In this case the problem is that mixing libraries is probably not a good idea, they will have different data formats and I will need to move the data from one to another back and forth, or something even worse." CreationDate="2014-08-31T01:58:32.983" UserId="1281" />
  <row Id="2191" PostId="1060" Score="0" Text="I know, that's why I'm asking for Scala libraries :) Making my own library would not be wise if there were good libraries out there, which is usually the case. I found some libraries like Breeze (and Breeze-Viz, and Breeze-Bokeh), Saddle, Spire, Spark, Scalding, etc. The problem is that I'm quite uncertain about which one to choose, to use or expand, the nicest ones in philosophy seem to be dead, the most efficient ones seem to be useless, and in general I found none that would suit this simple use case :/" CreationDate="2014-08-31T02:07:13.160" UserId="1281" />
  <row Id="2192" PostId="1053" Score="0" Text="I found [a thread](https://groups.google.com/forum/#!topic/cascading-user/YlCKDmjlkP0) discussing something similar for cascading, mahout, hadoop and some other technologies. I have to check it into detail, but I can't now, so I'll simply leave it here..." CreationDate="2014-08-31T02:13:36.947" UserId="1281" />
  <row Id="2194" PostId="1058" Score="0" Text="Try Fermi Estimation: http://en.wikipedia.org/wiki/Fermi_problem" CreationDate="2014-09-01T08:30:00.547" UserId="471" />
  <row Id="2195" PostId="1063" Score="0" Text="CSV data is usually read using `FileBasedDatabaseConnection` and `NumberVectorLabelParser`. Quantiles can be determined using `QuickSelect`.&#xA;If you don't want to mix libraries, then don't *ask* about libraries..." CreationDate="2014-09-01T08:58:01.877" UserId="924" />
  <row Id="2196" PostId="1061" Score="1" Text="The &quot;best&quot; depends very much on your application and your data. Without you expanding on that, this Q will just become a list of &quot;Howabout datafoo.io?&quot;. Even with that expansion, its still probably too much opinion." CreationDate="2014-09-01T12:14:02.260" UserId="471" />
  <row Id="2197" PostId="1058" Score="3" Text="As soon as you wrote a scientific article on this, you would be wrong." CreationDate="2014-09-01T12:21:10.427" UserId="471" />
  <row Id="2198" PostId="1063" Score="0" Text="1. I would have never guessed those names, thank you. 2. Spark (MLib) uses Breeze over Hadoop clusters, the combination is very natural for those libraries, AFAIK, because they are quite agnostic in the information representation. When moving to higher level (aka less raw) libraries decisions will be made and that means more compatibility problems, specially when that means decisions about information representation." CreationDate="2014-09-01T17:37:10.143" UserId="1281" />
  <row Id="2199" PostId="1066" Score="0" Text="Hi, thanks for your answer. &#xA;&#xA;I'm looking for an estimation of the amount of scientific knowledge. After that I want to study the distribution between different fields. Maybe if I use a wordcount and I don't take care about the pictures the problem is more easy to resolve." CreationDate="2014-09-02T00:23:28.073" UserId="3128" />
  <row Id="2200" PostId="1058" Score="0" Text="Why would be an error? &#xA;&#xA;I thing that to resolve this problem I need data science tools. After know the total amount I want to extract patters of activity in each field for every year and study the dynamics of science and how this would be under the influence of historical events" CreationDate="2014-09-02T00:26:47.553" UserId="3128" />
  <row Id="2201" PostId="1072" Score="0" Text="I am interested in analytics. I know HANA has a lot of transnational functionality but I am more interested what can be done in terms of machine learning, custom python reducers and hive interface. For example I know HANA has an library of machine learning algorithms built right in. Does Exasol?" CreationDate="2014-09-02T14:28:16.337" UserId="2511" />
  <row Id="2202" PostId="1072" Score="0" Text="EXASOL has a framework called &quot;EXAPowerlytics&quot; which uses protobuf and ZeroMQ to connect with Hadoop (for example). It also allows the ability to write User Defined Functions in Java, Lua, Python and R - for example importing any R machine-learning libraries you need without limit and running them in-memory and in parallel (where the algorithm is parallelisable). Definitely recommend signing-up at [link](www.exasol.com/portal) to get some more information." CreationDate="2014-09-02T14:57:12.140" UserId="3181" />
  <row Id="2203" PostId="1058" Score="0" Text="I'm not confident about the feasibility of an attempt to solve this problem. You can certainly come up with a very rough estimate (and I see such numbers from time to time), but the accuracy is pretty low, considering the diversity of the outlets human knowledge can found at, as well as frequency of appearing of new research studies and even research repositories." CreationDate="2014-09-02T15:22:54.523" UserId="2452" />
  <row Id="2204" PostId="1071" Score="1" Text="The variety of perspectives for this particular topic is not obvious for me. I'd say that: Data = symbols (of an alphabet). Information = data + syntax. Knowledge = information + semantics." CreationDate="2014-09-02T16:27:09.260" UserId="1281" />
  <row Id="2205" PostId="866" Score="0" Text="This is only a little bit related, but our most recent data science challenge concerned predicting claims from other claims. http://www.cloudera.com/content/cloudera/en/training/certification/ccp-ds/challenge/challenge2.html  When the solution is released it may contain a few interesting ideas." CreationDate="2014-09-03T14:20:04.223" UserId="21" />
  <row Id="2206" PostId="1043" Score="0" Text="KPIs? Key Performance Indicators? For performance of what?" CreationDate="2014-09-04T11:39:14.873" UserId="471" />
  <row Id="2207" PostId="1075" Score="1" Text="How much memory one dataset takes? I.e. what is approximate size of one row?" CreationDate="2014-09-04T18:34:29.907" UserId="1279" />
  <row Id="2208" PostId="1043" Score="0" Text="Well I would be interested in any really. I have not been able to find a single paper on such a common feature. I would think that message length, response writing time and time until response send would be the most interesting KPIs." CreationDate="2014-09-04T22:46:40.717" UserId="2511" />
  <row Id="2209" PostId="1076" Score="0" Text="Thanks for your comments. I am very aware and use scikit and caret regularly but I am looking for more of a state of the art online learning package, and neither scikit or caret are these. I am looking for people who are actually using something and can give their experience with a given package." CreationDate="2014-09-05T13:23:49.067" UserId="802" />
  <row Id="2210" PostId="1077" Score="2" Text="Few corrections. Firstly, Hadoop is a common name for a set of tools, file system is called HDFS. Secondly, data may be processed in parallel without distributing it on HDFS, e.g. sending data tuples to Storm or even RabbitMQ with a number of workers on a consumer side will do the trick as well. Thirdly, in this specific case you need to distribute not just 2 separate datasets, but instead all pairs from both (90 B tuples). Spark already has `.cartesian()` method which does exactly this, but custom lazy generator + RabbitMQ will work fine too." CreationDate="2014-09-05T23:44:41.593" UserId="1279" />
  <row Id="2211" PostId="1075" Score="0" Text="The size of rows differs, but on average about 30-50. Data sets are about 100Mb each." CreationDate="2014-09-06T12:44:19.140" UserId="3203" />
  <row Id="2212" PostId="1077" Score="0" Text="Splitting up using a queue and calculating in a distributed fashion is an option. just assume this will come with a price tag of handling a lot of complexity thats been solved in other distributed products. Spark seems like a good candidate on paper." CreationDate="2014-09-06T12:46:38.340" UserId="3203" />
  <row Id="2214" PostId="1084" Score="0" Text="Original Poster used the word &quot;classify&quot; but &quot;cluster&quot; is a more accurate description of his problem because he has no a priori definitions of categories. Therefore, this is not necessarily a supervised learning problem." CreationDate="2014-09-08T02:28:56.183" UserId="609" />
  <row Id="2215" PostId="1084" Score="0" Text="@MrMeritology: hmm, from context I would say that author is just not sure about concrete classes he's going to use, but still wants classification, not clustering. Anyway, he's the only person who knows the truth :)" CreationDate="2014-09-08T05:31:22.507" UserId="1279" />
  <row Id="2216" PostId="1084" Score="0" Text="Maybe i was not clear at the point. The categories are going to be selected in advice, so it is rather classification than clustering problem. The idea of creating a complex features vector seems to be quite reasonable - especially, that there are some particular tags, that are most likely probably to quickly classify some of samples. I'm not sure if SVM are going to fit the problem, as I predict high nonlinearities, but decision trees and Bayes seem to be applicable. I'm starting also to think about application of a hybrid algorithm (SVM based decision trees)." CreationDate="2014-09-08T05:46:48.257" UserId="3215" />
  <row Id="2217" PostId="1084" Score="0" Text="@GrzegorzE. -- If your categories are defined in advance, then please list these three categories in your question.  In my opinion, you are too focused on ML algorithms and not enough on the nature of your problem and the nature of your data. For example, you predict &quot;nonlinearies&quot; in features for web sites of unknown structure. Why? Also, you are mixing tags with web page text with who-knows-what-else, and they have different semantic significance." CreationDate="2014-09-08T06:07:25.307" UserId="609" />
  <row Id="2218" PostId="1084" Score="0" Text="@GrzegorzE. -- I strongly suggest that your classification method should be primarily driven by the nature of your a priori categories and the nature of the data. There are an infinite number of ways to categorize arbitrary web sites into 3 categories. Each way of categorizing will suggest salient features in the data or salient patterns. There's no substitute for manual analysis of individual data elements (web pages) and their context." CreationDate="2014-09-08T06:17:58.650" UserId="609" />
  <row Id="2219" PostId="1084" Score="0" Text="@MrMeritology -- I wan to classify if the particular page is a personal page (or social profile page). As I am a newbie in data science, my control engineering background drives my modelling and algorithmic focus in some particular areas. Similarly, the nonlinearities in feature vectors space and selection of appropriate algorithms have background in control..." CreationDate="2014-09-08T06:42:40.300" UserId="3215" />
  <row Id="2220" PostId="1086" Score="0" Text="From this description, I'm not at all clear what the ranking is supposed to be. There is no such thing as ordering by &quot;unique content&quot;. Clarify please what the ordering is supposed to be determined by?" CreationDate="2014-09-08T09:15:13.517" UserId="21" />
  <row Id="2221" PostId="1075" Score="0" Text="Meta-comment: N^2 algorithms are always bad news at scale. I would in any event investigate algorithms that do not require all-pairs computation. Cartesian joins destroy data locality optimization." CreationDate="2014-09-08T11:51:40.437" UserId="21" />
  <row Id="2222" PostId="1084" Score="0" Text="@GrzegorzE. - I'm familiar with Control Engineering as I have a EE background, so I can understand how you'd quickly focus on algorithms, nonlinearity, etc. I renew my suggestion to focus first on problem definition, understanding the data, and feature selection. Your problem is essentially a Social Science problem, not just a &quot;data classification problem.&quot; Insightful selection of features is worth more than 50% improvement in algorithm effectiveness." CreationDate="2014-09-08T18:17:17.203" UserId="609" />
  <row Id="2223" PostId="1084" Score="0" Text="@MrMeritology - great thanks for suggestions. Since yesterday I'm building a feature vector, focusing on text corpus, tags and tag arguments. I'll drop a line, with project status when get first results." CreationDate="2014-09-09T05:32:08.690" UserId="3215" />
  <row Id="2224" PostId="1075" Score="0" Text="You haven't specified the constraintd..eg response time, throughput etc" CreationDate="2014-09-09T06:32:03.607" UserId="1256" />
  <row Id="2225" PostId="1092" Score="0" Text="I would be interested to know where this sits, too, as currently I feel obliged to learn Python, R and Octave, just so I have access for tools for a hobby (whilst I know Ruby for professional reasons). I don't know enough about it to suggest an answer, but have known about http://sciruby.com/ for a while. My gut feel is that it is not ready yet" CreationDate="2014-09-09T09:29:27.130" UserId="836" />
  <row Id="2226" PostId="1092" Score="0" Text="Yeah, we took a look at sciruby, and while it looks nice, it seems limited to providing some data structures and linear algebra operations. If someone were to build a unified ML library for Ruby it would probably be a great basis for that." CreationDate="2014-09-09T16:59:37.457" UserId="2487" />
  <row Id="2227" PostId="1073" Score="0" Text="Dumb question, but do you mean `online` as in &quot;non-batch mode&quot;, or as in &quot;processed in the cloud&quot;?" CreationDate="2014-09-09T17:14:02.150" UserId="3248" />
  <row Id="2228" PostId="1081" Score="0" Text="I'm not sure data is a &quot;thing&quot;. If it is a thing, it should exist somewhere. Data doesn't exist anywhere. Take your own examples: (1) ethnicity: belonging to a social group - this quality does not &quot;exist&quot;, but is assigned to an individual and a group of individuals. What counts as an ethnicity is a social construction and may differ between individuals, societies, and eras; (2) housing prices do not have existence either, but are a manifestation of perceived property/material/social value. But anyway, the point is taken that data is a construction that is used to make meaningful inferences." CreationDate="2014-09-10T05:56:32.147" UserId="3178" />
  <row Id="2229" PostId="1086" Score="0" Text="@SeanOwen All documents come under the same theme for instance Tissue engineering for Bonemarrow related cancer. Within this theme we know that most research article will definetly speak about genes (ABC) so this becomes the predominant theme within those topics. Most of these articles dwell within these same set of genes. But very few articles start speak about latent themes which involves  genes (ADF) and (XYZ). The current thought process is we want to rank the articles based on the  content (XYZ),(FDA),(ABC). This is what I meant by unique content." CreationDate="2014-09-10T10:49:37.477" UserId="3232" />
  <row Id="2230" PostId="1089" Score="0" Text="Thanks! this is usefull" CreationDate="2014-09-10T10:56:00.230" UserId="3232" />
  <row Id="2231" PostId="1087" Score="0" Text="Thanks. Im looking at weighted TF-IDF in cases were the key-words are already known . any suggestions for that ?" CreationDate="2014-09-10T11:00:57.753" UserId="3232" />
  <row Id="2232" PostId="1096" Score="0" Text="I did look at GATE, i was not successful. I am not familiar with python. Any other tool.. I know spotfire but its way too expensive.." CreationDate="2014-09-10T16:33:45.287" UserId="3244" />
  <row Id="2233" PostId="1098" Score="0" Text="Thanks Harjeet, SKlearn is simple but it does not solve my problem. SKlearn is more of analytical algorithm approach which R also provides but I am looking for something like we do in Google, put in n-words and it gives all suitable hits.." CreationDate="2014-09-10T16:43:49.457" UserId="3244" />
  <row Id="2234" PostId="1101" Score="0" Text="Gini coefficient is not Gini impurity. See the links in the question" CreationDate="2014-09-10T19:15:20.863" UserId="21" />
  <row Id="2235" PostId="1073" Score="0" Text="Hi Mike, I mean non-batch mode. So soon after a prediction is made we learn the true label and then use that in the training. So think in terms of predicting a stock price, in a few minutes we would learn the true value, and then use that value in our training." CreationDate="2014-09-11T13:16:04.657" UserId="802" />
  <row Id="2236" PostId="1101" Score="0" Text="Wikipedia ist not always a reliable source of information :-)" CreationDate="2014-09-11T13:40:48.833" UserId="979" />
  <row Id="2237" PostId="1096" Score="0" Text="Sorry, I don't know of any easy-to-use free tools that do exactly what you want to do." CreationDate="2014-09-11T13:56:59.770" UserId="819" />
  <row Id="2238" PostId="1087" Score="0" Text="What are you hoping to achieve by boosting the TF-IDF scores of known keywords?" CreationDate="2014-09-11T13:58:46.060" UserId="819" />
  <row Id="2239" PostId="1101" Score="0" Text="Sure. Go look it up somewhere else: http://mathworld.wolfram.com/GiniCoefficient.html What makes you think Gini coefficient = Gini impurity?" CreationDate="2014-09-11T14:03:08.870" UserId="21" />
  <row Id="2240" PostId="1101" Score="0" Text="Look it up: http://books.google.de/books?id=DQXhYAgXRpEC&amp;pg=PA373&amp;dq=gini+coefficient+classification&amp;hl=de&amp;sa=X&amp;ei=Q7sRVOKDOcvAPLDugbAP&amp;ved=0CCkQ6AEwAQ#v=onepage&amp;q=gini%20coefficient%20classification&amp;f=false" CreationDate="2014-09-11T15:10:51.260" UserId="979" />
  <row Id="2241" PostId="1101" Score="0" Text="By the way: I have never seen a publication that cites mathworld.wolrfram.com !" CreationDate="2014-09-11T15:12:19.297" UserId="979" />
  <row Id="2242" PostId="1101" Score="0" Text="I'm sure you can find people that use coefficient and impurity interchangeably within the field of ML. Gini coefficient is not misclassification error, outside of ML. It has a meaning, whose interpretation when brought back into ML is something else. If the question is, are they the same thing, then I can't see how the answer is &quot;yes&quot;. Search for &quot;gini coefficient&quot; and tell me those are all about misclassification error?" CreationDate="2014-09-11T16:01:22.573" UserId="21" />
  <row Id="2243" PostId="1104" Score="0" Text="I'm sure this is tagged incorrectly and not enough info.  But i dont know what i dont know at this point" CreationDate="2014-09-12T04:23:19.613" UserId="3279" />
  <row Id="2244" PostId="1085" Score="0" Text="I do not understand the down vote.  This is a perfectly reasonable answer.  There are platforms other than Hadoop for computational grid computing." CreationDate="2014-09-12T12:57:55.210" UserId="961" />
  <row Id="2245" PostId="1106" Score="0" Text="Huge topic, have a look at [Into to IR](http://nlp.stanford.edu/IR-book/), this walks you from basic first principles how to build what you are asking about. Something to lookup is [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) then realise this doesn't solve everything and look at bayesian probability" CreationDate="2014-09-12T13:00:09.070" UserId="95" />
  <row Id="2246" PostId="1105" Score="0" Text="This looks more like a programming question or maybe a stats question. Its not a data science question. Try StackOverflow?" CreationDate="2014-09-12T13:10:54.800" UserId="471" />
  <row Id="2247" PostId="1105" Score="0" Text="I was not that clear about it either. I think it'll be quite hard for someone who's not into the topic to see the problem, but I may give it a try" CreationDate="2014-09-12T13:40:05.153" UserId="3283" />
  <row Id="2249" PostId="1100" Score="0" Text="Why aren't other classification methods appropriate here? Sounds like you are concerned with multiple cases leading to a reduced set of outputs. But this is basically every case of representation learning in classification. Neural nets will help you sort out what interaction effects are predictive when base features are not too predictive. Otherwise you could use other methods." CreationDate="2014-09-13T19:03:16.963" UserId="92" />
  <row Id="2250" PostId="1113" Score="0" Text="How do text responses look like? And what are you trying to achieve with the groping?" CreationDate="2014-09-13T21:19:46.030" UserId="1279" />
  <row Id="2251" PostId="1054" Score="0" Text="magicharp, thank you for your answer." CreationDate="2014-09-13T21:26:27.317" UserId="3068" />
  <row Id="2252" PostId="1113" Score="0" Text="The text responses are sentences or phrases of 50-200 characters. The goal is just to group the numeric variable (that is, create cut points) based on the text responses." CreationDate="2014-09-13T23:02:06.283" UserId="36" />
  <row Id="2253" PostId="1113" Score="0" Text="Then you can group them by first letter of texts. It will satisfy your current description, but most probably you want something different. So what you actually want to achieve? Do you want to split people in age categories based on words they use? Or you are looking for specific signs in responses? Even assuming you want clustering, there are literally dozens of features that may extracted from text and same number of distance measures to use. But in current wording I can't even be sure we are talking about clustering. So please provide *example* and/or *context* of what you want to achieve." CreationDate="2014-09-14T00:18:08.677" UserId="1279" />
  <row Id="2254" PostId="1111" Score="0" Text="I'm not sure I understand your problem. Aren't you using the Twitter search API?" CreationDate="2014-09-14T01:29:32.217" UserId="381" />
</comments>